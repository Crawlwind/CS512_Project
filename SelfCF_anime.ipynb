{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3572e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SELFRec import SELFRec\n",
    "from util.conf import ModelConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee04cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "   SELFRec: A library for self-supervised recommendation.   \n",
      "================================================================================\n",
      "Baseline Models:\n",
      "LightGCN   DirectAU   MF\n",
      "--------------------------------------------------------------------------------\n",
      "Graph-Based Models:\n",
      "SGL   SimGCL   SEPT   MHCN   BUIR   SelfCF   SSL4Rec   XSimGCL   NCL   MixGCF\n",
      "--------------------------------------------------------------------------------\n",
      "Applying Models:\n",
      "SelfCF_anime1   SelfCF_anime2   XSimGCL_anime1   XSimGCL_anime2   NCL_anime1   NCL_anime2\n",
      "================================================================================\n",
      "Please enter the model you want to run:SelfCF_anime1\n",
      "Reading data and preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/windy/512Project/SELFRec/base/torch_interface.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /scratch/env/opence1.5.1/conda-bld/pytorch-base_1643072044833/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  i = torch.LongTensor([coo.row, coo.col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SelfCF\n",
      "Training Set: /home/windy/512Project/SELFRec/dataset/anime1/train.txt\n",
      "Test Set: /home/windy/512Project/SELFRec/dataset/anime1/test.txt\n",
      "Embedding Dimension: 64\n",
      "Maximum Epoch: 100\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 2048\n",
      "Regularization Parameter: 0.0001\n",
      "Specific parameters: n_layer:2  tau:0.05  \n",
      "Training Set Size: (user number: 10173, item number 9874, interaction number: 1180276)\n",
      "Test Set Size: (user number: 10173, item number 7568, interaction number: 300391)\n",
      "================================================================================\n",
      "Initializing and building model...\n",
      "Training Model...\n",
      "training: 1 batch 0 batch_loss: 1.0029070377349854\n",
      "training: 1 batch 1 batch_loss: 1.0006791353225708\n",
      "training: 1 batch 2 batch_loss: 1.002922773361206\n",
      "training: 1 batch 3 batch_loss: 1.0076904296875\n",
      "training: 1 batch 4 batch_loss: 1.0169460773468018\n",
      "training: 1 batch 5 batch_loss: 1.016465187072754\n",
      "training: 1 batch 6 batch_loss: 1.0182324647903442\n",
      "training: 1 batch 7 batch_loss: 1.0190404653549194\n",
      "training: 1 batch 8 batch_loss: 1.0152215957641602\n",
      "training: 1 batch 9 batch_loss: 1.006002426147461\n",
      "training: 1 batch 10 batch_loss: 0.9994969367980957\n",
      "training: 1 batch 11 batch_loss: 0.9884349703788757\n",
      "training: 1 batch 12 batch_loss: 0.9696801900863647\n",
      "training: 1 batch 13 batch_loss: 0.9476933479309082\n",
      "training: 1 batch 14 batch_loss: 0.9233202934265137\n",
      "training: 1 batch 15 batch_loss: 0.8962876796722412\n",
      "training: 1 batch 16 batch_loss: 0.8654977083206177\n",
      "training: 1 batch 17 batch_loss: 0.829700767993927\n",
      "training: 1 batch 18 batch_loss: 0.7954038381576538\n",
      "training: 1 batch 19 batch_loss: 0.7551997900009155\n",
      "training: 1 batch 20 batch_loss: 0.7130459547042847\n",
      "training: 1 batch 21 batch_loss: 0.6739659905433655\n",
      "training: 1 batch 22 batch_loss: 0.638251781463623\n",
      "training: 1 batch 23 batch_loss: 0.5955186486244202\n",
      "training: 1 batch 24 batch_loss: 0.5583318471908569\n",
      "training: 1 batch 25 batch_loss: 0.5230264663696289\n",
      "training: 1 batch 26 batch_loss: 0.48363739252090454\n",
      "training: 1 batch 27 batch_loss: 0.45275983214378357\n",
      "training: 1 batch 28 batch_loss: 0.4169217348098755\n",
      "training: 1 batch 29 batch_loss: 0.38496696949005127\n",
      "training: 1 batch 30 batch_loss: 0.3553716540336609\n",
      "training: 1 batch 31 batch_loss: 0.32817792892456055\n",
      "training: 1 batch 32 batch_loss: 0.3020034730434418\n",
      "training: 1 batch 33 batch_loss: 0.274361252784729\n",
      "training: 1 batch 34 batch_loss: 0.2557849884033203\n",
      "training: 1 batch 35 batch_loss: 0.23402470350265503\n",
      "training: 1 batch 36 batch_loss: 0.21564871072769165\n",
      "training: 1 batch 37 batch_loss: 0.1992182433605194\n",
      "training: 1 batch 38 batch_loss: 0.17781519889831543\n",
      "training: 1 batch 39 batch_loss: 0.16541561484336853\n",
      "training: 1 batch 40 batch_loss: 0.15255016088485718\n",
      "training: 1 batch 41 batch_loss: 0.14041325449943542\n",
      "training: 1 batch 42 batch_loss: 0.12693050503730774\n",
      "training: 1 batch 43 batch_loss: 0.11907914280891418\n",
      "training: 1 batch 44 batch_loss: 0.10647836327552795\n",
      "training: 1 batch 45 batch_loss: 0.09715458750724792\n",
      "training: 1 batch 46 batch_loss: 0.08743047714233398\n",
      "training: 1 batch 47 batch_loss: 0.08398967981338501\n",
      "training: 1 batch 48 batch_loss: 0.07645183801651001\n",
      "training: 1 batch 49 batch_loss: 0.07278141379356384\n",
      "training: 1 batch 50 batch_loss: 0.06476837396621704\n",
      "training: 1 batch 51 batch_loss: 0.0635770857334137\n",
      "training: 1 batch 52 batch_loss: 0.05782321095466614\n",
      "training: 1 batch 53 batch_loss: 0.05376580357551575\n",
      "training: 1 batch 54 batch_loss: 0.04962503910064697\n",
      "training: 1 batch 55 batch_loss: 0.04778248071670532\n",
      "training: 1 batch 56 batch_loss: 0.0440751314163208\n",
      "training: 1 batch 57 batch_loss: 0.0424380898475647\n",
      "training: 1 batch 58 batch_loss: 0.043507128953933716\n",
      "training: 1 batch 59 batch_loss: 0.03947672247886658\n",
      "training: 1 batch 60 batch_loss: 0.03772863745689392\n",
      "training: 1 batch 61 batch_loss: 0.03550595045089722\n",
      "training: 1 batch 62 batch_loss: 0.03597560524940491\n",
      "training: 1 batch 63 batch_loss: 0.033674389123916626\n",
      "training: 1 batch 64 batch_loss: 0.031889140605926514\n",
      "training: 1 batch 65 batch_loss: 0.03142285346984863\n",
      "training: 1 batch 66 batch_loss: 0.02841070294380188\n",
      "training: 1 batch 67 batch_loss: 0.027339518070220947\n",
      "training: 1 batch 68 batch_loss: 0.028921186923980713\n",
      "training: 1 batch 69 batch_loss: 0.028625518083572388\n",
      "training: 1 batch 70 batch_loss: 0.02673521637916565\n",
      "training: 1 batch 71 batch_loss: 0.02735602855682373\n",
      "training: 1 batch 72 batch_loss: 0.02512747049331665\n",
      "training: 1 batch 73 batch_loss: 0.02863338589668274\n",
      "training: 1 batch 74 batch_loss: 0.027403295040130615\n",
      "training: 1 batch 75 batch_loss: 0.02584788203239441\n",
      "training: 1 batch 76 batch_loss: 0.0257779061794281\n",
      "training: 1 batch 77 batch_loss: 0.022883057594299316\n",
      "training: 1 batch 78 batch_loss: 0.02553996443748474\n",
      "training: 1 batch 79 batch_loss: 0.024495452642440796\n",
      "training: 1 batch 80 batch_loss: 0.024515539407730103\n",
      "training: 1 batch 81 batch_loss: 0.023188918828964233\n",
      "training: 1 batch 82 batch_loss: 0.022359132766723633\n",
      "training: 1 batch 83 batch_loss: 0.02172982692718506\n",
      "training: 1 batch 84 batch_loss: 0.02328956127166748\n",
      "training: 1 batch 85 batch_loss: 0.022187530994415283\n",
      "training: 1 batch 86 batch_loss: 0.020231276750564575\n",
      "training: 1 batch 87 batch_loss: 0.021226823329925537\n",
      "training: 1 batch 88 batch_loss: 0.021230101585388184\n",
      "training: 1 batch 89 batch_loss: 0.02016720175743103\n",
      "training: 1 batch 90 batch_loss: 0.02101290225982666\n",
      "training: 1 batch 91 batch_loss: 0.022449225187301636\n",
      "training: 1 batch 92 batch_loss: 0.019639045000076294\n",
      "training: 1 batch 93 batch_loss: 0.02155327796936035\n",
      "training: 1 batch 94 batch_loss: 0.021125704050064087\n",
      "training: 1 batch 95 batch_loss: 0.018490731716156006\n",
      "training: 1 batch 96 batch_loss: 0.01912611722946167\n",
      "training: 1 batch 97 batch_loss: 0.02085477113723755\n",
      "training: 1 batch 98 batch_loss: 0.018488407135009766\n",
      "training: 1 batch 99 batch_loss: 0.01904115080833435\n",
      "training: 1 batch 100 batch_loss: 0.01960238814353943\n",
      "training: 1 batch 101 batch_loss: 0.01924353837966919\n",
      "training: 1 batch 102 batch_loss: 0.020639151334762573\n",
      "training: 1 batch 103 batch_loss: 0.018859922885894775\n",
      "training: 1 batch 104 batch_loss: 0.018367767333984375\n",
      "training: 1 batch 105 batch_loss: 0.018399953842163086\n",
      "training: 1 batch 106 batch_loss: 0.016814619302749634\n",
      "training: 1 batch 107 batch_loss: 0.018359750509262085\n",
      "training: 1 batch 108 batch_loss: 0.016978591680526733\n",
      "training: 1 batch 109 batch_loss: 0.01761794090270996\n",
      "training: 1 batch 110 batch_loss: 0.017978012561798096\n",
      "training: 1 batch 111 batch_loss: 0.017232775688171387\n",
      "training: 1 batch 112 batch_loss: 0.01713654398918152\n",
      "training: 1 batch 113 batch_loss: 0.017274320125579834\n",
      "training: 1 batch 114 batch_loss: 0.019101440906524658\n",
      "training: 1 batch 115 batch_loss: 0.017127811908721924\n",
      "training: 1 batch 116 batch_loss: 0.0165998637676239\n",
      "training: 1 batch 117 batch_loss: 0.01786249876022339\n",
      "training: 1 batch 118 batch_loss: 0.017352789640426636\n",
      "training: 1 batch 119 batch_loss: 0.0167694091796875\n",
      "training: 1 batch 120 batch_loss: 0.016880810260772705\n",
      "training: 1 batch 121 batch_loss: 0.01717054843902588\n",
      "training: 1 batch 122 batch_loss: 0.015887200832366943\n",
      "training: 1 batch 123 batch_loss: 0.016245096921920776\n",
      "training: 1 batch 124 batch_loss: 0.01583084464073181\n",
      "training: 1 batch 125 batch_loss: 0.016498148441314697\n",
      "training: 1 batch 126 batch_loss: 0.016930758953094482\n",
      "training: 1 batch 127 batch_loss: 0.015498936176300049\n",
      "training: 1 batch 128 batch_loss: 0.01672878861427307\n",
      "training: 1 batch 129 batch_loss: 0.014970481395721436\n",
      "training: 1 batch 130 batch_loss: 0.015517055988311768\n",
      "training: 1 batch 131 batch_loss: 0.016677111387252808\n",
      "training: 1 batch 132 batch_loss: 0.016393989324569702\n",
      "training: 1 batch 133 batch_loss: 0.014938265085220337\n",
      "training: 1 batch 134 batch_loss: 0.015180528163909912\n",
      "training: 1 batch 135 batch_loss: 0.015485972166061401\n",
      "training: 1 batch 136 batch_loss: 0.014957845211029053\n",
      "training: 1 batch 137 batch_loss: 0.013902395963668823\n",
      "training: 1 batch 138 batch_loss: 0.01531568169593811\n",
      "training: 1 batch 139 batch_loss: 0.014387458562850952\n",
      "training: 1 batch 140 batch_loss: 0.015606969594955444\n",
      "training: 1 batch 141 batch_loss: 0.015348881483078003\n",
      "training: 1 batch 142 batch_loss: 0.014761865139007568\n",
      "training: 1 batch 143 batch_loss: 0.014982402324676514\n",
      "training: 1 batch 144 batch_loss: 0.014241307973861694\n",
      "training: 1 batch 145 batch_loss: 0.015352457761764526\n",
      "training: 1 batch 146 batch_loss: 0.013545632362365723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 1 batch 147 batch_loss: 0.014603972434997559\n",
      "training: 1 batch 148 batch_loss: 0.013734936714172363\n",
      "training: 1 batch 149 batch_loss: 0.013895750045776367\n",
      "training: 1 batch 150 batch_loss: 0.01438644528388977\n",
      "training: 1 batch 151 batch_loss: 0.014320164918899536\n",
      "training: 1 batch 152 batch_loss: 0.01483297348022461\n",
      "training: 1 batch 153 batch_loss: 0.015153259038925171\n",
      "training: 1 batch 154 batch_loss: 0.012904942035675049\n",
      "training: 1 batch 155 batch_loss: 0.012387216091156006\n",
      "training: 1 batch 156 batch_loss: 0.013313710689544678\n",
      "training: 1 batch 157 batch_loss: 0.013453841209411621\n",
      "training: 1 batch 158 batch_loss: 0.013145208358764648\n",
      "training: 1 batch 159 batch_loss: 0.014004439115524292\n",
      "training: 1 batch 160 batch_loss: 0.01391822099685669\n",
      "training: 1 batch 161 batch_loss: 0.014036446809768677\n",
      "training: 1 batch 162 batch_loss: 0.013950586318969727\n",
      "training: 1 batch 163 batch_loss: 0.012631237506866455\n",
      "training: 1 batch 164 batch_loss: 0.014966368675231934\n",
      "training: 1 batch 165 batch_loss: 0.013321220874786377\n",
      "training: 1 batch 166 batch_loss: 0.012765675783157349\n",
      "training: 1 batch 167 batch_loss: 0.013879448175430298\n",
      "training: 1 batch 168 batch_loss: 0.01338997483253479\n",
      "training: 1 batch 169 batch_loss: 0.012497097253799438\n",
      "training: 1 batch 170 batch_loss: 0.012395262718200684\n",
      "training: 1 batch 171 batch_loss: 0.011882632970809937\n",
      "training: 1 batch 172 batch_loss: 0.01196742057800293\n",
      "training: 1 batch 173 batch_loss: 0.012692153453826904\n",
      "training: 1 batch 174 batch_loss: 0.011847138404846191\n",
      "training: 1 batch 175 batch_loss: 0.012663573026657104\n",
      "training: 1 batch 176 batch_loss: 0.013287395238876343\n",
      "training: 1 batch 177 batch_loss: 0.011115670204162598\n",
      "training: 1 batch 178 batch_loss: 0.0120047926902771\n",
      "training: 1 batch 179 batch_loss: 0.011471420526504517\n",
      "training: 1 batch 180 batch_loss: 0.011971205472946167\n",
      "training: 1 batch 181 batch_loss: 0.012252181768417358\n",
      "training: 1 batch 182 batch_loss: 0.011236131191253662\n",
      "training: 1 batch 183 batch_loss: 0.012005388736724854\n",
      "training: 1 batch 184 batch_loss: 0.011972665786743164\n",
      "training: 1 batch 185 batch_loss: 0.011779725551605225\n",
      "training: 1 batch 186 batch_loss: 0.01132500171661377\n",
      "training: 1 batch 187 batch_loss: 0.01200336217880249\n",
      "training: 1 batch 188 batch_loss: 0.012091785669326782\n",
      "training: 1 batch 189 batch_loss: 0.011892110109329224\n",
      "training: 1 batch 190 batch_loss: 0.011370152235031128\n",
      "training: 1 batch 191 batch_loss: 0.012933790683746338\n",
      "training: 1 batch 192 batch_loss: 0.012225180864334106\n",
      "training: 1 batch 193 batch_loss: 0.01207306981086731\n",
      "training: 1 batch 194 batch_loss: 0.012178003787994385\n",
      "training: 1 batch 195 batch_loss: 0.012201786041259766\n",
      "training: 1 batch 196 batch_loss: 0.011643797159194946\n",
      "training: 1 batch 197 batch_loss: 0.011206328868865967\n",
      "training: 1 batch 198 batch_loss: 0.011128544807434082\n",
      "training: 1 batch 199 batch_loss: 0.01130068302154541\n",
      "training: 1 batch 200 batch_loss: 0.011763334274291992\n",
      "training: 1 batch 201 batch_loss: 0.010892152786254883\n",
      "training: 1 batch 202 batch_loss: 0.012551933526992798\n",
      "training: 1 batch 203 batch_loss: 0.010599225759506226\n",
      "training: 1 batch 204 batch_loss: 0.010606378316879272\n",
      "training: 1 batch 205 batch_loss: 0.01112249493598938\n",
      "training: 1 batch 206 batch_loss: 0.010319411754608154\n",
      "training: 1 batch 207 batch_loss: 0.011119991540908813\n",
      "training: 1 batch 208 batch_loss: 0.01205626130104065\n",
      "training: 1 batch 209 batch_loss: 0.01088908314704895\n",
      "training: 1 batch 210 batch_loss: 0.011531859636306763\n",
      "training: 1 batch 211 batch_loss: 0.010762333869934082\n",
      "training: 1 batch 212 batch_loss: 0.010897517204284668\n",
      "training: 1 batch 213 batch_loss: 0.011914074420928955\n",
      "training: 1 batch 214 batch_loss: 0.010469794273376465\n",
      "training: 1 batch 215 batch_loss: 0.010848253965377808\n",
      "training: 1 batch 216 batch_loss: 0.009919017553329468\n",
      "training: 1 batch 217 batch_loss: 0.010619878768920898\n",
      "training: 1 batch 218 batch_loss: 0.010154664516448975\n",
      "training: 1 batch 219 batch_loss: 0.010708332061767578\n",
      "training: 1 batch 220 batch_loss: 0.011797279119491577\n",
      "training: 1 batch 221 batch_loss: 0.011049091815948486\n",
      "training: 1 batch 222 batch_loss: 0.011178076267242432\n",
      "training: 1 batch 223 batch_loss: 0.010313421487808228\n",
      "training: 1 batch 224 batch_loss: 0.010319292545318604\n",
      "training: 1 batch 225 batch_loss: 0.00995945930480957\n",
      "training: 1 batch 226 batch_loss: 0.011063188314437866\n",
      "training: 1 batch 227 batch_loss: 0.010002613067626953\n",
      "training: 1 batch 228 batch_loss: 0.0101107656955719\n",
      "training: 1 batch 229 batch_loss: 0.009736388921737671\n",
      "training: 1 batch 230 batch_loss: 0.010432243347167969\n",
      "training: 1 batch 231 batch_loss: 0.011663079261779785\n",
      "training: 1 batch 232 batch_loss: 0.009853899478912354\n",
      "training: 1 batch 233 batch_loss: 0.009600907564163208\n",
      "training: 1 batch 234 batch_loss: 0.01011723279953003\n",
      "training: 1 batch 235 batch_loss: 0.009983211755752563\n",
      "training: 1 batch 236 batch_loss: 0.010395973920822144\n",
      "training: 1 batch 237 batch_loss: 0.010134667158126831\n",
      "training: 1 batch 238 batch_loss: 0.009844839572906494\n",
      "training: 1 batch 239 batch_loss: 0.010038614273071289\n",
      "training: 1 batch 240 batch_loss: 0.010497301816940308\n",
      "training: 1 batch 241 batch_loss: 0.009774923324584961\n",
      "training: 1 batch 242 batch_loss: 0.010144799947738647\n",
      "training: 1 batch 243 batch_loss: 0.011333644390106201\n",
      "training: 1 batch 244 batch_loss: 0.009381860494613647\n",
      "training: 1 batch 245 batch_loss: 0.009876489639282227\n",
      "training: 1 batch 246 batch_loss: 0.009554177522659302\n",
      "training: 1 batch 247 batch_loss: 0.010633587837219238\n",
      "training: 1 batch 248 batch_loss: 0.010308653116226196\n",
      "training: 1 batch 249 batch_loss: 0.00990450382232666\n",
      "training: 1 batch 250 batch_loss: 0.010079056024551392\n",
      "training: 1 batch 251 batch_loss: 0.009008139371871948\n",
      "training: 1 batch 252 batch_loss: 0.009823441505432129\n",
      "training: 1 batch 253 batch_loss: 0.009373188018798828\n",
      "training: 1 batch 254 batch_loss: 0.009766161441802979\n",
      "training: 1 batch 255 batch_loss: 0.009591788053512573\n",
      "training: 1 batch 256 batch_loss: 0.01057499647140503\n",
      "training: 1 batch 257 batch_loss: 0.01002657413482666\n",
      "training: 1 batch 258 batch_loss: 0.00925987958908081\n",
      "training: 1 batch 259 batch_loss: 0.010138720273971558\n",
      "training: 1 batch 260 batch_loss: 0.009839683771133423\n",
      "training: 1 batch 261 batch_loss: 0.009363830089569092\n",
      "training: 1 batch 262 batch_loss: 0.008818775415420532\n",
      "training: 1 batch 263 batch_loss: 0.009547919034957886\n",
      "training: 1 batch 264 batch_loss: 0.009890973567962646\n",
      "training: 1 batch 265 batch_loss: 0.00973406434059143\n",
      "training: 1 batch 266 batch_loss: 0.009270042181015015\n",
      "training: 1 batch 267 batch_loss: 0.008510798215866089\n",
      "training: 1 batch 268 batch_loss: 0.009830653667449951\n",
      "training: 1 batch 269 batch_loss: 0.009317755699157715\n",
      "training: 1 batch 270 batch_loss: 0.009177565574645996\n",
      "training: 1 batch 271 batch_loss: 0.009313851594924927\n",
      "training: 1 batch 272 batch_loss: 0.008558541536331177\n",
      "training: 1 batch 273 batch_loss: 0.009373843669891357\n",
      "training: 1 batch 274 batch_loss: 0.009099841117858887\n",
      "training: 1 batch 275 batch_loss: 0.009040594100952148\n",
      "training: 1 batch 276 batch_loss: 0.009002268314361572\n",
      "training: 1 batch 277 batch_loss: 0.008403182029724121\n",
      "training: 1 batch 278 batch_loss: 0.009248048067092896\n",
      "training: 1 batch 279 batch_loss: 0.0087394118309021\n",
      "training: 1 batch 280 batch_loss: 0.008362740278244019\n",
      "training: 1 batch 281 batch_loss: 0.008584797382354736\n",
      "training: 1 batch 282 batch_loss: 0.008277714252471924\n",
      "training: 1 batch 283 batch_loss: 0.008360117673873901\n",
      "training: 1 batch 284 batch_loss: 0.008659958839416504\n",
      "training: 1 batch 285 batch_loss: 0.008578658103942871\n",
      "training: 1 batch 286 batch_loss: 0.008654206991195679\n",
      "training: 1 batch 287 batch_loss: 0.00849980115890503\n",
      "training: 1 batch 288 batch_loss: 0.009163469076156616\n",
      "training: 1 batch 289 batch_loss: 0.008429199457168579\n",
      "training: 1 batch 290 batch_loss: 0.00966346263885498\n",
      "training: 1 batch 291 batch_loss: 0.008908599615097046\n",
      "training: 1 batch 292 batch_loss: 0.008846789598464966\n",
      "training: 1 batch 293 batch_loss: 0.009276747703552246\n",
      "training: 1 batch 294 batch_loss: 0.008395791053771973\n",
      "training: 1 batch 295 batch_loss: 0.008957624435424805\n",
      "training: 1 batch 296 batch_loss: 0.008493751287460327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 1 batch 297 batch_loss: 0.008340954780578613\n",
      "training: 1 batch 298 batch_loss: 0.008216708898544312\n",
      "training: 1 batch 299 batch_loss: 0.008554846048355103\n",
      "training: 1 batch 300 batch_loss: 0.008668959140777588\n",
      "training: 1 batch 301 batch_loss: 0.008665412664413452\n",
      "training: 1 batch 302 batch_loss: 0.008668601512908936\n",
      "training: 1 batch 303 batch_loss: 0.008255094289779663\n",
      "training: 1 batch 304 batch_loss: 0.008875995874404907\n",
      "training: 1 batch 305 batch_loss: 0.00885125994682312\n",
      "training: 1 batch 306 batch_loss: 0.008228600025177002\n",
      "training: 1 batch 307 batch_loss: 0.008016496896743774\n",
      "training: 1 batch 308 batch_loss: 0.008487284183502197\n",
      "training: 1 batch 309 batch_loss: 0.009843379259109497\n",
      "training: 1 batch 310 batch_loss: 0.007854580879211426\n",
      "training: 1 batch 311 batch_loss: 0.008500784635543823\n",
      "training: 1 batch 312 batch_loss: 0.008396446704864502\n",
      "training: 1 batch 313 batch_loss: 0.007894277572631836\n",
      "training: 1 batch 314 batch_loss: 0.008170008659362793\n",
      "training: 1 batch 315 batch_loss: 0.008052736520767212\n",
      "training: 1 batch 316 batch_loss: 0.008099943399429321\n",
      "training: 1 batch 317 batch_loss: 0.007580578327178955\n",
      "training: 1 batch 318 batch_loss: 0.007831990718841553\n",
      "training: 1 batch 319 batch_loss: 0.00799223780632019\n",
      "training: 1 batch 320 batch_loss: 0.008065491914749146\n",
      "training: 1 batch 321 batch_loss: 0.007757246494293213\n",
      "training: 1 batch 322 batch_loss: 0.008034676313400269\n",
      "training: 1 batch 323 batch_loss: 0.007541358470916748\n",
      "training: 1 batch 324 batch_loss: 0.007795512676239014\n",
      "training: 1 batch 325 batch_loss: 0.007909446954727173\n",
      "training: 1 batch 326 batch_loss: 0.008668392896652222\n",
      "training: 1 batch 327 batch_loss: 0.007949203252792358\n",
      "training: 1 batch 328 batch_loss: 0.008106410503387451\n",
      "training: 1 batch 329 batch_loss: 0.007671713829040527\n",
      "training: 1 batch 330 batch_loss: 0.007784098386764526\n",
      "training: 1 batch 331 batch_loss: 0.00809943675994873\n",
      "training: 1 batch 332 batch_loss: 0.00833982229232788\n",
      "training: 1 batch 333 batch_loss: 0.007901191711425781\n",
      "training: 1 batch 334 batch_loss: 0.007707655429840088\n",
      "training: 1 batch 335 batch_loss: 0.0074052512645721436\n",
      "training: 1 batch 336 batch_loss: 0.007993370294570923\n",
      "training: 1 batch 337 batch_loss: 0.00804629921913147\n",
      "training: 1 batch 338 batch_loss: 0.008253753185272217\n",
      "training: 1 batch 339 batch_loss: 0.008656173944473267\n",
      "training: 1 batch 340 batch_loss: 0.007852792739868164\n",
      "training: 1 batch 341 batch_loss: 0.0080280601978302\n",
      "training: 1 batch 342 batch_loss: 0.007376194000244141\n",
      "training: 1 batch 343 batch_loss: 0.007690876722335815\n",
      "training: 1 batch 344 batch_loss: 0.007863461971282959\n",
      "training: 1 batch 345 batch_loss: 0.007795423269271851\n",
      "training: 1 batch 346 batch_loss: 0.007311373949050903\n",
      "training: 1 batch 347 batch_loss: 0.008653104305267334\n",
      "training: 1 batch 348 batch_loss: 0.007795065641403198\n",
      "training: 1 batch 349 batch_loss: 0.007965087890625\n",
      "training: 1 batch 350 batch_loss: 0.008497148752212524\n",
      "training: 1 batch 351 batch_loss: 0.0072931647300720215\n",
      "training: 1 batch 352 batch_loss: 0.007579803466796875\n",
      "training: 1 batch 353 batch_loss: 0.007057905197143555\n",
      "training: 1 batch 354 batch_loss: 0.00771939754486084\n",
      "training: 1 batch 355 batch_loss: 0.007947415113449097\n",
      "training: 1 batch 356 batch_loss: 0.007982075214385986\n",
      "training: 1 batch 357 batch_loss: 0.007488250732421875\n",
      "training: 1 batch 358 batch_loss: 0.008040040731430054\n",
      "training: 1 batch 359 batch_loss: 0.0075826942920684814\n",
      "training: 1 batch 360 batch_loss: 0.007047474384307861\n",
      "training: 1 batch 361 batch_loss: 0.007711082696914673\n",
      "training: 1 batch 362 batch_loss: 0.007641047239303589\n",
      "training: 1 batch 363 batch_loss: 0.007263004779815674\n",
      "training: 1 batch 364 batch_loss: 0.0076454877853393555\n",
      "training: 1 batch 365 batch_loss: 0.007713735103607178\n",
      "training: 1 batch 366 batch_loss: 0.007986336946487427\n",
      "training: 1 batch 367 batch_loss: 0.007938295602798462\n",
      "training: 1 batch 368 batch_loss: 0.007322102785110474\n",
      "training: 1 batch 369 batch_loss: 0.0070719122886657715\n",
      "training: 1 batch 370 batch_loss: 0.007697969675064087\n",
      "training: 1 batch 371 batch_loss: 0.006826221942901611\n",
      "training: 1 batch 372 batch_loss: 0.006944864988327026\n",
      "training: 1 batch 373 batch_loss: 0.007030367851257324\n",
      "training: 1 batch 374 batch_loss: 0.007086515426635742\n",
      "training: 1 batch 375 batch_loss: 0.007497131824493408\n",
      "training: 1 batch 376 batch_loss: 0.0076771080493927\n",
      "training: 1 batch 377 batch_loss: 0.008107542991638184\n",
      "training: 1 batch 378 batch_loss: 0.007183969020843506\n",
      "training: 1 batch 379 batch_loss: 0.007415115833282471\n",
      "training: 1 batch 380 batch_loss: 0.007650107145309448\n",
      "training: 1 batch 381 batch_loss: 0.007495105266571045\n",
      "training: 1 batch 382 batch_loss: 0.007160156965255737\n",
      "training: 1 batch 383 batch_loss: 0.007460236549377441\n",
      "training: 1 batch 384 batch_loss: 0.008260637521743774\n",
      "training: 1 batch 385 batch_loss: 0.007530421018600464\n",
      "training: 1 batch 386 batch_loss: 0.006768077611923218\n",
      "training: 1 batch 387 batch_loss: 0.007383584976196289\n",
      "training: 1 batch 388 batch_loss: 0.007183760404586792\n",
      "training: 1 batch 389 batch_loss: 0.008031338453292847\n",
      "training: 1 batch 390 batch_loss: 0.007437288761138916\n",
      "training: 1 batch 391 batch_loss: 0.006751120090484619\n",
      "training: 1 batch 392 batch_loss: 0.006974697113037109\n",
      "training: 1 batch 393 batch_loss: 0.006777554750442505\n",
      "training: 1 batch 394 batch_loss: 0.007913917303085327\n",
      "training: 1 batch 395 batch_loss: 0.007345139980316162\n",
      "training: 1 batch 396 batch_loss: 0.006669878959655762\n",
      "training: 1 batch 397 batch_loss: 0.007540255784988403\n",
      "training: 1 batch 398 batch_loss: 0.006494015455245972\n",
      "training: 1 batch 399 batch_loss: 0.007956355810165405\n",
      "training: 1 batch 400 batch_loss: 0.007242083549499512\n",
      "training: 1 batch 401 batch_loss: 0.0071572065353393555\n",
      "training: 1 batch 402 batch_loss: 0.00709301233291626\n",
      "training: 1 batch 403 batch_loss: 0.007474452257156372\n",
      "training: 1 batch 404 batch_loss: 0.007746219635009766\n",
      "training: 1 batch 405 batch_loss: 0.007203131914138794\n",
      "training: 1 batch 406 batch_loss: 0.0077432990074157715\n",
      "training: 1 batch 407 batch_loss: 0.007201492786407471\n",
      "training: 1 batch 408 batch_loss: 0.006742715835571289\n",
      "training: 1 batch 409 batch_loss: 0.007325500249862671\n",
      "training: 1 batch 410 batch_loss: 0.006711483001708984\n",
      "training: 1 batch 411 batch_loss: 0.006508857011795044\n",
      "training: 1 batch 412 batch_loss: 0.007254242897033691\n",
      "training: 1 batch 413 batch_loss: 0.007116466760635376\n",
      "training: 1 batch 414 batch_loss: 0.00742650032043457\n",
      "training: 1 batch 415 batch_loss: 0.006634831428527832\n",
      "training: 1 batch 416 batch_loss: 0.007245838642120361\n",
      "training: 1 batch 417 batch_loss: 0.0069567859172821045\n",
      "training: 1 batch 418 batch_loss: 0.0066311657428741455\n",
      "training: 1 batch 419 batch_loss: 0.007174432277679443\n",
      "training: 1 batch 420 batch_loss: 0.007269382476806641\n",
      "training: 1 batch 421 batch_loss: 0.0067864954471588135\n",
      "training: 1 batch 422 batch_loss: 0.007086336612701416\n",
      "training: 1 batch 423 batch_loss: 0.0066935718059539795\n",
      "training: 1 batch 424 batch_loss: 0.006339132785797119\n",
      "training: 1 batch 425 batch_loss: 0.006958067417144775\n",
      "training: 1 batch 426 batch_loss: 0.006929278373718262\n",
      "training: 1 batch 427 batch_loss: 0.00646248459815979\n",
      "training: 1 batch 428 batch_loss: 0.006888329982757568\n",
      "training: 1 batch 429 batch_loss: 0.0072078704833984375\n",
      "training: 1 batch 430 batch_loss: 0.007020771503448486\n",
      "training: 1 batch 431 batch_loss: 0.0066620707511901855\n",
      "training: 1 batch 432 batch_loss: 0.0062965452671051025\n",
      "training: 1 batch 433 batch_loss: 0.006578266620635986\n",
      "training: 1 batch 434 batch_loss: 0.007266730070114136\n",
      "training: 1 batch 435 batch_loss: 0.00684848427772522\n",
      "training: 1 batch 436 batch_loss: 0.006752371788024902\n",
      "training: 1 batch 437 batch_loss: 0.006954550743103027\n",
      "training: 1 batch 438 batch_loss: 0.006270319223403931\n",
      "training: 1 batch 439 batch_loss: 0.006411194801330566\n",
      "training: 1 batch 440 batch_loss: 0.006292134523391724\n",
      "training: 1 batch 441 batch_loss: 0.006683379411697388\n",
      "training: 1 batch 442 batch_loss: 0.006296932697296143\n",
      "training: 1 batch 443 batch_loss: 0.006813943386077881\n",
      "training: 1 batch 444 batch_loss: 0.006471544504165649\n",
      "training: 1 batch 445 batch_loss: 0.007215231657028198\n",
      "training: 1 batch 446 batch_loss: 0.007083088159561157\n",
      "training: 1 batch 447 batch_loss: 0.006771087646484375\n",
      "training: 1 batch 448 batch_loss: 0.0071627795696258545\n",
      "training: 1 batch 449 batch_loss: 0.006428271532058716\n",
      "training: 1 batch 450 batch_loss: 0.007552683353424072\n",
      "training: 1 batch 451 batch_loss: 0.006855905055999756\n",
      "training: 1 batch 452 batch_loss: 0.006686806678771973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 1 batch 453 batch_loss: 0.006101101636886597\n",
      "training: 1 batch 454 batch_loss: 0.006073623895645142\n",
      "training: 1 batch 455 batch_loss: 0.005953729152679443\n",
      "training: 1 batch 456 batch_loss: 0.007077157497406006\n",
      "training: 1 batch 457 batch_loss: 0.006217986345291138\n",
      "training: 1 batch 458 batch_loss: 0.006424158811569214\n",
      "training: 1 batch 459 batch_loss: 0.006955951452255249\n",
      "training: 1 batch 460 batch_loss: 0.006499618291854858\n",
      "training: 1 batch 461 batch_loss: 0.0061334967613220215\n",
      "training: 1 batch 462 batch_loss: 0.006932258605957031\n",
      "training: 1 batch 463 batch_loss: 0.006251335144042969\n",
      "training: 1 batch 464 batch_loss: 0.00634390115737915\n",
      "training: 1 batch 465 batch_loss: 0.006228148937225342\n",
      "training: 1 batch 466 batch_loss: 0.006217837333679199\n",
      "training: 1 batch 467 batch_loss: 0.006699860095977783\n",
      "training: 1 batch 468 batch_loss: 0.006725579500198364\n",
      "training: 1 batch 469 batch_loss: 0.006223589181900024\n",
      "training: 1 batch 470 batch_loss: 0.0066998302936553955\n",
      "training: 1 batch 471 batch_loss: 0.006523847579956055\n",
      "training: 1 batch 472 batch_loss: 0.006530940532684326\n",
      "training: 1 batch 473 batch_loss: 0.006256729364395142\n",
      "training: 1 batch 474 batch_loss: 0.006460100412368774\n",
      "training: 1 batch 475 batch_loss: 0.00593218207359314\n",
      "training: 1 batch 476 batch_loss: 0.006189525127410889\n",
      "training: 1 batch 477 batch_loss: 0.006099671125411987\n",
      "training: 1 batch 478 batch_loss: 0.006877243518829346\n",
      "training: 1 batch 479 batch_loss: 0.006531119346618652\n",
      "training: 1 batch 480 batch_loss: 0.0063530802726745605\n",
      "training: 1 batch 481 batch_loss: 0.006133764982223511\n",
      "training: 1 batch 482 batch_loss: 0.00612950325012207\n",
      "training: 1 batch 483 batch_loss: 0.006594210863113403\n",
      "training: 1 batch 484 batch_loss: 0.006793767213821411\n",
      "training: 1 batch 485 batch_loss: 0.00613170862197876\n",
      "training: 1 batch 486 batch_loss: 0.006241917610168457\n",
      "training: 1 batch 487 batch_loss: 0.006736576557159424\n",
      "training: 1 batch 488 batch_loss: 0.0065291523933410645\n",
      "training: 1 batch 489 batch_loss: 0.006181150674819946\n",
      "training: 1 batch 490 batch_loss: 0.006479531526565552\n",
      "training: 1 batch 491 batch_loss: 0.00672227144241333\n",
      "training: 1 batch 492 batch_loss: 0.005887717008590698\n",
      "training: 1 batch 493 batch_loss: 0.006373345851898193\n",
      "training: 1 batch 494 batch_loss: 0.006376981735229492\n",
      "training: 1 batch 495 batch_loss: 0.00621303915977478\n",
      "training: 1 batch 496 batch_loss: 0.0060280561447143555\n",
      "training: 1 batch 497 batch_loss: 0.00617903470993042\n",
      "training: 1 batch 498 batch_loss: 0.006266593933105469\n",
      "training: 1 batch 499 batch_loss: 0.006400465965270996\n",
      "training: 1 batch 500 batch_loss: 0.006112962961196899\n",
      "training: 1 batch 501 batch_loss: 0.006535589694976807\n",
      "training: 1 batch 502 batch_loss: 0.005871623754501343\n",
      "training: 1 batch 503 batch_loss: 0.006219446659088135\n",
      "training: 1 batch 504 batch_loss: 0.005998969078063965\n",
      "training: 1 batch 505 batch_loss: 0.006693333387374878\n",
      "training: 1 batch 506 batch_loss: 0.006531357765197754\n",
      "training: 1 batch 507 batch_loss: 0.006179869174957275\n",
      "training: 1 batch 508 batch_loss: 0.006332218647003174\n",
      "training: 1 batch 509 batch_loss: 0.006362736225128174\n",
      "training: 1 batch 510 batch_loss: 0.006103783845901489\n",
      "training: 1 batch 511 batch_loss: 0.006991863250732422\n",
      "training: 1 batch 512 batch_loss: 0.0062094926834106445\n",
      "training: 1 batch 513 batch_loss: 0.006319999694824219\n",
      "training: 1 batch 514 batch_loss: 0.006536722183227539\n",
      "training: 1 batch 515 batch_loss: 0.00607261061668396\n",
      "training: 1 batch 516 batch_loss: 0.005666464567184448\n",
      "training: 1 batch 517 batch_loss: 0.006199568510055542\n",
      "training: 1 batch 518 batch_loss: 0.006043881177902222\n",
      "training: 1 batch 519 batch_loss: 0.006248891353607178\n",
      "training: 1 batch 520 batch_loss: 0.00597187876701355\n",
      "training: 1 batch 521 batch_loss: 0.00562673807144165\n",
      "training: 1 batch 522 batch_loss: 0.005825161933898926\n",
      "training: 1 batch 523 batch_loss: 0.0066062211990356445\n",
      "training: 1 batch 524 batch_loss: 0.006299138069152832\n",
      "training: 1 batch 525 batch_loss: 0.006112337112426758\n",
      "training: 1 batch 526 batch_loss: 0.00663524866104126\n",
      "training: 1 batch 527 batch_loss: 0.0064223408699035645\n",
      "training: 1 batch 528 batch_loss: 0.00598713755607605\n",
      "training: 1 batch 529 batch_loss: 0.006141990423202515\n",
      "training: 1 batch 530 batch_loss: 0.005637109279632568\n",
      "training: 1 batch 531 batch_loss: 0.005656450986862183\n",
      "training: 1 batch 532 batch_loss: 0.006008446216583252\n",
      "training: 1 batch 533 batch_loss: 0.006301403045654297\n",
      "training: 1 batch 534 batch_loss: 0.0065586864948272705\n",
      "training: 1 batch 535 batch_loss: 0.006331264972686768\n",
      "training: 1 batch 536 batch_loss: 0.00655403733253479\n",
      "training: 1 batch 537 batch_loss: 0.006589531898498535\n",
      "training: 1 batch 538 batch_loss: 0.0058126747608184814\n",
      "training: 1 batch 539 batch_loss: 0.005972832441329956\n",
      "training: 1 batch 540 batch_loss: 0.005798935890197754\n",
      "training: 1 batch 541 batch_loss: 0.005812585353851318\n",
      "training: 1 batch 542 batch_loss: 0.006614714860916138\n",
      "training: 1 batch 543 batch_loss: 0.005934089422225952\n",
      "training: 1 batch 544 batch_loss: 0.006516009569168091\n",
      "training: 1 batch 545 batch_loss: 0.005666017532348633\n",
      "training: 1 batch 546 batch_loss: 0.00591200590133667\n",
      "training: 1 batch 547 batch_loss: 0.006057113409042358\n",
      "training: 1 batch 548 batch_loss: 0.006061345338821411\n",
      "training: 1 batch 549 batch_loss: 0.006163835525512695\n",
      "training: 1 batch 550 batch_loss: 0.005729645490646362\n",
      "training: 1 batch 551 batch_loss: 0.006720691919326782\n",
      "training: 1 batch 552 batch_loss: 0.00663447380065918\n",
      "training: 1 batch 553 batch_loss: 0.005540251731872559\n",
      "training: 1 batch 554 batch_loss: 0.006330132484436035\n",
      "training: 1 batch 555 batch_loss: 0.0060648322105407715\n",
      "training: 1 batch 556 batch_loss: 0.005663692951202393\n",
      "training: 1 batch 557 batch_loss: 0.005572617053985596\n",
      "training: 1 batch 558 batch_loss: 0.005941033363342285\n",
      "training: 1 batch 559 batch_loss: 0.006871223449707031\n",
      "training: 1 batch 560 batch_loss: 0.006032496690750122\n",
      "training: 1 batch 561 batch_loss: 0.006237924098968506\n",
      "training: 1 batch 562 batch_loss: 0.00603523850440979\n",
      "training: 1 batch 563 batch_loss: 0.006195574998855591\n",
      "training: 1 batch 564 batch_loss: 0.005677640438079834\n",
      "training: 1 batch 565 batch_loss: 0.00611954927444458\n",
      "training: 1 batch 566 batch_loss: 0.006060481071472168\n",
      "training: 1 batch 567 batch_loss: 0.005707383155822754\n",
      "training: 1 batch 568 batch_loss: 0.00588536262512207\n",
      "training: 1 batch 569 batch_loss: 0.006273746490478516\n",
      "training: 1 batch 570 batch_loss: 0.006283462047576904\n",
      "training: 1 batch 571 batch_loss: 0.005852758884429932\n",
      "training: 1 batch 572 batch_loss: 0.005945563316345215\n",
      "training: 1 batch 573 batch_loss: 0.006040245294570923\n",
      "training: 1 batch 574 batch_loss: 0.005643665790557861\n",
      "training: 1 batch 575 batch_loss: 0.006312042474746704\n",
      "training: 1 batch 576 batch_loss: 0.006063193082809448\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | NDCG:0.04755010954365849\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 2 batch 0 batch_loss: 0.0055103600025177\n",
      "training: 2 batch 1 batch_loss: 0.005570441484451294\n",
      "training: 2 batch 2 batch_loss: 0.005559027194976807\n",
      "training: 2 batch 3 batch_loss: 0.005560338497161865\n",
      "training: 2 batch 4 batch_loss: 0.005382180213928223\n",
      "training: 2 batch 5 batch_loss: 0.00562557578086853\n",
      "training: 2 batch 6 batch_loss: 0.005415797233581543\n",
      "training: 2 batch 7 batch_loss: 0.005384117364883423\n",
      "training: 2 batch 8 batch_loss: 0.005388975143432617\n",
      "training: 2 batch 9 batch_loss: 0.005457401275634766\n",
      "training: 2 batch 10 batch_loss: 0.005620419979095459\n",
      "training: 2 batch 11 batch_loss: 0.005498766899108887\n",
      "training: 2 batch 12 batch_loss: 0.0054759681224823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 2 batch 13 batch_loss: 0.005500495433807373\n",
      "training: 2 batch 14 batch_loss: 0.00560528039932251\n",
      "training: 2 batch 15 batch_loss: 0.005421817302703857\n",
      "training: 2 batch 16 batch_loss: 0.005653321743011475\n",
      "training: 2 batch 17 batch_loss: 0.005549788475036621\n",
      "training: 2 batch 18 batch_loss: 0.005550801753997803\n",
      "training: 2 batch 19 batch_loss: 0.005511879920959473\n",
      "training: 2 batch 20 batch_loss: 0.005446583032608032\n",
      "training: 2 batch 21 batch_loss: 0.0054547786712646484\n",
      "training: 2 batch 22 batch_loss: 0.005558013916015625\n",
      "training: 2 batch 23 batch_loss: 0.00549963116645813\n",
      "training: 2 batch 24 batch_loss: 0.005451560020446777\n",
      "training: 2 batch 25 batch_loss: 0.005456507205963135\n",
      "training: 2 batch 26 batch_loss: 0.005634725093841553\n",
      "training: 2 batch 27 batch_loss: 0.005451381206512451\n",
      "training: 2 batch 28 batch_loss: 0.005671083927154541\n",
      "training: 2 batch 29 batch_loss: 0.0055283308029174805\n",
      "training: 2 batch 30 batch_loss: 0.00542062520980835\n",
      "training: 2 batch 31 batch_loss: 0.005442142486572266\n",
      "training: 2 batch 32 batch_loss: 0.005506306886672974\n",
      "training: 2 batch 33 batch_loss: 0.0055580735206604\n",
      "training: 2 batch 34 batch_loss: 0.005431056022644043\n",
      "training: 2 batch 35 batch_loss: 0.005439460277557373\n",
      "training: 2 batch 36 batch_loss: 0.005682229995727539\n",
      "training: 2 batch 37 batch_loss: 0.005403697490692139\n",
      "training: 2 batch 38 batch_loss: 0.005627095699310303\n",
      "training: 2 batch 39 batch_loss: 0.005514323711395264\n",
      "training: 2 batch 40 batch_loss: 0.005530416965484619\n",
      "training: 2 batch 41 batch_loss: 0.0056220293045043945\n",
      "training: 2 batch 42 batch_loss: 0.005533009767532349\n",
      "training: 2 batch 43 batch_loss: 0.00546187162399292\n",
      "training: 2 batch 44 batch_loss: 0.0055318474769592285\n",
      "training: 2 batch 45 batch_loss: 0.005577385425567627\n",
      "training: 2 batch 46 batch_loss: 0.005446583032608032\n",
      "training: 2 batch 47 batch_loss: 0.005513012409210205\n",
      "training: 2 batch 48 batch_loss: 0.0055921077728271484\n",
      "training: 2 batch 49 batch_loss: 0.005660742521286011\n",
      "training: 2 batch 50 batch_loss: 0.005467653274536133\n",
      "training: 2 batch 51 batch_loss: 0.005604535341262817\n",
      "training: 2 batch 52 batch_loss: 0.005509883165359497\n",
      "training: 2 batch 53 batch_loss: 0.005517065525054932\n",
      "training: 2 batch 54 batch_loss: 0.005485862493515015\n",
      "training: 2 batch 55 batch_loss: 0.005500197410583496\n",
      "training: 2 batch 56 batch_loss: 0.005493670701980591\n",
      "training: 2 batch 57 batch_loss: 0.005520880222320557\n",
      "training: 2 batch 58 batch_loss: 0.0055990517139434814\n",
      "training: 2 batch 59 batch_loss: 0.005474358797073364\n",
      "training: 2 batch 60 batch_loss: 0.005536168813705444\n",
      "training: 2 batch 61 batch_loss: 0.005524784326553345\n",
      "training: 2 batch 62 batch_loss: 0.005654722452163696\n",
      "training: 2 batch 63 batch_loss: 0.005524486303329468\n",
      "training: 2 batch 64 batch_loss: 0.0054901838302612305\n",
      "training: 2 batch 65 batch_loss: 0.005498707294464111\n",
      "training: 2 batch 66 batch_loss: 0.00555005669593811\n",
      "training: 2 batch 67 batch_loss: 0.0055084228515625\n",
      "training: 2 batch 68 batch_loss: 0.00550389289855957\n",
      "training: 2 batch 69 batch_loss: 0.0054905712604522705\n",
      "training: 2 batch 70 batch_loss: 0.0053141117095947266\n",
      "training: 2 batch 71 batch_loss: 0.005611717700958252\n",
      "training: 2 batch 72 batch_loss: 0.005532801151275635\n",
      "training: 2 batch 73 batch_loss: 0.005519092082977295\n",
      "training: 2 batch 74 batch_loss: 0.005448848009109497\n",
      "training: 2 batch 75 batch_loss: 0.005609363317489624\n",
      "training: 2 batch 76 batch_loss: 0.0056056976318359375\n",
      "training: 2 batch 77 batch_loss: 0.0054776668548583984\n",
      "training: 2 batch 78 batch_loss: 0.005556941032409668\n",
      "training: 2 batch 79 batch_loss: 0.005502820014953613\n",
      "training: 2 batch 80 batch_loss: 0.005597829818725586\n",
      "training: 2 batch 81 batch_loss: 0.005466639995574951\n",
      "training: 2 batch 82 batch_loss: 0.00561097264289856\n",
      "training: 2 batch 83 batch_loss: 0.005496710538864136\n",
      "training: 2 batch 84 batch_loss: 0.005467593669891357\n",
      "training: 2 batch 85 batch_loss: 0.00548398494720459\n",
      "training: 2 batch 86 batch_loss: 0.005549788475036621\n",
      "training: 2 batch 87 batch_loss: 0.005499809980392456\n",
      "training: 2 batch 88 batch_loss: 0.005588889122009277\n",
      "training: 2 batch 89 batch_loss: 0.005506277084350586\n",
      "training: 2 batch 90 batch_loss: 0.0055151283740997314\n",
      "training: 2 batch 91 batch_loss: 0.005460530519485474\n",
      "training: 2 batch 92 batch_loss: 0.005597293376922607\n",
      "training: 2 batch 93 batch_loss: 0.005565226078033447\n",
      "training: 2 batch 94 batch_loss: 0.005480051040649414\n",
      "training: 2 batch 95 batch_loss: 0.005474358797073364\n",
      "training: 2 batch 96 batch_loss: 0.005673408508300781\n",
      "training: 2 batch 97 batch_loss: 0.005491346120834351\n",
      "training: 2 batch 98 batch_loss: 0.00555729866027832\n",
      "training: 2 batch 99 batch_loss: 0.00561407208442688\n",
      "training: 2 batch 100 batch_loss: 0.005585789680480957\n",
      "training: 2 batch 101 batch_loss: 0.005691111087799072\n",
      "training: 2 batch 102 batch_loss: 0.005574047565460205\n",
      "training: 2 batch 103 batch_loss: 0.005707263946533203\n",
      "training: 2 batch 104 batch_loss: 0.005511969327926636\n",
      "training: 2 batch 105 batch_loss: 0.005696743726730347\n",
      "training: 2 batch 106 batch_loss: 0.005579054355621338\n",
      "training: 2 batch 107 batch_loss: 0.00559312105178833\n",
      "training: 2 batch 108 batch_loss: 0.005552738904953003\n",
      "training: 2 batch 109 batch_loss: 0.0056324005126953125\n",
      "training: 2 batch 110 batch_loss: 0.00567665696144104\n",
      "training: 2 batch 111 batch_loss: 0.0056198835372924805\n",
      "training: 2 batch 112 batch_loss: 0.005589514970779419\n",
      "training: 2 batch 113 batch_loss: 0.005615830421447754\n",
      "training: 2 batch 114 batch_loss: 0.005699425935745239\n",
      "training: 2 batch 115 batch_loss: 0.0055814385414123535\n",
      "training: 2 batch 116 batch_loss: 0.00563928484916687\n",
      "training: 2 batch 117 batch_loss: 0.005641818046569824\n",
      "training: 2 batch 118 batch_loss: 0.005630373954772949\n",
      "training: 2 batch 119 batch_loss: 0.005579620599746704\n",
      "training: 2 batch 120 batch_loss: 0.0056476891040802\n",
      "training: 2 batch 121 batch_loss: 0.005604952573776245\n",
      "training: 2 batch 122 batch_loss: 0.005627363920211792\n",
      "training: 2 batch 123 batch_loss: 0.0056983232498168945\n",
      "training: 2 batch 124 batch_loss: 0.005551040172576904\n",
      "training: 2 batch 125 batch_loss: 0.005645483732223511\n",
      "training: 2 batch 126 batch_loss: 0.005607783794403076\n",
      "training: 2 batch 127 batch_loss: 0.005517005920410156\n",
      "training: 2 batch 128 batch_loss: 0.005746722221374512\n",
      "training: 2 batch 129 batch_loss: 0.0056768059730529785\n",
      "training: 2 batch 130 batch_loss: 0.005649387836456299\n",
      "training: 2 batch 131 batch_loss: 0.005731314420700073\n",
      "training: 2 batch 132 batch_loss: 0.005807220935821533\n",
      "training: 2 batch 133 batch_loss: 0.005648910999298096\n",
      "training: 2 batch 134 batch_loss: 0.005685389041900635\n",
      "training: 2 batch 135 batch_loss: 0.005788624286651611\n",
      "training: 2 batch 136 batch_loss: 0.005650758743286133\n",
      "training: 2 batch 137 batch_loss: 0.0057630836963653564\n",
      "training: 2 batch 138 batch_loss: 0.005775958299636841\n",
      "training: 2 batch 139 batch_loss: 0.005780518054962158\n",
      "training: 2 batch 140 batch_loss: 0.005788862705230713\n",
      "training: 2 batch 141 batch_loss: 0.005692183971405029\n",
      "training: 2 batch 142 batch_loss: 0.005685389041900635\n",
      "training: 2 batch 143 batch_loss: 0.005778670310974121\n",
      "training: 2 batch 144 batch_loss: 0.0057822465896606445\n",
      "training: 2 batch 145 batch_loss: 0.005744755268096924\n",
      "training: 2 batch 146 batch_loss: 0.0057183802127838135\n",
      "training: 2 batch 147 batch_loss: 0.006345093250274658\n",
      "training: 2 batch 148 batch_loss: 0.005728930234909058\n",
      "training: 2 batch 149 batch_loss: 0.0058574676513671875\n",
      "training: 2 batch 150 batch_loss: 0.005823791027069092\n",
      "training: 2 batch 151 batch_loss: 0.005813866853713989\n",
      "training: 2 batch 152 batch_loss: 0.005772411823272705\n",
      "training: 2 batch 153 batch_loss: 0.005882292985916138\n",
      "training: 2 batch 154 batch_loss: 0.0058408379554748535\n",
      "training: 2 batch 155 batch_loss: 0.005873501300811768\n",
      "training: 2 batch 156 batch_loss: 0.005895882844924927\n",
      "training: 2 batch 157 batch_loss: 0.005995333194732666\n",
      "training: 2 batch 158 batch_loss: 0.005845099687576294\n",
      "training: 2 batch 159 batch_loss: 0.005770891904830933\n",
      "training: 2 batch 160 batch_loss: 0.005825608968734741\n",
      "training: 2 batch 161 batch_loss: 0.005933701992034912\n",
      "training: 2 batch 162 batch_loss: 0.005863398313522339\n",
      "training: 2 batch 163 batch_loss: 0.005808323621749878\n",
      "training: 2 batch 164 batch_loss: 0.0058518946170806885\n",
      "training: 2 batch 165 batch_loss: 0.005988329648971558\n",
      "training: 2 batch 166 batch_loss: 0.005883932113647461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 2 batch 167 batch_loss: 0.005968570709228516\n",
      "training: 2 batch 168 batch_loss: 0.005939185619354248\n",
      "training: 2 batch 169 batch_loss: 0.005997598171234131\n",
      "training: 2 batch 170 batch_loss: 0.0059184730052948\n",
      "training: 2 batch 171 batch_loss: 0.005901902914047241\n",
      "training: 2 batch 172 batch_loss: 0.006160110235214233\n",
      "training: 2 batch 173 batch_loss: 0.006016075611114502\n",
      "training: 2 batch 174 batch_loss: 0.005910933017730713\n",
      "training: 2 batch 175 batch_loss: 0.0059954822063446045\n",
      "training: 2 batch 176 batch_loss: 0.006060868501663208\n",
      "training: 2 batch 177 batch_loss: 0.006001383066177368\n",
      "training: 2 batch 178 batch_loss: 0.006061762571334839\n",
      "training: 2 batch 179 batch_loss: 0.005979567766189575\n",
      "training: 2 batch 180 batch_loss: 0.006040811538696289\n",
      "training: 2 batch 181 batch_loss: 0.006031334400177002\n",
      "training: 2 batch 182 batch_loss: 0.006068587303161621\n",
      "training: 2 batch 183 batch_loss: 0.006140708923339844\n",
      "training: 2 batch 184 batch_loss: 0.005956709384918213\n",
      "training: 2 batch 185 batch_loss: 0.006131350994110107\n",
      "training: 2 batch 186 batch_loss: 0.00616985559463501\n",
      "training: 2 batch 187 batch_loss: 0.006202042102813721\n",
      "training: 2 batch 188 batch_loss: 0.006039053201675415\n",
      "training: 2 batch 189 batch_loss: 0.006170004606246948\n",
      "training: 2 batch 190 batch_loss: 0.00621458888053894\n",
      "training: 2 batch 191 batch_loss: 0.006082206964492798\n",
      "training: 2 batch 192 batch_loss: 0.006232023239135742\n",
      "training: 2 batch 193 batch_loss: 0.0062146782875061035\n",
      "training: 2 batch 194 batch_loss: 0.006202340126037598\n",
      "training: 2 batch 195 batch_loss: 0.006178677082061768\n",
      "training: 2 batch 196 batch_loss: 0.00640106201171875\n",
      "training: 2 batch 197 batch_loss: 0.0063169002532958984\n",
      "training: 2 batch 198 batch_loss: 0.00637894868850708\n",
      "training: 2 batch 199 batch_loss: 0.006285816431045532\n",
      "training: 2 batch 200 batch_loss: 0.006344497203826904\n",
      "training: 2 batch 201 batch_loss: 0.0063841938972473145\n",
      "training: 2 batch 202 batch_loss: 0.00636562705039978\n",
      "training: 2 batch 203 batch_loss: 0.006218671798706055\n",
      "training: 2 batch 204 batch_loss: 0.0063593387603759766\n",
      "training: 2 batch 205 batch_loss: 0.006463974714279175\n",
      "training: 2 batch 206 batch_loss: 0.006457716226577759\n",
      "training: 2 batch 207 batch_loss: 0.006334632635116577\n",
      "training: 2 batch 208 batch_loss: 0.006461411714553833\n",
      "training: 2 batch 209 batch_loss: 0.0063936710357666016\n",
      "training: 2 batch 210 batch_loss: 0.00653684139251709\n",
      "training: 2 batch 211 batch_loss: 0.006511896848678589\n",
      "training: 2 batch 212 batch_loss: 0.006614595651626587\n",
      "training: 2 batch 213 batch_loss: 0.006475687026977539\n",
      "training: 2 batch 214 batch_loss: 0.006639301776885986\n",
      "training: 2 batch 215 batch_loss: 0.00662568211555481\n",
      "training: 2 batch 216 batch_loss: 0.006594598293304443\n",
      "training: 2 batch 217 batch_loss: 0.006505817174911499\n",
      "training: 2 batch 218 batch_loss: 0.006687343120574951\n",
      "training: 2 batch 219 batch_loss: 0.006741642951965332\n",
      "training: 2 batch 220 batch_loss: 0.006619691848754883\n",
      "training: 2 batch 221 batch_loss: 0.00672343373298645\n",
      "training: 2 batch 222 batch_loss: 0.006663680076599121\n",
      "training: 2 batch 223 batch_loss: 0.006763041019439697\n",
      "training: 2 batch 224 batch_loss: 0.0066550374031066895\n",
      "training: 2 batch 225 batch_loss: 0.006858170032501221\n",
      "training: 2 batch 226 batch_loss: 0.006909966468811035\n",
      "training: 2 batch 227 batch_loss: 0.006768912076950073\n",
      "training: 2 batch 228 batch_loss: 0.0068344175815582275\n",
      "training: 2 batch 229 batch_loss: 0.006840139627456665\n",
      "training: 2 batch 230 batch_loss: 0.006924539804458618\n",
      "training: 2 batch 231 batch_loss: 0.0071020424365997314\n",
      "training: 2 batch 232 batch_loss: 0.006989657878875732\n",
      "training: 2 batch 233 batch_loss: 0.007041871547698975\n",
      "training: 2 batch 234 batch_loss: 0.007033675909042358\n",
      "training: 2 batch 235 batch_loss: 0.007390528917312622\n",
      "training: 2 batch 236 batch_loss: 0.007058829069137573\n",
      "training: 2 batch 237 batch_loss: 0.007059246301651001\n",
      "training: 2 batch 238 batch_loss: 0.00703805685043335\n",
      "training: 2 batch 239 batch_loss: 0.007014155387878418\n",
      "training: 2 batch 240 batch_loss: 0.007074505090713501\n",
      "training: 2 batch 241 batch_loss: 0.007137417793273926\n",
      "training: 2 batch 242 batch_loss: 0.00734025239944458\n",
      "training: 2 batch 243 batch_loss: 0.007296055555343628\n",
      "training: 2 batch 244 batch_loss: 0.00734671950340271\n",
      "training: 2 batch 245 batch_loss: 0.007376194000244141\n",
      "training: 2 batch 246 batch_loss: 0.007388293743133545\n",
      "training: 2 batch 247 batch_loss: 0.007380306720733643\n",
      "training: 2 batch 248 batch_loss: 0.007522463798522949\n",
      "training: 2 batch 249 batch_loss: 0.007569551467895508\n",
      "training: 2 batch 250 batch_loss: 0.007432341575622559\n",
      "training: 2 batch 251 batch_loss: 0.007744342088699341\n",
      "training: 2 batch 252 batch_loss: 0.007503211498260498\n",
      "training: 2 batch 253 batch_loss: 0.007768601179122925\n",
      "training: 2 batch 254 batch_loss: 0.007727235555648804\n",
      "training: 2 batch 255 batch_loss: 0.007595688104629517\n",
      "training: 2 batch 256 batch_loss: 0.007775664329528809\n",
      "training: 2 batch 257 batch_loss: 0.007800638675689697\n",
      "training: 2 batch 258 batch_loss: 0.00773245096206665\n",
      "training: 2 batch 259 batch_loss: 0.007915705442428589\n",
      "training: 2 batch 260 batch_loss: 0.007972747087478638\n",
      "training: 2 batch 261 batch_loss: 0.00796365737915039\n",
      "training: 2 batch 262 batch_loss: 0.007996529340744019\n",
      "training: 2 batch 263 batch_loss: 0.008125156164169312\n",
      "training: 2 batch 264 batch_loss: 0.008048385381698608\n",
      "training: 2 batch 265 batch_loss: 0.0081101655960083\n",
      "training: 2 batch 266 batch_loss: 0.00813359022140503\n",
      "training: 2 batch 267 batch_loss: 0.008471786975860596\n",
      "training: 2 batch 268 batch_loss: 0.008341550827026367\n",
      "training: 2 batch 269 batch_loss: 0.008479148149490356\n",
      "training: 2 batch 270 batch_loss: 0.008528918027877808\n",
      "training: 2 batch 271 batch_loss: 0.008426845073699951\n",
      "training: 2 batch 272 batch_loss: 0.008405059576034546\n",
      "training: 2 batch 273 batch_loss: 0.008681267499923706\n",
      "training: 2 batch 274 batch_loss: 0.008781731128692627\n",
      "training: 2 batch 275 batch_loss: 0.008718758821487427\n",
      "training: 2 batch 276 batch_loss: 0.00876656174659729\n",
      "training: 2 batch 277 batch_loss: 0.00865069031715393\n",
      "training: 2 batch 278 batch_loss: 0.008767485618591309\n",
      "training: 2 batch 279 batch_loss: 0.008944064378738403\n",
      "training: 2 batch 280 batch_loss: 0.009057700634002686\n",
      "training: 2 batch 281 batch_loss: 0.009166836738586426\n",
      "training: 2 batch 282 batch_loss: 0.009212374687194824\n",
      "training: 2 batch 283 batch_loss: 0.009267926216125488\n",
      "training: 2 batch 284 batch_loss: 0.009539902210235596\n",
      "training: 2 batch 285 batch_loss: 0.009414494037628174\n",
      "training: 2 batch 286 batch_loss: 0.009552538394927979\n",
      "training: 2 batch 287 batch_loss: 0.009282082319259644\n",
      "training: 2 batch 288 batch_loss: 0.009567290544509888\n",
      "training: 2 batch 289 batch_loss: 0.00963658094406128\n",
      "training: 2 batch 290 batch_loss: 0.009911596775054932\n",
      "training: 2 batch 291 batch_loss: 0.010169059038162231\n",
      "training: 2 batch 292 batch_loss: 0.00992843508720398\n",
      "training: 2 batch 293 batch_loss: 0.010151177644729614\n",
      "training: 2 batch 294 batch_loss: 0.010272562503814697\n",
      "training: 2 batch 295 batch_loss: 0.010174393653869629\n",
      "training: 2 batch 296 batch_loss: 0.010441362857818604\n",
      "training: 2 batch 297 batch_loss: 0.01045989990234375\n",
      "training: 2 batch 298 batch_loss: 0.010699987411499023\n",
      "training: 2 batch 299 batch_loss: 0.010719597339630127\n",
      "training: 2 batch 300 batch_loss: 0.010625362396240234\n",
      "training: 2 batch 301 batch_loss: 0.010969221591949463\n",
      "training: 2 batch 302 batch_loss: 0.011109471321105957\n",
      "training: 2 batch 303 batch_loss: 0.011256486177444458\n",
      "training: 2 batch 304 batch_loss: 0.011394530534744263\n",
      "training: 2 batch 305 batch_loss: 0.011284798383712769\n",
      "training: 2 batch 306 batch_loss: 0.011427253484725952\n",
      "training: 2 batch 307 batch_loss: 0.01135319471359253\n",
      "training: 2 batch 308 batch_loss: 0.011353075504302979\n",
      "training: 2 batch 309 batch_loss: 0.01173323392868042\n",
      "training: 2 batch 310 batch_loss: 0.011814802885055542\n",
      "training: 2 batch 311 batch_loss: 0.012020468711853027\n",
      "training: 2 batch 312 batch_loss: 0.012478947639465332\n",
      "training: 2 batch 313 batch_loss: 0.012516260147094727\n",
      "training: 2 batch 314 batch_loss: 0.012851357460021973\n",
      "training: 2 batch 315 batch_loss: 0.012754052877426147\n",
      "training: 2 batch 316 batch_loss: 0.012942880392074585\n",
      "training: 2 batch 317 batch_loss: 0.01305285096168518\n",
      "training: 2 batch 318 batch_loss: 0.013482868671417236\n",
      "training: 2 batch 319 batch_loss: 0.013319313526153564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 2 batch 320 batch_loss: 0.013453841209411621\n",
      "training: 2 batch 321 batch_loss: 0.013987720012664795\n",
      "training: 2 batch 322 batch_loss: 0.014301061630249023\n",
      "training: 2 batch 323 batch_loss: 0.0141010582447052\n",
      "training: 2 batch 324 batch_loss: 0.014217376708984375\n",
      "training: 2 batch 325 batch_loss: 0.014305263757705688\n",
      "training: 2 batch 326 batch_loss: 0.01499795913696289\n",
      "training: 2 batch 327 batch_loss: 0.015065550804138184\n",
      "training: 2 batch 328 batch_loss: 0.015291720628738403\n",
      "training: 2 batch 329 batch_loss: 0.01528388261795044\n",
      "training: 2 batch 330 batch_loss: 0.015489041805267334\n",
      "training: 2 batch 331 batch_loss: 0.01564568281173706\n",
      "training: 2 batch 332 batch_loss: 0.01615014672279358\n",
      "training: 2 batch 333 batch_loss: 0.016789346933364868\n",
      "training: 2 batch 334 batch_loss: 0.01666581630706787\n",
      "training: 2 batch 335 batch_loss: 0.016840249300003052\n",
      "training: 2 batch 336 batch_loss: 0.0172654390335083\n",
      "training: 2 batch 337 batch_loss: 0.017300665378570557\n",
      "training: 2 batch 338 batch_loss: 0.01781398057937622\n",
      "training: 2 batch 339 batch_loss: 0.017631351947784424\n",
      "training: 2 batch 340 batch_loss: 0.01806536316871643\n",
      "training: 2 batch 341 batch_loss: 0.018403589725494385\n",
      "training: 2 batch 342 batch_loss: 0.018749475479125977\n",
      "training: 2 batch 343 batch_loss: 0.01891234517097473\n",
      "training: 2 batch 344 batch_loss: 0.01919114589691162\n",
      "training: 2 batch 345 batch_loss: 0.01944422721862793\n",
      "training: 2 batch 346 batch_loss: 0.0200653076171875\n",
      "training: 2 batch 347 batch_loss: 0.019867509603500366\n",
      "training: 2 batch 348 batch_loss: 0.020722925662994385\n",
      "training: 2 batch 349 batch_loss: 0.020559877157211304\n",
      "training: 2 batch 350 batch_loss: 0.021493107080459595\n",
      "training: 2 batch 351 batch_loss: 0.021574348211288452\n",
      "training: 2 batch 352 batch_loss: 0.021514922380447388\n",
      "training: 2 batch 353 batch_loss: 0.021876245737075806\n",
      "training: 2 batch 354 batch_loss: 0.022909164428710938\n",
      "training: 2 batch 355 batch_loss: 0.023159503936767578\n",
      "training: 2 batch 356 batch_loss: 0.02351263165473938\n",
      "training: 2 batch 357 batch_loss: 0.024499118328094482\n",
      "training: 2 batch 358 batch_loss: 0.02388584613800049\n",
      "training: 2 batch 359 batch_loss: 0.024128079414367676\n",
      "training: 2 batch 360 batch_loss: 0.024704277515411377\n",
      "training: 2 batch 361 batch_loss: 0.025340259075164795\n",
      "training: 2 batch 362 batch_loss: 0.02555367350578308\n",
      "training: 2 batch 363 batch_loss: 0.026130080223083496\n",
      "training: 2 batch 364 batch_loss: 0.026808232069015503\n",
      "training: 2 batch 365 batch_loss: 0.027307361364364624\n",
      "training: 2 batch 366 batch_loss: 0.027142345905303955\n",
      "training: 2 batch 367 batch_loss: 0.02797919511795044\n",
      "training: 2 batch 368 batch_loss: 0.0276699960231781\n",
      "training: 2 batch 369 batch_loss: 0.028318792581558228\n",
      "training: 2 batch 370 batch_loss: 0.02972349524497986\n",
      "training: 2 batch 371 batch_loss: 0.03020864725112915\n",
      "training: 2 batch 372 batch_loss: 0.02982237935066223\n",
      "training: 2 batch 373 batch_loss: 0.03160539269447327\n",
      "training: 2 batch 374 batch_loss: 0.03198644518852234\n",
      "training: 2 batch 375 batch_loss: 0.03152400255203247\n",
      "training: 2 batch 376 batch_loss: 0.03255930542945862\n",
      "training: 2 batch 377 batch_loss: 0.03281491994857788\n",
      "training: 2 batch 378 batch_loss: 0.03375530242919922\n",
      "training: 2 batch 379 batch_loss: 0.03404480218887329\n",
      "training: 2 batch 380 batch_loss: 0.034405261278152466\n",
      "training: 2 batch 381 batch_loss: 0.03480607271194458\n",
      "training: 2 batch 382 batch_loss: 0.0355096161365509\n",
      "training: 2 batch 383 batch_loss: 0.03610795736312866\n",
      "training: 2 batch 384 batch_loss: 0.03542625904083252\n",
      "training: 2 batch 385 batch_loss: 0.03722035884857178\n",
      "training: 2 batch 386 batch_loss: 0.03716456890106201\n",
      "training: 2 batch 387 batch_loss: 0.03778684139251709\n",
      "training: 2 batch 388 batch_loss: 0.03878670930862427\n",
      "training: 2 batch 389 batch_loss: 0.03799021244049072\n",
      "training: 2 batch 390 batch_loss: 0.039339661598205566\n",
      "training: 2 batch 391 batch_loss: 0.039643704891204834\n",
      "training: 2 batch 392 batch_loss: 0.04107236862182617\n",
      "training: 2 batch 393 batch_loss: 0.04151126742362976\n",
      "training: 2 batch 394 batch_loss: 0.04064542055130005\n",
      "training: 2 batch 395 batch_loss: 0.042598068714141846\n",
      "training: 2 batch 396 batch_loss: 0.04311612248420715\n",
      "training: 2 batch 397 batch_loss: 0.04338392615318298\n",
      "training: 2 batch 398 batch_loss: 0.04447096586227417\n",
      "training: 2 batch 399 batch_loss: 0.044770658016204834\n",
      "training: 2 batch 400 batch_loss: 0.04493868350982666\n",
      "training: 2 batch 401 batch_loss: 0.046289294958114624\n",
      "training: 2 batch 402 batch_loss: 0.045349180698394775\n",
      "training: 2 batch 403 batch_loss: 0.04576188325881958\n",
      "training: 2 batch 404 batch_loss: 0.047144949436187744\n",
      "training: 2 batch 405 batch_loss: 0.047389715909957886\n",
      "training: 2 batch 406 batch_loss: 0.046295881271362305\n",
      "training: 2 batch 407 batch_loss: 0.048734575510025024\n",
      "training: 2 batch 408 batch_loss: 0.048162758350372314\n",
      "training: 2 batch 409 batch_loss: 0.048639148473739624\n",
      "training: 2 batch 410 batch_loss: 0.049693167209625244\n",
      "training: 2 batch 411 batch_loss: 0.05150902271270752\n",
      "training: 2 batch 412 batch_loss: 0.05177164077758789\n",
      "training: 2 batch 413 batch_loss: 0.04955807328224182\n",
      "training: 2 batch 414 batch_loss: 0.05099806189537048\n",
      "training: 2 batch 415 batch_loss: 0.05146586894989014\n",
      "training: 2 batch 416 batch_loss: 0.052752554416656494\n",
      "training: 2 batch 417 batch_loss: 0.051944881677627563\n",
      "training: 2 batch 418 batch_loss: 0.054186999797821045\n",
      "training: 2 batch 419 batch_loss: 0.05285915732383728\n",
      "training: 2 batch 420 batch_loss: 0.05337822437286377\n",
      "training: 2 batch 421 batch_loss: 0.05385088920593262\n",
      "training: 2 batch 422 batch_loss: 0.05604797601699829\n",
      "training: 2 batch 423 batch_loss: 0.0555824339389801\n",
      "training: 2 batch 424 batch_loss: 0.053018152713775635\n",
      "training: 2 batch 425 batch_loss: 0.0564078688621521\n",
      "training: 2 batch 426 batch_loss: 0.05541875958442688\n",
      "training: 2 batch 427 batch_loss: 0.05585041642189026\n",
      "training: 2 batch 428 batch_loss: 0.05620616674423218\n",
      "training: 2 batch 429 batch_loss: 0.05615606904029846\n",
      "training: 2 batch 430 batch_loss: 0.056392133235931396\n",
      "training: 2 batch 431 batch_loss: 0.05708587169647217\n",
      "training: 2 batch 432 batch_loss: 0.056725114583969116\n",
      "training: 2 batch 433 batch_loss: 0.05744123458862305\n",
      "training: 2 batch 434 batch_loss: 0.05857938528060913\n",
      "training: 2 batch 435 batch_loss: 0.06040722131729126\n",
      "training: 2 batch 436 batch_loss: 0.059416234493255615\n",
      "training: 2 batch 437 batch_loss: 0.061590731143951416\n",
      "training: 2 batch 438 batch_loss: 0.05967181921005249\n",
      "training: 2 batch 439 batch_loss: 0.057585835456848145\n",
      "training: 2 batch 440 batch_loss: 0.059773147106170654\n",
      "training: 2 batch 441 batch_loss: 0.05943092703819275\n",
      "training: 2 batch 442 batch_loss: 0.05963018536567688\n",
      "training: 2 batch 443 batch_loss: 0.059486836194992065\n",
      "training: 2 batch 444 batch_loss: 0.05944931507110596\n",
      "training: 2 batch 445 batch_loss: 0.06147083640098572\n",
      "training: 2 batch 446 batch_loss: 0.06302738189697266\n",
      "training: 2 batch 447 batch_loss: 0.06293797492980957\n",
      "training: 2 batch 448 batch_loss: 0.06301817297935486\n",
      "training: 2 batch 449 batch_loss: 0.05959576368331909\n",
      "training: 2 batch 450 batch_loss: 0.061729252338409424\n",
      "training: 2 batch 451 batch_loss: 0.05963453650474548\n",
      "training: 2 batch 452 batch_loss: 0.06188392639160156\n",
      "training: 2 batch 453 batch_loss: 0.061663150787353516\n",
      "training: 2 batch 454 batch_loss: 0.06162440776824951\n",
      "training: 2 batch 455 batch_loss: 0.06144261360168457\n",
      "training: 2 batch 456 batch_loss: 0.060628294944763184\n",
      "training: 2 batch 457 batch_loss: 0.06375598907470703\n",
      "training: 2 batch 458 batch_loss: 0.06406846642494202\n",
      "training: 2 batch 459 batch_loss: 0.06065073609352112\n",
      "training: 2 batch 460 batch_loss: 0.06294983625411987\n",
      "training: 2 batch 461 batch_loss: 0.06202688813209534\n",
      "training: 2 batch 462 batch_loss: 0.06279101967811584\n",
      "training: 2 batch 463 batch_loss: 0.06098005175590515\n",
      "training: 2 batch 464 batch_loss: 0.06199479103088379\n",
      "training: 2 batch 465 batch_loss: 0.061317503452301025\n",
      "training: 2 batch 466 batch_loss: 0.06178903579711914\n",
      "training: 2 batch 467 batch_loss: 0.06474819779396057\n",
      "training: 2 batch 468 batch_loss: 0.06122475862503052\n",
      "training: 2 batch 469 batch_loss: 0.06287488341331482\n",
      "training: 2 batch 470 batch_loss: 0.061450839042663574\n",
      "training: 2 batch 471 batch_loss: 0.061071038246154785\n",
      "training: 2 batch 472 batch_loss: 0.060399025678634644\n",
      "training: 2 batch 473 batch_loss: 0.06312289834022522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 2 batch 474 batch_loss: 0.061166733503341675\n",
      "training: 2 batch 475 batch_loss: 0.06116947531700134\n",
      "training: 2 batch 476 batch_loss: 0.06163632869720459\n",
      "training: 2 batch 477 batch_loss: 0.06119716167449951\n",
      "training: 2 batch 478 batch_loss: 0.0628637969493866\n",
      "training: 2 batch 479 batch_loss: 0.06215471029281616\n",
      "training: 2 batch 480 batch_loss: 0.06346964836120605\n",
      "training: 2 batch 481 batch_loss: 0.06287902593612671\n",
      "training: 2 batch 482 batch_loss: 0.06432098150253296\n",
      "training: 2 batch 483 batch_loss: 0.06281983852386475\n",
      "training: 2 batch 484 batch_loss: 0.061768144369125366\n",
      "training: 2 batch 485 batch_loss: 0.06213784217834473\n",
      "training: 2 batch 486 batch_loss: 0.06297463178634644\n",
      "training: 2 batch 487 batch_loss: 0.06601691246032715\n",
      "training: 2 batch 488 batch_loss: 0.06434351205825806\n",
      "training: 2 batch 489 batch_loss: 0.06129652261734009\n",
      "training: 2 batch 490 batch_loss: 0.06380066275596619\n",
      "training: 2 batch 491 batch_loss: 0.06170418858528137\n",
      "training: 2 batch 492 batch_loss: 0.06257802248001099\n",
      "training: 2 batch 493 batch_loss: 0.06172877550125122\n",
      "training: 2 batch 494 batch_loss: 0.06507530808448792\n",
      "training: 2 batch 495 batch_loss: 0.06556379795074463\n",
      "training: 2 batch 496 batch_loss: 0.06231403350830078\n",
      "training: 2 batch 497 batch_loss: 0.0622425377368927\n",
      "training: 2 batch 498 batch_loss: 0.06257361173629761\n",
      "training: 2 batch 499 batch_loss: 0.06239381432533264\n",
      "training: 2 batch 500 batch_loss: 0.06355857849121094\n",
      "training: 2 batch 501 batch_loss: 0.0630846619606018\n",
      "training: 2 batch 502 batch_loss: 0.061077386140823364\n",
      "training: 2 batch 503 batch_loss: 0.06008648872375488\n",
      "training: 2 batch 504 batch_loss: 0.06302744150161743\n",
      "training: 2 batch 505 batch_loss: 0.06174004077911377\n",
      "training: 2 batch 506 batch_loss: 0.0612412691116333\n",
      "training: 2 batch 507 batch_loss: 0.06271660327911377\n",
      "training: 2 batch 508 batch_loss: 0.06326323747634888\n",
      "training: 2 batch 509 batch_loss: 0.06260353326797485\n",
      "training: 2 batch 510 batch_loss: 0.060966312885284424\n",
      "training: 2 batch 511 batch_loss: 0.06326231360435486\n",
      "training: 2 batch 512 batch_loss: 0.06227964162826538\n",
      "training: 2 batch 513 batch_loss: 0.0653219223022461\n",
      "training: 2 batch 514 batch_loss: 0.06276094913482666\n",
      "training: 2 batch 515 batch_loss: 0.06412947177886963\n",
      "training: 2 batch 516 batch_loss: 0.0649823546409607\n",
      "training: 2 batch 517 batch_loss: 0.06199204921722412\n",
      "training: 2 batch 518 batch_loss: 0.06277191638946533\n",
      "training: 2 batch 519 batch_loss: 0.06462085247039795\n",
      "training: 2 batch 520 batch_loss: 0.06269633769989014\n",
      "training: 2 batch 521 batch_loss: 0.06285789608955383\n",
      "training: 2 batch 522 batch_loss: 0.06568726897239685\n",
      "training: 2 batch 523 batch_loss: 0.063862144947052\n",
      "training: 2 batch 524 batch_loss: 0.06431198120117188\n",
      "training: 2 batch 525 batch_loss: 0.06307831406593323\n",
      "training: 2 batch 526 batch_loss: 0.061227381229400635\n",
      "training: 2 batch 527 batch_loss: 0.06314414739608765\n",
      "training: 2 batch 528 batch_loss: 0.06489840149879456\n",
      "training: 2 batch 529 batch_loss: 0.06392818689346313\n",
      "training: 2 batch 530 batch_loss: 0.06526383757591248\n",
      "training: 2 batch 531 batch_loss: 0.06358286738395691\n",
      "training: 2 batch 532 batch_loss: 0.06454327702522278\n",
      "training: 2 batch 533 batch_loss: 0.06260019540786743\n",
      "training: 2 batch 534 batch_loss: 0.06227710843086243\n",
      "training: 2 batch 535 batch_loss: 0.062159568071365356\n",
      "training: 2 batch 536 batch_loss: 0.06364086270332336\n",
      "training: 2 batch 537 batch_loss: 0.06329262256622314\n",
      "training: 2 batch 538 batch_loss: 0.06416785717010498\n",
      "training: 2 batch 539 batch_loss: 0.06555956602096558\n",
      "training: 2 batch 540 batch_loss: 0.06372484564781189\n",
      "training: 2 batch 541 batch_loss: 0.06586617231369019\n",
      "training: 2 batch 542 batch_loss: 0.06349688768386841\n",
      "training: 2 batch 543 batch_loss: 0.06397908926010132\n",
      "training: 2 batch 544 batch_loss: 0.0638340413570404\n",
      "training: 2 batch 545 batch_loss: 0.06366673111915588\n",
      "training: 2 batch 546 batch_loss: 0.06621775031089783\n",
      "training: 2 batch 547 batch_loss: 0.06421181559562683\n",
      "training: 2 batch 548 batch_loss: 0.06419229507446289\n",
      "training: 2 batch 549 batch_loss: 0.0643085241317749\n",
      "training: 2 batch 550 batch_loss: 0.06610530614852905\n",
      "training: 2 batch 551 batch_loss: 0.06172835826873779\n",
      "training: 2 batch 552 batch_loss: 0.06538140773773193\n",
      "training: 2 batch 553 batch_loss: 0.06543803215026855\n",
      "training: 2 batch 554 batch_loss: 0.06411579251289368\n",
      "training: 2 batch 555 batch_loss: 0.061626940965652466\n",
      "training: 2 batch 556 batch_loss: 0.06531259417533875\n",
      "training: 2 batch 557 batch_loss: 0.06434175372123718\n",
      "training: 2 batch 558 batch_loss: 0.06506907939910889\n",
      "training: 2 batch 559 batch_loss: 0.06347763538360596\n",
      "training: 2 batch 560 batch_loss: 0.06378644704818726\n",
      "training: 2 batch 561 batch_loss: 0.06673038005828857\n",
      "training: 2 batch 562 batch_loss: 0.06219637393951416\n",
      "training: 2 batch 563 batch_loss: 0.06580433249473572\n",
      "training: 2 batch 564 batch_loss: 0.06700018048286438\n",
      "training: 2 batch 565 batch_loss: 0.06444147229194641\n",
      "training: 2 batch 566 batch_loss: 0.06276527047157288\n",
      "training: 2 batch 567 batch_loss: 0.06604140996932983\n",
      "training: 2 batch 568 batch_loss: 0.06251028180122375\n",
      "training: 2 batch 569 batch_loss: 0.06330779194831848\n",
      "training: 2 batch 570 batch_loss: 0.06401363015174866\n",
      "training: 2 batch 571 batch_loss: 0.06412526965141296\n",
      "training: 2 batch 572 batch_loss: 0.06813442707061768\n",
      "training: 2 batch 573 batch_loss: 0.06285983324050903\n",
      "training: 2 batch 574 batch_loss: 0.0655466616153717\n",
      "training: 2 batch 575 batch_loss: 0.06566306948661804\n",
      "training: 2 batch 576 batch_loss: 0.06336206197738647\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 2, Hit Ratio:0.01942083926007602 | Precision:0.028654280939742455 | Recall:0.04004768246090024 | NDCG:0.035189717112375715\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 3 batch 0 batch_loss: 0.06339392066001892\n",
      "training: 3 batch 1 batch_loss: 0.0636795163154602\n",
      "training: 3 batch 2 batch_loss: 0.0630146861076355\n",
      "training: 3 batch 3 batch_loss: 0.062089890241622925\n",
      "training: 3 batch 4 batch_loss: 0.06493967771530151\n",
      "training: 3 batch 5 batch_loss: 0.061950236558914185\n",
      "training: 3 batch 6 batch_loss: 0.0627281665802002\n",
      "training: 3 batch 7 batch_loss: 0.06486815214157104\n",
      "training: 3 batch 8 batch_loss: 0.06119602918624878\n",
      "training: 3 batch 9 batch_loss: 0.06522560119628906\n",
      "training: 3 batch 10 batch_loss: 0.06651318073272705\n",
      "training: 3 batch 11 batch_loss: 0.0657062828540802\n",
      "training: 3 batch 12 batch_loss: 0.0642220675945282\n",
      "training: 3 batch 13 batch_loss: 0.0661756694316864\n",
      "training: 3 batch 14 batch_loss: 0.0659136176109314\n",
      "training: 3 batch 15 batch_loss: 0.06543397903442383\n",
      "training: 3 batch 16 batch_loss: 0.0680311918258667\n",
      "training: 3 batch 17 batch_loss: 0.06708109378814697\n",
      "training: 3 batch 18 batch_loss: 0.06502881646156311\n",
      "training: 3 batch 19 batch_loss: 0.0659295916557312\n",
      "training: 3 batch 20 batch_loss: 0.06450515985488892\n",
      "training: 3 batch 21 batch_loss: 0.06546896696090698\n",
      "training: 3 batch 22 batch_loss: 0.0632396936416626\n",
      "training: 3 batch 23 batch_loss: 0.06442251801490784\n",
      "training: 3 batch 24 batch_loss: 0.06576067209243774\n",
      "training: 3 batch 25 batch_loss: 0.06660455465316772\n",
      "training: 3 batch 26 batch_loss: 0.0667189359664917\n",
      "training: 3 batch 27 batch_loss: 0.06540524959564209\n",
      "training: 3 batch 28 batch_loss: 0.06501919031143188\n",
      "training: 3 batch 29 batch_loss: 0.06602451205253601\n",
      "training: 3 batch 30 batch_loss: 0.06588906049728394\n",
      "training: 3 batch 31 batch_loss: 0.0667232871055603\n",
      "training: 3 batch 32 batch_loss: 0.06520867347717285\n",
      "training: 3 batch 33 batch_loss: 0.06729313731193542\n",
      "training: 3 batch 34 batch_loss: 0.06555530428886414\n",
      "training: 3 batch 35 batch_loss: 0.06664964556694031\n",
      "training: 3 batch 36 batch_loss: 0.06377291679382324\n",
      "training: 3 batch 37 batch_loss: 0.06508234143257141\n",
      "training: 3 batch 38 batch_loss: 0.06554299592971802\n",
      "training: 3 batch 39 batch_loss: 0.06385838985443115\n",
      "training: 3 batch 40 batch_loss: 0.06544899940490723\n",
      "training: 3 batch 41 batch_loss: 0.06819713115692139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 3 batch 42 batch_loss: 0.06953692436218262\n",
      "training: 3 batch 43 batch_loss: 0.0653676986694336\n",
      "training: 3 batch 44 batch_loss: 0.0657852292060852\n",
      "training: 3 batch 45 batch_loss: 0.0646631121635437\n",
      "training: 3 batch 46 batch_loss: 0.06906217336654663\n",
      "training: 3 batch 47 batch_loss: 0.06612426042556763\n",
      "training: 3 batch 48 batch_loss: 0.06789752840995789\n",
      "training: 3 batch 49 batch_loss: 0.06588304042816162\n",
      "training: 3 batch 50 batch_loss: 0.06559985876083374\n",
      "training: 3 batch 51 batch_loss: 0.06752225756645203\n",
      "training: 3 batch 52 batch_loss: 0.06463056802749634\n",
      "training: 3 batch 53 batch_loss: 0.06443247199058533\n",
      "training: 3 batch 54 batch_loss: 0.0666024386882782\n",
      "training: 3 batch 55 batch_loss: 0.06615680456161499\n",
      "training: 3 batch 56 batch_loss: 0.06745103001594543\n",
      "training: 3 batch 57 batch_loss: 0.06585940718650818\n",
      "training: 3 batch 58 batch_loss: 0.06567758321762085\n",
      "training: 3 batch 59 batch_loss: 0.0673803985118866\n",
      "training: 3 batch 60 batch_loss: 0.06354761123657227\n",
      "training: 3 batch 61 batch_loss: 0.06327134370803833\n",
      "training: 3 batch 62 batch_loss: 0.06653338670730591\n",
      "training: 3 batch 63 batch_loss: 0.06623739004135132\n",
      "training: 3 batch 64 batch_loss: 0.0650908350944519\n",
      "training: 3 batch 65 batch_loss: 0.06575796008110046\n",
      "training: 3 batch 66 batch_loss: 0.06624707579612732\n",
      "training: 3 batch 67 batch_loss: 0.06583774089813232\n",
      "training: 3 batch 68 batch_loss: 0.0660509467124939\n",
      "training: 3 batch 69 batch_loss: 0.06704464554786682\n",
      "training: 3 batch 70 batch_loss: 0.06766313314437866\n",
      "training: 3 batch 71 batch_loss: 0.0675087571144104\n",
      "training: 3 batch 72 batch_loss: 0.06991854310035706\n",
      "training: 3 batch 73 batch_loss: 0.06480753421783447\n",
      "training: 3 batch 74 batch_loss: 0.06591853499412537\n",
      "training: 3 batch 75 batch_loss: 0.06751203536987305\n",
      "training: 3 batch 76 batch_loss: 0.06659206748008728\n",
      "training: 3 batch 77 batch_loss: 0.06666263937950134\n",
      "training: 3 batch 78 batch_loss: 0.06579440832138062\n",
      "training: 3 batch 79 batch_loss: 0.06319516897201538\n",
      "training: 3 batch 80 batch_loss: 0.06466597318649292\n",
      "training: 3 batch 81 batch_loss: 0.06588727235794067\n",
      "training: 3 batch 82 batch_loss: 0.06801769137382507\n",
      "training: 3 batch 83 batch_loss: 0.06589657068252563\n",
      "training: 3 batch 84 batch_loss: 0.06719538569450378\n",
      "training: 3 batch 85 batch_loss: 0.06882420182228088\n",
      "training: 3 batch 86 batch_loss: 0.06534814834594727\n",
      "training: 3 batch 87 batch_loss: 0.0683383047580719\n",
      "training: 3 batch 88 batch_loss: 0.06782302260398865\n",
      "training: 3 batch 89 batch_loss: 0.06761389970779419\n",
      "training: 3 batch 90 batch_loss: 0.06651204824447632\n",
      "training: 3 batch 91 batch_loss: 0.06703820824623108\n",
      "training: 3 batch 92 batch_loss: 0.06518754363059998\n",
      "training: 3 batch 93 batch_loss: 0.0679410994052887\n",
      "training: 3 batch 94 batch_loss: 0.06643381714820862\n",
      "training: 3 batch 95 batch_loss: 0.06867700815200806\n",
      "training: 3 batch 96 batch_loss: 0.06617313623428345\n",
      "training: 3 batch 97 batch_loss: 0.0679192841053009\n",
      "training: 3 batch 98 batch_loss: 0.06986069679260254\n",
      "training: 3 batch 99 batch_loss: 0.06450924277305603\n",
      "training: 3 batch 100 batch_loss: 0.06523489952087402\n",
      "training: 3 batch 101 batch_loss: 0.06478086113929749\n",
      "training: 3 batch 102 batch_loss: 0.06789141893386841\n",
      "training: 3 batch 103 batch_loss: 0.06867438554763794\n",
      "training: 3 batch 104 batch_loss: 0.06754767894744873\n",
      "training: 3 batch 105 batch_loss: 0.0669550895690918\n",
      "training: 3 batch 106 batch_loss: 0.06760361790657043\n",
      "training: 3 batch 107 batch_loss: 0.06676673889160156\n",
      "training: 3 batch 108 batch_loss: 0.06364521384239197\n",
      "training: 3 batch 109 batch_loss: 0.06787928938865662\n",
      "training: 3 batch 110 batch_loss: 0.06853550672531128\n",
      "training: 3 batch 111 batch_loss: 0.06686845421791077\n",
      "training: 3 batch 112 batch_loss: 0.06806966662406921\n",
      "training: 3 batch 113 batch_loss: 0.0666460394859314\n",
      "training: 3 batch 114 batch_loss: 0.06866976618766785\n",
      "training: 3 batch 115 batch_loss: 0.06553113460540771\n",
      "training: 3 batch 116 batch_loss: 0.06774500012397766\n",
      "training: 3 batch 117 batch_loss: 0.06854739785194397\n",
      "training: 3 batch 118 batch_loss: 0.06650400161743164\n",
      "training: 3 batch 119 batch_loss: 0.067018061876297\n",
      "training: 3 batch 120 batch_loss: 0.06705769896507263\n",
      "training: 3 batch 121 batch_loss: 0.06760817766189575\n",
      "training: 3 batch 122 batch_loss: 0.0673539936542511\n",
      "training: 3 batch 123 batch_loss: 0.06883341073989868\n",
      "training: 3 batch 124 batch_loss: 0.07008510828018188\n",
      "training: 3 batch 125 batch_loss: 0.06679481267929077\n",
      "training: 3 batch 126 batch_loss: 0.06752878427505493\n",
      "training: 3 batch 127 batch_loss: 0.06639981269836426\n",
      "training: 3 batch 128 batch_loss: 0.06802904605865479\n",
      "training: 3 batch 129 batch_loss: 0.06805932521820068\n",
      "training: 3 batch 130 batch_loss: 0.06864455342292786\n",
      "training: 3 batch 131 batch_loss: 0.06808343529701233\n",
      "training: 3 batch 132 batch_loss: 0.0686497688293457\n",
      "training: 3 batch 133 batch_loss: 0.07014733552932739\n",
      "training: 3 batch 134 batch_loss: 0.06827014684677124\n",
      "training: 3 batch 135 batch_loss: 0.0661931037902832\n",
      "training: 3 batch 136 batch_loss: 0.0658414363861084\n",
      "training: 3 batch 137 batch_loss: 0.06660476326942444\n",
      "training: 3 batch 138 batch_loss: 0.06928986310958862\n",
      "training: 3 batch 139 batch_loss: 0.06729719042778015\n",
      "training: 3 batch 140 batch_loss: 0.06863617897033691\n",
      "training: 3 batch 141 batch_loss: 0.07076448202133179\n",
      "training: 3 batch 142 batch_loss: 0.06805333495140076\n",
      "training: 3 batch 143 batch_loss: 0.06718605756759644\n",
      "training: 3 batch 144 batch_loss: 0.06895798444747925\n",
      "training: 3 batch 145 batch_loss: 0.06808260083198547\n",
      "training: 3 batch 146 batch_loss: 0.0677981972694397\n",
      "training: 3 batch 147 batch_loss: 0.06867307424545288\n",
      "training: 3 batch 148 batch_loss: 0.06845563650131226\n",
      "training: 3 batch 149 batch_loss: 0.07002967596054077\n",
      "training: 3 batch 150 batch_loss: 0.06904551386833191\n",
      "training: 3 batch 151 batch_loss: 0.07094469666481018\n",
      "training: 3 batch 152 batch_loss: 0.06855162978172302\n",
      "training: 3 batch 153 batch_loss: 0.06801927089691162\n",
      "training: 3 batch 154 batch_loss: 0.06973746418952942\n",
      "training: 3 batch 155 batch_loss: 0.06920242309570312\n",
      "training: 3 batch 156 batch_loss: 0.06917411088943481\n",
      "training: 3 batch 157 batch_loss: 0.0699521005153656\n",
      "training: 3 batch 158 batch_loss: 0.06786000728607178\n",
      "training: 3 batch 159 batch_loss: 0.06933164596557617\n",
      "training: 3 batch 160 batch_loss: 0.06437668204307556\n",
      "training: 3 batch 161 batch_loss: 0.06740576028823853\n",
      "training: 3 batch 162 batch_loss: 0.06821215152740479\n",
      "training: 3 batch 163 batch_loss: 0.07086026668548584\n",
      "training: 3 batch 164 batch_loss: 0.0693807601928711\n",
      "training: 3 batch 165 batch_loss: 0.07039862871170044\n",
      "training: 3 batch 166 batch_loss: 0.06875461339950562\n",
      "training: 3 batch 167 batch_loss: 0.06916201114654541\n",
      "training: 3 batch 168 batch_loss: 0.0673513412475586\n",
      "training: 3 batch 169 batch_loss: 0.06974583864212036\n",
      "training: 3 batch 170 batch_loss: 0.06595131754875183\n",
      "training: 3 batch 171 batch_loss: 0.06674495339393616\n",
      "training: 3 batch 172 batch_loss: 0.06924885511398315\n",
      "training: 3 batch 173 batch_loss: 0.06822144985198975\n",
      "training: 3 batch 174 batch_loss: 0.06908267736434937\n",
      "training: 3 batch 175 batch_loss: 0.06872737407684326\n",
      "training: 3 batch 176 batch_loss: 0.06856086850166321\n",
      "training: 3 batch 177 batch_loss: 0.06799307465553284\n",
      "training: 3 batch 178 batch_loss: 0.066974937915802\n",
      "training: 3 batch 179 batch_loss: 0.06878858804702759\n",
      "training: 3 batch 180 batch_loss: 0.06921136379241943\n",
      "training: 3 batch 181 batch_loss: 0.06766611337661743\n",
      "training: 3 batch 182 batch_loss: 0.06959518790245056\n",
      "training: 3 batch 183 batch_loss: 0.06958374381065369\n",
      "training: 3 batch 184 batch_loss: 0.06831872463226318\n",
      "training: 3 batch 185 batch_loss: 0.07198607921600342\n",
      "training: 3 batch 186 batch_loss: 0.06709551811218262\n",
      "training: 3 batch 187 batch_loss: 0.06780779361724854\n",
      "training: 3 batch 188 batch_loss: 0.06891769170761108\n",
      "training: 3 batch 189 batch_loss: 0.06714725494384766\n",
      "training: 3 batch 190 batch_loss: 0.06969881057739258\n",
      "training: 3 batch 191 batch_loss: 0.06929823756217957\n",
      "training: 3 batch 192 batch_loss: 0.07010436058044434\n",
      "training: 3 batch 193 batch_loss: 0.06754109263420105\n",
      "training: 3 batch 194 batch_loss: 0.06868496537208557\n",
      "training: 3 batch 195 batch_loss: 0.06792736053466797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 3 batch 196 batch_loss: 0.07060348987579346\n",
      "training: 3 batch 197 batch_loss: 0.07018890976905823\n",
      "training: 3 batch 198 batch_loss: 0.06706303358078003\n",
      "training: 3 batch 199 batch_loss: 0.0663403868675232\n",
      "training: 3 batch 200 batch_loss: 0.07004451751708984\n",
      "training: 3 batch 201 batch_loss: 0.06984373927116394\n",
      "training: 3 batch 202 batch_loss: 0.06674948334693909\n",
      "training: 3 batch 203 batch_loss: 0.06874501705169678\n",
      "training: 3 batch 204 batch_loss: 0.06886297464370728\n",
      "training: 3 batch 205 batch_loss: 0.07025808095932007\n",
      "training: 3 batch 206 batch_loss: 0.06939703226089478\n",
      "training: 3 batch 207 batch_loss: 0.07042312622070312\n",
      "training: 3 batch 208 batch_loss: 0.07111001014709473\n",
      "training: 3 batch 209 batch_loss: 0.06887328624725342\n",
      "training: 3 batch 210 batch_loss: 0.06747210025787354\n",
      "training: 3 batch 211 batch_loss: 0.06708326935768127\n",
      "training: 3 batch 212 batch_loss: 0.06899744272232056\n",
      "training: 3 batch 213 batch_loss: 0.07054328918457031\n",
      "training: 3 batch 214 batch_loss: 0.06664913892745972\n",
      "training: 3 batch 215 batch_loss: 0.06934720277786255\n",
      "training: 3 batch 216 batch_loss: 0.06939840316772461\n",
      "training: 3 batch 217 batch_loss: 0.06612029671669006\n",
      "training: 3 batch 218 batch_loss: 0.07091084122657776\n",
      "training: 3 batch 219 batch_loss: 0.0675278902053833\n",
      "training: 3 batch 220 batch_loss: 0.07058817148208618\n",
      "training: 3 batch 221 batch_loss: 0.06706872582435608\n",
      "training: 3 batch 222 batch_loss: 0.06785669922828674\n",
      "training: 3 batch 223 batch_loss: 0.07202109694480896\n",
      "training: 3 batch 224 batch_loss: 0.0687132179737091\n",
      "training: 3 batch 225 batch_loss: 0.06865847110748291\n",
      "training: 3 batch 226 batch_loss: 0.06955289840698242\n",
      "training: 3 batch 227 batch_loss: 0.06975990533828735\n",
      "training: 3 batch 228 batch_loss: 0.06897038221359253\n",
      "training: 3 batch 229 batch_loss: 0.0692288875579834\n",
      "training: 3 batch 230 batch_loss: 0.07124832272529602\n",
      "training: 3 batch 231 batch_loss: 0.06901678442955017\n",
      "training: 3 batch 232 batch_loss: 0.06897634267807007\n",
      "training: 3 batch 233 batch_loss: 0.07120642066001892\n",
      "training: 3 batch 234 batch_loss: 0.07020565867424011\n",
      "training: 3 batch 235 batch_loss: 0.06790059804916382\n",
      "training: 3 batch 236 batch_loss: 0.06974154710769653\n",
      "training: 3 batch 237 batch_loss: 0.07058164477348328\n",
      "training: 3 batch 238 batch_loss: 0.06889018416404724\n",
      "training: 3 batch 239 batch_loss: 0.06961420178413391\n",
      "training: 3 batch 240 batch_loss: 0.06819292902946472\n",
      "training: 3 batch 241 batch_loss: 0.0692969262599945\n",
      "training: 3 batch 242 batch_loss: 0.06907308101654053\n",
      "training: 3 batch 243 batch_loss: 0.06998834013938904\n",
      "training: 3 batch 244 batch_loss: 0.07097858190536499\n",
      "training: 3 batch 245 batch_loss: 0.06931954622268677\n",
      "training: 3 batch 246 batch_loss: 0.067271888256073\n",
      "training: 3 batch 247 batch_loss: 0.06846177577972412\n",
      "training: 3 batch 248 batch_loss: 0.07127326726913452\n",
      "training: 3 batch 249 batch_loss: 0.06983625888824463\n",
      "training: 3 batch 250 batch_loss: 0.07394886016845703\n",
      "training: 3 batch 251 batch_loss: 0.07067957520484924\n",
      "training: 3 batch 252 batch_loss: 0.06941711902618408\n",
      "training: 3 batch 253 batch_loss: 0.06958204507827759\n",
      "training: 3 batch 254 batch_loss: 0.07075321674346924\n",
      "training: 3 batch 255 batch_loss: 0.06968435645103455\n",
      "training: 3 batch 256 batch_loss: 0.0676756203174591\n",
      "training: 3 batch 257 batch_loss: 0.07237857580184937\n",
      "training: 3 batch 258 batch_loss: 0.07094043493270874\n",
      "training: 3 batch 259 batch_loss: 0.06877008080482483\n",
      "training: 3 batch 260 batch_loss: 0.07311144471168518\n",
      "training: 3 batch 261 batch_loss: 0.06983673572540283\n",
      "training: 3 batch 262 batch_loss: 0.06939893960952759\n",
      "training: 3 batch 263 batch_loss: 0.07117897272109985\n",
      "training: 3 batch 264 batch_loss: 0.06851121783256531\n",
      "training: 3 batch 265 batch_loss: 0.06804075837135315\n",
      "training: 3 batch 266 batch_loss: 0.07055413722991943\n",
      "training: 3 batch 267 batch_loss: 0.06953170895576477\n",
      "training: 3 batch 268 batch_loss: 0.07209932804107666\n",
      "training: 3 batch 269 batch_loss: 0.07186609506607056\n",
      "training: 3 batch 270 batch_loss: 0.07064962387084961\n",
      "training: 3 batch 271 batch_loss: 0.06956404447555542\n",
      "training: 3 batch 272 batch_loss: 0.06981280446052551\n",
      "training: 3 batch 273 batch_loss: 0.06998562812805176\n",
      "training: 3 batch 274 batch_loss: 0.07001709938049316\n",
      "training: 3 batch 275 batch_loss: 0.06890636682510376\n",
      "training: 3 batch 276 batch_loss: 0.07399585843086243\n",
      "training: 3 batch 277 batch_loss: 0.07343032956123352\n",
      "training: 3 batch 278 batch_loss: 0.06895172595977783\n",
      "training: 3 batch 279 batch_loss: 0.06994330883026123\n",
      "training: 3 batch 280 batch_loss: 0.0711512565612793\n",
      "training: 3 batch 281 batch_loss: 0.06742602586746216\n",
      "training: 3 batch 282 batch_loss: 0.07236143946647644\n",
      "training: 3 batch 283 batch_loss: 0.07143956422805786\n",
      "training: 3 batch 284 batch_loss: 0.07071489095687866\n",
      "training: 3 batch 285 batch_loss: 0.06803348660469055\n",
      "training: 3 batch 286 batch_loss: 0.0716770589351654\n",
      "training: 3 batch 287 batch_loss: 0.0710354745388031\n",
      "training: 3 batch 288 batch_loss: 0.07117262482643127\n",
      "training: 3 batch 289 batch_loss: 0.07195210456848145\n",
      "training: 3 batch 290 batch_loss: 0.06889563798904419\n",
      "training: 3 batch 291 batch_loss: 0.06989192962646484\n",
      "training: 3 batch 292 batch_loss: 0.06915566325187683\n",
      "training: 3 batch 293 batch_loss: 0.07184046506881714\n",
      "training: 3 batch 294 batch_loss: 0.0727609395980835\n",
      "training: 3 batch 295 batch_loss: 0.07042115926742554\n",
      "training: 3 batch 296 batch_loss: 0.07033586502075195\n",
      "training: 3 batch 297 batch_loss: 0.07139933109283447\n",
      "training: 3 batch 298 batch_loss: 0.06830111145973206\n",
      "training: 3 batch 299 batch_loss: 0.06880062818527222\n",
      "training: 3 batch 300 batch_loss: 0.07128000259399414\n",
      "training: 3 batch 301 batch_loss: 0.07152852416038513\n",
      "training: 3 batch 302 batch_loss: 0.06996554136276245\n",
      "training: 3 batch 303 batch_loss: 0.06838029623031616\n",
      "training: 3 batch 304 batch_loss: 0.07175296545028687\n",
      "training: 3 batch 305 batch_loss: 0.07125702500343323\n",
      "training: 3 batch 306 batch_loss: 0.07332411408424377\n",
      "training: 3 batch 307 batch_loss: 0.07160267233848572\n",
      "training: 3 batch 308 batch_loss: 0.07121589779853821\n",
      "training: 3 batch 309 batch_loss: 0.0712507963180542\n",
      "training: 3 batch 310 batch_loss: 0.07337701320648193\n",
      "training: 3 batch 311 batch_loss: 0.07081657648086548\n",
      "training: 3 batch 312 batch_loss: 0.07265675067901611\n",
      "training: 3 batch 313 batch_loss: 0.07190501689910889\n",
      "training: 3 batch 314 batch_loss: 0.06924748420715332\n",
      "training: 3 batch 315 batch_loss: 0.07014492154121399\n",
      "training: 3 batch 316 batch_loss: 0.07253783941268921\n",
      "training: 3 batch 317 batch_loss: 0.07281321287155151\n",
      "training: 3 batch 318 batch_loss: 0.0735233724117279\n",
      "training: 3 batch 319 batch_loss: 0.07068872451782227\n",
      "training: 3 batch 320 batch_loss: 0.07252910733222961\n",
      "training: 3 batch 321 batch_loss: 0.071361243724823\n",
      "training: 3 batch 322 batch_loss: 0.07371169328689575\n",
      "training: 3 batch 323 batch_loss: 0.07267588376998901\n",
      "training: 3 batch 324 batch_loss: 0.07573485374450684\n",
      "training: 3 batch 325 batch_loss: 0.07248550653457642\n",
      "training: 3 batch 326 batch_loss: 0.07304680347442627\n",
      "training: 3 batch 327 batch_loss: 0.07306662201881409\n",
      "training: 3 batch 328 batch_loss: 0.07242199778556824\n",
      "training: 3 batch 329 batch_loss: 0.07035773992538452\n",
      "training: 3 batch 330 batch_loss: 0.07228603959083557\n",
      "training: 3 batch 331 batch_loss: 0.0724119246006012\n",
      "training: 3 batch 332 batch_loss: 0.07322174310684204\n",
      "training: 3 batch 333 batch_loss: 0.07048052549362183\n",
      "training: 3 batch 334 batch_loss: 0.07216805219650269\n",
      "training: 3 batch 335 batch_loss: 0.06999149918556213\n",
      "training: 3 batch 336 batch_loss: 0.07379436492919922\n",
      "training: 3 batch 337 batch_loss: 0.07400554418563843\n",
      "training: 3 batch 338 batch_loss: 0.07345867156982422\n",
      "training: 3 batch 339 batch_loss: 0.07422956824302673\n",
      "training: 3 batch 340 batch_loss: 0.07512155175209045\n",
      "training: 3 batch 341 batch_loss: 0.07167211174964905\n",
      "training: 3 batch 342 batch_loss: 0.07297539710998535\n",
      "training: 3 batch 343 batch_loss: 0.07233473658561707\n",
      "training: 3 batch 344 batch_loss: 0.07385778427124023\n",
      "training: 3 batch 345 batch_loss: 0.0726645290851593\n",
      "training: 3 batch 346 batch_loss: 0.07311856746673584\n",
      "training: 3 batch 347 batch_loss: 0.07128554582595825\n",
      "training: 3 batch 348 batch_loss: 0.07412725687026978\n",
      "training: 3 batch 349 batch_loss: 0.07295840978622437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 3 batch 350 batch_loss: 0.07238343358039856\n",
      "training: 3 batch 351 batch_loss: 0.07047837972640991\n",
      "training: 3 batch 352 batch_loss: 0.0749271810054779\n",
      "training: 3 batch 353 batch_loss: 0.0742914080619812\n",
      "training: 3 batch 354 batch_loss: 0.07281345129013062\n",
      "training: 3 batch 355 batch_loss: 0.07480505108833313\n",
      "training: 3 batch 356 batch_loss: 0.07183700799942017\n",
      "training: 3 batch 357 batch_loss: 0.07231283187866211\n",
      "training: 3 batch 358 batch_loss: 0.07233390212059021\n",
      "training: 3 batch 359 batch_loss: 0.07242131233215332\n",
      "training: 3 batch 360 batch_loss: 0.07491609454154968\n",
      "training: 3 batch 361 batch_loss: 0.07271984219551086\n",
      "training: 3 batch 362 batch_loss: 0.07216617465019226\n",
      "training: 3 batch 363 batch_loss: 0.07257577776908875\n",
      "training: 3 batch 364 batch_loss: 0.07199937105178833\n",
      "training: 3 batch 365 batch_loss: 0.07191604375839233\n",
      "training: 3 batch 366 batch_loss: 0.07401400804519653\n",
      "training: 3 batch 367 batch_loss: 0.07311254739761353\n",
      "training: 3 batch 368 batch_loss: 0.07555526494979858\n",
      "training: 3 batch 369 batch_loss: 0.07460695505142212\n",
      "training: 3 batch 370 batch_loss: 0.07444804906845093\n",
      "training: 3 batch 371 batch_loss: 0.07399609684944153\n",
      "training: 3 batch 372 batch_loss: 0.07341963052749634\n",
      "training: 3 batch 373 batch_loss: 0.07351452112197876\n",
      "training: 3 batch 374 batch_loss: 0.07281658053398132\n",
      "training: 3 batch 375 batch_loss: 0.07510495185852051\n",
      "training: 3 batch 376 batch_loss: 0.07369858026504517\n",
      "training: 3 batch 377 batch_loss: 0.07515576481819153\n",
      "training: 3 batch 378 batch_loss: 0.07737386226654053\n",
      "training: 3 batch 379 batch_loss: 0.07679873704910278\n",
      "training: 3 batch 380 batch_loss: 0.07362833619117737\n",
      "training: 3 batch 381 batch_loss: 0.07497212290763855\n",
      "training: 3 batch 382 batch_loss: 0.07710516452789307\n",
      "training: 3 batch 383 batch_loss: 0.07663965225219727\n",
      "training: 3 batch 384 batch_loss: 0.07425975799560547\n",
      "training: 3 batch 385 batch_loss: 0.07367831468582153\n",
      "training: 3 batch 386 batch_loss: 0.07597014307975769\n",
      "training: 3 batch 387 batch_loss: 0.07302820682525635\n",
      "training: 3 batch 388 batch_loss: 0.07346680760383606\n",
      "training: 3 batch 389 batch_loss: 0.07873257994651794\n",
      "training: 3 batch 390 batch_loss: 0.07431375980377197\n",
      "training: 3 batch 391 batch_loss: 0.07395589351654053\n",
      "training: 3 batch 392 batch_loss: 0.07607251405715942\n",
      "training: 3 batch 393 batch_loss: 0.07343387603759766\n",
      "training: 3 batch 394 batch_loss: 0.075238436460495\n",
      "training: 3 batch 395 batch_loss: 0.07605257630348206\n",
      "training: 3 batch 396 batch_loss: 0.0757887065410614\n",
      "training: 3 batch 397 batch_loss: 0.07297182083129883\n",
      "training: 3 batch 398 batch_loss: 0.07519093155860901\n",
      "training: 3 batch 399 batch_loss: 0.07669702172279358\n",
      "training: 3 batch 400 batch_loss: 0.0732940137386322\n",
      "training: 3 batch 401 batch_loss: 0.07774803042411804\n",
      "training: 3 batch 402 batch_loss: 0.07511043548583984\n",
      "training: 3 batch 403 batch_loss: 0.07464218139648438\n",
      "training: 3 batch 404 batch_loss: 0.0749063491821289\n",
      "training: 3 batch 405 batch_loss: 0.07521340250968933\n",
      "training: 3 batch 406 batch_loss: 0.07422381639480591\n",
      "training: 3 batch 407 batch_loss: 0.07391500473022461\n",
      "training: 3 batch 408 batch_loss: 0.07736533880233765\n",
      "training: 3 batch 409 batch_loss: 0.07723897695541382\n",
      "training: 3 batch 410 batch_loss: 0.07276275753974915\n",
      "training: 3 batch 411 batch_loss: 0.07498425245285034\n",
      "training: 3 batch 412 batch_loss: 0.0775727927684784\n",
      "training: 3 batch 413 batch_loss: 0.07607638835906982\n",
      "training: 3 batch 414 batch_loss: 0.07809203863143921\n",
      "training: 3 batch 415 batch_loss: 0.07312962412834167\n",
      "training: 3 batch 416 batch_loss: 0.07478708028793335\n",
      "training: 3 batch 417 batch_loss: 0.07482743263244629\n",
      "training: 3 batch 418 batch_loss: 0.0743401050567627\n",
      "training: 3 batch 419 batch_loss: 0.0740218460559845\n",
      "training: 3 batch 420 batch_loss: 0.07917985320091248\n",
      "training: 3 batch 421 batch_loss: 0.08082124590873718\n",
      "training: 3 batch 422 batch_loss: 0.07560479640960693\n",
      "training: 3 batch 423 batch_loss: 0.07619303464889526\n",
      "training: 3 batch 424 batch_loss: 0.07458871603012085\n",
      "training: 3 batch 425 batch_loss: 0.07475113868713379\n",
      "training: 3 batch 426 batch_loss: 0.0781216025352478\n",
      "training: 3 batch 427 batch_loss: 0.07903975248336792\n",
      "training: 3 batch 428 batch_loss: 0.0791168212890625\n",
      "training: 3 batch 429 batch_loss: 0.07725712656974792\n",
      "training: 3 batch 430 batch_loss: 0.07583478093147278\n",
      "training: 3 batch 431 batch_loss: 0.0756104588508606\n",
      "training: 3 batch 432 batch_loss: 0.07902073860168457\n",
      "training: 3 batch 433 batch_loss: 0.07743480801582336\n",
      "training: 3 batch 434 batch_loss: 0.07759639620780945\n",
      "training: 3 batch 435 batch_loss: 0.07965493202209473\n",
      "training: 3 batch 436 batch_loss: 0.07561272382736206\n",
      "training: 3 batch 437 batch_loss: 0.07731294631958008\n",
      "training: 3 batch 438 batch_loss: 0.0767621397972107\n",
      "training: 3 batch 439 batch_loss: 0.07730859518051147\n",
      "training: 3 batch 440 batch_loss: 0.07775998115539551\n",
      "training: 3 batch 441 batch_loss: 0.07580044865608215\n",
      "training: 3 batch 442 batch_loss: 0.0768120288848877\n",
      "training: 3 batch 443 batch_loss: 0.07886868715286255\n",
      "training: 3 batch 444 batch_loss: 0.08000856637954712\n",
      "training: 3 batch 445 batch_loss: 0.07836514711380005\n",
      "training: 3 batch 446 batch_loss: 0.077839195728302\n",
      "training: 3 batch 447 batch_loss: 0.07825437188148499\n",
      "training: 3 batch 448 batch_loss: 0.07796728610992432\n",
      "training: 3 batch 449 batch_loss: 0.07673108577728271\n",
      "training: 3 batch 450 batch_loss: 0.07875275611877441\n",
      "training: 3 batch 451 batch_loss: 0.07979586720466614\n",
      "training: 3 batch 452 batch_loss: 0.07715365290641785\n",
      "training: 3 batch 453 batch_loss: 0.07628089189529419\n",
      "training: 3 batch 454 batch_loss: 0.07745048403739929\n",
      "training: 3 batch 455 batch_loss: 0.08173710107803345\n",
      "training: 3 batch 456 batch_loss: 0.07802700996398926\n",
      "training: 3 batch 457 batch_loss: 0.07679712772369385\n",
      "training: 3 batch 458 batch_loss: 0.080078125\n",
      "training: 3 batch 459 batch_loss: 0.08018842339515686\n",
      "training: 3 batch 460 batch_loss: 0.07842251658439636\n",
      "training: 3 batch 461 batch_loss: 0.07756084203720093\n",
      "training: 3 batch 462 batch_loss: 0.08083456754684448\n",
      "training: 3 batch 463 batch_loss: 0.07904824614524841\n",
      "training: 3 batch 464 batch_loss: 0.07852071523666382\n",
      "training: 3 batch 465 batch_loss: 0.07950717210769653\n",
      "training: 3 batch 466 batch_loss: 0.0789402425289154\n",
      "training: 3 batch 467 batch_loss: 0.08076849579811096\n",
      "training: 3 batch 468 batch_loss: 0.07850953936576843\n",
      "training: 3 batch 469 batch_loss: 0.08046945929527283\n",
      "training: 3 batch 470 batch_loss: 0.08228558301925659\n",
      "training: 3 batch 471 batch_loss: 0.07985979318618774\n",
      "training: 3 batch 472 batch_loss: 0.07945901155471802\n",
      "training: 3 batch 473 batch_loss: 0.07868161797523499\n",
      "training: 3 batch 474 batch_loss: 0.07696878910064697\n",
      "training: 3 batch 475 batch_loss: 0.0759534239768982\n",
      "training: 3 batch 476 batch_loss: 0.07994464039802551\n",
      "training: 3 batch 477 batch_loss: 0.07713273167610168\n",
      "training: 3 batch 478 batch_loss: 0.08021986484527588\n",
      "training: 3 batch 479 batch_loss: 0.07926926016807556\n",
      "training: 3 batch 480 batch_loss: 0.07776114344596863\n",
      "training: 3 batch 481 batch_loss: 0.08062610030174255\n",
      "training: 3 batch 482 batch_loss: 0.07939291000366211\n",
      "training: 3 batch 483 batch_loss: 0.07748198509216309\n",
      "training: 3 batch 484 batch_loss: 0.08037364482879639\n",
      "training: 3 batch 485 batch_loss: 0.07695868611335754\n",
      "training: 3 batch 486 batch_loss: 0.08125755190849304\n",
      "training: 3 batch 487 batch_loss: 0.08220440149307251\n",
      "training: 3 batch 488 batch_loss: 0.08128881454467773\n",
      "training: 3 batch 489 batch_loss: 0.07990998029708862\n",
      "training: 3 batch 490 batch_loss: 0.08004772663116455\n",
      "training: 3 batch 491 batch_loss: 0.07928574085235596\n",
      "training: 3 batch 492 batch_loss: 0.07956746220588684\n",
      "training: 3 batch 493 batch_loss: 0.07866644859313965\n",
      "training: 3 batch 494 batch_loss: 0.07987603545188904\n",
      "training: 3 batch 495 batch_loss: 0.07979393005371094\n",
      "training: 3 batch 496 batch_loss: 0.08288827538490295\n",
      "training: 3 batch 497 batch_loss: 0.07889169454574585\n",
      "training: 3 batch 498 batch_loss: 0.08031472563743591\n",
      "training: 3 batch 499 batch_loss: 0.08044591546058655\n",
      "training: 3 batch 500 batch_loss: 0.07833355665206909\n",
      "training: 3 batch 501 batch_loss: 0.08103224635124207\n",
      "training: 3 batch 502 batch_loss: 0.07839423418045044\n",
      "training: 3 batch 503 batch_loss: 0.08078944683074951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 3 batch 504 batch_loss: 0.0808928906917572\n",
      "training: 3 batch 505 batch_loss: 0.07935211062431335\n",
      "training: 3 batch 506 batch_loss: 0.08205801248550415\n",
      "training: 3 batch 507 batch_loss: 0.08205682039260864\n",
      "training: 3 batch 508 batch_loss: 0.08072292804718018\n",
      "training: 3 batch 509 batch_loss: 0.08292171359062195\n",
      "training: 3 batch 510 batch_loss: 0.07999473810195923\n",
      "training: 3 batch 511 batch_loss: 0.08180642127990723\n",
      "training: 3 batch 512 batch_loss: 0.08176717162132263\n",
      "training: 3 batch 513 batch_loss: 0.0828750729560852\n",
      "training: 3 batch 514 batch_loss: 0.0845678448677063\n",
      "training: 3 batch 515 batch_loss: 0.08203133940696716\n",
      "training: 3 batch 516 batch_loss: 0.08318653702735901\n",
      "training: 3 batch 517 batch_loss: 0.08120575547218323\n",
      "training: 3 batch 518 batch_loss: 0.08501657843589783\n",
      "training: 3 batch 519 batch_loss: 0.08139598369598389\n",
      "training: 3 batch 520 batch_loss: 0.08224710822105408\n",
      "training: 3 batch 521 batch_loss: 0.07992637157440186\n",
      "training: 3 batch 522 batch_loss: 0.0828859806060791\n",
      "training: 3 batch 523 batch_loss: 0.08389735221862793\n",
      "training: 3 batch 524 batch_loss: 0.0784115195274353\n",
      "training: 3 batch 525 batch_loss: 0.08137887716293335\n",
      "training: 3 batch 526 batch_loss: 0.0811205506324768\n",
      "training: 3 batch 527 batch_loss: 0.07767871022224426\n",
      "training: 3 batch 528 batch_loss: 0.08298209309577942\n",
      "training: 3 batch 529 batch_loss: 0.08052590489387512\n",
      "training: 3 batch 530 batch_loss: 0.08319732546806335\n",
      "training: 3 batch 531 batch_loss: 0.08391481637954712\n",
      "training: 3 batch 532 batch_loss: 0.08544126152992249\n",
      "training: 3 batch 533 batch_loss: 0.08109995722770691\n",
      "training: 3 batch 534 batch_loss: 0.07913053035736084\n",
      "training: 3 batch 535 batch_loss: 0.08091488480567932\n",
      "training: 3 batch 536 batch_loss: 0.07890364527702332\n",
      "training: 3 batch 537 batch_loss: 0.08181712031364441\n",
      "training: 3 batch 538 batch_loss: 0.0788225531578064\n",
      "training: 3 batch 539 batch_loss: 0.08275935053825378\n",
      "training: 3 batch 540 batch_loss: 0.08097869157791138\n",
      "training: 3 batch 541 batch_loss: 0.08135205507278442\n",
      "training: 3 batch 542 batch_loss: 0.0826098620891571\n",
      "training: 3 batch 543 batch_loss: 0.08321356773376465\n",
      "training: 3 batch 544 batch_loss: 0.08589321374893188\n",
      "training: 3 batch 545 batch_loss: 0.08496803045272827\n",
      "training: 3 batch 546 batch_loss: 0.07920873165130615\n",
      "training: 3 batch 547 batch_loss: 0.08411240577697754\n",
      "training: 3 batch 548 batch_loss: 0.08125874400138855\n",
      "training: 3 batch 549 batch_loss: 0.08270660042762756\n",
      "training: 3 batch 550 batch_loss: 0.0813860297203064\n",
      "training: 3 batch 551 batch_loss: 0.0846245288848877\n",
      "training: 3 batch 552 batch_loss: 0.08277866244316101\n",
      "training: 3 batch 553 batch_loss: 0.08355575799942017\n",
      "training: 3 batch 554 batch_loss: 0.08199936151504517\n",
      "training: 3 batch 555 batch_loss: 0.08424711227416992\n",
      "training: 3 batch 556 batch_loss: 0.08265292644500732\n",
      "training: 3 batch 557 batch_loss: 0.08463132381439209\n",
      "training: 3 batch 558 batch_loss: 0.0818021297454834\n",
      "training: 3 batch 559 batch_loss: 0.0845409631729126\n",
      "training: 3 batch 560 batch_loss: 0.08224555850028992\n",
      "training: 3 batch 561 batch_loss: 0.08652442693710327\n",
      "training: 3 batch 562 batch_loss: 0.08406627178192139\n",
      "training: 3 batch 563 batch_loss: 0.08548054099082947\n",
      "training: 3 batch 564 batch_loss: 0.0820881724357605\n",
      "training: 3 batch 565 batch_loss: 0.08223789930343628\n",
      "training: 3 batch 566 batch_loss: 0.08209377527236938\n",
      "training: 3 batch 567 batch_loss: 0.08267122507095337\n",
      "training: 3 batch 568 batch_loss: 0.0846337378025055\n",
      "training: 3 batch 569 batch_loss: 0.08052709698677063\n",
      "training: 3 batch 570 batch_loss: 0.08399796485900879\n",
      "training: 3 batch 571 batch_loss: 0.08130061626434326\n",
      "training: 3 batch 572 batch_loss: 0.08479315042495728\n",
      "training: 3 batch 573 batch_loss: 0.08346965909004211\n",
      "training: 3 batch 574 batch_loss: 0.08496099710464478\n",
      "training: 3 batch 575 batch_loss: 0.08115848898887634\n",
      "training: 3 batch 576 batch_loss: 0.08702784776687622\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 3, Hit Ratio:0.020593418234269287 | Precision:0.03038435073233068 | Recall:0.04385914565342987 | NDCG:0.037492785253389865\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 4 batch 0 batch_loss: 0.08141493797302246\n",
      "training: 4 batch 1 batch_loss: 0.0838523805141449\n",
      "training: 4 batch 2 batch_loss: 0.08134636282920837\n",
      "training: 4 batch 3 batch_loss: 0.08225670456886292\n",
      "training: 4 batch 4 batch_loss: 0.08423781394958496\n",
      "training: 4 batch 5 batch_loss: 0.08125936985015869\n",
      "training: 4 batch 6 batch_loss: 0.0791783332824707\n",
      "training: 4 batch 7 batch_loss: 0.08087220788002014\n",
      "training: 4 batch 8 batch_loss: 0.08105823397636414\n",
      "training: 4 batch 9 batch_loss: 0.08324369788169861\n",
      "training: 4 batch 10 batch_loss: 0.08104437589645386\n",
      "training: 4 batch 11 batch_loss: 0.08379906415939331\n",
      "training: 4 batch 12 batch_loss: 0.08440512418746948\n",
      "training: 4 batch 13 batch_loss: 0.08540093898773193\n",
      "training: 4 batch 14 batch_loss: 0.08215567469596863\n",
      "training: 4 batch 15 batch_loss: 0.08315420150756836\n",
      "training: 4 batch 16 batch_loss: 0.08126777410507202\n",
      "training: 4 batch 17 batch_loss: 0.08324435353279114\n",
      "training: 4 batch 18 batch_loss: 0.08398449420928955\n",
      "training: 4 batch 19 batch_loss: 0.08396190404891968\n",
      "training: 4 batch 20 batch_loss: 0.085263192653656\n",
      "training: 4 batch 21 batch_loss: 0.08476990461349487\n",
      "training: 4 batch 22 batch_loss: 0.08428770303726196\n",
      "training: 4 batch 23 batch_loss: 0.08451342582702637\n",
      "training: 4 batch 24 batch_loss: 0.08283466100692749\n",
      "training: 4 batch 25 batch_loss: 0.0814443826675415\n",
      "training: 4 batch 26 batch_loss: 0.08282935619354248\n",
      "training: 4 batch 27 batch_loss: 0.0811612606048584\n",
      "training: 4 batch 28 batch_loss: 0.08189615607261658\n",
      "training: 4 batch 29 batch_loss: 0.08185189962387085\n",
      "training: 4 batch 30 batch_loss: 0.08364161849021912\n",
      "training: 4 batch 31 batch_loss: 0.0871572494506836\n",
      "training: 4 batch 32 batch_loss: 0.0845121443271637\n",
      "training: 4 batch 33 batch_loss: 0.08233189582824707\n",
      "training: 4 batch 34 batch_loss: 0.08052361011505127\n",
      "training: 4 batch 35 batch_loss: 0.08476132154464722\n",
      "training: 4 batch 36 batch_loss: 0.08292350172996521\n",
      "training: 4 batch 37 batch_loss: 0.08486852049827576\n",
      "training: 4 batch 38 batch_loss: 0.08506044745445251\n",
      "training: 4 batch 39 batch_loss: 0.08935791254043579\n",
      "training: 4 batch 40 batch_loss: 0.08299031853675842\n",
      "training: 4 batch 41 batch_loss: 0.08363887667655945\n",
      "training: 4 batch 42 batch_loss: 0.08670967817306519\n",
      "training: 4 batch 43 batch_loss: 0.08331787586212158\n",
      "training: 4 batch 44 batch_loss: 0.08454000949859619\n",
      "training: 4 batch 45 batch_loss: 0.0853625237941742\n",
      "training: 4 batch 46 batch_loss: 0.0878337025642395\n",
      "training: 4 batch 47 batch_loss: 0.0828661322593689\n",
      "training: 4 batch 48 batch_loss: 0.08269056677818298\n",
      "training: 4 batch 49 batch_loss: 0.08501842617988586\n",
      "training: 4 batch 50 batch_loss: 0.08459755778312683\n",
      "training: 4 batch 51 batch_loss: 0.08813723921775818\n",
      "training: 4 batch 52 batch_loss: 0.08592873811721802\n",
      "training: 4 batch 53 batch_loss: 0.08400365710258484\n",
      "training: 4 batch 54 batch_loss: 0.08100587129592896\n",
      "training: 4 batch 55 batch_loss: 0.08521369099617004\n",
      "training: 4 batch 56 batch_loss: 0.08208781480789185\n",
      "training: 4 batch 57 batch_loss: 0.08467021584510803\n",
      "training: 4 batch 58 batch_loss: 0.0866500735282898\n",
      "training: 4 batch 59 batch_loss: 0.08455461263656616\n",
      "training: 4 batch 60 batch_loss: 0.0829942524433136\n",
      "training: 4 batch 61 batch_loss: 0.08057728409767151\n",
      "training: 4 batch 62 batch_loss: 0.08434617519378662\n",
      "training: 4 batch 63 batch_loss: 0.08501774072647095\n",
      "training: 4 batch 64 batch_loss: 0.08454325795173645\n",
      "training: 4 batch 65 batch_loss: 0.08509957790374756\n",
      "training: 4 batch 66 batch_loss: 0.08459687232971191\n",
      "training: 4 batch 67 batch_loss: 0.08498319983482361\n",
      "training: 4 batch 68 batch_loss: 0.08133375644683838\n",
      "training: 4 batch 69 batch_loss: 0.08523944020271301\n",
      "training: 4 batch 70 batch_loss: 0.08105841279029846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 4 batch 71 batch_loss: 0.08257004618644714\n",
      "training: 4 batch 72 batch_loss: 0.08114466071128845\n",
      "training: 4 batch 73 batch_loss: 0.08570989966392517\n",
      "training: 4 batch 74 batch_loss: 0.08372640609741211\n",
      "training: 4 batch 75 batch_loss: 0.08671525120735168\n",
      "training: 4 batch 76 batch_loss: 0.08542981743812561\n",
      "training: 4 batch 77 batch_loss: 0.0865660309791565\n",
      "training: 4 batch 78 batch_loss: 0.08680364489555359\n",
      "training: 4 batch 79 batch_loss: 0.08659526705741882\n",
      "training: 4 batch 80 batch_loss: 0.08410695195198059\n",
      "training: 4 batch 81 batch_loss: 0.08724448084831238\n",
      "training: 4 batch 82 batch_loss: 0.08430922031402588\n",
      "training: 4 batch 83 batch_loss: 0.08536872267723083\n",
      "training: 4 batch 84 batch_loss: 0.08577939867973328\n",
      "training: 4 batch 85 batch_loss: 0.08286920189857483\n",
      "training: 4 batch 86 batch_loss: 0.08431437611579895\n",
      "training: 4 batch 87 batch_loss: 0.08281078934669495\n",
      "training: 4 batch 88 batch_loss: 0.08414095640182495\n",
      "training: 4 batch 89 batch_loss: 0.08473089337348938\n",
      "training: 4 batch 90 batch_loss: 0.08355635404586792\n",
      "training: 4 batch 91 batch_loss: 0.08520635962486267\n",
      "training: 4 batch 92 batch_loss: 0.08575636148452759\n",
      "training: 4 batch 93 batch_loss: 0.08387243747711182\n",
      "training: 4 batch 94 batch_loss: 0.08436787128448486\n",
      "training: 4 batch 95 batch_loss: 0.08411607146263123\n",
      "training: 4 batch 96 batch_loss: 0.08484315872192383\n",
      "training: 4 batch 97 batch_loss: 0.08460956811904907\n",
      "training: 4 batch 98 batch_loss: 0.08544731140136719\n",
      "training: 4 batch 99 batch_loss: 0.08567339181900024\n",
      "training: 4 batch 100 batch_loss: 0.08571910858154297\n",
      "training: 4 batch 101 batch_loss: 0.08528414368629456\n",
      "training: 4 batch 102 batch_loss: 0.0825299620628357\n",
      "training: 4 batch 103 batch_loss: 0.08702057600021362\n",
      "training: 4 batch 104 batch_loss: 0.0832967460155487\n",
      "training: 4 batch 105 batch_loss: 0.08403077721595764\n",
      "training: 4 batch 106 batch_loss: 0.08550310134887695\n",
      "training: 4 batch 107 batch_loss: 0.08646267652511597\n",
      "training: 4 batch 108 batch_loss: 0.08349394798278809\n",
      "training: 4 batch 109 batch_loss: 0.0871228277683258\n",
      "training: 4 batch 110 batch_loss: 0.08450406789779663\n",
      "training: 4 batch 111 batch_loss: 0.0845465362071991\n",
      "training: 4 batch 112 batch_loss: 0.0840926468372345\n",
      "training: 4 batch 113 batch_loss: 0.08551627397537231\n",
      "training: 4 batch 114 batch_loss: 0.08639377355575562\n",
      "training: 4 batch 115 batch_loss: 0.08397573232650757\n",
      "training: 4 batch 116 batch_loss: 0.08478802442550659\n",
      "training: 4 batch 117 batch_loss: 0.08701464533805847\n",
      "training: 4 batch 118 batch_loss: 0.08668676018714905\n",
      "training: 4 batch 119 batch_loss: 0.08705773949623108\n",
      "training: 4 batch 120 batch_loss: 0.08754944801330566\n",
      "training: 4 batch 121 batch_loss: 0.08571693301200867\n",
      "training: 4 batch 122 batch_loss: 0.0841742753982544\n",
      "training: 4 batch 123 batch_loss: 0.08358806371688843\n",
      "training: 4 batch 124 batch_loss: 0.08463817834854126\n",
      "training: 4 batch 125 batch_loss: 0.08831089735031128\n",
      "training: 4 batch 126 batch_loss: 0.08519980311393738\n",
      "training: 4 batch 127 batch_loss: 0.08456894755363464\n",
      "training: 4 batch 128 batch_loss: 0.08554711937904358\n",
      "training: 4 batch 129 batch_loss: 0.0855521559715271\n",
      "training: 4 batch 130 batch_loss: 0.0857301652431488\n",
      "training: 4 batch 131 batch_loss: 0.08494871854782104\n",
      "training: 4 batch 132 batch_loss: 0.08454856276512146\n",
      "training: 4 batch 133 batch_loss: 0.08443507552146912\n",
      "training: 4 batch 134 batch_loss: 0.08476519584655762\n",
      "training: 4 batch 135 batch_loss: 0.08580923080444336\n",
      "training: 4 batch 136 batch_loss: 0.08348917961120605\n",
      "training: 4 batch 137 batch_loss: 0.08356419205665588\n",
      "training: 4 batch 138 batch_loss: 0.08500605821609497\n",
      "training: 4 batch 139 batch_loss: 0.0857183039188385\n",
      "training: 4 batch 140 batch_loss: 0.08604961633682251\n",
      "training: 4 batch 141 batch_loss: 0.08667171001434326\n",
      "training: 4 batch 142 batch_loss: 0.0830230712890625\n",
      "training: 4 batch 143 batch_loss: 0.08355680108070374\n",
      "training: 4 batch 144 batch_loss: 0.08384460210800171\n",
      "training: 4 batch 145 batch_loss: 0.08551055192947388\n",
      "training: 4 batch 146 batch_loss: 0.08841833472251892\n",
      "training: 4 batch 147 batch_loss: 0.08911427855491638\n",
      "training: 4 batch 148 batch_loss: 0.08630871772766113\n",
      "training: 4 batch 149 batch_loss: 0.08676734566688538\n",
      "training: 4 batch 150 batch_loss: 0.08490064740180969\n",
      "training: 4 batch 151 batch_loss: 0.08597013354301453\n",
      "training: 4 batch 152 batch_loss: 0.08436918258666992\n",
      "training: 4 batch 153 batch_loss: 0.08327829837799072\n",
      "training: 4 batch 154 batch_loss: 0.08756780624389648\n",
      "training: 4 batch 155 batch_loss: 0.08482655882835388\n",
      "training: 4 batch 156 batch_loss: 0.0855606198310852\n",
      "training: 4 batch 157 batch_loss: 0.08526796102523804\n",
      "training: 4 batch 158 batch_loss: 0.08853799104690552\n",
      "training: 4 batch 159 batch_loss: 0.08813947439193726\n",
      "training: 4 batch 160 batch_loss: 0.08878600597381592\n",
      "training: 4 batch 161 batch_loss: 0.08403900265693665\n",
      "training: 4 batch 162 batch_loss: 0.08603259921073914\n",
      "training: 4 batch 163 batch_loss: 0.08703303337097168\n",
      "training: 4 batch 164 batch_loss: 0.08697846531867981\n",
      "training: 4 batch 165 batch_loss: 0.08692270517349243\n",
      "training: 4 batch 166 batch_loss: 0.08781129121780396\n",
      "training: 4 batch 167 batch_loss: 0.08521640300750732\n",
      "training: 4 batch 168 batch_loss: 0.08562487363815308\n",
      "training: 4 batch 169 batch_loss: 0.08798795938491821\n",
      "training: 4 batch 170 batch_loss: 0.08285784721374512\n",
      "training: 4 batch 171 batch_loss: 0.08682012557983398\n",
      "training: 4 batch 172 batch_loss: 0.08593866229057312\n",
      "training: 4 batch 173 batch_loss: 0.08458030223846436\n",
      "training: 4 batch 174 batch_loss: 0.08186203241348267\n",
      "training: 4 batch 175 batch_loss: 0.08578979969024658\n",
      "training: 4 batch 176 batch_loss: 0.08755865693092346\n",
      "training: 4 batch 177 batch_loss: 0.0859934389591217\n",
      "training: 4 batch 178 batch_loss: 0.08746606111526489\n",
      "training: 4 batch 179 batch_loss: 0.08582335710525513\n",
      "training: 4 batch 180 batch_loss: 0.08552148938179016\n",
      "training: 4 batch 181 batch_loss: 0.08467373251914978\n",
      "training: 4 batch 182 batch_loss: 0.0861959159374237\n",
      "training: 4 batch 183 batch_loss: 0.0881032943725586\n",
      "training: 4 batch 184 batch_loss: 0.08521810173988342\n",
      "training: 4 batch 185 batch_loss: 0.08513998985290527\n",
      "training: 4 batch 186 batch_loss: 0.08635786175727844\n",
      "training: 4 batch 187 batch_loss: 0.08722853660583496\n",
      "training: 4 batch 188 batch_loss: 0.08681294322013855\n",
      "training: 4 batch 189 batch_loss: 0.08768725395202637\n",
      "training: 4 batch 190 batch_loss: 0.08766692876815796\n",
      "training: 4 batch 191 batch_loss: 0.08894741535186768\n",
      "training: 4 batch 192 batch_loss: 0.08600738644599915\n",
      "training: 4 batch 193 batch_loss: 0.08494487404823303\n",
      "training: 4 batch 194 batch_loss: 0.08565512299537659\n",
      "training: 4 batch 195 batch_loss: 0.08683022856712341\n",
      "training: 4 batch 196 batch_loss: 0.08437389135360718\n",
      "training: 4 batch 197 batch_loss: 0.0870884358882904\n",
      "training: 4 batch 198 batch_loss: 0.0873575210571289\n",
      "training: 4 batch 199 batch_loss: 0.08511781692504883\n",
      "training: 4 batch 200 batch_loss: 0.09107637405395508\n",
      "training: 4 batch 201 batch_loss: 0.08754467964172363\n",
      "training: 4 batch 202 batch_loss: 0.08623206615447998\n",
      "training: 4 batch 203 batch_loss: 0.0851532518863678\n",
      "training: 4 batch 204 batch_loss: 0.08574852347373962\n",
      "training: 4 batch 205 batch_loss: 0.08500108122825623\n",
      "training: 4 batch 206 batch_loss: 0.08669862151145935\n",
      "training: 4 batch 207 batch_loss: 0.0883762538433075\n",
      "training: 4 batch 208 batch_loss: 0.08365753293037415\n",
      "training: 4 batch 209 batch_loss: 0.08789336681365967\n",
      "training: 4 batch 210 batch_loss: 0.08668822050094604\n",
      "training: 4 batch 211 batch_loss: 0.08656451106071472\n",
      "training: 4 batch 212 batch_loss: 0.0853508710861206\n",
      "training: 4 batch 213 batch_loss: 0.08628794550895691\n",
      "training: 4 batch 214 batch_loss: 0.08575600385665894\n",
      "training: 4 batch 215 batch_loss: 0.08767908811569214\n",
      "training: 4 batch 216 batch_loss: 0.08832240104675293\n",
      "training: 4 batch 217 batch_loss: 0.08594545722007751\n",
      "training: 4 batch 218 batch_loss: 0.08467930555343628\n",
      "training: 4 batch 219 batch_loss: 0.08909446001052856\n",
      "training: 4 batch 220 batch_loss: 0.08539500832557678\n",
      "training: 4 batch 221 batch_loss: 0.08525288105010986\n",
      "training: 4 batch 222 batch_loss: 0.08953437209129333\n",
      "training: 4 batch 223 batch_loss: 0.0873519778251648\n",
      "training: 4 batch 224 batch_loss: 0.08532357215881348\n",
      "training: 4 batch 225 batch_loss: 0.08749896287918091\n",
      "training: 4 batch 226 batch_loss: 0.08275651931762695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 4 batch 227 batch_loss: 0.08559238910675049\n",
      "training: 4 batch 228 batch_loss: 0.08962225914001465\n",
      "training: 4 batch 229 batch_loss: 0.0891021192073822\n",
      "training: 4 batch 230 batch_loss: 0.08655145764350891\n",
      "training: 4 batch 231 batch_loss: 0.08653557300567627\n",
      "training: 4 batch 232 batch_loss: 0.08763912320137024\n",
      "training: 4 batch 233 batch_loss: 0.08683016896247864\n",
      "training: 4 batch 234 batch_loss: 0.08795934915542603\n",
      "training: 4 batch 235 batch_loss: 0.08398434519767761\n",
      "training: 4 batch 236 batch_loss: 0.08649605512619019\n",
      "training: 4 batch 237 batch_loss: 0.08699339628219604\n",
      "training: 4 batch 238 batch_loss: 0.08687484264373779\n",
      "training: 4 batch 239 batch_loss: 0.08878439664840698\n",
      "training: 4 batch 240 batch_loss: 0.08796104788780212\n",
      "training: 4 batch 241 batch_loss: 0.0852205753326416\n",
      "training: 4 batch 242 batch_loss: 0.08995163440704346\n",
      "training: 4 batch 243 batch_loss: 0.08653241395950317\n",
      "training: 4 batch 244 batch_loss: 0.08713454008102417\n",
      "training: 4 batch 245 batch_loss: 0.08452323079109192\n",
      "training: 4 batch 246 batch_loss: 0.08611816167831421\n",
      "training: 4 batch 247 batch_loss: 0.08801808953285217\n",
      "training: 4 batch 248 batch_loss: 0.08982142806053162\n",
      "training: 4 batch 249 batch_loss: 0.0879596471786499\n",
      "training: 4 batch 250 batch_loss: 0.08998990058898926\n",
      "training: 4 batch 251 batch_loss: 0.08976051211357117\n",
      "training: 4 batch 252 batch_loss: 0.08702462911605835\n",
      "training: 4 batch 253 batch_loss: 0.09058347344398499\n",
      "training: 4 batch 254 batch_loss: 0.08439922332763672\n",
      "training: 4 batch 255 batch_loss: 0.08940112590789795\n",
      "training: 4 batch 256 batch_loss: 0.08698168396949768\n",
      "training: 4 batch 257 batch_loss: 0.0881592333316803\n",
      "training: 4 batch 258 batch_loss: 0.08737808465957642\n",
      "training: 4 batch 259 batch_loss: 0.08730903267860413\n",
      "training: 4 batch 260 batch_loss: 0.08732277154922485\n",
      "training: 4 batch 261 batch_loss: 0.08845126628875732\n",
      "training: 4 batch 262 batch_loss: 0.08700096607208252\n",
      "training: 4 batch 263 batch_loss: 0.08731365203857422\n",
      "training: 4 batch 264 batch_loss: 0.08582630753517151\n",
      "training: 4 batch 265 batch_loss: 0.08761587738990784\n",
      "training: 4 batch 266 batch_loss: 0.08708244562149048\n",
      "training: 4 batch 267 batch_loss: 0.08926635980606079\n",
      "training: 4 batch 268 batch_loss: 0.08497306704521179\n",
      "training: 4 batch 269 batch_loss: 0.08530169725418091\n",
      "training: 4 batch 270 batch_loss: 0.08963194489479065\n",
      "training: 4 batch 271 batch_loss: 0.0878857672214508\n",
      "training: 4 batch 272 batch_loss: 0.08806130290031433\n",
      "training: 4 batch 273 batch_loss: 0.09194323420524597\n",
      "training: 4 batch 274 batch_loss: 0.09098365902900696\n",
      "training: 4 batch 275 batch_loss: 0.08819562196731567\n",
      "training: 4 batch 276 batch_loss: 0.0877622663974762\n",
      "training: 4 batch 277 batch_loss: 0.0868905782699585\n",
      "training: 4 batch 278 batch_loss: 0.08592915534973145\n",
      "training: 4 batch 279 batch_loss: 0.08677339553833008\n",
      "training: 4 batch 280 batch_loss: 0.08898270130157471\n",
      "training: 4 batch 281 batch_loss: 0.08550775051116943\n",
      "training: 4 batch 282 batch_loss: 0.08725607395172119\n",
      "training: 4 batch 283 batch_loss: 0.08901369571685791\n",
      "training: 4 batch 284 batch_loss: 0.08682698011398315\n",
      "training: 4 batch 285 batch_loss: 0.08711758255958557\n",
      "training: 4 batch 286 batch_loss: 0.08857372403144836\n",
      "training: 4 batch 287 batch_loss: 0.08946144580841064\n",
      "training: 4 batch 288 batch_loss: 0.08914506435394287\n",
      "training: 4 batch 289 batch_loss: 0.08825567364692688\n",
      "training: 4 batch 290 batch_loss: 0.08750095963478088\n",
      "training: 4 batch 291 batch_loss: 0.08606764674186707\n",
      "training: 4 batch 292 batch_loss: 0.08677655458450317\n",
      "training: 4 batch 293 batch_loss: 0.08530962467193604\n",
      "training: 4 batch 294 batch_loss: 0.08688798546791077\n",
      "training: 4 batch 295 batch_loss: 0.08581116795539856\n",
      "training: 4 batch 296 batch_loss: 0.0890432596206665\n",
      "training: 4 batch 297 batch_loss: 0.09006392955780029\n",
      "training: 4 batch 298 batch_loss: 0.08472129702568054\n",
      "training: 4 batch 299 batch_loss: 0.09013155102729797\n",
      "training: 4 batch 300 batch_loss: 0.08887815475463867\n",
      "training: 4 batch 301 batch_loss: 0.08635410666465759\n",
      "training: 4 batch 302 batch_loss: 0.0873364806175232\n",
      "training: 4 batch 303 batch_loss: 0.08550697565078735\n",
      "training: 4 batch 304 batch_loss: 0.08675634860992432\n",
      "training: 4 batch 305 batch_loss: 0.09187644720077515\n",
      "training: 4 batch 306 batch_loss: 0.08784577250480652\n",
      "training: 4 batch 307 batch_loss: 0.08885937929153442\n",
      "training: 4 batch 308 batch_loss: 0.08608001470565796\n",
      "training: 4 batch 309 batch_loss: 0.08798283338546753\n",
      "training: 4 batch 310 batch_loss: 0.09011662006378174\n",
      "training: 4 batch 311 batch_loss: 0.0883464515209198\n",
      "training: 4 batch 312 batch_loss: 0.09293526411056519\n",
      "training: 4 batch 313 batch_loss: 0.08900430798530579\n",
      "training: 4 batch 314 batch_loss: 0.09050571918487549\n",
      "training: 4 batch 315 batch_loss: 0.08892989158630371\n",
      "training: 4 batch 316 batch_loss: 0.08754387497901917\n",
      "training: 4 batch 317 batch_loss: 0.09168186783790588\n",
      "training: 4 batch 318 batch_loss: 0.08983024954795837\n",
      "training: 4 batch 319 batch_loss: 0.08917278051376343\n",
      "training: 4 batch 320 batch_loss: 0.08735829591751099\n",
      "training: 4 batch 321 batch_loss: 0.08953839540481567\n",
      "training: 4 batch 322 batch_loss: 0.09003511071205139\n",
      "training: 4 batch 323 batch_loss: 0.08804228901863098\n",
      "training: 4 batch 324 batch_loss: 0.08892202377319336\n",
      "training: 4 batch 325 batch_loss: 0.08893454074859619\n",
      "training: 4 batch 326 batch_loss: 0.08692163228988647\n",
      "training: 4 batch 327 batch_loss: 0.08741796016693115\n",
      "training: 4 batch 328 batch_loss: 0.08845552802085876\n",
      "training: 4 batch 329 batch_loss: 0.08875051140785217\n",
      "training: 4 batch 330 batch_loss: 0.08862659335136414\n",
      "training: 4 batch 331 batch_loss: 0.0898086428642273\n",
      "training: 4 batch 332 batch_loss: 0.08532911539077759\n",
      "training: 4 batch 333 batch_loss: 0.09032511711120605\n",
      "training: 4 batch 334 batch_loss: 0.08880013227462769\n",
      "training: 4 batch 335 batch_loss: 0.0861348807811737\n",
      "training: 4 batch 336 batch_loss: 0.08973556756973267\n",
      "training: 4 batch 337 batch_loss: 0.08831676840782166\n",
      "training: 4 batch 338 batch_loss: 0.08646366000175476\n",
      "training: 4 batch 339 batch_loss: 0.08799386024475098\n",
      "training: 4 batch 340 batch_loss: 0.08834680914878845\n",
      "training: 4 batch 341 batch_loss: 0.08761483430862427\n",
      "training: 4 batch 342 batch_loss: 0.08757403492927551\n",
      "training: 4 batch 343 batch_loss: 0.08887678384780884\n",
      "training: 4 batch 344 batch_loss: 0.08647048473358154\n",
      "training: 4 batch 345 batch_loss: 0.09452164173126221\n",
      "training: 4 batch 346 batch_loss: 0.0878477692604065\n",
      "training: 4 batch 347 batch_loss: 0.0893515944480896\n",
      "training: 4 batch 348 batch_loss: 0.08606541156768799\n",
      "training: 4 batch 349 batch_loss: 0.08940565586090088\n",
      "training: 4 batch 350 batch_loss: 0.09037351608276367\n",
      "training: 4 batch 351 batch_loss: 0.0850839614868164\n",
      "training: 4 batch 352 batch_loss: 0.09127205610275269\n",
      "training: 4 batch 353 batch_loss: 0.08867877721786499\n",
      "training: 4 batch 354 batch_loss: 0.08748742938041687\n",
      "training: 4 batch 355 batch_loss: 0.09177407622337341\n",
      "training: 4 batch 356 batch_loss: 0.08777007460594177\n",
      "training: 4 batch 357 batch_loss: 0.09264576435089111\n",
      "training: 4 batch 358 batch_loss: 0.08988878130912781\n",
      "training: 4 batch 359 batch_loss: 0.09046792984008789\n",
      "training: 4 batch 360 batch_loss: 0.08676862716674805\n",
      "training: 4 batch 361 batch_loss: 0.08876782655715942\n",
      "training: 4 batch 362 batch_loss: 0.08976778388023376\n",
      "training: 4 batch 363 batch_loss: 0.08990862965583801\n",
      "training: 4 batch 364 batch_loss: 0.0857326090335846\n",
      "training: 4 batch 365 batch_loss: 0.08888739347457886\n",
      "training: 4 batch 366 batch_loss: 0.09143540263175964\n",
      "training: 4 batch 367 batch_loss: 0.08775472640991211\n",
      "training: 4 batch 368 batch_loss: 0.09041357040405273\n",
      "training: 4 batch 369 batch_loss: 0.08870354294776917\n",
      "training: 4 batch 370 batch_loss: 0.09070044755935669\n",
      "training: 4 batch 371 batch_loss: 0.09103760123252869\n",
      "training: 4 batch 372 batch_loss: 0.09063145518302917\n",
      "training: 4 batch 373 batch_loss: 0.08793789148330688\n",
      "training: 4 batch 374 batch_loss: 0.08969226479530334\n",
      "training: 4 batch 375 batch_loss: 0.09093818068504333\n",
      "training: 4 batch 376 batch_loss: 0.09127074480056763\n",
      "training: 4 batch 377 batch_loss: 0.08706310391426086\n",
      "training: 4 batch 378 batch_loss: 0.09159570932388306\n",
      "training: 4 batch 379 batch_loss: 0.09055289626121521\n",
      "training: 4 batch 380 batch_loss: 0.08910804986953735\n",
      "training: 4 batch 381 batch_loss: 0.09120017290115356\n",
      "training: 4 batch 382 batch_loss: 0.09072816371917725\n",
      "training: 4 batch 383 batch_loss: 0.08759397268295288\n",
      "training: 4 batch 384 batch_loss: 0.08751672506332397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 4 batch 385 batch_loss: 0.08832085132598877\n",
      "training: 4 batch 386 batch_loss: 0.0900900661945343\n",
      "training: 4 batch 387 batch_loss: 0.09108081459999084\n",
      "training: 4 batch 388 batch_loss: 0.0898527204990387\n",
      "training: 4 batch 389 batch_loss: 0.08804875612258911\n",
      "training: 4 batch 390 batch_loss: 0.09104889631271362\n",
      "training: 4 batch 391 batch_loss: 0.09198194742202759\n",
      "training: 4 batch 392 batch_loss: 0.09284862875938416\n",
      "training: 4 batch 393 batch_loss: 0.09213542938232422\n",
      "training: 4 batch 394 batch_loss: 0.0896051824092865\n",
      "training: 4 batch 395 batch_loss: 0.08818715810775757\n",
      "training: 4 batch 396 batch_loss: 0.08894258737564087\n",
      "training: 4 batch 397 batch_loss: 0.08927807211875916\n",
      "training: 4 batch 398 batch_loss: 0.08868223428726196\n",
      "training: 4 batch 399 batch_loss: 0.09292122721672058\n",
      "training: 4 batch 400 batch_loss: 0.08976423740386963\n",
      "training: 4 batch 401 batch_loss: 0.09082889556884766\n",
      "training: 4 batch 402 batch_loss: 0.08719116449356079\n",
      "training: 4 batch 403 batch_loss: 0.08734747767448425\n",
      "training: 4 batch 404 batch_loss: 0.09094291925430298\n",
      "training: 4 batch 405 batch_loss: 0.09262746572494507\n",
      "training: 4 batch 406 batch_loss: 0.09036034345626831\n",
      "training: 4 batch 407 batch_loss: 0.08850893378257751\n",
      "training: 4 batch 408 batch_loss: 0.0890740156173706\n",
      "training: 4 batch 409 batch_loss: 0.08716690540313721\n",
      "training: 4 batch 410 batch_loss: 0.09216788411140442\n",
      "training: 4 batch 411 batch_loss: 0.0884491503238678\n",
      "training: 4 batch 412 batch_loss: 0.09013068675994873\n",
      "training: 4 batch 413 batch_loss: 0.09040907025337219\n",
      "training: 4 batch 414 batch_loss: 0.0890917181968689\n",
      "training: 4 batch 415 batch_loss: 0.09265369176864624\n",
      "training: 4 batch 416 batch_loss: 0.08976119756698608\n",
      "training: 4 batch 417 batch_loss: 0.08872020244598389\n",
      "training: 4 batch 418 batch_loss: 0.09107246994972229\n",
      "training: 4 batch 419 batch_loss: 0.08983227610588074\n",
      "training: 4 batch 420 batch_loss: 0.0922875702381134\n",
      "training: 4 batch 421 batch_loss: 0.08817580342292786\n",
      "training: 4 batch 422 batch_loss: 0.09243899583816528\n",
      "training: 4 batch 423 batch_loss: 0.09476667642593384\n",
      "training: 4 batch 424 batch_loss: 0.09023362398147583\n",
      "training: 4 batch 425 batch_loss: 0.0911332368850708\n",
      "training: 4 batch 426 batch_loss: 0.09072989225387573\n",
      "training: 4 batch 427 batch_loss: 0.08846661448478699\n",
      "training: 4 batch 428 batch_loss: 0.08823701739311218\n",
      "training: 4 batch 429 batch_loss: 0.09344568848609924\n",
      "training: 4 batch 430 batch_loss: 0.08941507339477539\n",
      "training: 4 batch 431 batch_loss: 0.09201154112815857\n",
      "training: 4 batch 432 batch_loss: 0.09103044867515564\n",
      "training: 4 batch 433 batch_loss: 0.09061798453330994\n",
      "training: 4 batch 434 batch_loss: 0.08936023712158203\n",
      "training: 4 batch 435 batch_loss: 0.09287989139556885\n",
      "training: 4 batch 436 batch_loss: 0.08656585216522217\n",
      "training: 4 batch 437 batch_loss: 0.08972722291946411\n",
      "training: 4 batch 438 batch_loss: 0.0935179591178894\n",
      "training: 4 batch 439 batch_loss: 0.09099388122558594\n",
      "training: 4 batch 440 batch_loss: 0.09076181054115295\n",
      "training: 4 batch 441 batch_loss: 0.08917468786239624\n",
      "training: 4 batch 442 batch_loss: 0.09187808632850647\n",
      "training: 4 batch 443 batch_loss: 0.09223306179046631\n",
      "training: 4 batch 444 batch_loss: 0.09122759103775024\n",
      "training: 4 batch 445 batch_loss: 0.08851680159568787\n",
      "training: 4 batch 446 batch_loss: 0.09019473195075989\n",
      "training: 4 batch 447 batch_loss: 0.09078764915466309\n",
      "training: 4 batch 448 batch_loss: 0.08827954530715942\n",
      "training: 4 batch 449 batch_loss: 0.08900067210197449\n",
      "training: 4 batch 450 batch_loss: 0.09238529205322266\n",
      "training: 4 batch 451 batch_loss: 0.08927300572395325\n",
      "training: 4 batch 452 batch_loss: 0.09316754341125488\n",
      "training: 4 batch 453 batch_loss: 0.09032878279685974\n",
      "training: 4 batch 454 batch_loss: 0.09323418140411377\n",
      "training: 4 batch 455 batch_loss: 0.08895611763000488\n",
      "training: 4 batch 456 batch_loss: 0.09258311986923218\n",
      "training: 4 batch 457 batch_loss: 0.09140345454216003\n",
      "training: 4 batch 458 batch_loss: 0.09063825011253357\n",
      "training: 4 batch 459 batch_loss: 0.08932989835739136\n",
      "training: 4 batch 460 batch_loss: 0.09119710326194763\n",
      "training: 4 batch 461 batch_loss: 0.09501492977142334\n",
      "training: 4 batch 462 batch_loss: 0.08895576000213623\n",
      "training: 4 batch 463 batch_loss: 0.09272235631942749\n",
      "training: 4 batch 464 batch_loss: 0.09245529770851135\n",
      "training: 4 batch 465 batch_loss: 0.09069561958312988\n",
      "training: 4 batch 466 batch_loss: 0.0922815203666687\n",
      "training: 4 batch 467 batch_loss: 0.09066763520240784\n",
      "training: 4 batch 468 batch_loss: 0.09248071908950806\n",
      "training: 4 batch 469 batch_loss: 0.09263047575950623\n",
      "training: 4 batch 470 batch_loss: 0.09220778942108154\n",
      "training: 4 batch 471 batch_loss: 0.09187933802604675\n",
      "training: 4 batch 472 batch_loss: 0.08995375037193298\n",
      "training: 4 batch 473 batch_loss: 0.09125566482543945\n",
      "training: 4 batch 474 batch_loss: 0.09127217531204224\n",
      "training: 4 batch 475 batch_loss: 0.09177207946777344\n",
      "training: 4 batch 476 batch_loss: 0.08887284994125366\n",
      "training: 4 batch 477 batch_loss: 0.08926135301589966\n",
      "training: 4 batch 478 batch_loss: 0.09165728092193604\n",
      "training: 4 batch 479 batch_loss: 0.09012061357498169\n",
      "training: 4 batch 480 batch_loss: 0.0905560851097107\n",
      "training: 4 batch 481 batch_loss: 0.09413087368011475\n",
      "training: 4 batch 482 batch_loss: 0.09246957302093506\n",
      "training: 4 batch 483 batch_loss: 0.08958485722541809\n",
      "training: 4 batch 484 batch_loss: 0.0930071473121643\n",
      "training: 4 batch 485 batch_loss: 0.09078264236450195\n",
      "training: 4 batch 486 batch_loss: 0.08971354365348816\n",
      "training: 4 batch 487 batch_loss: 0.09036117792129517\n",
      "training: 4 batch 488 batch_loss: 0.0910743772983551\n",
      "training: 4 batch 489 batch_loss: 0.09200263023376465\n",
      "training: 4 batch 490 batch_loss: 0.09055072069168091\n",
      "training: 4 batch 491 batch_loss: 0.08923202753067017\n",
      "training: 4 batch 492 batch_loss: 0.09218549728393555\n",
      "training: 4 batch 493 batch_loss: 0.09253674745559692\n",
      "training: 4 batch 494 batch_loss: 0.08999314904212952\n",
      "training: 4 batch 495 batch_loss: 0.09292399883270264\n",
      "training: 4 batch 496 batch_loss: 0.09173065423965454\n",
      "training: 4 batch 497 batch_loss: 0.09069442749023438\n",
      "training: 4 batch 498 batch_loss: 0.09163302183151245\n",
      "training: 4 batch 499 batch_loss: 0.09476873278617859\n",
      "training: 4 batch 500 batch_loss: 0.09287905693054199\n",
      "training: 4 batch 501 batch_loss: 0.09148037433624268\n",
      "training: 4 batch 502 batch_loss: 0.09314695000648499\n",
      "training: 4 batch 503 batch_loss: 0.0962037742137909\n",
      "training: 4 batch 504 batch_loss: 0.088507741689682\n",
      "training: 4 batch 505 batch_loss: 0.08879882097244263\n",
      "training: 4 batch 506 batch_loss: 0.09238767623901367\n",
      "training: 4 batch 507 batch_loss: 0.09334182739257812\n",
      "training: 4 batch 508 batch_loss: 0.0925135612487793\n",
      "training: 4 batch 509 batch_loss: 0.09321406483650208\n",
      "training: 4 batch 510 batch_loss: 0.09501960873603821\n",
      "training: 4 batch 511 batch_loss: 0.09121584892272949\n",
      "training: 4 batch 512 batch_loss: 0.09112560749053955\n",
      "training: 4 batch 513 batch_loss: 0.09106233716011047\n",
      "training: 4 batch 514 batch_loss: 0.0922861099243164\n",
      "training: 4 batch 515 batch_loss: 0.09351596236228943\n",
      "training: 4 batch 516 batch_loss: 0.09219145774841309\n",
      "training: 4 batch 517 batch_loss: 0.09495386481285095\n",
      "training: 4 batch 518 batch_loss: 0.09177848696708679\n",
      "training: 4 batch 519 batch_loss: 0.09375691413879395\n",
      "training: 4 batch 520 batch_loss: 0.09091377258300781\n",
      "training: 4 batch 521 batch_loss: 0.09287810325622559\n",
      "training: 4 batch 522 batch_loss: 0.09093523025512695\n",
      "training: 4 batch 523 batch_loss: 0.09280520677566528\n",
      "training: 4 batch 524 batch_loss: 0.09419035911560059\n",
      "training: 4 batch 525 batch_loss: 0.0901145339012146\n",
      "training: 4 batch 526 batch_loss: 0.0931350588798523\n",
      "training: 4 batch 527 batch_loss: 0.08957016468048096\n",
      "training: 4 batch 528 batch_loss: 0.09143441915512085\n",
      "training: 4 batch 529 batch_loss: 0.09571799635887146\n",
      "training: 4 batch 530 batch_loss: 0.09095814824104309\n",
      "training: 4 batch 531 batch_loss: 0.089957594871521\n",
      "training: 4 batch 532 batch_loss: 0.08884689211845398\n",
      "training: 4 batch 533 batch_loss: 0.09341657161712646\n",
      "training: 4 batch 534 batch_loss: 0.09269464015960693\n",
      "training: 4 batch 535 batch_loss: 0.09398907423019409\n",
      "training: 4 batch 536 batch_loss: 0.09471768140792847\n",
      "training: 4 batch 537 batch_loss: 0.09494388103485107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 4 batch 538 batch_loss: 0.09078952670097351\n",
      "training: 4 batch 539 batch_loss: 0.09159556031227112\n",
      "training: 4 batch 540 batch_loss: 0.09477075934410095\n",
      "training: 4 batch 541 batch_loss: 0.09605377912521362\n",
      "training: 4 batch 542 batch_loss: 0.09479233622550964\n",
      "training: 4 batch 543 batch_loss: 0.09502053260803223\n",
      "training: 4 batch 544 batch_loss: 0.09281408786773682\n",
      "training: 4 batch 545 batch_loss: 0.09258219599723816\n",
      "training: 4 batch 546 batch_loss: 0.0920906662940979\n",
      "training: 4 batch 547 batch_loss: 0.09374290704727173\n",
      "training: 4 batch 548 batch_loss: 0.09103018045425415\n",
      "training: 4 batch 549 batch_loss: 0.09051144123077393\n",
      "training: 4 batch 550 batch_loss: 0.09372574090957642\n",
      "training: 4 batch 551 batch_loss: 0.09435519576072693\n",
      "training: 4 batch 552 batch_loss: 0.09327420592308044\n",
      "training: 4 batch 553 batch_loss: 0.09558001160621643\n",
      "training: 4 batch 554 batch_loss: 0.09227460622787476\n",
      "training: 4 batch 555 batch_loss: 0.09372544288635254\n",
      "training: 4 batch 556 batch_loss: 0.09342402219772339\n",
      "training: 4 batch 557 batch_loss: 0.09432575106620789\n",
      "training: 4 batch 558 batch_loss: 0.09225159883499146\n",
      "training: 4 batch 559 batch_loss: 0.09102943539619446\n",
      "training: 4 batch 560 batch_loss: 0.09213343262672424\n",
      "training: 4 batch 561 batch_loss: 0.09140443801879883\n",
      "training: 4 batch 562 batch_loss: 0.09381702542304993\n",
      "training: 4 batch 563 batch_loss: 0.09346306324005127\n",
      "training: 4 batch 564 batch_loss: 0.09121429920196533\n",
      "training: 4 batch 565 batch_loss: 0.0936916172504425\n",
      "training: 4 batch 566 batch_loss: 0.09458661079406738\n",
      "training: 4 batch 567 batch_loss: 0.09265357255935669\n",
      "training: 4 batch 568 batch_loss: 0.09311971068382263\n",
      "training: 4 batch 569 batch_loss: 0.0947190523147583\n",
      "training: 4 batch 570 batch_loss: 0.09216529130935669\n",
      "training: 4 batch 571 batch_loss: 0.09445765614509583\n",
      "training: 4 batch 572 batch_loss: 0.09284979104995728\n",
      "training: 4 batch 573 batch_loss: 0.09084391593933105\n",
      "training: 4 batch 574 batch_loss: 0.09440553188323975\n",
      "training: 4 batch 575 batch_loss: 0.09080713987350464\n",
      "training: 4 batch 576 batch_loss: 0.09444615244865417\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 4, Hit Ratio:0.020710009893635094 | Precision:0.030556374717389166 | Recall:0.0446617251204095 | NDCG:0.03837801849094106\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 5 batch 0 batch_loss: 0.08951696753501892\n",
      "training: 5 batch 1 batch_loss: 0.09072265028953552\n",
      "training: 5 batch 2 batch_loss: 0.09098070859909058\n",
      "training: 5 batch 3 batch_loss: 0.09166789054870605\n",
      "training: 5 batch 4 batch_loss: 0.09360253810882568\n",
      "training: 5 batch 5 batch_loss: 0.0906030535697937\n",
      "training: 5 batch 6 batch_loss: 0.0916801393032074\n",
      "training: 5 batch 7 batch_loss: 0.09089532494544983\n",
      "training: 5 batch 8 batch_loss: 0.08985376358032227\n",
      "training: 5 batch 9 batch_loss: 0.09076729416847229\n",
      "training: 5 batch 10 batch_loss: 0.09253153204917908\n",
      "training: 5 batch 11 batch_loss: 0.08875170350074768\n",
      "training: 5 batch 12 batch_loss: 0.08879673480987549\n",
      "training: 5 batch 13 batch_loss: 0.09228518605232239\n",
      "training: 5 batch 14 batch_loss: 0.0916355550289154\n",
      "training: 5 batch 15 batch_loss: 0.09237149357795715\n",
      "training: 5 batch 16 batch_loss: 0.09027335047721863\n",
      "training: 5 batch 17 batch_loss: 0.09269678592681885\n",
      "training: 5 batch 18 batch_loss: 0.09167656302452087\n",
      "training: 5 batch 19 batch_loss: 0.09470599889755249\n",
      "training: 5 batch 20 batch_loss: 0.09175288677215576\n",
      "training: 5 batch 21 batch_loss: 0.0929935872554779\n",
      "training: 5 batch 22 batch_loss: 0.0919884443283081\n",
      "training: 5 batch 23 batch_loss: 0.09309181571006775\n",
      "training: 5 batch 24 batch_loss: 0.0931866466999054\n",
      "training: 5 batch 25 batch_loss: 0.09091296792030334\n",
      "training: 5 batch 26 batch_loss: 0.09336230158805847\n",
      "training: 5 batch 27 batch_loss: 0.09210699796676636\n",
      "training: 5 batch 28 batch_loss: 0.09113302826881409\n",
      "training: 5 batch 29 batch_loss: 0.09509408473968506\n",
      "training: 5 batch 30 batch_loss: 0.0910835862159729\n",
      "training: 5 batch 31 batch_loss: 0.09477317333221436\n",
      "training: 5 batch 32 batch_loss: 0.09346085786819458\n",
      "training: 5 batch 33 batch_loss: 0.09159749746322632\n",
      "training: 5 batch 34 batch_loss: 0.0952802300453186\n",
      "training: 5 batch 35 batch_loss: 0.08927768468856812\n",
      "training: 5 batch 36 batch_loss: 0.09503036737442017\n",
      "training: 5 batch 37 batch_loss: 0.09079527854919434\n",
      "training: 5 batch 38 batch_loss: 0.09296000003814697\n",
      "training: 5 batch 39 batch_loss: 0.09396788477897644\n",
      "training: 5 batch 40 batch_loss: 0.09341654181480408\n",
      "training: 5 batch 41 batch_loss: 0.09393259882926941\n",
      "training: 5 batch 42 batch_loss: 0.09133237600326538\n",
      "training: 5 batch 43 batch_loss: 0.0903281569480896\n",
      "training: 5 batch 44 batch_loss: 0.09130701422691345\n",
      "training: 5 batch 45 batch_loss: 0.08906763792037964\n",
      "training: 5 batch 46 batch_loss: 0.09383368492126465\n",
      "training: 5 batch 47 batch_loss: 0.09239894151687622\n",
      "training: 5 batch 48 batch_loss: 0.09304654598236084\n",
      "training: 5 batch 49 batch_loss: 0.09247863292694092\n",
      "training: 5 batch 50 batch_loss: 0.09206229448318481\n",
      "training: 5 batch 51 batch_loss: 0.09060633182525635\n",
      "training: 5 batch 52 batch_loss: 0.09172803163528442\n",
      "training: 5 batch 53 batch_loss: 0.0966031551361084\n",
      "training: 5 batch 54 batch_loss: 0.09155511856079102\n",
      "training: 5 batch 55 batch_loss: 0.09226340055465698\n",
      "training: 5 batch 56 batch_loss: 0.09041571617126465\n",
      "training: 5 batch 57 batch_loss: 0.09206116199493408\n",
      "training: 5 batch 58 batch_loss: 0.09240734577178955\n",
      "training: 5 batch 59 batch_loss: 0.09264156222343445\n",
      "training: 5 batch 60 batch_loss: 0.0916861891746521\n",
      "training: 5 batch 61 batch_loss: 0.09281212091445923\n",
      "training: 5 batch 62 batch_loss: 0.09513366222381592\n",
      "training: 5 batch 63 batch_loss: 0.09430548548698425\n",
      "training: 5 batch 64 batch_loss: 0.09284275770187378\n",
      "training: 5 batch 65 batch_loss: 0.09350118041038513\n",
      "training: 5 batch 66 batch_loss: 0.09595274925231934\n",
      "training: 5 batch 67 batch_loss: 0.09307342767715454\n",
      "training: 5 batch 68 batch_loss: 0.09078836441040039\n",
      "training: 5 batch 69 batch_loss: 0.09362286329269409\n",
      "training: 5 batch 70 batch_loss: 0.09095734357833862\n",
      "training: 5 batch 71 batch_loss: 0.09153175354003906\n",
      "training: 5 batch 72 batch_loss: 0.08851790428161621\n",
      "training: 5 batch 73 batch_loss: 0.0943739116191864\n",
      "training: 5 batch 74 batch_loss: 0.0923384428024292\n",
      "training: 5 batch 75 batch_loss: 0.09394329786300659\n",
      "training: 5 batch 76 batch_loss: 0.09285324811935425\n",
      "training: 5 batch 77 batch_loss: 0.09866386651992798\n",
      "training: 5 batch 78 batch_loss: 0.09207352995872498\n",
      "training: 5 batch 79 batch_loss: 0.09433546662330627\n",
      "training: 5 batch 80 batch_loss: 0.09441009163856506\n",
      "training: 5 batch 81 batch_loss: 0.09505712985992432\n",
      "training: 5 batch 82 batch_loss: 0.0911509096622467\n",
      "training: 5 batch 83 batch_loss: 0.09121972322463989\n",
      "training: 5 batch 84 batch_loss: 0.09318703413009644\n",
      "training: 5 batch 85 batch_loss: 0.09303551912307739\n",
      "training: 5 batch 86 batch_loss: 0.09189081192016602\n",
      "training: 5 batch 87 batch_loss: 0.09235233068466187\n",
      "training: 5 batch 88 batch_loss: 0.09244680404663086\n",
      "training: 5 batch 89 batch_loss: 0.09307804703712463\n",
      "training: 5 batch 90 batch_loss: 0.095081627368927\n",
      "training: 5 batch 91 batch_loss: 0.09164077043533325\n",
      "training: 5 batch 92 batch_loss: 0.09202611446380615\n",
      "training: 5 batch 93 batch_loss: 0.09746193885803223\n",
      "training: 5 batch 94 batch_loss: 0.09437763690948486\n",
      "training: 5 batch 95 batch_loss: 0.09299582242965698\n",
      "training: 5 batch 96 batch_loss: 0.09440383315086365\n",
      "training: 5 batch 97 batch_loss: 0.09206554293632507\n",
      "training: 5 batch 98 batch_loss: 0.09501785039901733\n",
      "training: 5 batch 99 batch_loss: 0.09258410334587097\n",
      "training: 5 batch 100 batch_loss: 0.09347230195999146\n",
      "training: 5 batch 101 batch_loss: 0.09302237629890442\n",
      "training: 5 batch 102 batch_loss: 0.09301954507827759\n",
      "training: 5 batch 103 batch_loss: 0.09356537461280823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 5 batch 104 batch_loss: 0.09343722462654114\n",
      "training: 5 batch 105 batch_loss: 0.0902494490146637\n",
      "training: 5 batch 106 batch_loss: 0.09255120158195496\n",
      "training: 5 batch 107 batch_loss: 0.09419357776641846\n",
      "training: 5 batch 108 batch_loss: 0.09139922261238098\n",
      "training: 5 batch 109 batch_loss: 0.09462326765060425\n",
      "training: 5 batch 110 batch_loss: 0.09282448887825012\n",
      "training: 5 batch 111 batch_loss: 0.09503883123397827\n",
      "training: 5 batch 112 batch_loss: 0.09658351540565491\n",
      "training: 5 batch 113 batch_loss: 0.09542793035507202\n",
      "training: 5 batch 114 batch_loss: 0.09437179565429688\n",
      "training: 5 batch 115 batch_loss: 0.09409719705581665\n",
      "training: 5 batch 116 batch_loss: 0.09439250826835632\n",
      "training: 5 batch 117 batch_loss: 0.09577065706253052\n",
      "training: 5 batch 118 batch_loss: 0.09045878052711487\n",
      "training: 5 batch 119 batch_loss: 0.09700408577919006\n",
      "training: 5 batch 120 batch_loss: 0.09122177958488464\n",
      "training: 5 batch 121 batch_loss: 0.09202474355697632\n",
      "training: 5 batch 122 batch_loss: 0.09352803230285645\n",
      "training: 5 batch 123 batch_loss: 0.09374138712882996\n",
      "training: 5 batch 124 batch_loss: 0.0932546854019165\n",
      "training: 5 batch 125 batch_loss: 0.09513720870018005\n",
      "training: 5 batch 126 batch_loss: 0.09291636943817139\n",
      "training: 5 batch 127 batch_loss: 0.0969952940940857\n",
      "training: 5 batch 128 batch_loss: 0.09293928742408752\n",
      "training: 5 batch 129 batch_loss: 0.09409049153327942\n",
      "training: 5 batch 130 batch_loss: 0.0914367139339447\n",
      "training: 5 batch 131 batch_loss: 0.09503990411758423\n",
      "training: 5 batch 132 batch_loss: 0.09555748105049133\n",
      "training: 5 batch 133 batch_loss: 0.09227699041366577\n",
      "training: 5 batch 134 batch_loss: 0.09307408332824707\n",
      "training: 5 batch 135 batch_loss: 0.09349313378334045\n",
      "training: 5 batch 136 batch_loss: 0.09281057119369507\n",
      "training: 5 batch 137 batch_loss: 0.09472236037254333\n",
      "training: 5 batch 138 batch_loss: 0.09280574321746826\n",
      "training: 5 batch 139 batch_loss: 0.0944371223449707\n",
      "training: 5 batch 140 batch_loss: 0.09601473808288574\n",
      "training: 5 batch 141 batch_loss: 0.09376335144042969\n",
      "training: 5 batch 142 batch_loss: 0.09411990642547607\n",
      "training: 5 batch 143 batch_loss: 0.09498158097267151\n",
      "training: 5 batch 144 batch_loss: 0.09546568989753723\n",
      "training: 5 batch 145 batch_loss: 0.096096932888031\n",
      "training: 5 batch 146 batch_loss: 0.09565156698226929\n",
      "training: 5 batch 147 batch_loss: 0.09296524524688721\n",
      "training: 5 batch 148 batch_loss: 0.09499719738960266\n",
      "training: 5 batch 149 batch_loss: 0.09307491779327393\n",
      "training: 5 batch 150 batch_loss: 0.09541159868240356\n",
      "training: 5 batch 151 batch_loss: 0.09507149457931519\n",
      "training: 5 batch 152 batch_loss: 0.0923791229724884\n",
      "training: 5 batch 153 batch_loss: 0.09556478261947632\n",
      "training: 5 batch 154 batch_loss: 0.09471556544303894\n",
      "training: 5 batch 155 batch_loss: 0.09535154700279236\n",
      "training: 5 batch 156 batch_loss: 0.09260958433151245\n",
      "training: 5 batch 157 batch_loss: 0.09531134366989136\n",
      "training: 5 batch 158 batch_loss: 0.09471827745437622\n",
      "training: 5 batch 159 batch_loss: 0.09698757529258728\n",
      "training: 5 batch 160 batch_loss: 0.09143823385238647\n",
      "training: 5 batch 161 batch_loss: 0.09438768029212952\n",
      "training: 5 batch 162 batch_loss: 0.09565883874893188\n",
      "training: 5 batch 163 batch_loss: 0.09603697061538696\n",
      "training: 5 batch 164 batch_loss: 0.0926564633846283\n",
      "training: 5 batch 165 batch_loss: 0.09070762991905212\n",
      "training: 5 batch 166 batch_loss: 0.0942949652671814\n",
      "training: 5 batch 167 batch_loss: 0.0945899486541748\n",
      "training: 5 batch 168 batch_loss: 0.09435778856277466\n",
      "training: 5 batch 169 batch_loss: 0.09470862150192261\n",
      "training: 5 batch 170 batch_loss: 0.09308657050132751\n",
      "training: 5 batch 171 batch_loss: 0.09315663576126099\n",
      "training: 5 batch 172 batch_loss: 0.09561088681221008\n",
      "training: 5 batch 173 batch_loss: 0.0963435173034668\n",
      "training: 5 batch 174 batch_loss: 0.09435909986495972\n",
      "training: 5 batch 175 batch_loss: 0.09295687079429626\n",
      "training: 5 batch 176 batch_loss: 0.09229224920272827\n",
      "training: 5 batch 177 batch_loss: 0.09582418203353882\n",
      "training: 5 batch 178 batch_loss: 0.09345927834510803\n",
      "training: 5 batch 179 batch_loss: 0.09034565091133118\n",
      "training: 5 batch 180 batch_loss: 0.09503871202468872\n",
      "training: 5 batch 181 batch_loss: 0.09638255834579468\n",
      "training: 5 batch 182 batch_loss: 0.0956067144870758\n",
      "training: 5 batch 183 batch_loss: 0.09437119960784912\n",
      "training: 5 batch 184 batch_loss: 0.09595540165901184\n",
      "training: 5 batch 185 batch_loss: 0.09493625164031982\n",
      "training: 5 batch 186 batch_loss: 0.09037768840789795\n",
      "training: 5 batch 187 batch_loss: 0.09370347857475281\n",
      "training: 5 batch 188 batch_loss: 0.09664615988731384\n",
      "training: 5 batch 189 batch_loss: 0.09048274159431458\n",
      "training: 5 batch 190 batch_loss: 0.09569278359413147\n",
      "training: 5 batch 191 batch_loss: 0.09628280997276306\n",
      "training: 5 batch 192 batch_loss: 0.09569588303565979\n",
      "training: 5 batch 193 batch_loss: 0.09559985995292664\n",
      "training: 5 batch 194 batch_loss: 0.09601742029190063\n",
      "training: 5 batch 195 batch_loss: 0.093146413564682\n",
      "training: 5 batch 196 batch_loss: 0.09545117616653442\n",
      "training: 5 batch 197 batch_loss: 0.0946684181690216\n",
      "training: 5 batch 198 batch_loss: 0.09260579943656921\n",
      "training: 5 batch 199 batch_loss: 0.09275531768798828\n",
      "training: 5 batch 200 batch_loss: 0.09178516268730164\n",
      "training: 5 batch 201 batch_loss: 0.09514057636260986\n",
      "training: 5 batch 202 batch_loss: 0.09401249885559082\n",
      "training: 5 batch 203 batch_loss: 0.09413319826126099\n",
      "training: 5 batch 204 batch_loss: 0.09513261914253235\n",
      "training: 5 batch 205 batch_loss: 0.09702372550964355\n",
      "training: 5 batch 206 batch_loss: 0.09088239073753357\n",
      "training: 5 batch 207 batch_loss: 0.0947820246219635\n",
      "training: 5 batch 208 batch_loss: 0.0988188087940216\n",
      "training: 5 batch 209 batch_loss: 0.09354740381240845\n",
      "training: 5 batch 210 batch_loss: 0.09768998622894287\n",
      "training: 5 batch 211 batch_loss: 0.09514760971069336\n",
      "training: 5 batch 212 batch_loss: 0.09636944532394409\n",
      "training: 5 batch 213 batch_loss: 0.09381935000419617\n",
      "training: 5 batch 214 batch_loss: 0.09415504336357117\n",
      "training: 5 batch 215 batch_loss: 0.09819802641868591\n",
      "training: 5 batch 216 batch_loss: 0.0978541374206543\n",
      "training: 5 batch 217 batch_loss: 0.09449368715286255\n",
      "training: 5 batch 218 batch_loss: 0.0958280861377716\n",
      "training: 5 batch 219 batch_loss: 0.09396189451217651\n",
      "training: 5 batch 220 batch_loss: 0.0964297354221344\n",
      "training: 5 batch 221 batch_loss: 0.09321928024291992\n",
      "training: 5 batch 222 batch_loss: 0.09254708886146545\n",
      "training: 5 batch 223 batch_loss: 0.09837663173675537\n",
      "training: 5 batch 224 batch_loss: 0.09502655267715454\n",
      "training: 5 batch 225 batch_loss: 0.09721565246582031\n",
      "training: 5 batch 226 batch_loss: 0.0945817232131958\n",
      "training: 5 batch 227 batch_loss: 0.09688040614128113\n",
      "training: 5 batch 228 batch_loss: 0.09561926126480103\n",
      "training: 5 batch 229 batch_loss: 0.0924730896949768\n",
      "training: 5 batch 230 batch_loss: 0.0955137312412262\n",
      "training: 5 batch 231 batch_loss: 0.09529972076416016\n",
      "training: 5 batch 232 batch_loss: 0.09570598602294922\n",
      "training: 5 batch 233 batch_loss: 0.09723681211471558\n",
      "training: 5 batch 234 batch_loss: 0.09451735019683838\n",
      "training: 5 batch 235 batch_loss: 0.0953550934791565\n",
      "training: 5 batch 236 batch_loss: 0.1000145673751831\n",
      "training: 5 batch 237 batch_loss: 0.09803825616836548\n",
      "training: 5 batch 238 batch_loss: 0.09474164247512817\n",
      "training: 5 batch 239 batch_loss: 0.09407329559326172\n",
      "training: 5 batch 240 batch_loss: 0.09765985608100891\n",
      "training: 5 batch 241 batch_loss: 0.09400612115859985\n",
      "training: 5 batch 242 batch_loss: 0.09357678890228271\n",
      "training: 5 batch 243 batch_loss: 0.09507536888122559\n",
      "training: 5 batch 244 batch_loss: 0.09601902961730957\n",
      "training: 5 batch 245 batch_loss: 0.09415161609649658\n",
      "training: 5 batch 246 batch_loss: 0.09631678462028503\n",
      "training: 5 batch 247 batch_loss: 0.09535998106002808\n",
      "training: 5 batch 248 batch_loss: 0.09540128707885742\n",
      "training: 5 batch 249 batch_loss: 0.09571579098701477\n",
      "training: 5 batch 250 batch_loss: 0.09597140550613403\n",
      "training: 5 batch 251 batch_loss: 0.0980755090713501\n",
      "training: 5 batch 252 batch_loss: 0.09552279114723206\n",
      "training: 5 batch 253 batch_loss: 0.09491986036300659\n",
      "training: 5 batch 254 batch_loss: 0.09470033645629883\n",
      "training: 5 batch 255 batch_loss: 0.09409412741661072\n",
      "training: 5 batch 256 batch_loss: 0.09412550926208496\n",
      "training: 5 batch 257 batch_loss: 0.09712666273117065\n",
      "training: 5 batch 258 batch_loss: 0.09509855508804321\n",
      "training: 5 batch 259 batch_loss: 0.09646746516227722\n",
      "training: 5 batch 260 batch_loss: 0.0980449914932251\n",
      "training: 5 batch 261 batch_loss: 0.09733772277832031\n",
      "training: 5 batch 262 batch_loss: 0.09554415941238403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 5 batch 263 batch_loss: 0.09592127799987793\n",
      "training: 5 batch 264 batch_loss: 0.09566429257392883\n",
      "training: 5 batch 265 batch_loss: 0.0956348180770874\n",
      "training: 5 batch 266 batch_loss: 0.09435489773750305\n",
      "training: 5 batch 267 batch_loss: 0.09766274690628052\n",
      "training: 5 batch 268 batch_loss: 0.09436440467834473\n",
      "training: 5 batch 269 batch_loss: 0.09616166353225708\n",
      "training: 5 batch 270 batch_loss: 0.09645470976829529\n",
      "training: 5 batch 271 batch_loss: 0.09842818975448608\n",
      "training: 5 batch 272 batch_loss: 0.0939406156539917\n",
      "training: 5 batch 273 batch_loss: 0.09688711166381836\n",
      "training: 5 batch 274 batch_loss: 0.09451508522033691\n",
      "training: 5 batch 275 batch_loss: 0.09622478485107422\n",
      "training: 5 batch 276 batch_loss: 0.0929574966430664\n",
      "training: 5 batch 277 batch_loss: 0.09524625539779663\n",
      "training: 5 batch 278 batch_loss: 0.09719651937484741\n",
      "training: 5 batch 279 batch_loss: 0.09524756669998169\n",
      "training: 5 batch 280 batch_loss: 0.09587851166725159\n",
      "training: 5 batch 281 batch_loss: 0.09766194224357605\n",
      "training: 5 batch 282 batch_loss: 0.09817853569984436\n",
      "training: 5 batch 283 batch_loss: 0.09365519881248474\n",
      "training: 5 batch 284 batch_loss: 0.09711384773254395\n",
      "training: 5 batch 285 batch_loss: 0.09785884618759155\n",
      "training: 5 batch 286 batch_loss: 0.09657710790634155\n",
      "training: 5 batch 287 batch_loss: 0.09751814603805542\n",
      "training: 5 batch 288 batch_loss: 0.09633156657218933\n",
      "training: 5 batch 289 batch_loss: 0.0929153561592102\n",
      "training: 5 batch 290 batch_loss: 0.09559968113899231\n",
      "training: 5 batch 291 batch_loss: 0.09525668621063232\n",
      "training: 5 batch 292 batch_loss: 0.10076913237571716\n",
      "training: 5 batch 293 batch_loss: 0.10065525770187378\n",
      "training: 5 batch 294 batch_loss: 0.0947115421295166\n",
      "training: 5 batch 295 batch_loss: 0.09608963131904602\n",
      "training: 5 batch 296 batch_loss: 0.09430056810379028\n",
      "training: 5 batch 297 batch_loss: 0.09711471199989319\n",
      "training: 5 batch 298 batch_loss: 0.09437870979309082\n",
      "training: 5 batch 299 batch_loss: 0.0947030782699585\n",
      "training: 5 batch 300 batch_loss: 0.09257364273071289\n",
      "training: 5 batch 301 batch_loss: 0.09559193253517151\n",
      "training: 5 batch 302 batch_loss: 0.0964072048664093\n",
      "training: 5 batch 303 batch_loss: 0.09679234027862549\n",
      "training: 5 batch 304 batch_loss: 0.0946359932422638\n",
      "training: 5 batch 305 batch_loss: 0.09457337856292725\n",
      "training: 5 batch 306 batch_loss: 0.09486979246139526\n",
      "training: 5 batch 307 batch_loss: 0.09609970450401306\n",
      "training: 5 batch 308 batch_loss: 0.09847420454025269\n",
      "training: 5 batch 309 batch_loss: 0.09762969613075256\n",
      "training: 5 batch 310 batch_loss: 0.0919312834739685\n",
      "training: 5 batch 311 batch_loss: 0.0949801504611969\n",
      "training: 5 batch 312 batch_loss: 0.09881353378295898\n",
      "training: 5 batch 313 batch_loss: 0.09672105312347412\n",
      "training: 5 batch 314 batch_loss: 0.09840616583824158\n",
      "training: 5 batch 315 batch_loss: 0.09590739011764526\n",
      "training: 5 batch 316 batch_loss: 0.09527051448822021\n",
      "training: 5 batch 317 batch_loss: 0.09804850816726685\n",
      "training: 5 batch 318 batch_loss: 0.09833651781082153\n",
      "training: 5 batch 319 batch_loss: 0.09696763753890991\n",
      "training: 5 batch 320 batch_loss: 0.09625768661499023\n",
      "training: 5 batch 321 batch_loss: 0.09544402360916138\n",
      "training: 5 batch 322 batch_loss: 0.0985436737537384\n",
      "training: 5 batch 323 batch_loss: 0.09524664282798767\n",
      "training: 5 batch 324 batch_loss: 0.09501627087593079\n",
      "training: 5 batch 325 batch_loss: 0.09544417262077332\n",
      "training: 5 batch 326 batch_loss: 0.0956171452999115\n",
      "training: 5 batch 327 batch_loss: 0.0977398157119751\n",
      "training: 5 batch 328 batch_loss: 0.09666520357131958\n",
      "training: 5 batch 329 batch_loss: 0.09318473935127258\n",
      "training: 5 batch 330 batch_loss: 0.09760686755180359\n",
      "training: 5 batch 331 batch_loss: 0.09941324591636658\n",
      "training: 5 batch 332 batch_loss: 0.09591391682624817\n",
      "training: 5 batch 333 batch_loss: 0.09322714805603027\n",
      "training: 5 batch 334 batch_loss: 0.09813421964645386\n",
      "training: 5 batch 335 batch_loss: 0.09929406642913818\n",
      "training: 5 batch 336 batch_loss: 0.0969133973121643\n",
      "training: 5 batch 337 batch_loss: 0.09563860297203064\n",
      "training: 5 batch 338 batch_loss: 0.1000308096408844\n",
      "training: 5 batch 339 batch_loss: 0.09701436758041382\n",
      "training: 5 batch 340 batch_loss: 0.09516763687133789\n",
      "training: 5 batch 341 batch_loss: 0.09845659136772156\n",
      "training: 5 batch 342 batch_loss: 0.09620335698127747\n",
      "training: 5 batch 343 batch_loss: 0.09644100069999695\n",
      "training: 5 batch 344 batch_loss: 0.09914851188659668\n",
      "training: 5 batch 345 batch_loss: 0.09779733419418335\n",
      "training: 5 batch 346 batch_loss: 0.09473508596420288\n",
      "training: 5 batch 347 batch_loss: 0.09525200724601746\n",
      "training: 5 batch 348 batch_loss: 0.09363996982574463\n",
      "training: 5 batch 349 batch_loss: 0.09752315282821655\n",
      "training: 5 batch 350 batch_loss: 0.09848520159721375\n",
      "training: 5 batch 351 batch_loss: 0.09498944878578186\n",
      "training: 5 batch 352 batch_loss: 0.09515678882598877\n",
      "training: 5 batch 353 batch_loss: 0.09420832991600037\n",
      "training: 5 batch 354 batch_loss: 0.09641045331954956\n",
      "training: 5 batch 355 batch_loss: 0.09955212473869324\n",
      "training: 5 batch 356 batch_loss: 0.09783551096916199\n",
      "training: 5 batch 357 batch_loss: 0.09549325704574585\n",
      "training: 5 batch 358 batch_loss: 0.09998533129692078\n",
      "training: 5 batch 359 batch_loss: 0.09871846437454224\n",
      "training: 5 batch 360 batch_loss: 0.09983760118484497\n",
      "training: 5 batch 361 batch_loss: 0.09774363040924072\n",
      "training: 5 batch 362 batch_loss: 0.09780514240264893\n",
      "training: 5 batch 363 batch_loss: 0.09775921702384949\n",
      "training: 5 batch 364 batch_loss: 0.09679850935935974\n",
      "training: 5 batch 365 batch_loss: 0.09989267587661743\n",
      "training: 5 batch 366 batch_loss: 0.09613054990768433\n",
      "training: 5 batch 367 batch_loss: 0.10008308291435242\n",
      "training: 5 batch 368 batch_loss: 0.09840768575668335\n",
      "training: 5 batch 369 batch_loss: 0.09832605719566345\n",
      "training: 5 batch 370 batch_loss: 0.09668532013893127\n",
      "training: 5 batch 371 batch_loss: 0.09763872623443604\n",
      "training: 5 batch 372 batch_loss: 0.0933951735496521\n",
      "training: 5 batch 373 batch_loss: 0.09607246518135071\n",
      "training: 5 batch 374 batch_loss: 0.09695702791213989\n",
      "training: 5 batch 375 batch_loss: 0.09661626815795898\n",
      "training: 5 batch 376 batch_loss: 0.09846657514572144\n",
      "training: 5 batch 377 batch_loss: 0.09695756435394287\n",
      "training: 5 batch 378 batch_loss: 0.09847047924995422\n",
      "training: 5 batch 379 batch_loss: 0.09707200527191162\n",
      "training: 5 batch 380 batch_loss: 0.09679847955703735\n",
      "training: 5 batch 381 batch_loss: 0.09510993957519531\n",
      "training: 5 batch 382 batch_loss: 0.0991608202457428\n",
      "training: 5 batch 383 batch_loss: 0.09556049108505249\n",
      "training: 5 batch 384 batch_loss: 0.0985342264175415\n",
      "training: 5 batch 385 batch_loss: 0.09894227981567383\n",
      "training: 5 batch 386 batch_loss: 0.0982528030872345\n",
      "training: 5 batch 387 batch_loss: 0.09933561086654663\n",
      "training: 5 batch 388 batch_loss: 0.09542480111122131\n",
      "training: 5 batch 389 batch_loss: 0.09627637267112732\n",
      "training: 5 batch 390 batch_loss: 0.09781447052955627\n",
      "training: 5 batch 391 batch_loss: 0.09904611110687256\n",
      "training: 5 batch 392 batch_loss: 0.09965252876281738\n",
      "training: 5 batch 393 batch_loss: 0.09526523947715759\n",
      "training: 5 batch 394 batch_loss: 0.0984153151512146\n",
      "training: 5 batch 395 batch_loss: 0.09904608130455017\n",
      "training: 5 batch 396 batch_loss: 0.09663540124893188\n",
      "training: 5 batch 397 batch_loss: 0.09744834899902344\n",
      "training: 5 batch 398 batch_loss: 0.10066857933998108\n",
      "training: 5 batch 399 batch_loss: 0.0968363881111145\n",
      "training: 5 batch 400 batch_loss: 0.09481513500213623\n",
      "training: 5 batch 401 batch_loss: 0.09736627340316772\n",
      "training: 5 batch 402 batch_loss: 0.09829959273338318\n",
      "training: 5 batch 403 batch_loss: 0.09549936652183533\n",
      "training: 5 batch 404 batch_loss: 0.0981283187866211\n",
      "training: 5 batch 405 batch_loss: 0.09725895524024963\n",
      "training: 5 batch 406 batch_loss: 0.09992697834968567\n",
      "training: 5 batch 407 batch_loss: 0.09760281443595886\n",
      "training: 5 batch 408 batch_loss: 0.09667152166366577\n",
      "training: 5 batch 409 batch_loss: 0.09720879793167114\n",
      "training: 5 batch 410 batch_loss: 0.09892779588699341\n",
      "training: 5 batch 411 batch_loss: 0.099022775888443\n",
      "training: 5 batch 412 batch_loss: 0.09747692942619324\n",
      "training: 5 batch 413 batch_loss: 0.09967082738876343\n",
      "training: 5 batch 414 batch_loss: 0.09975698590278625\n",
      "training: 5 batch 415 batch_loss: 0.09747070074081421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 5 batch 416 batch_loss: 0.09780481457710266\n",
      "training: 5 batch 417 batch_loss: 0.09823811054229736\n",
      "training: 5 batch 418 batch_loss: 0.09626215696334839\n",
      "training: 5 batch 419 batch_loss: 0.0995846688747406\n",
      "training: 5 batch 420 batch_loss: 0.0996353030204773\n",
      "training: 5 batch 421 batch_loss: 0.09439292550086975\n",
      "training: 5 batch 422 batch_loss: 0.09887498617172241\n",
      "training: 5 batch 423 batch_loss: 0.09609788656234741\n",
      "training: 5 batch 424 batch_loss: 0.09629184007644653\n",
      "training: 5 batch 425 batch_loss: 0.09454599022865295\n",
      "training: 5 batch 426 batch_loss: 0.09961652755737305\n",
      "training: 5 batch 427 batch_loss: 0.10022541880607605\n",
      "training: 5 batch 428 batch_loss: 0.09749746322631836\n",
      "training: 5 batch 429 batch_loss: 0.09875112771987915\n",
      "training: 5 batch 430 batch_loss: 0.10096129775047302\n",
      "training: 5 batch 431 batch_loss: 0.09789222478866577\n",
      "training: 5 batch 432 batch_loss: 0.0965333878993988\n",
      "training: 5 batch 433 batch_loss: 0.09790277481079102\n",
      "training: 5 batch 434 batch_loss: 0.09691566228866577\n",
      "training: 5 batch 435 batch_loss: 0.09734362363815308\n",
      "training: 5 batch 436 batch_loss: 0.09815073013305664\n",
      "training: 5 batch 437 batch_loss: 0.09969675540924072\n",
      "training: 5 batch 438 batch_loss: 0.09898906946182251\n",
      "training: 5 batch 439 batch_loss: 0.09651204943656921\n",
      "training: 5 batch 440 batch_loss: 0.09927350282669067\n",
      "training: 5 batch 441 batch_loss: 0.09977281093597412\n",
      "training: 5 batch 442 batch_loss: 0.09954893589019775\n",
      "training: 5 batch 443 batch_loss: 0.09528416395187378\n",
      "training: 5 batch 444 batch_loss: 0.09968850016593933\n",
      "training: 5 batch 445 batch_loss: 0.09899699687957764\n",
      "training: 5 batch 446 batch_loss: 0.09890085458755493\n",
      "training: 5 batch 447 batch_loss: 0.09937560558319092\n",
      "training: 5 batch 448 batch_loss: 0.09800094366073608\n",
      "training: 5 batch 449 batch_loss: 0.0991378128528595\n",
      "training: 5 batch 450 batch_loss: 0.10036134719848633\n",
      "training: 5 batch 451 batch_loss: 0.09967941045761108\n",
      "training: 5 batch 452 batch_loss: 0.09843945503234863\n",
      "training: 5 batch 453 batch_loss: 0.10129261016845703\n",
      "training: 5 batch 454 batch_loss: 0.09830284118652344\n",
      "training: 5 batch 455 batch_loss: 0.0985051691532135\n",
      "training: 5 batch 456 batch_loss: 0.09800633788108826\n",
      "training: 5 batch 457 batch_loss: 0.10035693645477295\n",
      "training: 5 batch 458 batch_loss: 0.09891641139984131\n",
      "training: 5 batch 459 batch_loss: 0.10141444206237793\n",
      "training: 5 batch 460 batch_loss: 0.10072976350784302\n",
      "training: 5 batch 461 batch_loss: 0.10282108187675476\n",
      "training: 5 batch 462 batch_loss: 0.09917080402374268\n",
      "training: 5 batch 463 batch_loss: 0.0987381637096405\n",
      "training: 5 batch 464 batch_loss: 0.09708139300346375\n",
      "training: 5 batch 465 batch_loss: 0.09922146797180176\n",
      "training: 5 batch 466 batch_loss: 0.09995061159133911\n",
      "training: 5 batch 467 batch_loss: 0.10068276524543762\n",
      "training: 5 batch 468 batch_loss: 0.0966377854347229\n",
      "training: 5 batch 469 batch_loss: 0.09806469082832336\n",
      "training: 5 batch 470 batch_loss: 0.0995696485042572\n",
      "training: 5 batch 471 batch_loss: 0.09801673889160156\n",
      "training: 5 batch 472 batch_loss: 0.09825533628463745\n",
      "training: 5 batch 473 batch_loss: 0.09924566745758057\n",
      "training: 5 batch 474 batch_loss: 0.10272285342216492\n",
      "training: 5 batch 475 batch_loss: 0.09647077322006226\n",
      "training: 5 batch 476 batch_loss: 0.10149860382080078\n",
      "training: 5 batch 477 batch_loss: 0.09813299775123596\n",
      "training: 5 batch 478 batch_loss: 0.09744682908058167\n",
      "training: 5 batch 479 batch_loss: 0.09970486164093018\n",
      "training: 5 batch 480 batch_loss: 0.09775304794311523\n",
      "training: 5 batch 481 batch_loss: 0.09726893901824951\n",
      "training: 5 batch 482 batch_loss: 0.09813416004180908\n",
      "training: 5 batch 483 batch_loss: 0.0971493124961853\n",
      "training: 5 batch 484 batch_loss: 0.09908783435821533\n",
      "training: 5 batch 485 batch_loss: 0.0981229841709137\n",
      "training: 5 batch 486 batch_loss: 0.09969338774681091\n",
      "training: 5 batch 487 batch_loss: 0.10118791460990906\n",
      "training: 5 batch 488 batch_loss: 0.09819972515106201\n",
      "training: 5 batch 489 batch_loss: 0.09849905967712402\n",
      "training: 5 batch 490 batch_loss: 0.0981343686580658\n",
      "training: 5 batch 491 batch_loss: 0.10353168845176697\n",
      "training: 5 batch 492 batch_loss: 0.10005167126655579\n",
      "training: 5 batch 493 batch_loss: 0.09882017970085144\n",
      "training: 5 batch 494 batch_loss: 0.10311758518218994\n",
      "training: 5 batch 495 batch_loss: 0.10241615772247314\n",
      "training: 5 batch 496 batch_loss: 0.09945818781852722\n",
      "training: 5 batch 497 batch_loss: 0.09832179546356201\n",
      "training: 5 batch 498 batch_loss: 0.09944015741348267\n",
      "training: 5 batch 499 batch_loss: 0.10241478681564331\n",
      "training: 5 batch 500 batch_loss: 0.09863534569740295\n",
      "training: 5 batch 501 batch_loss: 0.09984472393989563\n",
      "training: 5 batch 502 batch_loss: 0.10067668557167053\n",
      "training: 5 batch 503 batch_loss: 0.10121190547943115\n",
      "training: 5 batch 504 batch_loss: 0.10149097442626953\n",
      "training: 5 batch 505 batch_loss: 0.1007280945777893\n",
      "training: 5 batch 506 batch_loss: 0.09619942307472229\n",
      "training: 5 batch 507 batch_loss: 0.10243359208106995\n",
      "training: 5 batch 508 batch_loss: 0.09727251529693604\n",
      "training: 5 batch 509 batch_loss: 0.0996827781200409\n",
      "training: 5 batch 510 batch_loss: 0.10091930627822876\n",
      "training: 5 batch 511 batch_loss: 0.10212337970733643\n",
      "training: 5 batch 512 batch_loss: 0.10048887133598328\n",
      "training: 5 batch 513 batch_loss: 0.09959280490875244\n",
      "training: 5 batch 514 batch_loss: 0.09973794221878052\n",
      "training: 5 batch 515 batch_loss: 0.09931975603103638\n",
      "training: 5 batch 516 batch_loss: 0.09905761480331421\n",
      "training: 5 batch 517 batch_loss: 0.10175850987434387\n",
      "training: 5 batch 518 batch_loss: 0.10277998447418213\n",
      "training: 5 batch 519 batch_loss: 0.10255306959152222\n",
      "training: 5 batch 520 batch_loss: 0.10050296783447266\n",
      "training: 5 batch 521 batch_loss: 0.09928536415100098\n",
      "training: 5 batch 522 batch_loss: 0.10021108388900757\n",
      "training: 5 batch 523 batch_loss: 0.09990349411964417\n",
      "training: 5 batch 524 batch_loss: 0.10094273090362549\n",
      "training: 5 batch 525 batch_loss: 0.10217833518981934\n",
      "training: 5 batch 526 batch_loss: 0.0993129312992096\n",
      "training: 5 batch 527 batch_loss: 0.10082995891571045\n",
      "training: 5 batch 528 batch_loss: 0.09862136840820312\n",
      "training: 5 batch 529 batch_loss: 0.09724408388137817\n",
      "training: 5 batch 530 batch_loss: 0.09697696566581726\n",
      "training: 5 batch 531 batch_loss: 0.09768208861351013\n",
      "training: 5 batch 532 batch_loss: 0.10104075074195862\n",
      "training: 5 batch 533 batch_loss: 0.09897202253341675\n",
      "training: 5 batch 534 batch_loss: 0.09916454553604126\n",
      "training: 5 batch 535 batch_loss: 0.09880918264389038\n",
      "training: 5 batch 536 batch_loss: 0.09900853037834167\n",
      "training: 5 batch 537 batch_loss: 0.10046830773353577\n",
      "training: 5 batch 538 batch_loss: 0.10158312320709229\n",
      "training: 5 batch 539 batch_loss: 0.09989291429519653\n",
      "training: 5 batch 540 batch_loss: 0.09915417432785034\n",
      "training: 5 batch 541 batch_loss: 0.09848427772521973\n",
      "training: 5 batch 542 batch_loss: 0.10074087977409363\n",
      "training: 5 batch 543 batch_loss: 0.10027340054512024\n",
      "training: 5 batch 544 batch_loss: 0.10204842686653137\n",
      "training: 5 batch 545 batch_loss: 0.10116532444953918\n",
      "training: 5 batch 546 batch_loss: 0.09607461094856262\n",
      "training: 5 batch 547 batch_loss: 0.09660977125167847\n",
      "training: 5 batch 548 batch_loss: 0.0990871787071228\n",
      "training: 5 batch 549 batch_loss: 0.09866601228713989\n",
      "training: 5 batch 550 batch_loss: 0.09827175736427307\n",
      "training: 5 batch 551 batch_loss: 0.10224825143814087\n",
      "training: 5 batch 552 batch_loss: 0.10048216581344604\n",
      "training: 5 batch 553 batch_loss: 0.10164684057235718\n",
      "training: 5 batch 554 batch_loss: 0.09963014721870422\n",
      "training: 5 batch 555 batch_loss: 0.09978675842285156\n",
      "training: 5 batch 556 batch_loss: 0.09616124629974365\n",
      "training: 5 batch 557 batch_loss: 0.09885406494140625\n",
      "training: 5 batch 558 batch_loss: 0.09985414147377014\n",
      "training: 5 batch 559 batch_loss: 0.10316762328147888\n",
      "training: 5 batch 560 batch_loss: 0.09972947835922241\n",
      "training: 5 batch 561 batch_loss: 0.10460862517356873\n",
      "training: 5 batch 562 batch_loss: 0.1003531813621521\n",
      "training: 5 batch 563 batch_loss: 0.10207116603851318\n",
      "training: 5 batch 564 batch_loss: 0.0994306206703186\n",
      "training: 5 batch 565 batch_loss: 0.09967663884162903\n",
      "training: 5 batch 566 batch_loss: 0.09994101524353027\n",
      "training: 5 batch 567 batch_loss: 0.09961175918579102\n",
      "training: 5 batch 568 batch_loss: 0.1004510223865509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 5 batch 569 batch_loss: 0.09955862164497375\n",
      "training: 5 batch 570 batch_loss: 0.1016186773777008\n",
      "training: 5 batch 571 batch_loss: 0.09976387023925781\n",
      "training: 5 batch 572 batch_loss: 0.10140550136566162\n",
      "training: 5 batch 573 batch_loss: 0.0961323082447052\n",
      "training: 5 batch 574 batch_loss: 0.10280230641365051\n",
      "training: 5 batch 575 batch_loss: 0.09802752733230591\n",
      "training: 5 batch 576 batch_loss: 0.105510413646698\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 5, Hit Ratio:0.020946524402634305 | Precision:0.030905337658507816 | Recall:0.045577049813353185 | NDCG:0.038658488036771105\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 6 batch 0 batch_loss: 0.09527656435966492\n",
      "training: 6 batch 1 batch_loss: 0.0938982367515564\n",
      "training: 6 batch 2 batch_loss: 0.10006669163703918\n",
      "training: 6 batch 3 batch_loss: 0.09801387786865234\n",
      "training: 6 batch 4 batch_loss: 0.09736058115959167\n",
      "training: 6 batch 5 batch_loss: 0.09880590438842773\n",
      "training: 6 batch 6 batch_loss: 0.0973709225654602\n",
      "training: 6 batch 7 batch_loss: 0.09786263108253479\n",
      "training: 6 batch 8 batch_loss: 0.09907951951026917\n",
      "training: 6 batch 9 batch_loss: 0.0997726321220398\n",
      "training: 6 batch 10 batch_loss: 0.09765815734863281\n",
      "training: 6 batch 11 batch_loss: 0.09970766305923462\n",
      "training: 6 batch 12 batch_loss: 0.09881219267845154\n",
      "training: 6 batch 13 batch_loss: 0.09778288006782532\n",
      "training: 6 batch 14 batch_loss: 0.10044825077056885\n",
      "training: 6 batch 15 batch_loss: 0.09843695163726807\n",
      "training: 6 batch 16 batch_loss: 0.10098263621330261\n",
      "training: 6 batch 17 batch_loss: 0.09809949994087219\n",
      "training: 6 batch 18 batch_loss: 0.09897679090499878\n",
      "training: 6 batch 19 batch_loss: 0.09779497981071472\n",
      "training: 6 batch 20 batch_loss: 0.09750518202781677\n",
      "training: 6 batch 21 batch_loss: 0.10043472051620483\n",
      "training: 6 batch 22 batch_loss: 0.09596890211105347\n",
      "training: 6 batch 23 batch_loss: 0.0983259379863739\n",
      "training: 6 batch 24 batch_loss: 0.09843635559082031\n",
      "training: 6 batch 25 batch_loss: 0.10005897283554077\n",
      "training: 6 batch 26 batch_loss: 0.09980043768882751\n",
      "training: 6 batch 27 batch_loss: 0.10020613670349121\n",
      "training: 6 batch 28 batch_loss: 0.09862467646598816\n",
      "training: 6 batch 29 batch_loss: 0.0979127287864685\n",
      "training: 6 batch 30 batch_loss: 0.10128843784332275\n",
      "training: 6 batch 31 batch_loss: 0.10162127017974854\n",
      "training: 6 batch 32 batch_loss: 0.09762391448020935\n",
      "training: 6 batch 33 batch_loss: 0.09876647591590881\n",
      "training: 6 batch 34 batch_loss: 0.10082656145095825\n",
      "training: 6 batch 35 batch_loss: 0.10042756795883179\n",
      "training: 6 batch 36 batch_loss: 0.09927442669868469\n",
      "training: 6 batch 37 batch_loss: 0.09627178311347961\n",
      "training: 6 batch 38 batch_loss: 0.10028669238090515\n",
      "training: 6 batch 39 batch_loss: 0.09653258323669434\n",
      "training: 6 batch 40 batch_loss: 0.10068589448928833\n",
      "training: 6 batch 41 batch_loss: 0.09930044412612915\n",
      "training: 6 batch 42 batch_loss: 0.09838339686393738\n",
      "training: 6 batch 43 batch_loss: 0.09686630964279175\n",
      "training: 6 batch 44 batch_loss: 0.1005704402923584\n",
      "training: 6 batch 45 batch_loss: 0.1026993989944458\n",
      "training: 6 batch 46 batch_loss: 0.09784838557243347\n",
      "training: 6 batch 47 batch_loss: 0.09717905521392822\n",
      "training: 6 batch 48 batch_loss: 0.09968554973602295\n",
      "training: 6 batch 49 batch_loss: 0.09804654121398926\n",
      "training: 6 batch 50 batch_loss: 0.1007927656173706\n",
      "training: 6 batch 51 batch_loss: 0.09601885080337524\n",
      "training: 6 batch 52 batch_loss: 0.10185804963111877\n",
      "training: 6 batch 53 batch_loss: 0.09974610805511475\n",
      "training: 6 batch 54 batch_loss: 0.10098159313201904\n",
      "training: 6 batch 55 batch_loss: 0.10099619626998901\n",
      "training: 6 batch 56 batch_loss: 0.10152846574783325\n",
      "training: 6 batch 57 batch_loss: 0.09827113151550293\n",
      "training: 6 batch 58 batch_loss: 0.09837913513183594\n",
      "training: 6 batch 59 batch_loss: 0.09849190711975098\n",
      "training: 6 batch 60 batch_loss: 0.10044920444488525\n",
      "training: 6 batch 61 batch_loss: 0.10034722089767456\n",
      "training: 6 batch 62 batch_loss: 0.0993979275226593\n",
      "training: 6 batch 63 batch_loss: 0.09765627980232239\n",
      "training: 6 batch 64 batch_loss: 0.09960445761680603\n",
      "training: 6 batch 65 batch_loss: 0.09977760910987854\n",
      "training: 6 batch 66 batch_loss: 0.09852069616317749\n",
      "training: 6 batch 67 batch_loss: 0.09871020913124084\n",
      "training: 6 batch 68 batch_loss: 0.09899455308914185\n",
      "training: 6 batch 69 batch_loss: 0.0997200608253479\n",
      "training: 6 batch 70 batch_loss: 0.09930086135864258\n",
      "training: 6 batch 71 batch_loss: 0.09874904155731201\n",
      "training: 6 batch 72 batch_loss: 0.09902593493461609\n",
      "training: 6 batch 73 batch_loss: 0.10140037536621094\n",
      "training: 6 batch 74 batch_loss: 0.10295301675796509\n",
      "training: 6 batch 75 batch_loss: 0.09962612390518188\n",
      "training: 6 batch 76 batch_loss: 0.10077458620071411\n",
      "training: 6 batch 77 batch_loss: 0.09935244917869568\n",
      "training: 6 batch 78 batch_loss: 0.10003775358200073\n",
      "training: 6 batch 79 batch_loss: 0.0986429750919342\n",
      "training: 6 batch 80 batch_loss: 0.10009720921516418\n",
      "training: 6 batch 81 batch_loss: 0.09887048602104187\n",
      "training: 6 batch 82 batch_loss: 0.09970700740814209\n",
      "training: 6 batch 83 batch_loss: 0.10242855548858643\n",
      "training: 6 batch 84 batch_loss: 0.10192528367042542\n",
      "training: 6 batch 85 batch_loss: 0.09814977645874023\n",
      "training: 6 batch 86 batch_loss: 0.10104021430015564\n",
      "training: 6 batch 87 batch_loss: 0.09875679016113281\n",
      "training: 6 batch 88 batch_loss: 0.09868696331977844\n",
      "training: 6 batch 89 batch_loss: 0.10035303235054016\n",
      "training: 6 batch 90 batch_loss: 0.10065317153930664\n",
      "training: 6 batch 91 batch_loss: 0.09929525852203369\n",
      "training: 6 batch 92 batch_loss: 0.0997186005115509\n",
      "training: 6 batch 93 batch_loss: 0.10058560967445374\n",
      "training: 6 batch 94 batch_loss: 0.10002580285072327\n",
      "training: 6 batch 95 batch_loss: 0.09980010986328125\n",
      "training: 6 batch 96 batch_loss: 0.10006704926490784\n",
      "training: 6 batch 97 batch_loss: 0.09830582141876221\n",
      "training: 6 batch 98 batch_loss: 0.09955623745918274\n",
      "training: 6 batch 99 batch_loss: 0.101482093334198\n",
      "training: 6 batch 100 batch_loss: 0.10147321224212646\n",
      "training: 6 batch 101 batch_loss: 0.10070160031318665\n",
      "training: 6 batch 102 batch_loss: 0.09828305244445801\n",
      "training: 6 batch 103 batch_loss: 0.10022050142288208\n",
      "training: 6 batch 104 batch_loss: 0.09817308187484741\n",
      "training: 6 batch 105 batch_loss: 0.10149380564689636\n",
      "training: 6 batch 106 batch_loss: 0.1002204418182373\n",
      "training: 6 batch 107 batch_loss: 0.10081276297569275\n",
      "training: 6 batch 108 batch_loss: 0.09850463271141052\n",
      "training: 6 batch 109 batch_loss: 0.09926608204841614\n",
      "training: 6 batch 110 batch_loss: 0.10027840733528137\n",
      "training: 6 batch 111 batch_loss: 0.10308709740638733\n",
      "training: 6 batch 112 batch_loss: 0.0991438627243042\n",
      "training: 6 batch 113 batch_loss: 0.09988173842430115\n",
      "training: 6 batch 114 batch_loss: 0.09962594509124756\n",
      "training: 6 batch 115 batch_loss: 0.10236617922782898\n",
      "training: 6 batch 116 batch_loss: 0.09846040606498718\n",
      "training: 6 batch 117 batch_loss: 0.09870624542236328\n",
      "training: 6 batch 118 batch_loss: 0.10200107097625732\n",
      "training: 6 batch 119 batch_loss: 0.10060080885887146\n",
      "training: 6 batch 120 batch_loss: 0.09791427850723267\n",
      "training: 6 batch 121 batch_loss: 0.10338258743286133\n",
      "training: 6 batch 122 batch_loss: 0.10244601964950562\n",
      "training: 6 batch 123 batch_loss: 0.10254573822021484\n",
      "training: 6 batch 124 batch_loss: 0.10106685757637024\n",
      "training: 6 batch 125 batch_loss: 0.10090568661689758\n",
      "training: 6 batch 126 batch_loss: 0.09881198406219482\n",
      "training: 6 batch 127 batch_loss: 0.1008777916431427\n",
      "training: 6 batch 128 batch_loss: 0.10191535949707031\n",
      "training: 6 batch 129 batch_loss: 0.10235190391540527\n",
      "training: 6 batch 130 batch_loss: 0.10289967060089111\n",
      "training: 6 batch 131 batch_loss: 0.10211771726608276\n",
      "training: 6 batch 132 batch_loss: 0.10165825486183167\n",
      "training: 6 batch 133 batch_loss: 0.10263165831565857\n",
      "training: 6 batch 134 batch_loss: 0.09968101978302002\n",
      "training: 6 batch 135 batch_loss: 0.10425376892089844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 6 batch 136 batch_loss: 0.10256040096282959\n",
      "training: 6 batch 137 batch_loss: 0.10079655051231384\n",
      "training: 6 batch 138 batch_loss: 0.10346665978431702\n",
      "training: 6 batch 139 batch_loss: 0.10347741842269897\n",
      "training: 6 batch 140 batch_loss: 0.10192656517028809\n",
      "training: 6 batch 141 batch_loss: 0.10155680775642395\n",
      "training: 6 batch 142 batch_loss: 0.099153071641922\n",
      "training: 6 batch 143 batch_loss: 0.10023230314254761\n",
      "training: 6 batch 144 batch_loss: 0.10057467222213745\n",
      "training: 6 batch 145 batch_loss: 0.10223537683486938\n",
      "training: 6 batch 146 batch_loss: 0.10083138942718506\n",
      "training: 6 batch 147 batch_loss: 0.10147354006767273\n",
      "training: 6 batch 148 batch_loss: 0.10046783089637756\n",
      "training: 6 batch 149 batch_loss: 0.09775185585021973\n",
      "training: 6 batch 150 batch_loss: 0.10161742568016052\n",
      "training: 6 batch 151 batch_loss: 0.10316038131713867\n",
      "training: 6 batch 152 batch_loss: 0.0988757312297821\n",
      "training: 6 batch 153 batch_loss: 0.10316860675811768\n",
      "training: 6 batch 154 batch_loss: 0.10279509425163269\n",
      "training: 6 batch 155 batch_loss: 0.0998048484325409\n",
      "training: 6 batch 156 batch_loss: 0.10335558652877808\n",
      "training: 6 batch 157 batch_loss: 0.10073503851890564\n",
      "training: 6 batch 158 batch_loss: 0.1005905270576477\n",
      "training: 6 batch 159 batch_loss: 0.1021432876586914\n",
      "training: 6 batch 160 batch_loss: 0.10255694389343262\n",
      "training: 6 batch 161 batch_loss: 0.10111087560653687\n",
      "training: 6 batch 162 batch_loss: 0.10253334045410156\n",
      "training: 6 batch 163 batch_loss: 0.09992486238479614\n",
      "training: 6 batch 164 batch_loss: 0.10048970580101013\n",
      "training: 6 batch 165 batch_loss: 0.10334664583206177\n",
      "training: 6 batch 166 batch_loss: 0.09975337982177734\n",
      "training: 6 batch 167 batch_loss: 0.10131034255027771\n",
      "training: 6 batch 168 batch_loss: 0.1008138656616211\n",
      "training: 6 batch 169 batch_loss: 0.09975400567054749\n",
      "training: 6 batch 170 batch_loss: 0.10174605250358582\n",
      "training: 6 batch 171 batch_loss: 0.10169464349746704\n",
      "training: 6 batch 172 batch_loss: 0.10135143995285034\n",
      "training: 6 batch 173 batch_loss: 0.10207250714302063\n",
      "training: 6 batch 174 batch_loss: 0.1000073254108429\n",
      "training: 6 batch 175 batch_loss: 0.10024610161781311\n",
      "training: 6 batch 176 batch_loss: 0.10389214754104614\n",
      "training: 6 batch 177 batch_loss: 0.1006840169429779\n",
      "training: 6 batch 178 batch_loss: 0.10043168067932129\n",
      "training: 6 batch 179 batch_loss: 0.09903162717819214\n",
      "training: 6 batch 180 batch_loss: 0.10158059000968933\n",
      "training: 6 batch 181 batch_loss: 0.10079145431518555\n",
      "training: 6 batch 182 batch_loss: 0.10288438200950623\n",
      "training: 6 batch 183 batch_loss: 0.10325366258621216\n",
      "training: 6 batch 184 batch_loss: 0.09961086511611938\n",
      "training: 6 batch 185 batch_loss: 0.10362863540649414\n",
      "training: 6 batch 186 batch_loss: 0.10395640134811401\n",
      "training: 6 batch 187 batch_loss: 0.10029518604278564\n",
      "training: 6 batch 188 batch_loss: 0.10060879588127136\n",
      "training: 6 batch 189 batch_loss: 0.10271763801574707\n",
      "training: 6 batch 190 batch_loss: 0.10203421115875244\n",
      "training: 6 batch 191 batch_loss: 0.1004677414894104\n",
      "training: 6 batch 192 batch_loss: 0.10087475180625916\n",
      "training: 6 batch 193 batch_loss: 0.10352325439453125\n",
      "training: 6 batch 194 batch_loss: 0.10093241930007935\n",
      "training: 6 batch 195 batch_loss: 0.10015901923179626\n",
      "training: 6 batch 196 batch_loss: 0.10066619515419006\n",
      "training: 6 batch 197 batch_loss: 0.10315895080566406\n",
      "training: 6 batch 198 batch_loss: 0.1031004786491394\n",
      "training: 6 batch 199 batch_loss: 0.10243463516235352\n",
      "training: 6 batch 200 batch_loss: 0.10198768973350525\n",
      "training: 6 batch 201 batch_loss: 0.10165584087371826\n",
      "training: 6 batch 202 batch_loss: 0.10442602634429932\n",
      "training: 6 batch 203 batch_loss: 0.10220450162887573\n",
      "training: 6 batch 204 batch_loss: 0.10262882709503174\n",
      "training: 6 batch 205 batch_loss: 0.10146456956863403\n",
      "training: 6 batch 206 batch_loss: 0.0999581515789032\n",
      "training: 6 batch 207 batch_loss: 0.10080528259277344\n",
      "training: 6 batch 208 batch_loss: 0.10061711072921753\n",
      "training: 6 batch 209 batch_loss: 0.10035267472267151\n",
      "training: 6 batch 210 batch_loss: 0.10215452313423157\n",
      "training: 6 batch 211 batch_loss: 0.10534650087356567\n",
      "training: 6 batch 212 batch_loss: 0.10279148817062378\n",
      "training: 6 batch 213 batch_loss: 0.10020944476127625\n",
      "training: 6 batch 214 batch_loss: 0.09880492091178894\n",
      "training: 6 batch 215 batch_loss: 0.09906896948814392\n",
      "training: 6 batch 216 batch_loss: 0.10423314571380615\n",
      "training: 6 batch 217 batch_loss: 0.09940952062606812\n",
      "training: 6 batch 218 batch_loss: 0.10041019320487976\n",
      "training: 6 batch 219 batch_loss: 0.09990978240966797\n",
      "training: 6 batch 220 batch_loss: 0.10380002856254578\n",
      "training: 6 batch 221 batch_loss: 0.09881585836410522\n",
      "training: 6 batch 222 batch_loss: 0.09934559464454651\n",
      "training: 6 batch 223 batch_loss: 0.10445153713226318\n",
      "training: 6 batch 224 batch_loss: 0.10209473967552185\n",
      "training: 6 batch 225 batch_loss: 0.1028016209602356\n",
      "training: 6 batch 226 batch_loss: 0.10404551029205322\n",
      "training: 6 batch 227 batch_loss: 0.10150527954101562\n",
      "training: 6 batch 228 batch_loss: 0.10270887613296509\n",
      "training: 6 batch 229 batch_loss: 0.10419011116027832\n",
      "training: 6 batch 230 batch_loss: 0.09853017330169678\n",
      "training: 6 batch 231 batch_loss: 0.10303366184234619\n",
      "training: 6 batch 232 batch_loss: 0.10111787915229797\n",
      "training: 6 batch 233 batch_loss: 0.09946736693382263\n",
      "training: 6 batch 234 batch_loss: 0.10246741771697998\n",
      "training: 6 batch 235 batch_loss: 0.10365712642669678\n",
      "training: 6 batch 236 batch_loss: 0.1023373007774353\n",
      "training: 6 batch 237 batch_loss: 0.10113599896430969\n",
      "training: 6 batch 238 batch_loss: 0.10242637991905212\n",
      "training: 6 batch 239 batch_loss: 0.0989428460597992\n",
      "training: 6 batch 240 batch_loss: 0.10103192925453186\n",
      "training: 6 batch 241 batch_loss: 0.10277411341667175\n",
      "training: 6 batch 242 batch_loss: 0.09873554110527039\n",
      "training: 6 batch 243 batch_loss: 0.10413163900375366\n",
      "training: 6 batch 244 batch_loss: 0.10308694839477539\n",
      "training: 6 batch 245 batch_loss: 0.10124263167381287\n",
      "training: 6 batch 246 batch_loss: 0.10377258062362671\n",
      "training: 6 batch 247 batch_loss: 0.10122162103652954\n",
      "training: 6 batch 248 batch_loss: 0.10255232453346252\n",
      "training: 6 batch 249 batch_loss: 0.10268199443817139\n",
      "training: 6 batch 250 batch_loss: 0.10187733173370361\n",
      "training: 6 batch 251 batch_loss: 0.10252609848976135\n",
      "training: 6 batch 252 batch_loss: 0.09904715418815613\n",
      "training: 6 batch 253 batch_loss: 0.1014651358127594\n",
      "training: 6 batch 254 batch_loss: 0.1031353771686554\n",
      "training: 6 batch 255 batch_loss: 0.1023416519165039\n",
      "training: 6 batch 256 batch_loss: 0.1029910147190094\n",
      "training: 6 batch 257 batch_loss: 0.09966501593589783\n",
      "training: 6 batch 258 batch_loss: 0.09956908226013184\n",
      "training: 6 batch 259 batch_loss: 0.10098952054977417\n",
      "training: 6 batch 260 batch_loss: 0.10174182057380676\n",
      "training: 6 batch 261 batch_loss: 0.10246652364730835\n",
      "training: 6 batch 262 batch_loss: 0.09982132911682129\n",
      "training: 6 batch 263 batch_loss: 0.10336330533027649\n",
      "training: 6 batch 264 batch_loss: 0.10386836528778076\n",
      "training: 6 batch 265 batch_loss: 0.10198545455932617\n",
      "training: 6 batch 266 batch_loss: 0.1008722186088562\n",
      "training: 6 batch 267 batch_loss: 0.10184350609779358\n",
      "training: 6 batch 268 batch_loss: 0.10341891646385193\n",
      "training: 6 batch 269 batch_loss: 0.10155674815177917\n",
      "training: 6 batch 270 batch_loss: 0.09980887174606323\n",
      "training: 6 batch 271 batch_loss: 0.09888413548469543\n",
      "training: 6 batch 272 batch_loss: 0.1037055253982544\n",
      "training: 6 batch 273 batch_loss: 0.10117873549461365\n",
      "training: 6 batch 274 batch_loss: 0.10134989023208618\n",
      "training: 6 batch 275 batch_loss: 0.10111767053604126\n",
      "training: 6 batch 276 batch_loss: 0.10078051686286926\n",
      "training: 6 batch 277 batch_loss: 0.1018994152545929\n",
      "training: 6 batch 278 batch_loss: 0.10538864135742188\n",
      "training: 6 batch 279 batch_loss: 0.10332340002059937\n",
      "training: 6 batch 280 batch_loss: 0.10255759954452515\n",
      "training: 6 batch 281 batch_loss: 0.10322737693786621\n",
      "training: 6 batch 282 batch_loss: 0.10182362794876099\n",
      "training: 6 batch 283 batch_loss: 0.09940892457962036\n",
      "training: 6 batch 284 batch_loss: 0.09990531206130981\n",
      "training: 6 batch 285 batch_loss: 0.10122960805892944\n",
      "training: 6 batch 286 batch_loss: 0.10367128252983093\n",
      "training: 6 batch 287 batch_loss: 0.10031536221504211\n",
      "training: 6 batch 288 batch_loss: 0.10246974229812622\n",
      "training: 6 batch 289 batch_loss: 0.10058337450027466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 6 batch 290 batch_loss: 0.10256129503250122\n",
      "training: 6 batch 291 batch_loss: 0.10161620378494263\n",
      "training: 6 batch 292 batch_loss: 0.10167080163955688\n",
      "training: 6 batch 293 batch_loss: 0.10075706243515015\n",
      "training: 6 batch 294 batch_loss: 0.10240808129310608\n",
      "training: 6 batch 295 batch_loss: 0.1000957190990448\n",
      "training: 6 batch 296 batch_loss: 0.09923028945922852\n",
      "training: 6 batch 297 batch_loss: 0.10523656010627747\n",
      "training: 6 batch 298 batch_loss: 0.10336422920227051\n",
      "training: 6 batch 299 batch_loss: 0.10851487517356873\n",
      "training: 6 batch 300 batch_loss: 0.10105431079864502\n",
      "training: 6 batch 301 batch_loss: 0.10216650366783142\n",
      "training: 6 batch 302 batch_loss: 0.10386231541633606\n",
      "training: 6 batch 303 batch_loss: 0.10176020860671997\n",
      "training: 6 batch 304 batch_loss: 0.1020355224609375\n",
      "training: 6 batch 305 batch_loss: 0.10497954487800598\n",
      "training: 6 batch 306 batch_loss: 0.10096266865730286\n",
      "training: 6 batch 307 batch_loss: 0.10305586457252502\n",
      "training: 6 batch 308 batch_loss: 0.10121950507164001\n",
      "training: 6 batch 309 batch_loss: 0.10220065712928772\n",
      "training: 6 batch 310 batch_loss: 0.10232502222061157\n",
      "training: 6 batch 311 batch_loss: 0.1015433669090271\n",
      "training: 6 batch 312 batch_loss: 0.10040009021759033\n",
      "training: 6 batch 313 batch_loss: 0.10290426015853882\n",
      "training: 6 batch 314 batch_loss: 0.10112243890762329\n",
      "training: 6 batch 315 batch_loss: 0.10520994663238525\n",
      "training: 6 batch 316 batch_loss: 0.10061821341514587\n",
      "training: 6 batch 317 batch_loss: 0.10404324531555176\n",
      "training: 6 batch 318 batch_loss: 0.10339334607124329\n",
      "training: 6 batch 319 batch_loss: 0.10112035274505615\n",
      "training: 6 batch 320 batch_loss: 0.10202664136886597\n",
      "training: 6 batch 321 batch_loss: 0.10243940353393555\n",
      "training: 6 batch 322 batch_loss: 0.1009625792503357\n",
      "training: 6 batch 323 batch_loss: 0.10506832599639893\n",
      "training: 6 batch 324 batch_loss: 0.1017380952835083\n",
      "training: 6 batch 325 batch_loss: 0.10239273309707642\n",
      "training: 6 batch 326 batch_loss: 0.10295233130455017\n",
      "training: 6 batch 327 batch_loss: 0.10262170433998108\n",
      "training: 6 batch 328 batch_loss: 0.10014382004737854\n",
      "training: 6 batch 329 batch_loss: 0.10111525654792786\n",
      "training: 6 batch 330 batch_loss: 0.10301652550697327\n",
      "training: 6 batch 331 batch_loss: 0.10592123866081238\n",
      "training: 6 batch 332 batch_loss: 0.10303422808647156\n",
      "training: 6 batch 333 batch_loss: 0.10328149795532227\n",
      "training: 6 batch 334 batch_loss: 0.1049601137638092\n",
      "training: 6 batch 335 batch_loss: 0.1039937436580658\n",
      "training: 6 batch 336 batch_loss: 0.10162019729614258\n",
      "training: 6 batch 337 batch_loss: 0.10111075639724731\n",
      "training: 6 batch 338 batch_loss: 0.10278519988059998\n",
      "training: 6 batch 339 batch_loss: 0.10382029414176941\n",
      "training: 6 batch 340 batch_loss: 0.10137170553207397\n",
      "training: 6 batch 341 batch_loss: 0.10423773527145386\n",
      "training: 6 batch 342 batch_loss: 0.10223519802093506\n",
      "training: 6 batch 343 batch_loss: 0.10431525111198425\n",
      "training: 6 batch 344 batch_loss: 0.10298168659210205\n",
      "training: 6 batch 345 batch_loss: 0.10426592826843262\n",
      "training: 6 batch 346 batch_loss: 0.10579878091812134\n",
      "training: 6 batch 347 batch_loss: 0.10281115770339966\n",
      "training: 6 batch 348 batch_loss: 0.10237684845924377\n",
      "training: 6 batch 349 batch_loss: 0.10295358300209045\n",
      "training: 6 batch 350 batch_loss: 0.10087853670120239\n",
      "training: 6 batch 351 batch_loss: 0.10345643758773804\n",
      "training: 6 batch 352 batch_loss: 0.103234201669693\n",
      "training: 6 batch 353 batch_loss: 0.10100182890892029\n",
      "training: 6 batch 354 batch_loss: 0.10085362195968628\n",
      "training: 6 batch 355 batch_loss: 0.10442280769348145\n",
      "training: 6 batch 356 batch_loss: 0.1055942177772522\n",
      "training: 6 batch 357 batch_loss: 0.10400909185409546\n",
      "training: 6 batch 358 batch_loss: 0.10101887583732605\n",
      "training: 6 batch 359 batch_loss: 0.10307103395462036\n",
      "training: 6 batch 360 batch_loss: 0.10480213165283203\n",
      "training: 6 batch 361 batch_loss: 0.10236462950706482\n",
      "training: 6 batch 362 batch_loss: 0.10339021682739258\n",
      "training: 6 batch 363 batch_loss: 0.10328084230422974\n",
      "training: 6 batch 364 batch_loss: 0.10278138518333435\n",
      "training: 6 batch 365 batch_loss: 0.10373187065124512\n",
      "training: 6 batch 366 batch_loss: 0.10158714652061462\n",
      "training: 6 batch 367 batch_loss: 0.10127848386764526\n",
      "training: 6 batch 368 batch_loss: 0.1022489070892334\n",
      "training: 6 batch 369 batch_loss: 0.10224378108978271\n",
      "training: 6 batch 370 batch_loss: 0.10246792435646057\n",
      "training: 6 batch 371 batch_loss: 0.10263460874557495\n",
      "training: 6 batch 372 batch_loss: 0.10200336575508118\n",
      "training: 6 batch 373 batch_loss: 0.1011885404586792\n",
      "training: 6 batch 374 batch_loss: 0.10598614811897278\n",
      "training: 6 batch 375 batch_loss: 0.10183772444725037\n",
      "training: 6 batch 376 batch_loss: 0.10151216387748718\n",
      "training: 6 batch 377 batch_loss: 0.10339388251304626\n",
      "training: 6 batch 378 batch_loss: 0.10168460011482239\n",
      "training: 6 batch 379 batch_loss: 0.10780128836631775\n",
      "training: 6 batch 380 batch_loss: 0.10647410154342651\n",
      "training: 6 batch 381 batch_loss: 0.1054944097995758\n",
      "training: 6 batch 382 batch_loss: 0.10455068945884705\n",
      "training: 6 batch 383 batch_loss: 0.1037900447845459\n",
      "training: 6 batch 384 batch_loss: 0.10594740509986877\n",
      "training: 6 batch 385 batch_loss: 0.1006249487400055\n",
      "training: 6 batch 386 batch_loss: 0.10261094570159912\n",
      "training: 6 batch 387 batch_loss: 0.1041375994682312\n",
      "training: 6 batch 388 batch_loss: 0.10564512014389038\n",
      "training: 6 batch 389 batch_loss: 0.10318383574485779\n",
      "training: 6 batch 390 batch_loss: 0.10305041074752808\n",
      "training: 6 batch 391 batch_loss: 0.10500189661979675\n",
      "training: 6 batch 392 batch_loss: 0.10223188996315002\n",
      "training: 6 batch 393 batch_loss: 0.10426858067512512\n",
      "training: 6 batch 394 batch_loss: 0.1036035418510437\n",
      "training: 6 batch 395 batch_loss: 0.10519558191299438\n",
      "training: 6 batch 396 batch_loss: 0.10215342044830322\n",
      "training: 6 batch 397 batch_loss: 0.10418969392776489\n",
      "training: 6 batch 398 batch_loss: 0.10398128628730774\n",
      "training: 6 batch 399 batch_loss: 0.10411158204078674\n",
      "training: 6 batch 400 batch_loss: 0.10287618637084961\n",
      "training: 6 batch 401 batch_loss: 0.1022484302520752\n",
      "training: 6 batch 402 batch_loss: 0.10341817140579224\n",
      "training: 6 batch 403 batch_loss: 0.10195016860961914\n",
      "training: 6 batch 404 batch_loss: 0.10263592004776001\n",
      "training: 6 batch 405 batch_loss: 0.10232621431350708\n",
      "training: 6 batch 406 batch_loss: 0.10242089629173279\n",
      "training: 6 batch 407 batch_loss: 0.10368627309799194\n",
      "training: 6 batch 408 batch_loss: 0.10494694113731384\n",
      "training: 6 batch 409 batch_loss: 0.10384517908096313\n",
      "training: 6 batch 410 batch_loss: 0.1002458930015564\n",
      "training: 6 batch 411 batch_loss: 0.10705310106277466\n",
      "training: 6 batch 412 batch_loss: 0.10385879874229431\n",
      "training: 6 batch 413 batch_loss: 0.10361576080322266\n",
      "training: 6 batch 414 batch_loss: 0.10086727142333984\n",
      "training: 6 batch 415 batch_loss: 0.10155218839645386\n",
      "training: 6 batch 416 batch_loss: 0.09948739409446716\n",
      "training: 6 batch 417 batch_loss: 0.10547506809234619\n",
      "training: 6 batch 418 batch_loss: 0.10258987545967102\n",
      "training: 6 batch 419 batch_loss: 0.09985005855560303\n",
      "training: 6 batch 420 batch_loss: 0.10588479042053223\n",
      "training: 6 batch 421 batch_loss: 0.10519441962242126\n",
      "training: 6 batch 422 batch_loss: 0.10224485397338867\n",
      "training: 6 batch 423 batch_loss: 0.10211461782455444\n",
      "training: 6 batch 424 batch_loss: 0.10337045788764954\n",
      "training: 6 batch 425 batch_loss: 0.10295361280441284\n",
      "training: 6 batch 426 batch_loss: 0.10536462068557739\n",
      "training: 6 batch 427 batch_loss: 0.10426604747772217\n",
      "training: 6 batch 428 batch_loss: 0.10220170021057129\n",
      "training: 6 batch 429 batch_loss: 0.10568839311599731\n",
      "training: 6 batch 430 batch_loss: 0.10565608739852905\n",
      "training: 6 batch 431 batch_loss: 0.10310107469558716\n",
      "training: 6 batch 432 batch_loss: 0.10222616791725159\n",
      "training: 6 batch 433 batch_loss: 0.10281229019165039\n",
      "training: 6 batch 434 batch_loss: 0.10523012280464172\n",
      "training: 6 batch 435 batch_loss: 0.10050806403160095\n",
      "training: 6 batch 436 batch_loss: 0.1022542417049408\n",
      "training: 6 batch 437 batch_loss: 0.10445049405097961\n",
      "training: 6 batch 438 batch_loss: 0.10497817397117615\n",
      "training: 6 batch 439 batch_loss: 0.10259246826171875\n",
      "training: 6 batch 440 batch_loss: 0.10430917143821716\n",
      "training: 6 batch 441 batch_loss: 0.10186830163002014\n",
      "training: 6 batch 442 batch_loss: 0.10561156272888184\n",
      "training: 6 batch 443 batch_loss: 0.10093766450881958\n",
      "training: 6 batch 444 batch_loss: 0.1000361442565918\n",
      "training: 6 batch 445 batch_loss: 0.10253733396530151\n",
      "training: 6 batch 446 batch_loss: 0.10594719648361206\n",
      "training: 6 batch 447 batch_loss: 0.10079315304756165\n",
      "training: 6 batch 448 batch_loss: 0.10390600562095642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 6 batch 449 batch_loss: 0.10515505075454712\n",
      "training: 6 batch 450 batch_loss: 0.10471653938293457\n",
      "training: 6 batch 451 batch_loss: 0.1061963140964508\n",
      "training: 6 batch 452 batch_loss: 0.10475057363510132\n",
      "training: 6 batch 453 batch_loss: 0.10480111837387085\n",
      "training: 6 batch 454 batch_loss: 0.1011715829372406\n",
      "training: 6 batch 455 batch_loss: 0.10304558277130127\n",
      "training: 6 batch 456 batch_loss: 0.10597431659698486\n",
      "training: 6 batch 457 batch_loss: 0.10168516635894775\n",
      "training: 6 batch 458 batch_loss: 0.10498464107513428\n",
      "training: 6 batch 459 batch_loss: 0.10466551780700684\n",
      "training: 6 batch 460 batch_loss: 0.10431316494941711\n",
      "training: 6 batch 461 batch_loss: 0.10532915592193604\n",
      "training: 6 batch 462 batch_loss: 0.10257989168167114\n",
      "training: 6 batch 463 batch_loss: 0.10322588682174683\n",
      "training: 6 batch 464 batch_loss: 0.1059231162071228\n",
      "training: 6 batch 465 batch_loss: 0.10648688673973083\n",
      "training: 6 batch 466 batch_loss: 0.10688024759292603\n",
      "training: 6 batch 467 batch_loss: 0.10519933700561523\n",
      "training: 6 batch 468 batch_loss: 0.10398361086845398\n",
      "training: 6 batch 469 batch_loss: 0.10091584920883179\n",
      "training: 6 batch 470 batch_loss: 0.10104390978813171\n",
      "training: 6 batch 471 batch_loss: 0.10305711627006531\n",
      "training: 6 batch 472 batch_loss: 0.10264632105827332\n",
      "training: 6 batch 473 batch_loss: 0.10585838556289673\n",
      "training: 6 batch 474 batch_loss: 0.10628366470336914\n",
      "training: 6 batch 475 batch_loss: 0.10611766576766968\n",
      "training: 6 batch 476 batch_loss: 0.10406669974327087\n",
      "training: 6 batch 477 batch_loss: 0.1043458878993988\n",
      "training: 6 batch 478 batch_loss: 0.09903883934020996\n",
      "training: 6 batch 479 batch_loss: 0.10465747117996216\n",
      "training: 6 batch 480 batch_loss: 0.10198843479156494\n",
      "training: 6 batch 481 batch_loss: 0.1014256477355957\n",
      "training: 6 batch 482 batch_loss: 0.10349875688552856\n",
      "training: 6 batch 483 batch_loss: 0.10520634055137634\n",
      "training: 6 batch 484 batch_loss: 0.10303997993469238\n",
      "training: 6 batch 485 batch_loss: 0.10412672162055969\n",
      "training: 6 batch 486 batch_loss: 0.10651078820228577\n",
      "training: 6 batch 487 batch_loss: 0.10423171520233154\n",
      "training: 6 batch 488 batch_loss: 0.10142645239830017\n",
      "training: 6 batch 489 batch_loss: 0.10437119007110596\n",
      "training: 6 batch 490 batch_loss: 0.10363861918449402\n",
      "training: 6 batch 491 batch_loss: 0.10619127750396729\n",
      "training: 6 batch 492 batch_loss: 0.10427999496459961\n",
      "training: 6 batch 493 batch_loss: 0.1067328155040741\n",
      "training: 6 batch 494 batch_loss: 0.10548844933509827\n",
      "training: 6 batch 495 batch_loss: 0.10600021481513977\n",
      "training: 6 batch 496 batch_loss: 0.1012546718120575\n",
      "training: 6 batch 497 batch_loss: 0.10626339912414551\n",
      "training: 6 batch 498 batch_loss: 0.10277929902076721\n",
      "training: 6 batch 499 batch_loss: 0.10406306385993958\n",
      "training: 6 batch 500 batch_loss: 0.10283991694450378\n",
      "training: 6 batch 501 batch_loss: 0.10386142134666443\n",
      "training: 6 batch 502 batch_loss: 0.10775598883628845\n",
      "training: 6 batch 503 batch_loss: 0.10400855541229248\n",
      "training: 6 batch 504 batch_loss: 0.10402581095695496\n",
      "training: 6 batch 505 batch_loss: 0.10284596681594849\n",
      "training: 6 batch 506 batch_loss: 0.10298830270767212\n",
      "training: 6 batch 507 batch_loss: 0.1044090986251831\n",
      "training: 6 batch 508 batch_loss: 0.10214480757713318\n",
      "training: 6 batch 509 batch_loss: 0.10272923111915588\n",
      "training: 6 batch 510 batch_loss: 0.10590839385986328\n",
      "training: 6 batch 511 batch_loss: 0.10526314377784729\n",
      "training: 6 batch 512 batch_loss: 0.10154598951339722\n",
      "training: 6 batch 513 batch_loss: 0.10458126664161682\n",
      "training: 6 batch 514 batch_loss: 0.10441538691520691\n",
      "training: 6 batch 515 batch_loss: 0.10522690415382385\n",
      "training: 6 batch 516 batch_loss: 0.10737231373786926\n",
      "training: 6 batch 517 batch_loss: 0.1060861349105835\n",
      "training: 6 batch 518 batch_loss: 0.10464456677436829\n",
      "training: 6 batch 519 batch_loss: 0.10333535075187683\n",
      "training: 6 batch 520 batch_loss: 0.10393211245536804\n",
      "training: 6 batch 521 batch_loss: 0.10747897624969482\n",
      "training: 6 batch 522 batch_loss: 0.10645642876625061\n",
      "training: 6 batch 523 batch_loss: 0.10320654511451721\n",
      "training: 6 batch 524 batch_loss: 0.10416179895401001\n",
      "training: 6 batch 525 batch_loss: 0.10906633734703064\n",
      "training: 6 batch 526 batch_loss: 0.10457712411880493\n",
      "training: 6 batch 527 batch_loss: 0.10314643383026123\n",
      "training: 6 batch 528 batch_loss: 0.10615605115890503\n",
      "training: 6 batch 529 batch_loss: 0.10300186276435852\n",
      "training: 6 batch 530 batch_loss: 0.10361772775650024\n",
      "training: 6 batch 531 batch_loss: 0.10394561290740967\n",
      "training: 6 batch 532 batch_loss: 0.10564666986465454\n",
      "training: 6 batch 533 batch_loss: 0.10388064384460449\n",
      "training: 6 batch 534 batch_loss: 0.10569360852241516\n",
      "training: 6 batch 535 batch_loss: 0.1080903708934784\n",
      "training: 6 batch 536 batch_loss: 0.10400465130805969\n",
      "training: 6 batch 537 batch_loss: 0.10627219080924988\n",
      "training: 6 batch 538 batch_loss: 0.10285183787345886\n",
      "training: 6 batch 539 batch_loss: 0.10452094674110413\n",
      "training: 6 batch 540 batch_loss: 0.10411939024925232\n",
      "training: 6 batch 541 batch_loss: 0.10734051465988159\n",
      "training: 6 batch 542 batch_loss: 0.10235071182250977\n",
      "training: 6 batch 543 batch_loss: 0.10237428545951843\n",
      "training: 6 batch 544 batch_loss: 0.10418444871902466\n",
      "training: 6 batch 545 batch_loss: 0.10272827744483948\n",
      "training: 6 batch 546 batch_loss: 0.10665345191955566\n",
      "training: 6 batch 547 batch_loss: 0.10237491130828857\n",
      "training: 6 batch 548 batch_loss: 0.10457432270050049\n",
      "training: 6 batch 549 batch_loss: 0.10452762246131897\n",
      "training: 6 batch 550 batch_loss: 0.10625314712524414\n",
      "training: 6 batch 551 batch_loss: 0.10647988319396973\n",
      "training: 6 batch 552 batch_loss: 0.10380175709724426\n",
      "training: 6 batch 553 batch_loss: 0.10403066873550415\n",
      "training: 6 batch 554 batch_loss: 0.1035776138305664\n",
      "training: 6 batch 555 batch_loss: 0.10326358675956726\n",
      "training: 6 batch 556 batch_loss: 0.10533702373504639\n",
      "training: 6 batch 557 batch_loss: 0.10435867309570312\n",
      "training: 6 batch 558 batch_loss: 0.10104784369468689\n",
      "training: 6 batch 559 batch_loss: 0.10569459199905396\n",
      "training: 6 batch 560 batch_loss: 0.10252675414085388\n",
      "training: 6 batch 561 batch_loss: 0.10623928904533386\n",
      "training: 6 batch 562 batch_loss: 0.10355126857757568\n",
      "training: 6 batch 563 batch_loss: 0.1058904230594635\n",
      "training: 6 batch 564 batch_loss: 0.10371538996696472\n",
      "training: 6 batch 565 batch_loss: 0.10583153367042542\n",
      "training: 6 batch 566 batch_loss: 0.10323894023895264\n",
      "training: 6 batch 567 batch_loss: 0.10548323392868042\n",
      "training: 6 batch 568 batch_loss: 0.10611549019813538\n",
      "training: 6 batch 569 batch_loss: 0.10564211010932922\n",
      "training: 6 batch 570 batch_loss: 0.10156828165054321\n",
      "training: 6 batch 571 batch_loss: 0.10719087719917297\n",
      "training: 6 batch 572 batch_loss: 0.1062612235546112\n",
      "training: 6 batch 573 batch_loss: 0.10641562938690186\n",
      "training: 6 batch 574 batch_loss: 0.10580429434776306\n",
      "training: 6 batch 575 batch_loss: 0.10375148057937622\n",
      "training: 6 batch 576 batch_loss: 0.10696807503700256\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 6, Hit Ratio:0.021442871752505886 | Precision:0.03163766833775681 | Recall:0.046223290856755805 | NDCG:0.039758016668452086\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 7 batch 0 batch_loss: 0.10174927115440369\n",
      "training: 7 batch 1 batch_loss: 0.10641202330589294\n",
      "training: 7 batch 2 batch_loss: 0.10233253240585327\n",
      "training: 7 batch 3 batch_loss: 0.10239660739898682\n",
      "training: 7 batch 4 batch_loss: 0.10313445329666138\n",
      "training: 7 batch 5 batch_loss: 0.10133031010627747\n",
      "training: 7 batch 6 batch_loss: 0.10479417443275452\n",
      "training: 7 batch 7 batch_loss: 0.10320082306861877\n",
      "training: 7 batch 8 batch_loss: 0.10358893871307373\n",
      "training: 7 batch 9 batch_loss: 0.10342779755592346\n",
      "training: 7 batch 10 batch_loss: 0.10377472639083862\n",
      "training: 7 batch 11 batch_loss: 0.10392719507217407\n",
      "training: 7 batch 12 batch_loss: 0.10353255271911621\n",
      "training: 7 batch 13 batch_loss: 0.10303163528442383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 7 batch 14 batch_loss: 0.10236182808876038\n",
      "training: 7 batch 15 batch_loss: 0.10357847809791565\n",
      "training: 7 batch 16 batch_loss: 0.10312503576278687\n",
      "training: 7 batch 17 batch_loss: 0.10378211736679077\n",
      "training: 7 batch 18 batch_loss: 0.1053052544593811\n",
      "training: 7 batch 19 batch_loss: 0.10439825057983398\n",
      "training: 7 batch 20 batch_loss: 0.10494044423103333\n",
      "training: 7 batch 21 batch_loss: 0.1039198637008667\n",
      "training: 7 batch 22 batch_loss: 0.10615432262420654\n",
      "training: 7 batch 23 batch_loss: 0.10350978374481201\n",
      "training: 7 batch 24 batch_loss: 0.10316741466522217\n",
      "training: 7 batch 25 batch_loss: 0.10320764780044556\n",
      "training: 7 batch 26 batch_loss: 0.10265278816223145\n",
      "training: 7 batch 27 batch_loss: 0.1058732271194458\n",
      "training: 7 batch 28 batch_loss: 0.10441771149635315\n",
      "training: 7 batch 29 batch_loss: 0.10376709699630737\n",
      "training: 7 batch 30 batch_loss: 0.10272997617721558\n",
      "training: 7 batch 31 batch_loss: 0.1021384596824646\n",
      "training: 7 batch 32 batch_loss: 0.10371437668800354\n",
      "training: 7 batch 33 batch_loss: 0.10246431827545166\n",
      "training: 7 batch 34 batch_loss: 0.10534203052520752\n",
      "training: 7 batch 35 batch_loss: 0.10109496116638184\n",
      "training: 7 batch 36 batch_loss: 0.10469174385070801\n",
      "training: 7 batch 37 batch_loss: 0.10692885518074036\n",
      "training: 7 batch 38 batch_loss: 0.10330092906951904\n",
      "training: 7 batch 39 batch_loss: 0.10330483317375183\n",
      "training: 7 batch 40 batch_loss: 0.10455629229545593\n",
      "training: 7 batch 41 batch_loss: 0.10300067067146301\n",
      "training: 7 batch 42 batch_loss: 0.10480165481567383\n",
      "training: 7 batch 43 batch_loss: 0.10459291934967041\n",
      "training: 7 batch 44 batch_loss: 0.10597515106201172\n",
      "training: 7 batch 45 batch_loss: 0.10695433616638184\n",
      "training: 7 batch 46 batch_loss: 0.10323905944824219\n",
      "training: 7 batch 47 batch_loss: 0.10539820790290833\n",
      "training: 7 batch 48 batch_loss: 0.10251453518867493\n",
      "training: 7 batch 49 batch_loss: 0.10177597403526306\n",
      "training: 7 batch 50 batch_loss: 0.10417228937149048\n",
      "training: 7 batch 51 batch_loss: 0.10085049271583557\n",
      "training: 7 batch 52 batch_loss: 0.1058911383152008\n",
      "training: 7 batch 53 batch_loss: 0.10274237394332886\n",
      "training: 7 batch 54 batch_loss: 0.10535117983818054\n",
      "training: 7 batch 55 batch_loss: 0.10318341851234436\n",
      "training: 7 batch 56 batch_loss: 0.10241717100143433\n",
      "training: 7 batch 57 batch_loss: 0.10470500588417053\n",
      "training: 7 batch 58 batch_loss: 0.10367241501808167\n",
      "training: 7 batch 59 batch_loss: 0.105444997549057\n",
      "training: 7 batch 60 batch_loss: 0.10427215695381165\n",
      "training: 7 batch 61 batch_loss: 0.10467788577079773\n",
      "training: 7 batch 62 batch_loss: 0.10639491677284241\n",
      "training: 7 batch 63 batch_loss: 0.10080105066299438\n",
      "training: 7 batch 64 batch_loss: 0.10482388734817505\n",
      "training: 7 batch 65 batch_loss: 0.10340029001235962\n",
      "training: 7 batch 66 batch_loss: 0.10397893190383911\n",
      "training: 7 batch 67 batch_loss: 0.10331958532333374\n",
      "training: 7 batch 68 batch_loss: 0.10494139790534973\n",
      "training: 7 batch 69 batch_loss: 0.10680437088012695\n",
      "training: 7 batch 70 batch_loss: 0.103655606508255\n",
      "training: 7 batch 71 batch_loss: 0.1050419807434082\n",
      "training: 7 batch 72 batch_loss: 0.10516837239265442\n",
      "training: 7 batch 73 batch_loss: 0.1028141975402832\n",
      "training: 7 batch 74 batch_loss: 0.10617730021476746\n",
      "training: 7 batch 75 batch_loss: 0.1019241213798523\n",
      "training: 7 batch 76 batch_loss: 0.1046217679977417\n",
      "training: 7 batch 77 batch_loss: 0.10721832513809204\n",
      "training: 7 batch 78 batch_loss: 0.10633176565170288\n",
      "training: 7 batch 79 batch_loss: 0.10289770364761353\n",
      "training: 7 batch 80 batch_loss: 0.10620790719985962\n",
      "training: 7 batch 81 batch_loss: 0.10534447431564331\n",
      "training: 7 batch 82 batch_loss: 0.10465359687805176\n",
      "training: 7 batch 83 batch_loss: 0.10382384061813354\n",
      "training: 7 batch 84 batch_loss: 0.10780811309814453\n",
      "training: 7 batch 85 batch_loss: 0.10712689161300659\n",
      "training: 7 batch 86 batch_loss: 0.10708171129226685\n",
      "training: 7 batch 87 batch_loss: 0.10611233115196228\n",
      "training: 7 batch 88 batch_loss: 0.10639333724975586\n",
      "training: 7 batch 89 batch_loss: 0.10276281833648682\n",
      "training: 7 batch 90 batch_loss: 0.10722914338111877\n",
      "training: 7 batch 91 batch_loss: 0.10429733991622925\n",
      "training: 7 batch 92 batch_loss: 0.10377484560012817\n",
      "training: 7 batch 93 batch_loss: 0.10627597570419312\n",
      "training: 7 batch 94 batch_loss: 0.10437148809432983\n",
      "training: 7 batch 95 batch_loss: 0.10336881875991821\n",
      "training: 7 batch 96 batch_loss: 0.10326492786407471\n",
      "training: 7 batch 97 batch_loss: 0.10483121871948242\n",
      "training: 7 batch 98 batch_loss: 0.10494798421859741\n",
      "training: 7 batch 99 batch_loss: 0.10455390810966492\n",
      "training: 7 batch 100 batch_loss: 0.10293987393379211\n",
      "training: 7 batch 101 batch_loss: 0.10250860452651978\n",
      "training: 7 batch 102 batch_loss: 0.10611525177955627\n",
      "training: 7 batch 103 batch_loss: 0.10561305284500122\n",
      "training: 7 batch 104 batch_loss: 0.10685721039772034\n",
      "training: 7 batch 105 batch_loss: 0.10572418570518494\n",
      "training: 7 batch 106 batch_loss: 0.10293588042259216\n",
      "training: 7 batch 107 batch_loss: 0.10752835869789124\n",
      "training: 7 batch 108 batch_loss: 0.10498037934303284\n",
      "training: 7 batch 109 batch_loss: 0.1078452467918396\n",
      "training: 7 batch 110 batch_loss: 0.10396221280097961\n",
      "training: 7 batch 111 batch_loss: 0.10237768292427063\n",
      "training: 7 batch 112 batch_loss: 0.10297957062721252\n",
      "training: 7 batch 113 batch_loss: 0.10460790991783142\n",
      "training: 7 batch 114 batch_loss: 0.10442450642585754\n",
      "training: 7 batch 115 batch_loss: 0.1023591160774231\n",
      "training: 7 batch 116 batch_loss: 0.10278993844985962\n",
      "training: 7 batch 117 batch_loss: 0.10522031784057617\n",
      "training: 7 batch 118 batch_loss: 0.1076623797416687\n",
      "training: 7 batch 119 batch_loss: 0.10494446754455566\n",
      "training: 7 batch 120 batch_loss: 0.10399818420410156\n",
      "training: 7 batch 121 batch_loss: 0.10825923085212708\n",
      "training: 7 batch 122 batch_loss: 0.1030576229095459\n",
      "training: 7 batch 123 batch_loss: 0.10459393262863159\n",
      "training: 7 batch 124 batch_loss: 0.10613837838172913\n",
      "training: 7 batch 125 batch_loss: 0.10709601640701294\n",
      "training: 7 batch 126 batch_loss: 0.1052386462688446\n",
      "training: 7 batch 127 batch_loss: 0.1027018129825592\n",
      "training: 7 batch 128 batch_loss: 0.10444924235343933\n",
      "training: 7 batch 129 batch_loss: 0.10845458507537842\n",
      "training: 7 batch 130 batch_loss: 0.10842511057853699\n",
      "training: 7 batch 131 batch_loss: 0.10667014122009277\n",
      "training: 7 batch 132 batch_loss: 0.10695737600326538\n",
      "training: 7 batch 133 batch_loss: 0.10448068380355835\n",
      "training: 7 batch 134 batch_loss: 0.10352221131324768\n",
      "training: 7 batch 135 batch_loss: 0.10539352893829346\n",
      "training: 7 batch 136 batch_loss: 0.10321182012557983\n",
      "training: 7 batch 137 batch_loss: 0.1051294207572937\n",
      "training: 7 batch 138 batch_loss: 0.10588577389717102\n",
      "training: 7 batch 139 batch_loss: 0.1025853157043457\n",
      "training: 7 batch 140 batch_loss: 0.10549351572990417\n",
      "training: 7 batch 141 batch_loss: 0.10242357850074768\n",
      "training: 7 batch 142 batch_loss: 0.10499098896980286\n",
      "training: 7 batch 143 batch_loss: 0.10545393824577332\n",
      "training: 7 batch 144 batch_loss: 0.10646069049835205\n",
      "training: 7 batch 145 batch_loss: 0.10553038120269775\n",
      "training: 7 batch 146 batch_loss: 0.10423725843429565\n",
      "training: 7 batch 147 batch_loss: 0.10239532589912415\n",
      "training: 7 batch 148 batch_loss: 0.10735595226287842\n",
      "training: 7 batch 149 batch_loss: 0.10503366589546204\n",
      "training: 7 batch 150 batch_loss: 0.1066930890083313\n",
      "training: 7 batch 151 batch_loss: 0.10543417930603027\n",
      "training: 7 batch 152 batch_loss: 0.10444357991218567\n",
      "training: 7 batch 153 batch_loss: 0.10365697741508484\n",
      "training: 7 batch 154 batch_loss: 0.10550889372825623\n",
      "training: 7 batch 155 batch_loss: 0.10394227504730225\n",
      "training: 7 batch 156 batch_loss: 0.10425460338592529\n",
      "training: 7 batch 157 batch_loss: 0.10699889063835144\n",
      "training: 7 batch 158 batch_loss: 0.10451334714889526\n",
      "training: 7 batch 159 batch_loss: 0.10306280851364136\n",
      "training: 7 batch 160 batch_loss: 0.10585886240005493\n",
      "training: 7 batch 161 batch_loss: 0.10551515221595764\n",
      "training: 7 batch 162 batch_loss: 0.10431462526321411\n",
      "training: 7 batch 163 batch_loss: 0.10719966888427734\n",
      "training: 7 batch 164 batch_loss: 0.10681867599487305\n",
      "training: 7 batch 165 batch_loss: 0.10345381498336792\n",
      "training: 7 batch 166 batch_loss: 0.10393482446670532\n",
      "training: 7 batch 167 batch_loss: 0.10381728410720825\n",
      "training: 7 batch 168 batch_loss: 0.10243403911590576\n",
      "training: 7 batch 169 batch_loss: 0.10526919364929199\n",
      "training: 7 batch 170 batch_loss: 0.10404616594314575\n",
      "training: 7 batch 171 batch_loss: 0.10622626543045044\n",
      "training: 7 batch 172 batch_loss: 0.1053047776222229\n",
      "training: 7 batch 173 batch_loss: 0.10296913981437683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 7 batch 174 batch_loss: 0.10480055212974548\n",
      "training: 7 batch 175 batch_loss: 0.10555925965309143\n",
      "training: 7 batch 176 batch_loss: 0.10455048084259033\n",
      "training: 7 batch 177 batch_loss: 0.10575941205024719\n",
      "training: 7 batch 178 batch_loss: 0.10609424114227295\n",
      "training: 7 batch 179 batch_loss: 0.10492128133773804\n",
      "training: 7 batch 180 batch_loss: 0.10509321093559265\n",
      "training: 7 batch 181 batch_loss: 0.10596698522567749\n",
      "training: 7 batch 182 batch_loss: 0.10375294089317322\n",
      "training: 7 batch 183 batch_loss: 0.10583740472793579\n",
      "training: 7 batch 184 batch_loss: 0.1029672920703888\n",
      "training: 7 batch 185 batch_loss: 0.1064879298210144\n",
      "training: 7 batch 186 batch_loss: 0.10730507969856262\n",
      "training: 7 batch 187 batch_loss: 0.10446768999099731\n",
      "training: 7 batch 188 batch_loss: 0.10468041896820068\n",
      "training: 7 batch 189 batch_loss: 0.10235074162483215\n",
      "training: 7 batch 190 batch_loss: 0.10547500848770142\n",
      "training: 7 batch 191 batch_loss: 0.10538136959075928\n",
      "training: 7 batch 192 batch_loss: 0.10666924715042114\n",
      "training: 7 batch 193 batch_loss: 0.10239508748054504\n",
      "training: 7 batch 194 batch_loss: 0.10678493976593018\n",
      "training: 7 batch 195 batch_loss: 0.10297635197639465\n",
      "training: 7 batch 196 batch_loss: 0.10742595791816711\n",
      "training: 7 batch 197 batch_loss: 0.10371130704879761\n",
      "training: 7 batch 198 batch_loss: 0.11082741618156433\n",
      "training: 7 batch 199 batch_loss: 0.10461762547492981\n",
      "training: 7 batch 200 batch_loss: 0.10266020894050598\n",
      "training: 7 batch 201 batch_loss: 0.1064683198928833\n",
      "training: 7 batch 202 batch_loss: 0.10689690709114075\n",
      "training: 7 batch 203 batch_loss: 0.10625162720680237\n",
      "training: 7 batch 204 batch_loss: 0.10635125637054443\n",
      "training: 7 batch 205 batch_loss: 0.10826325416564941\n",
      "training: 7 batch 206 batch_loss: 0.10962024331092834\n",
      "training: 7 batch 207 batch_loss: 0.10580599308013916\n",
      "training: 7 batch 208 batch_loss: 0.1063576340675354\n",
      "training: 7 batch 209 batch_loss: 0.10306495428085327\n",
      "training: 7 batch 210 batch_loss: 0.10620564222335815\n",
      "training: 7 batch 211 batch_loss: 0.10720911622047424\n",
      "training: 7 batch 212 batch_loss: 0.10558322072029114\n",
      "training: 7 batch 213 batch_loss: 0.10764878988265991\n",
      "training: 7 batch 214 batch_loss: 0.10637706518173218\n",
      "training: 7 batch 215 batch_loss: 0.10633406043052673\n",
      "training: 7 batch 216 batch_loss: 0.1053454577922821\n",
      "training: 7 batch 217 batch_loss: 0.10499238967895508\n",
      "training: 7 batch 218 batch_loss: 0.1026497483253479\n",
      "training: 7 batch 219 batch_loss: 0.1071423590183258\n",
      "training: 7 batch 220 batch_loss: 0.10704457759857178\n",
      "training: 7 batch 221 batch_loss: 0.10637372732162476\n",
      "training: 7 batch 222 batch_loss: 0.1061423122882843\n",
      "training: 7 batch 223 batch_loss: 0.10093271732330322\n",
      "training: 7 batch 224 batch_loss: 0.10523277521133423\n",
      "training: 7 batch 225 batch_loss: 0.10569947957992554\n",
      "training: 7 batch 226 batch_loss: 0.10297870635986328\n",
      "training: 7 batch 227 batch_loss: 0.1055801510810852\n",
      "training: 7 batch 228 batch_loss: 0.10549312829971313\n",
      "training: 7 batch 229 batch_loss: 0.1067640483379364\n",
      "training: 7 batch 230 batch_loss: 0.10439056158065796\n",
      "training: 7 batch 231 batch_loss: 0.10756537318229675\n",
      "training: 7 batch 232 batch_loss: 0.1057889461517334\n",
      "training: 7 batch 233 batch_loss: 0.1042310893535614\n",
      "training: 7 batch 234 batch_loss: 0.1045607328414917\n",
      "training: 7 batch 235 batch_loss: 0.10756933689117432\n",
      "training: 7 batch 236 batch_loss: 0.10606265068054199\n",
      "training: 7 batch 237 batch_loss: 0.1085067093372345\n",
      "training: 7 batch 238 batch_loss: 0.10743439197540283\n",
      "training: 7 batch 239 batch_loss: 0.10718286037445068\n",
      "training: 7 batch 240 batch_loss: 0.10493230819702148\n",
      "training: 7 batch 241 batch_loss: 0.10567876696586609\n",
      "training: 7 batch 242 batch_loss: 0.10771334171295166\n",
      "training: 7 batch 243 batch_loss: 0.10470712184906006\n",
      "training: 7 batch 244 batch_loss: 0.10759007930755615\n",
      "training: 7 batch 245 batch_loss: 0.10430875420570374\n",
      "training: 7 batch 246 batch_loss: 0.10557818412780762\n",
      "training: 7 batch 247 batch_loss: 0.10351160168647766\n",
      "training: 7 batch 248 batch_loss: 0.10354277491569519\n",
      "training: 7 batch 249 batch_loss: 0.10466033220291138\n",
      "training: 7 batch 250 batch_loss: 0.10604071617126465\n",
      "training: 7 batch 251 batch_loss: 0.1047426164150238\n",
      "training: 7 batch 252 batch_loss: 0.10402590036392212\n",
      "training: 7 batch 253 batch_loss: 0.1064576804637909\n",
      "training: 7 batch 254 batch_loss: 0.10606208443641663\n",
      "training: 7 batch 255 batch_loss: 0.10709413886070251\n",
      "training: 7 batch 256 batch_loss: 0.1048465371131897\n",
      "training: 7 batch 257 batch_loss: 0.1049073338508606\n",
      "training: 7 batch 258 batch_loss: 0.10584929585456848\n",
      "training: 7 batch 259 batch_loss: 0.10611644387245178\n",
      "training: 7 batch 260 batch_loss: 0.1065039336681366\n",
      "training: 7 batch 261 batch_loss: 0.10678821802139282\n",
      "training: 7 batch 262 batch_loss: 0.1058725118637085\n",
      "training: 7 batch 263 batch_loss: 0.10554271936416626\n",
      "training: 7 batch 264 batch_loss: 0.10552433133125305\n",
      "training: 7 batch 265 batch_loss: 0.10648611187934875\n",
      "training: 7 batch 266 batch_loss: 0.10693433880805969\n",
      "training: 7 batch 267 batch_loss: 0.10615471005439758\n",
      "training: 7 batch 268 batch_loss: 0.10352063179016113\n",
      "training: 7 batch 269 batch_loss: 0.10598897933959961\n",
      "training: 7 batch 270 batch_loss: 0.10735470056533813\n",
      "training: 7 batch 271 batch_loss: 0.10627669095993042\n",
      "training: 7 batch 272 batch_loss: 0.10768526792526245\n",
      "training: 7 batch 273 batch_loss: 0.10620078444480896\n",
      "training: 7 batch 274 batch_loss: 0.10715612769126892\n",
      "training: 7 batch 275 batch_loss: 0.10892659425735474\n",
      "training: 7 batch 276 batch_loss: 0.10735860466957092\n",
      "training: 7 batch 277 batch_loss: 0.10514175891876221\n",
      "training: 7 batch 278 batch_loss: 0.10634690523147583\n",
      "training: 7 batch 279 batch_loss: 0.10725033283233643\n",
      "training: 7 batch 280 batch_loss: 0.10684129595756531\n",
      "training: 7 batch 281 batch_loss: 0.10408151149749756\n",
      "training: 7 batch 282 batch_loss: 0.10603448748588562\n",
      "training: 7 batch 283 batch_loss: 0.1059744656085968\n",
      "training: 7 batch 284 batch_loss: 0.10451185703277588\n",
      "training: 7 batch 285 batch_loss: 0.10705429315567017\n",
      "training: 7 batch 286 batch_loss: 0.1056833565235138\n",
      "training: 7 batch 287 batch_loss: 0.10412269830703735\n",
      "training: 7 batch 288 batch_loss: 0.10742169618606567\n",
      "training: 7 batch 289 batch_loss: 0.10322079062461853\n",
      "training: 7 batch 290 batch_loss: 0.10713079571723938\n",
      "training: 7 batch 291 batch_loss: 0.10624957084655762\n",
      "training: 7 batch 292 batch_loss: 0.10505330562591553\n",
      "training: 7 batch 293 batch_loss: 0.10503658652305603\n",
      "training: 7 batch 294 batch_loss: 0.10925406217575073\n",
      "training: 7 batch 295 batch_loss: 0.10489717125892639\n",
      "training: 7 batch 296 batch_loss: 0.10487979650497437\n",
      "training: 7 batch 297 batch_loss: 0.10463005304336548\n",
      "training: 7 batch 298 batch_loss: 0.10678547620773315\n",
      "training: 7 batch 299 batch_loss: 0.1055593490600586\n",
      "training: 7 batch 300 batch_loss: 0.10857704281806946\n",
      "training: 7 batch 301 batch_loss: 0.10369998216629028\n",
      "training: 7 batch 302 batch_loss: 0.10610690712928772\n",
      "training: 7 batch 303 batch_loss: 0.10652026534080505\n",
      "training: 7 batch 304 batch_loss: 0.10758066177368164\n",
      "training: 7 batch 305 batch_loss: 0.1064491868019104\n",
      "training: 7 batch 306 batch_loss: 0.10568389296531677\n",
      "training: 7 batch 307 batch_loss: 0.10570377111434937\n",
      "training: 7 batch 308 batch_loss: 0.10544899106025696\n",
      "training: 7 batch 309 batch_loss: 0.10840508341789246\n",
      "training: 7 batch 310 batch_loss: 0.10604885220527649\n",
      "training: 7 batch 311 batch_loss: 0.10578367114067078\n",
      "training: 7 batch 312 batch_loss: 0.10710245370864868\n",
      "training: 7 batch 313 batch_loss: 0.10653296113014221\n",
      "training: 7 batch 314 batch_loss: 0.10799092054367065\n",
      "training: 7 batch 315 batch_loss: 0.10976055264472961\n",
      "training: 7 batch 316 batch_loss: 0.10628324747085571\n",
      "training: 7 batch 317 batch_loss: 0.10686784982681274\n",
      "training: 7 batch 318 batch_loss: 0.10724326968193054\n",
      "training: 7 batch 319 batch_loss: 0.10665646195411682\n",
      "training: 7 batch 320 batch_loss: 0.10664838552474976\n",
      "training: 7 batch 321 batch_loss: 0.10732007026672363\n",
      "training: 7 batch 322 batch_loss: 0.10715225338935852\n",
      "training: 7 batch 323 batch_loss: 0.10727730393409729\n",
      "training: 7 batch 324 batch_loss: 0.10579541325569153\n",
      "training: 7 batch 325 batch_loss: 0.10438123345375061\n",
      "training: 7 batch 326 batch_loss: 0.10743799805641174\n",
      "training: 7 batch 327 batch_loss: 0.10609006881713867\n",
      "training: 7 batch 328 batch_loss: 0.1050681471824646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 7 batch 329 batch_loss: 0.10504081845283508\n",
      "training: 7 batch 330 batch_loss: 0.10679563879966736\n",
      "training: 7 batch 331 batch_loss: 0.10856187343597412\n",
      "training: 7 batch 332 batch_loss: 0.10553660988807678\n",
      "training: 7 batch 333 batch_loss: 0.1053362488746643\n",
      "training: 7 batch 334 batch_loss: 0.10716050863265991\n",
      "training: 7 batch 335 batch_loss: 0.10447737574577332\n",
      "training: 7 batch 336 batch_loss: 0.10533690452575684\n",
      "training: 7 batch 337 batch_loss: 0.10491719841957092\n",
      "training: 7 batch 338 batch_loss: 0.10631868243217468\n",
      "training: 7 batch 339 batch_loss: 0.10612863302230835\n",
      "training: 7 batch 340 batch_loss: 0.10363078117370605\n",
      "training: 7 batch 341 batch_loss: 0.107707679271698\n",
      "training: 7 batch 342 batch_loss: 0.1047849953174591\n",
      "training: 7 batch 343 batch_loss: 0.10504069924354553\n",
      "training: 7 batch 344 batch_loss: 0.10651832818984985\n",
      "training: 7 batch 345 batch_loss: 0.10810187458992004\n",
      "training: 7 batch 346 batch_loss: 0.11059927940368652\n",
      "training: 7 batch 347 batch_loss: 0.11071974039077759\n",
      "training: 7 batch 348 batch_loss: 0.10680133104324341\n",
      "training: 7 batch 349 batch_loss: 0.10554343461990356\n",
      "training: 7 batch 350 batch_loss: 0.10617184638977051\n",
      "training: 7 batch 351 batch_loss: 0.10713085532188416\n",
      "training: 7 batch 352 batch_loss: 0.10522711277008057\n",
      "training: 7 batch 353 batch_loss: 0.10851225256919861\n",
      "training: 7 batch 354 batch_loss: 0.10480007529258728\n",
      "training: 7 batch 355 batch_loss: 0.10631036758422852\n",
      "training: 7 batch 356 batch_loss: 0.10715606808662415\n",
      "training: 7 batch 357 batch_loss: 0.10699829459190369\n",
      "training: 7 batch 358 batch_loss: 0.10714942216873169\n",
      "training: 7 batch 359 batch_loss: 0.10832160711288452\n",
      "training: 7 batch 360 batch_loss: 0.1051868200302124\n",
      "training: 7 batch 361 batch_loss: 0.10693991184234619\n",
      "training: 7 batch 362 batch_loss: 0.10550478100776672\n",
      "training: 7 batch 363 batch_loss: 0.10604286193847656\n",
      "training: 7 batch 364 batch_loss: 0.10506510734558105\n",
      "training: 7 batch 365 batch_loss: 0.10671201348304749\n",
      "training: 7 batch 366 batch_loss: 0.10362660884857178\n",
      "training: 7 batch 367 batch_loss: 0.10643109679222107\n",
      "training: 7 batch 368 batch_loss: 0.106366366147995\n",
      "training: 7 batch 369 batch_loss: 0.108117014169693\n",
      "training: 7 batch 370 batch_loss: 0.10676127672195435\n",
      "training: 7 batch 371 batch_loss: 0.10701790452003479\n",
      "training: 7 batch 372 batch_loss: 0.1077379584312439\n",
      "training: 7 batch 373 batch_loss: 0.10778763890266418\n",
      "training: 7 batch 374 batch_loss: 0.10779806971549988\n",
      "training: 7 batch 375 batch_loss: 0.10769623517990112\n",
      "training: 7 batch 376 batch_loss: 0.10459566116333008\n",
      "training: 7 batch 377 batch_loss: 0.10755592584609985\n",
      "training: 7 batch 378 batch_loss: 0.10903874039649963\n",
      "training: 7 batch 379 batch_loss: 0.10421335697174072\n",
      "training: 7 batch 380 batch_loss: 0.10764726996421814\n",
      "training: 7 batch 381 batch_loss: 0.10836523771286011\n",
      "training: 7 batch 382 batch_loss: 0.10738974809646606\n",
      "training: 7 batch 383 batch_loss: 0.10455363988876343\n",
      "training: 7 batch 384 batch_loss: 0.10497358441352844\n",
      "training: 7 batch 385 batch_loss: 0.10868841409683228\n",
      "training: 7 batch 386 batch_loss: 0.10649046301841736\n",
      "training: 7 batch 387 batch_loss: 0.10737508535385132\n",
      "training: 7 batch 388 batch_loss: 0.10992100834846497\n",
      "training: 7 batch 389 batch_loss: 0.10626605153083801\n",
      "training: 7 batch 390 batch_loss: 0.10688936710357666\n",
      "training: 7 batch 391 batch_loss: 0.10761573910713196\n",
      "training: 7 batch 392 batch_loss: 0.1049150824546814\n",
      "training: 7 batch 393 batch_loss: 0.10830569267272949\n",
      "training: 7 batch 394 batch_loss: 0.10840991139411926\n",
      "training: 7 batch 395 batch_loss: 0.10447144508361816\n",
      "training: 7 batch 396 batch_loss: 0.10548961162567139\n",
      "training: 7 batch 397 batch_loss: 0.10650914907455444\n",
      "training: 7 batch 398 batch_loss: 0.10596305131912231\n",
      "training: 7 batch 399 batch_loss: 0.1087002158164978\n",
      "training: 7 batch 400 batch_loss: 0.10675573348999023\n",
      "training: 7 batch 401 batch_loss: 0.10737839341163635\n",
      "training: 7 batch 402 batch_loss: 0.10341429710388184\n",
      "training: 7 batch 403 batch_loss: 0.10697358846664429\n",
      "training: 7 batch 404 batch_loss: 0.1071532666683197\n",
      "training: 7 batch 405 batch_loss: 0.10797762870788574\n",
      "training: 7 batch 406 batch_loss: 0.10530716180801392\n",
      "training: 7 batch 407 batch_loss: 0.10653895139694214\n",
      "training: 7 batch 408 batch_loss: 0.1058359146118164\n",
      "training: 7 batch 409 batch_loss: 0.10590952634811401\n",
      "training: 7 batch 410 batch_loss: 0.10723617672920227\n",
      "training: 7 batch 411 batch_loss: 0.10564273595809937\n",
      "training: 7 batch 412 batch_loss: 0.1076464056968689\n",
      "training: 7 batch 413 batch_loss: 0.10676965117454529\n",
      "training: 7 batch 414 batch_loss: 0.10743334889411926\n",
      "training: 7 batch 415 batch_loss: 0.10547769069671631\n",
      "training: 7 batch 416 batch_loss: 0.10684233903884888\n",
      "training: 7 batch 417 batch_loss: 0.1081463098526001\n",
      "training: 7 batch 418 batch_loss: 0.10787928104400635\n",
      "training: 7 batch 419 batch_loss: 0.10111907124519348\n",
      "training: 7 batch 420 batch_loss: 0.1059388816356659\n",
      "training: 7 batch 421 batch_loss: 0.10709971189498901\n",
      "training: 7 batch 422 batch_loss: 0.10763269662857056\n",
      "training: 7 batch 423 batch_loss: 0.11151829361915588\n",
      "training: 7 batch 424 batch_loss: 0.10511195659637451\n",
      "training: 7 batch 425 batch_loss: 0.1059141755104065\n",
      "training: 7 batch 426 batch_loss: 0.1044580340385437\n",
      "training: 7 batch 427 batch_loss: 0.10959309339523315\n",
      "training: 7 batch 428 batch_loss: 0.10585376620292664\n",
      "training: 7 batch 429 batch_loss: 0.10656899213790894\n",
      "training: 7 batch 430 batch_loss: 0.10671338438987732\n",
      "training: 7 batch 431 batch_loss: 0.10930991172790527\n",
      "training: 7 batch 432 batch_loss: 0.10827368497848511\n",
      "training: 7 batch 433 batch_loss: 0.10765039920806885\n",
      "training: 7 batch 434 batch_loss: 0.1072620153427124\n",
      "training: 7 batch 435 batch_loss: 0.10758191347122192\n",
      "training: 7 batch 436 batch_loss: 0.10722321271896362\n",
      "training: 7 batch 437 batch_loss: 0.11013767123222351\n",
      "training: 7 batch 438 batch_loss: 0.11073264479637146\n",
      "training: 7 batch 439 batch_loss: 0.1056097149848938\n",
      "training: 7 batch 440 batch_loss: 0.10653406381607056\n",
      "training: 7 batch 441 batch_loss: 0.11023387312889099\n",
      "training: 7 batch 442 batch_loss: 0.10871312022209167\n",
      "training: 7 batch 443 batch_loss: 0.1068192720413208\n",
      "training: 7 batch 444 batch_loss: 0.10833302140235901\n",
      "training: 7 batch 445 batch_loss: 0.1065714955329895\n",
      "training: 7 batch 446 batch_loss: 0.10836508870124817\n",
      "training: 7 batch 447 batch_loss: 0.10599088668823242\n",
      "training: 7 batch 448 batch_loss: 0.10648146271705627\n",
      "training: 7 batch 449 batch_loss: 0.1056588888168335\n",
      "training: 7 batch 450 batch_loss: 0.10684677958488464\n",
      "training: 7 batch 451 batch_loss: 0.10417312383651733\n",
      "training: 7 batch 452 batch_loss: 0.10704594850540161\n",
      "training: 7 batch 453 batch_loss: 0.10707667469978333\n",
      "training: 7 batch 454 batch_loss: 0.10789895057678223\n",
      "training: 7 batch 455 batch_loss: 0.10777842998504639\n",
      "training: 7 batch 456 batch_loss: 0.10840928554534912\n",
      "training: 7 batch 457 batch_loss: 0.10620978474617004\n",
      "training: 7 batch 458 batch_loss: 0.10728335380554199\n",
      "training: 7 batch 459 batch_loss: 0.10523560643196106\n",
      "training: 7 batch 460 batch_loss: 0.1073814332485199\n",
      "training: 7 batch 461 batch_loss: 0.10789301991462708\n",
      "training: 7 batch 462 batch_loss: 0.10574981570243835\n",
      "training: 7 batch 463 batch_loss: 0.1075066328048706\n",
      "training: 7 batch 464 batch_loss: 0.10894310474395752\n",
      "training: 7 batch 465 batch_loss: 0.11023658514022827\n",
      "training: 7 batch 466 batch_loss: 0.1080450713634491\n",
      "training: 7 batch 467 batch_loss: 0.1074722409248352\n",
      "training: 7 batch 468 batch_loss: 0.10707014799118042\n",
      "training: 7 batch 469 batch_loss: 0.10622060298919678\n",
      "training: 7 batch 470 batch_loss: 0.10721665620803833\n",
      "training: 7 batch 471 batch_loss: 0.10864049196243286\n",
      "training: 7 batch 472 batch_loss: 0.10937413573265076\n",
      "training: 7 batch 473 batch_loss: 0.10623693466186523\n",
      "training: 7 batch 474 batch_loss: 0.10931092500686646\n",
      "training: 7 batch 475 batch_loss: 0.10519126057624817\n",
      "training: 7 batch 476 batch_loss: 0.10889831185340881\n",
      "training: 7 batch 477 batch_loss: 0.10840648412704468\n",
      "training: 7 batch 478 batch_loss: 0.10825219750404358\n",
      "training: 7 batch 479 batch_loss: 0.10715001821517944\n",
      "training: 7 batch 480 batch_loss: 0.1071183979511261\n",
      "training: 7 batch 481 batch_loss: 0.10998079180717468\n",
      "training: 7 batch 482 batch_loss: 0.10388314723968506\n",
      "training: 7 batch 483 batch_loss: 0.1080746054649353\n",
      "training: 7 batch 484 batch_loss: 0.10836344957351685\n",
      "training: 7 batch 485 batch_loss: 0.10964006185531616\n",
      "training: 7 batch 486 batch_loss: 0.10646229982376099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 7 batch 487 batch_loss: 0.10760605335235596\n",
      "training: 7 batch 488 batch_loss: 0.10746511816978455\n",
      "training: 7 batch 489 batch_loss: 0.10974740982055664\n",
      "training: 7 batch 490 batch_loss: 0.10908663272857666\n",
      "training: 7 batch 491 batch_loss: 0.10877391695976257\n",
      "training: 7 batch 492 batch_loss: 0.10683399438858032\n",
      "training: 7 batch 493 batch_loss: 0.10558643937110901\n",
      "training: 7 batch 494 batch_loss: 0.11174291372299194\n",
      "training: 7 batch 495 batch_loss: 0.10568380355834961\n",
      "training: 7 batch 496 batch_loss: 0.10727006196975708\n",
      "training: 7 batch 497 batch_loss: 0.10589206218719482\n",
      "training: 7 batch 498 batch_loss: 0.10660505294799805\n",
      "training: 7 batch 499 batch_loss: 0.1085200309753418\n",
      "training: 7 batch 500 batch_loss: 0.10832583904266357\n",
      "training: 7 batch 501 batch_loss: 0.11010599136352539\n",
      "training: 7 batch 502 batch_loss: 0.10536789894104004\n",
      "training: 7 batch 503 batch_loss: 0.10656625032424927\n",
      "training: 7 batch 504 batch_loss: 0.108244389295578\n",
      "training: 7 batch 505 batch_loss: 0.10785776376724243\n",
      "training: 7 batch 506 batch_loss: 0.10758617520332336\n",
      "training: 7 batch 507 batch_loss: 0.10788613557815552\n",
      "training: 7 batch 508 batch_loss: 0.10593116283416748\n",
      "training: 7 batch 509 batch_loss: 0.10768502950668335\n",
      "training: 7 batch 510 batch_loss: 0.10590338706970215\n",
      "training: 7 batch 511 batch_loss: 0.1093035340309143\n",
      "training: 7 batch 512 batch_loss: 0.10931757092475891\n",
      "training: 7 batch 513 batch_loss: 0.11171403527259827\n",
      "training: 7 batch 514 batch_loss: 0.1062922477722168\n",
      "training: 7 batch 515 batch_loss: 0.10607314109802246\n",
      "training: 7 batch 516 batch_loss: 0.11019361019134521\n",
      "training: 7 batch 517 batch_loss: 0.11038914322853088\n",
      "training: 7 batch 518 batch_loss: 0.10619723796844482\n",
      "training: 7 batch 519 batch_loss: 0.10480445623397827\n",
      "training: 7 batch 520 batch_loss: 0.10701140761375427\n",
      "training: 7 batch 521 batch_loss: 0.10672169923782349\n",
      "training: 7 batch 522 batch_loss: 0.10670110583305359\n",
      "training: 7 batch 523 batch_loss: 0.1086198091506958\n",
      "training: 7 batch 524 batch_loss: 0.10953313112258911\n",
      "training: 7 batch 525 batch_loss: 0.10726135969161987\n",
      "training: 7 batch 526 batch_loss: 0.1082964539527893\n",
      "training: 7 batch 527 batch_loss: 0.10551947355270386\n",
      "training: 7 batch 528 batch_loss: 0.10715577006340027\n",
      "training: 7 batch 529 batch_loss: 0.1088016927242279\n",
      "training: 7 batch 530 batch_loss: 0.10804787278175354\n",
      "training: 7 batch 531 batch_loss: 0.10877403616905212\n",
      "training: 7 batch 532 batch_loss: 0.10776811838150024\n",
      "training: 7 batch 533 batch_loss: 0.10810244083404541\n",
      "training: 7 batch 534 batch_loss: 0.10762724280357361\n",
      "training: 7 batch 535 batch_loss: 0.11033228039741516\n",
      "training: 7 batch 536 batch_loss: 0.10961386561393738\n",
      "training: 7 batch 537 batch_loss: 0.10803747177124023\n",
      "training: 7 batch 538 batch_loss: 0.10540500283241272\n",
      "training: 7 batch 539 batch_loss: 0.10893741250038147\n",
      "training: 7 batch 540 batch_loss: 0.11070767045021057\n",
      "training: 7 batch 541 batch_loss: 0.10970500111579895\n",
      "training: 7 batch 542 batch_loss: 0.10818237066268921\n",
      "training: 7 batch 543 batch_loss: 0.10683810710906982\n",
      "training: 7 batch 544 batch_loss: 0.11156642436981201\n",
      "training: 7 batch 545 batch_loss: 0.10859286785125732\n",
      "training: 7 batch 546 batch_loss: 0.10951417684555054\n",
      "training: 7 batch 547 batch_loss: 0.10751059651374817\n",
      "training: 7 batch 548 batch_loss: 0.10707718133926392\n",
      "training: 7 batch 549 batch_loss: 0.10830706357955933\n",
      "training: 7 batch 550 batch_loss: 0.10892057418823242\n",
      "training: 7 batch 551 batch_loss: 0.10755857825279236\n",
      "training: 7 batch 552 batch_loss: 0.10834139585494995\n",
      "training: 7 batch 553 batch_loss: 0.11063963174819946\n",
      "training: 7 batch 554 batch_loss: 0.10968762636184692\n",
      "training: 7 batch 555 batch_loss: 0.10880520939826965\n",
      "training: 7 batch 556 batch_loss: 0.10925242304801941\n",
      "training: 7 batch 557 batch_loss: 0.10745677351951599\n",
      "training: 7 batch 558 batch_loss: 0.10976600646972656\n",
      "training: 7 batch 559 batch_loss: 0.10857659578323364\n",
      "training: 7 batch 560 batch_loss: 0.10878077149391174\n",
      "training: 7 batch 561 batch_loss: 0.10966658592224121\n",
      "training: 7 batch 562 batch_loss: 0.10820099711418152\n",
      "training: 7 batch 563 batch_loss: 0.10817307233810425\n",
      "training: 7 batch 564 batch_loss: 0.10608997941017151\n",
      "training: 7 batch 565 batch_loss: 0.10853525996208191\n",
      "training: 7 batch 566 batch_loss: 0.11269769072532654\n",
      "training: 7 batch 567 batch_loss: 0.10954451560974121\n",
      "training: 7 batch 568 batch_loss: 0.10930824279785156\n",
      "training: 7 batch 569 batch_loss: 0.10832172632217407\n",
      "training: 7 batch 570 batch_loss: 0.10778111219406128\n",
      "training: 7 batch 571 batch_loss: 0.11133527755737305\n",
      "training: 7 batch 572 batch_loss: 0.10889190435409546\n",
      "training: 7 batch 573 batch_loss: 0.10956162214279175\n",
      "training: 7 batch 574 batch_loss: 0.10750868916511536\n",
      "training: 7 batch 575 batch_loss: 0.1072385311126709\n",
      "training: 7 batch 576 batch_loss: 0.10611668229103088\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 7, Hit Ratio:0.021815965062476475 | Precision:0.03218814508994397 | Recall:0.04716230934257042 | NDCG:0.040471962326654454\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 8 batch 0 batch_loss: 0.10629251599311829\n",
      "training: 8 batch 1 batch_loss: 0.10424625873565674\n",
      "training: 8 batch 2 batch_loss: 0.10788458585739136\n",
      "training: 8 batch 3 batch_loss: 0.10356086492538452\n",
      "training: 8 batch 4 batch_loss: 0.10622933506965637\n",
      "training: 8 batch 5 batch_loss: 0.10813310742378235\n",
      "training: 8 batch 6 batch_loss: 0.10641980171203613\n",
      "training: 8 batch 7 batch_loss: 0.10556507110595703\n",
      "training: 8 batch 8 batch_loss: 0.10555565357208252\n",
      "training: 8 batch 9 batch_loss: 0.10813841223716736\n",
      "training: 8 batch 10 batch_loss: 0.10863882303237915\n",
      "training: 8 batch 11 batch_loss: 0.10536611080169678\n",
      "training: 8 batch 12 batch_loss: 0.10616549849510193\n",
      "training: 8 batch 13 batch_loss: 0.10998305678367615\n",
      "training: 8 batch 14 batch_loss: 0.10582664608955383\n",
      "training: 8 batch 15 batch_loss: 0.10630142688751221\n",
      "training: 8 batch 16 batch_loss: 0.10630989074707031\n",
      "training: 8 batch 17 batch_loss: 0.10871559381484985\n",
      "training: 8 batch 18 batch_loss: 0.10637259483337402\n",
      "training: 8 batch 19 batch_loss: 0.10747972130775452\n",
      "training: 8 batch 20 batch_loss: 0.1069478988647461\n",
      "training: 8 batch 21 batch_loss: 0.10751423239707947\n",
      "training: 8 batch 22 batch_loss: 0.10628330707550049\n",
      "training: 8 batch 23 batch_loss: 0.10587304830551147\n",
      "training: 8 batch 24 batch_loss: 0.1089085042476654\n",
      "training: 8 batch 25 batch_loss: 0.10661020874977112\n",
      "training: 8 batch 26 batch_loss: 0.10907578468322754\n",
      "training: 8 batch 27 batch_loss: 0.10754722356796265\n",
      "training: 8 batch 28 batch_loss: 0.10974609851837158\n",
      "training: 8 batch 29 batch_loss: 0.10764086246490479\n",
      "training: 8 batch 30 batch_loss: 0.1070934534072876\n",
      "training: 8 batch 31 batch_loss: 0.10596531629562378\n",
      "training: 8 batch 32 batch_loss: 0.11221745610237122\n",
      "training: 8 batch 33 batch_loss: 0.10718536376953125\n",
      "training: 8 batch 34 batch_loss: 0.1087113618850708\n",
      "training: 8 batch 35 batch_loss: 0.10715687274932861\n",
      "training: 8 batch 36 batch_loss: 0.10590308904647827\n",
      "training: 8 batch 37 batch_loss: 0.1086416244506836\n",
      "training: 8 batch 38 batch_loss: 0.10631352663040161\n",
      "training: 8 batch 39 batch_loss: 0.1067061722278595\n",
      "training: 8 batch 40 batch_loss: 0.10555216670036316\n",
      "training: 8 batch 41 batch_loss: 0.10750362277030945\n",
      "training: 8 batch 42 batch_loss: 0.10530614852905273\n",
      "training: 8 batch 43 batch_loss: 0.10283973813056946\n",
      "training: 8 batch 44 batch_loss: 0.10631182789802551\n",
      "training: 8 batch 45 batch_loss: 0.10646575689315796\n",
      "training: 8 batch 46 batch_loss: 0.10964488983154297\n",
      "training: 8 batch 47 batch_loss: 0.10855892300605774\n",
      "training: 8 batch 48 batch_loss: 0.10655200481414795\n",
      "training: 8 batch 49 batch_loss: 0.10702091455459595\n",
      "training: 8 batch 50 batch_loss: 0.10893276333808899\n",
      "training: 8 batch 51 batch_loss: 0.10675117373466492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 8 batch 52 batch_loss: 0.10536244511604309\n",
      "training: 8 batch 53 batch_loss: 0.10946840047836304\n",
      "training: 8 batch 54 batch_loss: 0.10866162180900574\n",
      "training: 8 batch 55 batch_loss: 0.10418492555618286\n",
      "training: 8 batch 56 batch_loss: 0.10816550254821777\n",
      "training: 8 batch 57 batch_loss: 0.10624322295188904\n",
      "training: 8 batch 58 batch_loss: 0.1087031364440918\n",
      "training: 8 batch 59 batch_loss: 0.10754448175430298\n",
      "training: 8 batch 60 batch_loss: 0.10666495561599731\n",
      "training: 8 batch 61 batch_loss: 0.10651317238807678\n",
      "training: 8 batch 62 batch_loss: 0.10711038112640381\n",
      "training: 8 batch 63 batch_loss: 0.10540109872817993\n",
      "training: 8 batch 64 batch_loss: 0.10641705989837646\n",
      "training: 8 batch 65 batch_loss: 0.10609295964241028\n",
      "training: 8 batch 66 batch_loss: 0.1099398136138916\n",
      "training: 8 batch 67 batch_loss: 0.10840374231338501\n",
      "training: 8 batch 68 batch_loss: 0.10751965641975403\n",
      "training: 8 batch 69 batch_loss: 0.11017096042633057\n",
      "training: 8 batch 70 batch_loss: 0.1073196530342102\n",
      "training: 8 batch 71 batch_loss: 0.10796964168548584\n",
      "training: 8 batch 72 batch_loss: 0.10556638240814209\n",
      "training: 8 batch 73 batch_loss: 0.10674956440925598\n",
      "training: 8 batch 74 batch_loss: 0.10778921842575073\n",
      "training: 8 batch 75 batch_loss: 0.10818305611610413\n",
      "training: 8 batch 76 batch_loss: 0.11090663075447083\n",
      "training: 8 batch 77 batch_loss: 0.10759276151657104\n",
      "training: 8 batch 78 batch_loss: 0.10792151093482971\n",
      "training: 8 batch 79 batch_loss: 0.10561197996139526\n",
      "training: 8 batch 80 batch_loss: 0.10578310489654541\n",
      "training: 8 batch 81 batch_loss: 0.10620808601379395\n",
      "training: 8 batch 82 batch_loss: 0.10813266038894653\n",
      "training: 8 batch 83 batch_loss: 0.10842198133468628\n",
      "training: 8 batch 84 batch_loss: 0.1106557548046112\n",
      "training: 8 batch 85 batch_loss: 0.10818368196487427\n",
      "training: 8 batch 86 batch_loss: 0.10829076170921326\n",
      "training: 8 batch 87 batch_loss: 0.1072297990322113\n",
      "training: 8 batch 88 batch_loss: 0.1087908148765564\n",
      "training: 8 batch 89 batch_loss: 0.1052064299583435\n",
      "training: 8 batch 90 batch_loss: 0.1102856993675232\n",
      "training: 8 batch 91 batch_loss: 0.10729324817657471\n",
      "training: 8 batch 92 batch_loss: 0.11139824986457825\n",
      "training: 8 batch 93 batch_loss: 0.10895103216171265\n",
      "training: 8 batch 94 batch_loss: 0.10714882612228394\n",
      "training: 8 batch 95 batch_loss: 0.10754060745239258\n",
      "training: 8 batch 96 batch_loss: 0.10578492283821106\n",
      "training: 8 batch 97 batch_loss: 0.10783714056015015\n",
      "training: 8 batch 98 batch_loss: 0.10811588168144226\n",
      "training: 8 batch 99 batch_loss: 0.10963907837867737\n",
      "training: 8 batch 100 batch_loss: 0.10935023427009583\n",
      "training: 8 batch 101 batch_loss: 0.10680598020553589\n",
      "training: 8 batch 102 batch_loss: 0.10790824890136719\n",
      "training: 8 batch 103 batch_loss: 0.11067637801170349\n",
      "training: 8 batch 104 batch_loss: 0.10971343517303467\n",
      "training: 8 batch 105 batch_loss: 0.10477706789970398\n",
      "training: 8 batch 106 batch_loss: 0.10704085230827332\n",
      "training: 8 batch 107 batch_loss: 0.10724186897277832\n",
      "training: 8 batch 108 batch_loss: 0.10851061344146729\n",
      "training: 8 batch 109 batch_loss: 0.11046388745307922\n",
      "training: 8 batch 110 batch_loss: 0.10711649060249329\n",
      "training: 8 batch 111 batch_loss: 0.10758218169212341\n",
      "training: 8 batch 112 batch_loss: 0.11019420623779297\n",
      "training: 8 batch 113 batch_loss: 0.10871744155883789\n",
      "training: 8 batch 114 batch_loss: 0.10797950625419617\n",
      "training: 8 batch 115 batch_loss: 0.10967883467674255\n",
      "training: 8 batch 116 batch_loss: 0.10837861895561218\n",
      "training: 8 batch 117 batch_loss: 0.1067081093788147\n",
      "training: 8 batch 118 batch_loss: 0.10670334100723267\n",
      "training: 8 batch 119 batch_loss: 0.10891839861869812\n",
      "training: 8 batch 120 batch_loss: 0.10631546378135681\n",
      "training: 8 batch 121 batch_loss: 0.10812273621559143\n",
      "training: 8 batch 122 batch_loss: 0.1088407039642334\n",
      "training: 8 batch 123 batch_loss: 0.1108449399471283\n",
      "training: 8 batch 124 batch_loss: 0.10933259129524231\n",
      "training: 8 batch 125 batch_loss: 0.10995432734489441\n",
      "training: 8 batch 126 batch_loss: 0.11071610450744629\n",
      "training: 8 batch 127 batch_loss: 0.10909739136695862\n",
      "training: 8 batch 128 batch_loss: 0.10717415809631348\n",
      "training: 8 batch 129 batch_loss: 0.10740885138511658\n",
      "training: 8 batch 130 batch_loss: 0.10595691204071045\n",
      "training: 8 batch 131 batch_loss: 0.10700738430023193\n",
      "training: 8 batch 132 batch_loss: 0.10866847634315491\n",
      "training: 8 batch 133 batch_loss: 0.10729855298995972\n",
      "training: 8 batch 134 batch_loss: 0.10785216093063354\n",
      "training: 8 batch 135 batch_loss: 0.10932210087776184\n",
      "training: 8 batch 136 batch_loss: 0.1060437560081482\n",
      "training: 8 batch 137 batch_loss: 0.1082211434841156\n",
      "training: 8 batch 138 batch_loss: 0.10780572891235352\n",
      "training: 8 batch 139 batch_loss: 0.10787591338157654\n",
      "training: 8 batch 140 batch_loss: 0.10760879516601562\n",
      "training: 8 batch 141 batch_loss: 0.10796120762825012\n",
      "training: 8 batch 142 batch_loss: 0.10872587561607361\n",
      "training: 8 batch 143 batch_loss: 0.10646390914916992\n",
      "training: 8 batch 144 batch_loss: 0.1096331775188446\n",
      "training: 8 batch 145 batch_loss: 0.10681763291358948\n",
      "training: 8 batch 146 batch_loss: 0.10916554927825928\n",
      "training: 8 batch 147 batch_loss: 0.1074790358543396\n",
      "training: 8 batch 148 batch_loss: 0.10924792289733887\n",
      "training: 8 batch 149 batch_loss: 0.10562121868133545\n",
      "training: 8 batch 150 batch_loss: 0.11029788851737976\n",
      "training: 8 batch 151 batch_loss: 0.10647973418235779\n",
      "training: 8 batch 152 batch_loss: 0.10675051808357239\n",
      "training: 8 batch 153 batch_loss: 0.1089501678943634\n",
      "training: 8 batch 154 batch_loss: 0.107145756483078\n",
      "training: 8 batch 155 batch_loss: 0.10870033502578735\n",
      "training: 8 batch 156 batch_loss: 0.10845828056335449\n",
      "training: 8 batch 157 batch_loss: 0.10862362384796143\n",
      "training: 8 batch 158 batch_loss: 0.11115133762359619\n",
      "training: 8 batch 159 batch_loss: 0.10906267166137695\n",
      "training: 8 batch 160 batch_loss: 0.10973566770553589\n",
      "training: 8 batch 161 batch_loss: 0.1096150279045105\n",
      "training: 8 batch 162 batch_loss: 0.11081269383430481\n",
      "training: 8 batch 163 batch_loss: 0.10937273502349854\n",
      "training: 8 batch 164 batch_loss: 0.10708341002464294\n",
      "training: 8 batch 165 batch_loss: 0.1099395751953125\n",
      "training: 8 batch 166 batch_loss: 0.10904049873352051\n",
      "training: 8 batch 167 batch_loss: 0.10873031616210938\n",
      "training: 8 batch 168 batch_loss: 0.10859614610671997\n",
      "training: 8 batch 169 batch_loss: 0.10849091410636902\n",
      "training: 8 batch 170 batch_loss: 0.10721033811569214\n",
      "training: 8 batch 171 batch_loss: 0.10796171426773071\n",
      "training: 8 batch 172 batch_loss: 0.11260485649108887\n",
      "training: 8 batch 173 batch_loss: 0.1054682731628418\n",
      "training: 8 batch 174 batch_loss: 0.10870790481567383\n",
      "training: 8 batch 175 batch_loss: 0.1083754301071167\n",
      "training: 8 batch 176 batch_loss: 0.10806813836097717\n",
      "training: 8 batch 177 batch_loss: 0.1092977225780487\n",
      "training: 8 batch 178 batch_loss: 0.10872557759284973\n",
      "training: 8 batch 179 batch_loss: 0.11109274625778198\n",
      "training: 8 batch 180 batch_loss: 0.10839343070983887\n",
      "training: 8 batch 181 batch_loss: 0.10804453492164612\n",
      "training: 8 batch 182 batch_loss: 0.1113702654838562\n",
      "training: 8 batch 183 batch_loss: 0.10659489035606384\n",
      "training: 8 batch 184 batch_loss: 0.1071215271949768\n",
      "training: 8 batch 185 batch_loss: 0.10904812812805176\n",
      "training: 8 batch 186 batch_loss: 0.10839119553565979\n",
      "training: 8 batch 187 batch_loss: 0.10494261980056763\n",
      "training: 8 batch 188 batch_loss: 0.10938289761543274\n",
      "training: 8 batch 189 batch_loss: 0.10884839296340942\n",
      "training: 8 batch 190 batch_loss: 0.10650947690010071\n",
      "training: 8 batch 191 batch_loss: 0.10737228393554688\n",
      "training: 8 batch 192 batch_loss: 0.1105620265007019\n",
      "training: 8 batch 193 batch_loss: 0.10857975482940674\n",
      "training: 8 batch 194 batch_loss: 0.1058153510093689\n",
      "training: 8 batch 195 batch_loss: 0.10628604888916016\n",
      "training: 8 batch 196 batch_loss: 0.10887742042541504\n",
      "training: 8 batch 197 batch_loss: 0.10855165123939514\n",
      "training: 8 batch 198 batch_loss: 0.11181718111038208\n",
      "training: 8 batch 199 batch_loss: 0.10657286643981934\n",
      "training: 8 batch 200 batch_loss: 0.11125627160072327\n",
      "training: 8 batch 201 batch_loss: 0.10772481560707092\n",
      "training: 8 batch 202 batch_loss: 0.10692098736763\n",
      "training: 8 batch 203 batch_loss: 0.10815921425819397\n",
      "training: 8 batch 204 batch_loss: 0.10878440737724304\n",
      "training: 8 batch 205 batch_loss: 0.107083261013031\n",
      "training: 8 batch 206 batch_loss: 0.10816025733947754\n",
      "training: 8 batch 207 batch_loss: 0.1121973991394043\n",
      "training: 8 batch 208 batch_loss: 0.1078403890132904\n",
      "training: 8 batch 209 batch_loss: 0.10963559150695801\n",
      "training: 8 batch 210 batch_loss: 0.10777142643928528\n",
      "training: 8 batch 211 batch_loss: 0.1090211570262909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 8 batch 212 batch_loss: 0.1070406436920166\n",
      "training: 8 batch 213 batch_loss: 0.11007162928581238\n",
      "training: 8 batch 214 batch_loss: 0.11079496145248413\n",
      "training: 8 batch 215 batch_loss: 0.10262519121170044\n",
      "training: 8 batch 216 batch_loss: 0.1097487211227417\n",
      "training: 8 batch 217 batch_loss: 0.10436540842056274\n",
      "training: 8 batch 218 batch_loss: 0.1109461784362793\n",
      "training: 8 batch 219 batch_loss: 0.10865530371665955\n",
      "training: 8 batch 220 batch_loss: 0.1075129508972168\n",
      "training: 8 batch 221 batch_loss: 0.10857564210891724\n",
      "training: 8 batch 222 batch_loss: 0.1072128415107727\n",
      "training: 8 batch 223 batch_loss: 0.11185628175735474\n",
      "training: 8 batch 224 batch_loss: 0.10898962616920471\n",
      "training: 8 batch 225 batch_loss: 0.10634499788284302\n",
      "training: 8 batch 226 batch_loss: 0.1079578697681427\n",
      "training: 8 batch 227 batch_loss: 0.1092054545879364\n",
      "training: 8 batch 228 batch_loss: 0.10705932974815369\n",
      "training: 8 batch 229 batch_loss: 0.10745421051979065\n",
      "training: 8 batch 230 batch_loss: 0.10882332921028137\n",
      "training: 8 batch 231 batch_loss: 0.1095537543296814\n",
      "training: 8 batch 232 batch_loss: 0.10927045345306396\n",
      "training: 8 batch 233 batch_loss: 0.10802078247070312\n",
      "training: 8 batch 234 batch_loss: 0.10872668027877808\n",
      "training: 8 batch 235 batch_loss: 0.10912734270095825\n",
      "training: 8 batch 236 batch_loss: 0.10709631443023682\n",
      "training: 8 batch 237 batch_loss: 0.10885629057884216\n",
      "training: 8 batch 238 batch_loss: 0.10758510231971741\n",
      "training: 8 batch 239 batch_loss: 0.1093592643737793\n",
      "training: 8 batch 240 batch_loss: 0.11002352833747864\n",
      "training: 8 batch 241 batch_loss: 0.10878020524978638\n",
      "training: 8 batch 242 batch_loss: 0.11339396238327026\n",
      "training: 8 batch 243 batch_loss: 0.11067947745323181\n",
      "training: 8 batch 244 batch_loss: 0.10968181490898132\n",
      "training: 8 batch 245 batch_loss: 0.11256685853004456\n",
      "training: 8 batch 246 batch_loss: 0.10728856921195984\n",
      "training: 8 batch 247 batch_loss: 0.10804226994514465\n",
      "training: 8 batch 248 batch_loss: 0.10968470573425293\n",
      "training: 8 batch 249 batch_loss: 0.10909479856491089\n",
      "training: 8 batch 250 batch_loss: 0.11078381538391113\n",
      "training: 8 batch 251 batch_loss: 0.10756286978721619\n",
      "training: 8 batch 252 batch_loss: 0.10894623398780823\n",
      "training: 8 batch 253 batch_loss: 0.10744261741638184\n",
      "training: 8 batch 254 batch_loss: 0.10472726821899414\n",
      "training: 8 batch 255 batch_loss: 0.10739976167678833\n",
      "training: 8 batch 256 batch_loss: 0.11078032851219177\n",
      "training: 8 batch 257 batch_loss: 0.11053723096847534\n",
      "training: 8 batch 258 batch_loss: 0.10700511932373047\n",
      "training: 8 batch 259 batch_loss: 0.10798776149749756\n",
      "training: 8 batch 260 batch_loss: 0.11115121841430664\n",
      "training: 8 batch 261 batch_loss: 0.10908257961273193\n",
      "training: 8 batch 262 batch_loss: 0.11011594533920288\n",
      "training: 8 batch 263 batch_loss: 0.1087300181388855\n",
      "training: 8 batch 264 batch_loss: 0.11151975393295288\n",
      "training: 8 batch 265 batch_loss: 0.10983073711395264\n",
      "training: 8 batch 266 batch_loss: 0.10884803533554077\n",
      "training: 8 batch 267 batch_loss: 0.11201068758964539\n",
      "training: 8 batch 268 batch_loss: 0.10963159799575806\n",
      "training: 8 batch 269 batch_loss: 0.11082276701927185\n",
      "training: 8 batch 270 batch_loss: 0.10853013396263123\n",
      "training: 8 batch 271 batch_loss: 0.10914057493209839\n",
      "training: 8 batch 272 batch_loss: 0.11237645149230957\n",
      "training: 8 batch 273 batch_loss: 0.10744258761405945\n",
      "training: 8 batch 274 batch_loss: 0.10820648074150085\n",
      "training: 8 batch 275 batch_loss: 0.10986027121543884\n",
      "training: 8 batch 276 batch_loss: 0.10818612575531006\n",
      "training: 8 batch 277 batch_loss: 0.11291760206222534\n",
      "training: 8 batch 278 batch_loss: 0.1092149019241333\n",
      "training: 8 batch 279 batch_loss: 0.1084800660610199\n",
      "training: 8 batch 280 batch_loss: 0.11298820376396179\n",
      "training: 8 batch 281 batch_loss: 0.10858482122421265\n",
      "training: 8 batch 282 batch_loss: 0.11106786131858826\n",
      "training: 8 batch 283 batch_loss: 0.1073157787322998\n",
      "training: 8 batch 284 batch_loss: 0.10903871059417725\n",
      "training: 8 batch 285 batch_loss: 0.10900139808654785\n",
      "training: 8 batch 286 batch_loss: 0.11295783519744873\n",
      "training: 8 batch 287 batch_loss: 0.10907086730003357\n",
      "training: 8 batch 288 batch_loss: 0.10859730839729309\n",
      "training: 8 batch 289 batch_loss: 0.10857903957366943\n",
      "training: 8 batch 290 batch_loss: 0.10926869511604309\n",
      "training: 8 batch 291 batch_loss: 0.10963660478591919\n",
      "training: 8 batch 292 batch_loss: 0.11089015007019043\n",
      "training: 8 batch 293 batch_loss: 0.11062061786651611\n",
      "training: 8 batch 294 batch_loss: 0.10908418893814087\n",
      "training: 8 batch 295 batch_loss: 0.11066380143165588\n",
      "training: 8 batch 296 batch_loss: 0.10859087109565735\n",
      "training: 8 batch 297 batch_loss: 0.10919749736785889\n",
      "training: 8 batch 298 batch_loss: 0.10920625925064087\n",
      "training: 8 batch 299 batch_loss: 0.1127493679523468\n",
      "training: 8 batch 300 batch_loss: 0.10918569564819336\n",
      "training: 8 batch 301 batch_loss: 0.10928216576576233\n",
      "training: 8 batch 302 batch_loss: 0.10795912146568298\n",
      "training: 8 batch 303 batch_loss: 0.11074167490005493\n",
      "training: 8 batch 304 batch_loss: 0.11072832345962524\n",
      "training: 8 batch 305 batch_loss: 0.10979318618774414\n",
      "training: 8 batch 306 batch_loss: 0.10660412907600403\n",
      "training: 8 batch 307 batch_loss: 0.10910406708717346\n",
      "training: 8 batch 308 batch_loss: 0.10901650786399841\n",
      "training: 8 batch 309 batch_loss: 0.10791206359863281\n",
      "training: 8 batch 310 batch_loss: 0.10692787170410156\n",
      "training: 8 batch 311 batch_loss: 0.10990434885025024\n",
      "training: 8 batch 312 batch_loss: 0.10888016223907471\n",
      "training: 8 batch 313 batch_loss: 0.10705414414405823\n",
      "training: 8 batch 314 batch_loss: 0.10968220233917236\n",
      "training: 8 batch 315 batch_loss: 0.11266198754310608\n",
      "training: 8 batch 316 batch_loss: 0.1077791154384613\n",
      "training: 8 batch 317 batch_loss: 0.1054459810256958\n",
      "training: 8 batch 318 batch_loss: 0.11027762293815613\n",
      "training: 8 batch 319 batch_loss: 0.11223873496055603\n",
      "training: 8 batch 320 batch_loss: 0.11017152667045593\n",
      "training: 8 batch 321 batch_loss: 0.10938417911529541\n",
      "training: 8 batch 322 batch_loss: 0.10836169123649597\n",
      "training: 8 batch 323 batch_loss: 0.11289337277412415\n",
      "training: 8 batch 324 batch_loss: 0.11043104529380798\n",
      "training: 8 batch 325 batch_loss: 0.10913288593292236\n",
      "training: 8 batch 326 batch_loss: 0.11208903789520264\n",
      "training: 8 batch 327 batch_loss: 0.10975241661071777\n",
      "training: 8 batch 328 batch_loss: 0.11043131351470947\n",
      "training: 8 batch 329 batch_loss: 0.10905638337135315\n",
      "training: 8 batch 330 batch_loss: 0.10750970244407654\n",
      "training: 8 batch 331 batch_loss: 0.10919404029846191\n",
      "training: 8 batch 332 batch_loss: 0.10682627558708191\n",
      "training: 8 batch 333 batch_loss: 0.1127314567565918\n",
      "training: 8 batch 334 batch_loss: 0.11076068878173828\n",
      "training: 8 batch 335 batch_loss: 0.10839727520942688\n",
      "training: 8 batch 336 batch_loss: 0.11187484860420227\n",
      "training: 8 batch 337 batch_loss: 0.10907959938049316\n",
      "training: 8 batch 338 batch_loss: 0.10847151279449463\n",
      "training: 8 batch 339 batch_loss: 0.11014357209205627\n",
      "training: 8 batch 340 batch_loss: 0.11395743489265442\n",
      "training: 8 batch 341 batch_loss: 0.11169201135635376\n",
      "training: 8 batch 342 batch_loss: 0.11040529608726501\n",
      "training: 8 batch 343 batch_loss: 0.10928970575332642\n",
      "training: 8 batch 344 batch_loss: 0.11080750823020935\n",
      "training: 8 batch 345 batch_loss: 0.10987472534179688\n",
      "training: 8 batch 346 batch_loss: 0.10984733700752258\n",
      "training: 8 batch 347 batch_loss: 0.11128479242324829\n",
      "training: 8 batch 348 batch_loss: 0.11017635464668274\n",
      "training: 8 batch 349 batch_loss: 0.10956805944442749\n",
      "training: 8 batch 350 batch_loss: 0.11059370636940002\n",
      "training: 8 batch 351 batch_loss: 0.11423102021217346\n",
      "training: 8 batch 352 batch_loss: 0.1102224588394165\n",
      "training: 8 batch 353 batch_loss: 0.11161097884178162\n",
      "training: 8 batch 354 batch_loss: 0.11195605993270874\n",
      "training: 8 batch 355 batch_loss: 0.11062487959861755\n",
      "training: 8 batch 356 batch_loss: 0.10584080219268799\n",
      "training: 8 batch 357 batch_loss: 0.10526812076568604\n",
      "training: 8 batch 358 batch_loss: 0.10875216126441956\n",
      "training: 8 batch 359 batch_loss: 0.10779392719268799\n",
      "training: 8 batch 360 batch_loss: 0.1086362898349762\n",
      "training: 8 batch 361 batch_loss: 0.10837972164154053\n",
      "training: 8 batch 362 batch_loss: 0.10957252979278564\n",
      "training: 8 batch 363 batch_loss: 0.11024916172027588\n",
      "training: 8 batch 364 batch_loss: 0.11264762282371521\n",
      "training: 8 batch 365 batch_loss: 0.10879331827163696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 8 batch 366 batch_loss: 0.11251187324523926\n",
      "training: 8 batch 367 batch_loss: 0.10965165495872498\n",
      "training: 8 batch 368 batch_loss: 0.11036473512649536\n",
      "training: 8 batch 369 batch_loss: 0.11075398325920105\n",
      "training: 8 batch 370 batch_loss: 0.1127169132232666\n",
      "training: 8 batch 371 batch_loss: 0.11251533031463623\n",
      "training: 8 batch 372 batch_loss: 0.10914498567581177\n",
      "training: 8 batch 373 batch_loss: 0.11071109771728516\n",
      "training: 8 batch 374 batch_loss: 0.10794609785079956\n",
      "training: 8 batch 375 batch_loss: 0.11015316843986511\n",
      "training: 8 batch 376 batch_loss: 0.11271893978118896\n",
      "training: 8 batch 377 batch_loss: 0.109397292137146\n",
      "training: 8 batch 378 batch_loss: 0.11050480604171753\n",
      "training: 8 batch 379 batch_loss: 0.1094031035900116\n",
      "training: 8 batch 380 batch_loss: 0.11230897903442383\n",
      "training: 8 batch 381 batch_loss: 0.11116516590118408\n",
      "training: 8 batch 382 batch_loss: 0.10997390747070312\n",
      "training: 8 batch 383 batch_loss: 0.10910886526107788\n",
      "training: 8 batch 384 batch_loss: 0.11126363277435303\n",
      "training: 8 batch 385 batch_loss: 0.11081293225288391\n",
      "training: 8 batch 386 batch_loss: 0.10896757245063782\n",
      "training: 8 batch 387 batch_loss: 0.11152878403663635\n",
      "training: 8 batch 388 batch_loss: 0.11228200793266296\n",
      "training: 8 batch 389 batch_loss: 0.11020779609680176\n",
      "training: 8 batch 390 batch_loss: 0.10920020937919617\n",
      "training: 8 batch 391 batch_loss: 0.11100423336029053\n",
      "training: 8 batch 392 batch_loss: 0.11003676056861877\n",
      "training: 8 batch 393 batch_loss: 0.11192229390144348\n",
      "training: 8 batch 394 batch_loss: 0.11138436198234558\n",
      "training: 8 batch 395 batch_loss: 0.1087334156036377\n",
      "training: 8 batch 396 batch_loss: 0.11140191555023193\n",
      "training: 8 batch 397 batch_loss: 0.10931706428527832\n",
      "training: 8 batch 398 batch_loss: 0.11144909262657166\n",
      "training: 8 batch 399 batch_loss: 0.11035332083702087\n",
      "training: 8 batch 400 batch_loss: 0.10984846949577332\n",
      "training: 8 batch 401 batch_loss: 0.11159700155258179\n",
      "training: 8 batch 402 batch_loss: 0.1104518473148346\n",
      "training: 8 batch 403 batch_loss: 0.11107456684112549\n",
      "training: 8 batch 404 batch_loss: 0.10996562242507935\n",
      "training: 8 batch 405 batch_loss: 0.1105954647064209\n",
      "training: 8 batch 406 batch_loss: 0.11190474033355713\n",
      "training: 8 batch 407 batch_loss: 0.10814863443374634\n",
      "training: 8 batch 408 batch_loss: 0.10766816139221191\n",
      "training: 8 batch 409 batch_loss: 0.11027371883392334\n",
      "training: 8 batch 410 batch_loss: 0.11088746786117554\n",
      "training: 8 batch 411 batch_loss: 0.10901311039924622\n",
      "training: 8 batch 412 batch_loss: 0.10945701599121094\n",
      "training: 8 batch 413 batch_loss: 0.10821959376335144\n",
      "training: 8 batch 414 batch_loss: 0.11045622825622559\n",
      "training: 8 batch 415 batch_loss: 0.11164930462837219\n",
      "training: 8 batch 416 batch_loss: 0.10899132490158081\n",
      "training: 8 batch 417 batch_loss: 0.11114779114723206\n",
      "training: 8 batch 418 batch_loss: 0.11176317930221558\n",
      "training: 8 batch 419 batch_loss: 0.1103980541229248\n",
      "training: 8 batch 420 batch_loss: 0.10660499334335327\n",
      "training: 8 batch 421 batch_loss: 0.1105424165725708\n",
      "training: 8 batch 422 batch_loss: 0.11036914587020874\n",
      "training: 8 batch 423 batch_loss: 0.11216351389884949\n",
      "training: 8 batch 424 batch_loss: 0.1112605631351471\n",
      "training: 8 batch 425 batch_loss: 0.11176559329032898\n",
      "training: 8 batch 426 batch_loss: 0.11136797070503235\n",
      "training: 8 batch 427 batch_loss: 0.1124834418296814\n",
      "training: 8 batch 428 batch_loss: 0.10955649614334106\n",
      "training: 8 batch 429 batch_loss: 0.11157476902008057\n",
      "training: 8 batch 430 batch_loss: 0.11165305972099304\n",
      "training: 8 batch 431 batch_loss: 0.11161595582962036\n",
      "training: 8 batch 432 batch_loss: 0.10999837517738342\n",
      "training: 8 batch 433 batch_loss: 0.10899925231933594\n",
      "training: 8 batch 434 batch_loss: 0.11065271496772766\n",
      "training: 8 batch 435 batch_loss: 0.10771557688713074\n",
      "training: 8 batch 436 batch_loss: 0.11176887154579163\n",
      "training: 8 batch 437 batch_loss: 0.10903248190879822\n",
      "training: 8 batch 438 batch_loss: 0.10979121923446655\n",
      "training: 8 batch 439 batch_loss: 0.1112423837184906\n",
      "training: 8 batch 440 batch_loss: 0.10914263129234314\n",
      "training: 8 batch 441 batch_loss: 0.1113695502281189\n",
      "training: 8 batch 442 batch_loss: 0.10817387700080872\n",
      "training: 8 batch 443 batch_loss: 0.10758262872695923\n",
      "training: 8 batch 444 batch_loss: 0.1139490008354187\n",
      "training: 8 batch 445 batch_loss: 0.11072099208831787\n",
      "training: 8 batch 446 batch_loss: 0.11219441890716553\n",
      "training: 8 batch 447 batch_loss: 0.1144181489944458\n",
      "training: 8 batch 448 batch_loss: 0.10987696051597595\n",
      "training: 8 batch 449 batch_loss: 0.1109158992767334\n",
      "training: 8 batch 450 batch_loss: 0.1096406877040863\n",
      "training: 8 batch 451 batch_loss: 0.11228477954864502\n",
      "training: 8 batch 452 batch_loss: 0.10969573259353638\n",
      "training: 8 batch 453 batch_loss: 0.11181563138961792\n",
      "training: 8 batch 454 batch_loss: 0.1105087399482727\n",
      "training: 8 batch 455 batch_loss: 0.11060348153114319\n",
      "training: 8 batch 456 batch_loss: 0.10877704620361328\n",
      "training: 8 batch 457 batch_loss: 0.11189514398574829\n",
      "training: 8 batch 458 batch_loss: 0.11313462257385254\n",
      "training: 8 batch 459 batch_loss: 0.11385846138000488\n",
      "training: 8 batch 460 batch_loss: 0.11426448822021484\n",
      "training: 8 batch 461 batch_loss: 0.10981258749961853\n",
      "training: 8 batch 462 batch_loss: 0.11173391342163086\n",
      "training: 8 batch 463 batch_loss: 0.11077913641929626\n",
      "training: 8 batch 464 batch_loss: 0.1121830940246582\n",
      "training: 8 batch 465 batch_loss: 0.11052024364471436\n",
      "training: 8 batch 466 batch_loss: 0.11111459136009216\n",
      "training: 8 batch 467 batch_loss: 0.1146913468837738\n",
      "training: 8 batch 468 batch_loss: 0.10970646142959595\n",
      "training: 8 batch 469 batch_loss: 0.10889148712158203\n",
      "training: 8 batch 470 batch_loss: 0.11183911561965942\n",
      "training: 8 batch 471 batch_loss: 0.11039367318153381\n",
      "training: 8 batch 472 batch_loss: 0.1133730411529541\n",
      "training: 8 batch 473 batch_loss: 0.11265400052070618\n",
      "training: 8 batch 474 batch_loss: 0.11209845542907715\n",
      "training: 8 batch 475 batch_loss: 0.11202308535575867\n",
      "training: 8 batch 476 batch_loss: 0.11114197969436646\n",
      "training: 8 batch 477 batch_loss: 0.11071494221687317\n",
      "training: 8 batch 478 batch_loss: 0.11195623874664307\n",
      "training: 8 batch 479 batch_loss: 0.11026999354362488\n",
      "training: 8 batch 480 batch_loss: 0.1140008270740509\n",
      "training: 8 batch 481 batch_loss: 0.10958299040794373\n",
      "training: 8 batch 482 batch_loss: 0.1114039421081543\n",
      "training: 8 batch 483 batch_loss: 0.11337444186210632\n",
      "training: 8 batch 484 batch_loss: 0.11235132813453674\n",
      "training: 8 batch 485 batch_loss: 0.1088135838508606\n",
      "training: 8 batch 486 batch_loss: 0.11370629072189331\n",
      "training: 8 batch 487 batch_loss: 0.11197212338447571\n",
      "training: 8 batch 488 batch_loss: 0.11112073063850403\n",
      "training: 8 batch 489 batch_loss: 0.11370378732681274\n",
      "training: 8 batch 490 batch_loss: 0.113697350025177\n",
      "training: 8 batch 491 batch_loss: 0.11084812879562378\n",
      "training: 8 batch 492 batch_loss: 0.1110195517539978\n",
      "training: 8 batch 493 batch_loss: 0.10995984077453613\n",
      "training: 8 batch 494 batch_loss: 0.10943940281867981\n",
      "training: 8 batch 495 batch_loss: 0.10995692014694214\n",
      "training: 8 batch 496 batch_loss: 0.11080393195152283\n",
      "training: 8 batch 497 batch_loss: 0.11110332608222961\n",
      "training: 8 batch 498 batch_loss: 0.11098664999008179\n",
      "training: 8 batch 499 batch_loss: 0.10954642295837402\n",
      "training: 8 batch 500 batch_loss: 0.11157095432281494\n",
      "training: 8 batch 501 batch_loss: 0.1129823625087738\n",
      "training: 8 batch 502 batch_loss: 0.10867947340011597\n",
      "training: 8 batch 503 batch_loss: 0.11184507608413696\n",
      "training: 8 batch 504 batch_loss: 0.1119677722454071\n",
      "training: 8 batch 505 batch_loss: 0.10953149199485779\n",
      "training: 8 batch 506 batch_loss: 0.1146191954612732\n",
      "training: 8 batch 507 batch_loss: 0.11133435368537903\n",
      "training: 8 batch 508 batch_loss: 0.11127650737762451\n",
      "training: 8 batch 509 batch_loss: 0.11091941595077515\n",
      "training: 8 batch 510 batch_loss: 0.11123865842819214\n",
      "training: 8 batch 511 batch_loss: 0.11154201626777649\n",
      "training: 8 batch 512 batch_loss: 0.11074045300483704\n",
      "training: 8 batch 513 batch_loss: 0.1091921329498291\n",
      "training: 8 batch 514 batch_loss: 0.10993894934654236\n",
      "training: 8 batch 515 batch_loss: 0.11016154289245605\n",
      "training: 8 batch 516 batch_loss: 0.11311537027359009\n",
      "training: 8 batch 517 batch_loss: 0.1095440685749054\n",
      "training: 8 batch 518 batch_loss: 0.11418727040290833\n",
      "training: 8 batch 519 batch_loss: 0.11315286159515381\n",
      "training: 8 batch 520 batch_loss: 0.10983186960220337\n",
      "training: 8 batch 521 batch_loss: 0.11283731460571289\n",
      "training: 8 batch 522 batch_loss: 0.11162939667701721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 8 batch 523 batch_loss: 0.11221525073051453\n",
      "training: 8 batch 524 batch_loss: 0.11309555172920227\n",
      "training: 8 batch 525 batch_loss: 0.1106402575969696\n",
      "training: 8 batch 526 batch_loss: 0.11306038498878479\n",
      "training: 8 batch 527 batch_loss: 0.1110025942325592\n",
      "training: 8 batch 528 batch_loss: 0.11227652430534363\n",
      "training: 8 batch 529 batch_loss: 0.11159360408782959\n",
      "training: 8 batch 530 batch_loss: 0.11108890175819397\n",
      "training: 8 batch 531 batch_loss: 0.11320823431015015\n",
      "training: 8 batch 532 batch_loss: 0.11246722936630249\n",
      "training: 8 batch 533 batch_loss: 0.1115759015083313\n",
      "training: 8 batch 534 batch_loss: 0.1147342324256897\n",
      "training: 8 batch 535 batch_loss: 0.1114012598991394\n",
      "training: 8 batch 536 batch_loss: 0.11109775304794312\n",
      "training: 8 batch 537 batch_loss: 0.1113913357257843\n",
      "training: 8 batch 538 batch_loss: 0.11139023303985596\n",
      "training: 8 batch 539 batch_loss: 0.11000698804855347\n",
      "training: 8 batch 540 batch_loss: 0.11262065172195435\n",
      "training: 8 batch 541 batch_loss: 0.11182928085327148\n",
      "training: 8 batch 542 batch_loss: 0.11256334185600281\n",
      "training: 8 batch 543 batch_loss: 0.10787376761436462\n",
      "training: 8 batch 544 batch_loss: 0.10790613293647766\n",
      "training: 8 batch 545 batch_loss: 0.11117947101593018\n",
      "training: 8 batch 546 batch_loss: 0.10870355367660522\n",
      "training: 8 batch 547 batch_loss: 0.11166277527809143\n",
      "training: 8 batch 548 batch_loss: 0.11171233654022217\n",
      "training: 8 batch 549 batch_loss: 0.11006131768226624\n",
      "training: 8 batch 550 batch_loss: 0.11179894208908081\n",
      "training: 8 batch 551 batch_loss: 0.11103543639183044\n",
      "training: 8 batch 552 batch_loss: 0.10850024223327637\n",
      "training: 8 batch 553 batch_loss: 0.11004078388214111\n",
      "training: 8 batch 554 batch_loss: 0.10944584012031555\n",
      "training: 8 batch 555 batch_loss: 0.11149141192436218\n",
      "training: 8 batch 556 batch_loss: 0.11272844672203064\n",
      "training: 8 batch 557 batch_loss: 0.11192587018013\n",
      "training: 8 batch 558 batch_loss: 0.1100892722606659\n",
      "training: 8 batch 559 batch_loss: 0.10777255892753601\n",
      "training: 8 batch 560 batch_loss: 0.11009976267814636\n",
      "training: 8 batch 561 batch_loss: 0.11069738864898682\n",
      "training: 8 batch 562 batch_loss: 0.11214151978492737\n",
      "training: 8 batch 563 batch_loss: 0.11004191637039185\n",
      "training: 8 batch 564 batch_loss: 0.11078077554702759\n",
      "training: 8 batch 565 batch_loss: 0.11234211921691895\n",
      "training: 8 batch 566 batch_loss: 0.11177045106887817\n",
      "training: 8 batch 567 batch_loss: 0.11003240942955017\n",
      "training: 8 batch 568 batch_loss: 0.11316022276878357\n",
      "training: 8 batch 569 batch_loss: 0.11248302459716797\n",
      "training: 8 batch 570 batch_loss: 0.11038446426391602\n",
      "training: 8 batch 571 batch_loss: 0.11070302128791809\n",
      "training: 8 batch 572 batch_loss: 0.1107751727104187\n",
      "training: 8 batch 573 batch_loss: 0.10741505026817322\n",
      "training: 8 batch 574 batch_loss: 0.1153768002986908\n",
      "training: 8 batch 575 batch_loss: 0.11276519298553467\n",
      "training: 8 batch 576 batch_loss: 0.11346641182899475\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 8, Hit Ratio:0.022512183828403726 | Precision:0.033215374029293226 | Recall:0.04841954453429587 | NDCG:0.041733194863830606\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 9 batch 0 batch_loss: 0.10963964462280273\n",
      "training: 9 batch 1 batch_loss: 0.1104770302772522\n",
      "training: 9 batch 2 batch_loss: 0.1108468770980835\n",
      "training: 9 batch 3 batch_loss: 0.10881555080413818\n",
      "training: 9 batch 4 batch_loss: 0.1083475649356842\n",
      "training: 9 batch 5 batch_loss: 0.10850769281387329\n",
      "training: 9 batch 6 batch_loss: 0.11050894856452942\n",
      "training: 9 batch 7 batch_loss: 0.11221489310264587\n",
      "training: 9 batch 8 batch_loss: 0.1086687445640564\n",
      "training: 9 batch 9 batch_loss: 0.11005601286888123\n",
      "training: 9 batch 10 batch_loss: 0.1095840334892273\n",
      "training: 9 batch 11 batch_loss: 0.10850286483764648\n",
      "training: 9 batch 12 batch_loss: 0.11245366930961609\n",
      "training: 9 batch 13 batch_loss: 0.10777115821838379\n",
      "training: 9 batch 14 batch_loss: 0.10968896746635437\n",
      "training: 9 batch 15 batch_loss: 0.11043846607208252\n",
      "training: 9 batch 16 batch_loss: 0.11205676198005676\n",
      "training: 9 batch 17 batch_loss: 0.10997498035430908\n",
      "training: 9 batch 18 batch_loss: 0.10995042324066162\n",
      "training: 9 batch 19 batch_loss: 0.11186408996582031\n",
      "training: 9 batch 20 batch_loss: 0.10796380043029785\n",
      "training: 9 batch 21 batch_loss: 0.1062297523021698\n",
      "training: 9 batch 22 batch_loss: 0.10809820890426636\n",
      "training: 9 batch 23 batch_loss: 0.11068639159202576\n",
      "training: 9 batch 24 batch_loss: 0.10977929830551147\n",
      "training: 9 batch 25 batch_loss: 0.1092715859413147\n",
      "training: 9 batch 26 batch_loss: 0.11096853017807007\n",
      "training: 9 batch 27 batch_loss: 0.11193209886550903\n",
      "training: 9 batch 28 batch_loss: 0.11122512817382812\n",
      "training: 9 batch 29 batch_loss: 0.109580397605896\n",
      "training: 9 batch 30 batch_loss: 0.1086249053478241\n",
      "training: 9 batch 31 batch_loss: 0.11061668395996094\n",
      "training: 9 batch 32 batch_loss: 0.11095377802848816\n",
      "training: 9 batch 33 batch_loss: 0.11197665333747864\n",
      "training: 9 batch 34 batch_loss: 0.10961681604385376\n",
      "training: 9 batch 35 batch_loss: 0.1098673939704895\n",
      "training: 9 batch 36 batch_loss: 0.11074906587600708\n",
      "training: 9 batch 37 batch_loss: 0.11299446225166321\n",
      "training: 9 batch 38 batch_loss: 0.11085984110832214\n",
      "training: 9 batch 39 batch_loss: 0.11214029788970947\n",
      "training: 9 batch 40 batch_loss: 0.10910683870315552\n",
      "training: 9 batch 41 batch_loss: 0.11059015989303589\n",
      "training: 9 batch 42 batch_loss: 0.11121484637260437\n",
      "training: 9 batch 43 batch_loss: 0.11320418119430542\n",
      "training: 9 batch 44 batch_loss: 0.10782790184020996\n",
      "training: 9 batch 45 batch_loss: 0.11213245987892151\n",
      "training: 9 batch 46 batch_loss: 0.11234846711158752\n",
      "training: 9 batch 47 batch_loss: 0.10991638898849487\n",
      "training: 9 batch 48 batch_loss: 0.1095876693725586\n",
      "training: 9 batch 49 batch_loss: 0.10901990532875061\n",
      "training: 9 batch 50 batch_loss: 0.11065834760665894\n",
      "training: 9 batch 51 batch_loss: 0.11081674695014954\n",
      "training: 9 batch 52 batch_loss: 0.11004289984703064\n",
      "training: 9 batch 53 batch_loss: 0.10906487703323364\n",
      "training: 9 batch 54 batch_loss: 0.11213517189025879\n",
      "training: 9 batch 55 batch_loss: 0.11327794194221497\n",
      "training: 9 batch 56 batch_loss: 0.11029234528541565\n",
      "training: 9 batch 57 batch_loss: 0.11406946182250977\n",
      "training: 9 batch 58 batch_loss: 0.11023429036140442\n",
      "training: 9 batch 59 batch_loss: 0.11073929071426392\n",
      "training: 9 batch 60 batch_loss: 0.10950511693954468\n",
      "training: 9 batch 61 batch_loss: 0.11029848456382751\n",
      "training: 9 batch 62 batch_loss: 0.11213749647140503\n",
      "training: 9 batch 63 batch_loss: 0.11208447813987732\n",
      "training: 9 batch 64 batch_loss: 0.11274781823158264\n",
      "training: 9 batch 65 batch_loss: 0.10918083786964417\n",
      "training: 9 batch 66 batch_loss: 0.10940888524055481\n",
      "training: 9 batch 67 batch_loss: 0.11038544774055481\n",
      "training: 9 batch 68 batch_loss: 0.1111900806427002\n",
      "training: 9 batch 69 batch_loss: 0.11022567749023438\n",
      "training: 9 batch 70 batch_loss: 0.11177608370780945\n",
      "training: 9 batch 71 batch_loss: 0.11070334911346436\n",
      "training: 9 batch 72 batch_loss: 0.10972321033477783\n",
      "training: 9 batch 73 batch_loss: 0.1106882095336914\n",
      "training: 9 batch 74 batch_loss: 0.11143821477890015\n",
      "training: 9 batch 75 batch_loss: 0.11045196652412415\n",
      "training: 9 batch 76 batch_loss: 0.11097070574760437\n",
      "training: 9 batch 77 batch_loss: 0.11187717318534851\n",
      "training: 9 batch 78 batch_loss: 0.1073879599571228\n",
      "training: 9 batch 79 batch_loss: 0.11015650629997253\n",
      "training: 9 batch 80 batch_loss: 0.10917875170707703\n",
      "training: 9 batch 81 batch_loss: 0.11048570275306702\n",
      "training: 9 batch 82 batch_loss: 0.1110769510269165\n",
      "training: 9 batch 83 batch_loss: 0.1147017776966095\n",
      "training: 9 batch 84 batch_loss: 0.11303222179412842\n",
      "training: 9 batch 85 batch_loss: 0.10924887657165527\n",
      "training: 9 batch 86 batch_loss: 0.11103206872940063\n",
      "training: 9 batch 87 batch_loss: 0.111533522605896\n",
      "training: 9 batch 88 batch_loss: 0.1103619933128357\n",
      "training: 9 batch 89 batch_loss: 0.11366784572601318\n",
      "training: 9 batch 90 batch_loss: 0.11121779680252075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 9 batch 91 batch_loss: 0.11234980821609497\n",
      "training: 9 batch 92 batch_loss: 0.10943782329559326\n",
      "training: 9 batch 93 batch_loss: 0.11245235800743103\n",
      "training: 9 batch 94 batch_loss: 0.10919937491416931\n",
      "training: 9 batch 95 batch_loss: 0.1099993884563446\n",
      "training: 9 batch 96 batch_loss: 0.11049959063529968\n",
      "training: 9 batch 97 batch_loss: 0.10939791798591614\n",
      "training: 9 batch 98 batch_loss: 0.11220231652259827\n",
      "training: 9 batch 99 batch_loss: 0.1099981963634491\n",
      "training: 9 batch 100 batch_loss: 0.11005648970603943\n",
      "training: 9 batch 101 batch_loss: 0.11013251543045044\n",
      "training: 9 batch 102 batch_loss: 0.11234897375106812\n",
      "training: 9 batch 103 batch_loss: 0.11491304636001587\n",
      "training: 9 batch 104 batch_loss: 0.11010721325874329\n",
      "training: 9 batch 105 batch_loss: 0.10840845108032227\n",
      "training: 9 batch 106 batch_loss: 0.1098983883857727\n",
      "training: 9 batch 107 batch_loss: 0.11442527174949646\n",
      "training: 9 batch 108 batch_loss: 0.11014750599861145\n",
      "training: 9 batch 109 batch_loss: 0.11074793338775635\n",
      "training: 9 batch 110 batch_loss: 0.11278325319290161\n",
      "training: 9 batch 111 batch_loss: 0.11261138319969177\n",
      "training: 9 batch 112 batch_loss: 0.11309689283370972\n",
      "training: 9 batch 113 batch_loss: 0.11254048347473145\n",
      "training: 9 batch 114 batch_loss: 0.11152011156082153\n",
      "training: 9 batch 115 batch_loss: 0.11239534616470337\n",
      "training: 9 batch 116 batch_loss: 0.1102323830127716\n",
      "training: 9 batch 117 batch_loss: 0.1095539927482605\n",
      "training: 9 batch 118 batch_loss: 0.109782874584198\n",
      "training: 9 batch 119 batch_loss: 0.11207741498947144\n",
      "training: 9 batch 120 batch_loss: 0.11440858244895935\n",
      "training: 9 batch 121 batch_loss: 0.10951629281044006\n",
      "training: 9 batch 122 batch_loss: 0.11264845728874207\n",
      "training: 9 batch 123 batch_loss: 0.10847100615501404\n",
      "training: 9 batch 124 batch_loss: 0.11129370331764221\n",
      "training: 9 batch 125 batch_loss: 0.1126314103603363\n",
      "training: 9 batch 126 batch_loss: 0.11016502976417542\n",
      "training: 9 batch 127 batch_loss: 0.11019781231880188\n",
      "training: 9 batch 128 batch_loss: 0.11138641834259033\n",
      "training: 9 batch 129 batch_loss: 0.11327871680259705\n",
      "training: 9 batch 130 batch_loss: 0.11314070224761963\n",
      "training: 9 batch 131 batch_loss: 0.11114686727523804\n",
      "training: 9 batch 132 batch_loss: 0.10836747288703918\n",
      "training: 9 batch 133 batch_loss: 0.11067312955856323\n",
      "training: 9 batch 134 batch_loss: 0.11511561274528503\n",
      "training: 9 batch 135 batch_loss: 0.11044090986251831\n",
      "training: 9 batch 136 batch_loss: 0.11070916056632996\n",
      "training: 9 batch 137 batch_loss: 0.10966750979423523\n",
      "training: 9 batch 138 batch_loss: 0.10826092958450317\n",
      "training: 9 batch 139 batch_loss: 0.11021298170089722\n",
      "training: 9 batch 140 batch_loss: 0.10884436964988708\n",
      "training: 9 batch 141 batch_loss: 0.11272823810577393\n",
      "training: 9 batch 142 batch_loss: 0.11149072647094727\n",
      "training: 9 batch 143 batch_loss: 0.10858866572380066\n",
      "training: 9 batch 144 batch_loss: 0.11236077547073364\n",
      "training: 9 batch 145 batch_loss: 0.11458998918533325\n",
      "training: 9 batch 146 batch_loss: 0.11065000295639038\n",
      "training: 9 batch 147 batch_loss: 0.11063522100448608\n",
      "training: 9 batch 148 batch_loss: 0.11354491114616394\n",
      "training: 9 batch 149 batch_loss: 0.10827028751373291\n",
      "training: 9 batch 150 batch_loss: 0.1107950210571289\n",
      "training: 9 batch 151 batch_loss: 0.112586110830307\n",
      "training: 9 batch 152 batch_loss: 0.11241263151168823\n",
      "training: 9 batch 153 batch_loss: 0.11179682612419128\n",
      "training: 9 batch 154 batch_loss: 0.11292177438735962\n",
      "training: 9 batch 155 batch_loss: 0.11307969689369202\n",
      "training: 9 batch 156 batch_loss: 0.11381438374519348\n",
      "training: 9 batch 157 batch_loss: 0.11054408550262451\n",
      "training: 9 batch 158 batch_loss: 0.11004245281219482\n",
      "training: 9 batch 159 batch_loss: 0.11102089285850525\n",
      "training: 9 batch 160 batch_loss: 0.11154058575630188\n",
      "training: 9 batch 161 batch_loss: 0.1107807457447052\n",
      "training: 9 batch 162 batch_loss: 0.11274021863937378\n",
      "training: 9 batch 163 batch_loss: 0.11145138740539551\n",
      "training: 9 batch 164 batch_loss: 0.11312872171401978\n",
      "training: 9 batch 165 batch_loss: 0.1111917793750763\n",
      "training: 9 batch 166 batch_loss: 0.113125741481781\n",
      "training: 9 batch 167 batch_loss: 0.10696065425872803\n",
      "training: 9 batch 168 batch_loss: 0.11211052536964417\n",
      "training: 9 batch 169 batch_loss: 0.111350417137146\n",
      "training: 9 batch 170 batch_loss: 0.11090818047523499\n",
      "training: 9 batch 171 batch_loss: 0.11181479692459106\n",
      "training: 9 batch 172 batch_loss: 0.10904890298843384\n",
      "training: 9 batch 173 batch_loss: 0.11060768365859985\n",
      "training: 9 batch 174 batch_loss: 0.10913652181625366\n",
      "training: 9 batch 175 batch_loss: 0.11062023043632507\n",
      "training: 9 batch 176 batch_loss: 0.11378157138824463\n",
      "training: 9 batch 177 batch_loss: 0.11149418354034424\n",
      "training: 9 batch 178 batch_loss: 0.11436444520950317\n",
      "training: 9 batch 179 batch_loss: 0.1117076575756073\n",
      "training: 9 batch 180 batch_loss: 0.1131584644317627\n",
      "training: 9 batch 181 batch_loss: 0.11139452457427979\n",
      "training: 9 batch 182 batch_loss: 0.11386799812316895\n",
      "training: 9 batch 183 batch_loss: 0.11269202828407288\n",
      "training: 9 batch 184 batch_loss: 0.11111706495285034\n",
      "training: 9 batch 185 batch_loss: 0.11361032724380493\n",
      "training: 9 batch 186 batch_loss: 0.11200538277626038\n",
      "training: 9 batch 187 batch_loss: 0.10591787099838257\n",
      "training: 9 batch 188 batch_loss: 0.11181461811065674\n",
      "training: 9 batch 189 batch_loss: 0.1119825541973114\n",
      "training: 9 batch 190 batch_loss: 0.1129465103149414\n",
      "training: 9 batch 191 batch_loss: 0.11029356718063354\n",
      "training: 9 batch 192 batch_loss: 0.11407271027565002\n",
      "training: 9 batch 193 batch_loss: 0.11024266481399536\n",
      "training: 9 batch 194 batch_loss: 0.11213815212249756\n",
      "training: 9 batch 195 batch_loss: 0.11097803711891174\n",
      "training: 9 batch 196 batch_loss: 0.11317932605743408\n",
      "training: 9 batch 197 batch_loss: 0.11347368359565735\n",
      "training: 9 batch 198 batch_loss: 0.11448919773101807\n",
      "training: 9 batch 199 batch_loss: 0.11140882968902588\n",
      "training: 9 batch 200 batch_loss: 0.1114271879196167\n",
      "training: 9 batch 201 batch_loss: 0.11108121275901794\n",
      "training: 9 batch 202 batch_loss: 0.10653209686279297\n",
      "training: 9 batch 203 batch_loss: 0.11134028434753418\n",
      "training: 9 batch 204 batch_loss: 0.10987520217895508\n",
      "training: 9 batch 205 batch_loss: 0.10896199941635132\n",
      "training: 9 batch 206 batch_loss: 0.1117686927318573\n",
      "training: 9 batch 207 batch_loss: 0.11223423480987549\n",
      "training: 9 batch 208 batch_loss: 0.11099043488502502\n",
      "training: 9 batch 209 batch_loss: 0.11215397715568542\n",
      "training: 9 batch 210 batch_loss: 0.1142687201499939\n",
      "training: 9 batch 211 batch_loss: 0.11174055933952332\n",
      "training: 9 batch 212 batch_loss: 0.10980120301246643\n",
      "training: 9 batch 213 batch_loss: 0.1114373505115509\n",
      "training: 9 batch 214 batch_loss: 0.11305421590805054\n",
      "training: 9 batch 215 batch_loss: 0.11347737908363342\n",
      "training: 9 batch 216 batch_loss: 0.11153724789619446\n",
      "training: 9 batch 217 batch_loss: 0.1146480143070221\n",
      "training: 9 batch 218 batch_loss: 0.11084407567977905\n",
      "training: 9 batch 219 batch_loss: 0.1111273467540741\n",
      "training: 9 batch 220 batch_loss: 0.11296972632408142\n",
      "training: 9 batch 221 batch_loss: 0.11400464177131653\n",
      "training: 9 batch 222 batch_loss: 0.11096197366714478\n",
      "training: 9 batch 223 batch_loss: 0.11403536796569824\n",
      "training: 9 batch 224 batch_loss: 0.11210250854492188\n",
      "training: 9 batch 225 batch_loss: 0.11288401484489441\n",
      "training: 9 batch 226 batch_loss: 0.11317145824432373\n",
      "training: 9 batch 227 batch_loss: 0.11319151520729065\n",
      "training: 9 batch 228 batch_loss: 0.1130760908126831\n",
      "training: 9 batch 229 batch_loss: 0.11215347051620483\n",
      "training: 9 batch 230 batch_loss: 0.111339271068573\n",
      "training: 9 batch 231 batch_loss: 0.11344528198242188\n",
      "training: 9 batch 232 batch_loss: 0.11362612247467041\n",
      "training: 9 batch 233 batch_loss: 0.11231297254562378\n",
      "training: 9 batch 234 batch_loss: 0.11131742596626282\n",
      "training: 9 batch 235 batch_loss: 0.10995864868164062\n",
      "training: 9 batch 236 batch_loss: 0.11480006575584412\n",
      "training: 9 batch 237 batch_loss: 0.1136651337146759\n",
      "training: 9 batch 238 batch_loss: 0.11164912581443787\n",
      "training: 9 batch 239 batch_loss: 0.11142140626907349\n",
      "training: 9 batch 240 batch_loss: 0.10994505882263184\n",
      "training: 9 batch 241 batch_loss: 0.10971996188163757\n",
      "training: 9 batch 242 batch_loss: 0.11282846331596375\n",
      "training: 9 batch 243 batch_loss: 0.11389291286468506\n",
      "training: 9 batch 244 batch_loss: 0.1081191897392273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 9 batch 245 batch_loss: 0.11261588335037231\n",
      "training: 9 batch 246 batch_loss: 0.11319828033447266\n",
      "training: 9 batch 247 batch_loss: 0.11367353796958923\n",
      "training: 9 batch 248 batch_loss: 0.11219027638435364\n",
      "training: 9 batch 249 batch_loss: 0.11175999045372009\n",
      "training: 9 batch 250 batch_loss: 0.11262443661689758\n",
      "training: 9 batch 251 batch_loss: 0.11298158764839172\n",
      "training: 9 batch 252 batch_loss: 0.11395454406738281\n",
      "training: 9 batch 253 batch_loss: 0.11239701509475708\n",
      "training: 9 batch 254 batch_loss: 0.11629882454872131\n",
      "training: 9 batch 255 batch_loss: 0.10950964689254761\n",
      "training: 9 batch 256 batch_loss: 0.11328038573265076\n",
      "training: 9 batch 257 batch_loss: 0.11145582795143127\n",
      "training: 9 batch 258 batch_loss: 0.11262807250022888\n",
      "training: 9 batch 259 batch_loss: 0.11190620064735413\n",
      "training: 9 batch 260 batch_loss: 0.11065801978111267\n",
      "training: 9 batch 261 batch_loss: 0.11066165566444397\n",
      "training: 9 batch 262 batch_loss: 0.11110004782676697\n",
      "training: 9 batch 263 batch_loss: 0.11015260219573975\n",
      "training: 9 batch 264 batch_loss: 0.11464393138885498\n",
      "training: 9 batch 265 batch_loss: 0.11074402928352356\n",
      "training: 9 batch 266 batch_loss: 0.11250686645507812\n",
      "training: 9 batch 267 batch_loss: 0.11421874165534973\n",
      "training: 9 batch 268 batch_loss: 0.11239561438560486\n",
      "training: 9 batch 269 batch_loss: 0.11138522624969482\n",
      "training: 9 batch 270 batch_loss: 0.1131669282913208\n",
      "training: 9 batch 271 batch_loss: 0.11305803060531616\n",
      "training: 9 batch 272 batch_loss: 0.11500605940818787\n",
      "training: 9 batch 273 batch_loss: 0.11192569136619568\n",
      "training: 9 batch 274 batch_loss: 0.11203357577323914\n",
      "training: 9 batch 275 batch_loss: 0.11429408192634583\n",
      "training: 9 batch 276 batch_loss: 0.11067080497741699\n",
      "training: 9 batch 277 batch_loss: 0.11097905039787292\n",
      "training: 9 batch 278 batch_loss: 0.11372959613800049\n",
      "training: 9 batch 279 batch_loss: 0.11089479923248291\n",
      "training: 9 batch 280 batch_loss: 0.11246076226234436\n",
      "training: 9 batch 281 batch_loss: 0.1120542585849762\n",
      "training: 9 batch 282 batch_loss: 0.11515280604362488\n",
      "training: 9 batch 283 batch_loss: 0.11186069250106812\n",
      "training: 9 batch 284 batch_loss: 0.11208897829055786\n",
      "training: 9 batch 285 batch_loss: 0.11030802130699158\n",
      "training: 9 batch 286 batch_loss: 0.11284300684928894\n",
      "training: 9 batch 287 batch_loss: 0.1099652349948883\n",
      "training: 9 batch 288 batch_loss: 0.11256107687950134\n",
      "training: 9 batch 289 batch_loss: 0.11066848039627075\n",
      "training: 9 batch 290 batch_loss: 0.11500522494316101\n",
      "training: 9 batch 291 batch_loss: 0.1100664734840393\n",
      "training: 9 batch 292 batch_loss: 0.11433002352714539\n",
      "training: 9 batch 293 batch_loss: 0.11459416151046753\n",
      "training: 9 batch 294 batch_loss: 0.11102497577667236\n",
      "training: 9 batch 295 batch_loss: 0.11251658201217651\n",
      "training: 9 batch 296 batch_loss: 0.11047568917274475\n",
      "training: 9 batch 297 batch_loss: 0.1137532889842987\n",
      "training: 9 batch 298 batch_loss: 0.11297926306724548\n",
      "training: 9 batch 299 batch_loss: 0.11563631892204285\n",
      "training: 9 batch 300 batch_loss: 0.11329042911529541\n",
      "training: 9 batch 301 batch_loss: 0.11366015672683716\n",
      "training: 9 batch 302 batch_loss: 0.11079564690589905\n",
      "training: 9 batch 303 batch_loss: 0.1147652268409729\n",
      "training: 9 batch 304 batch_loss: 0.111726313829422\n",
      "training: 9 batch 305 batch_loss: 0.11389511823654175\n",
      "training: 9 batch 306 batch_loss: 0.11363548040390015\n",
      "training: 9 batch 307 batch_loss: 0.11404538154602051\n",
      "training: 9 batch 308 batch_loss: 0.11176365613937378\n",
      "training: 9 batch 309 batch_loss: 0.11300548911094666\n",
      "training: 9 batch 310 batch_loss: 0.1107979416847229\n",
      "training: 9 batch 311 batch_loss: 0.1127103865146637\n",
      "training: 9 batch 312 batch_loss: 0.11131978034973145\n",
      "training: 9 batch 313 batch_loss: 0.11176842451095581\n",
      "training: 9 batch 314 batch_loss: 0.11532393097877502\n",
      "training: 9 batch 315 batch_loss: 0.11339542269706726\n",
      "training: 9 batch 316 batch_loss: 0.11087614297866821\n",
      "training: 9 batch 317 batch_loss: 0.11108177900314331\n",
      "training: 9 batch 318 batch_loss: 0.11408847570419312\n",
      "training: 9 batch 319 batch_loss: 0.11369508504867554\n",
      "training: 9 batch 320 batch_loss: 0.1113923192024231\n",
      "training: 9 batch 321 batch_loss: 0.11401212215423584\n",
      "training: 9 batch 322 batch_loss: 0.11518445611000061\n",
      "training: 9 batch 323 batch_loss: 0.114865243434906\n",
      "training: 9 batch 324 batch_loss: 0.1149490475654602\n",
      "training: 9 batch 325 batch_loss: 0.11359256505966187\n",
      "training: 9 batch 326 batch_loss: 0.11395370960235596\n",
      "training: 9 batch 327 batch_loss: 0.11350950598716736\n",
      "training: 9 batch 328 batch_loss: 0.11261090636253357\n",
      "training: 9 batch 329 batch_loss: 0.11634770035743713\n",
      "training: 9 batch 330 batch_loss: 0.11079967021942139\n",
      "training: 9 batch 331 batch_loss: 0.11091399192810059\n",
      "training: 9 batch 332 batch_loss: 0.11451736092567444\n",
      "training: 9 batch 333 batch_loss: 0.1089094877243042\n",
      "training: 9 batch 334 batch_loss: 0.10999929904937744\n",
      "training: 9 batch 335 batch_loss: 0.11377465724945068\n",
      "training: 9 batch 336 batch_loss: 0.1101609468460083\n",
      "training: 9 batch 337 batch_loss: 0.1114148497581482\n",
      "training: 9 batch 338 batch_loss: 0.116445392370224\n",
      "training: 9 batch 339 batch_loss: 0.11710751056671143\n",
      "training: 9 batch 340 batch_loss: 0.11449465155601501\n",
      "training: 9 batch 341 batch_loss: 0.11176970601081848\n",
      "training: 9 batch 342 batch_loss: 0.1084551215171814\n",
      "training: 9 batch 343 batch_loss: 0.11442741751670837\n",
      "training: 9 batch 344 batch_loss: 0.11597439646720886\n",
      "training: 9 batch 345 batch_loss: 0.1142212450504303\n",
      "training: 9 batch 346 batch_loss: 0.11522248387336731\n",
      "training: 9 batch 347 batch_loss: 0.11185172200202942\n",
      "training: 9 batch 348 batch_loss: 0.10940209031105042\n",
      "training: 9 batch 349 batch_loss: 0.11323714256286621\n",
      "training: 9 batch 350 batch_loss: 0.11532929539680481\n",
      "training: 9 batch 351 batch_loss: 0.11113592982292175\n",
      "training: 9 batch 352 batch_loss: 0.11517620086669922\n",
      "training: 9 batch 353 batch_loss: 0.11173224449157715\n",
      "training: 9 batch 354 batch_loss: 0.11531409621238708\n",
      "training: 9 batch 355 batch_loss: 0.11256524920463562\n",
      "training: 9 batch 356 batch_loss: 0.11126682162284851\n",
      "training: 9 batch 357 batch_loss: 0.11159348487854004\n",
      "training: 9 batch 358 batch_loss: 0.11198419332504272\n",
      "training: 9 batch 359 batch_loss: 0.11172643303871155\n",
      "training: 9 batch 360 batch_loss: 0.11236971616744995\n",
      "training: 9 batch 361 batch_loss: 0.1124914288520813\n",
      "training: 9 batch 362 batch_loss: 0.11239004135131836\n",
      "training: 9 batch 363 batch_loss: 0.1118060052394867\n",
      "training: 9 batch 364 batch_loss: 0.11271774768829346\n",
      "training: 9 batch 365 batch_loss: 0.11236101388931274\n",
      "training: 9 batch 366 batch_loss: 0.11646333336830139\n",
      "training: 9 batch 367 batch_loss: 0.11203595995903015\n",
      "training: 9 batch 368 batch_loss: 0.11163100600242615\n",
      "training: 9 batch 369 batch_loss: 0.11490890383720398\n",
      "training: 9 batch 370 batch_loss: 0.1112084686756134\n",
      "training: 9 batch 371 batch_loss: 0.11266648769378662\n",
      "training: 9 batch 372 batch_loss: 0.11491325497627258\n",
      "training: 9 batch 373 batch_loss: 0.11188527941703796\n",
      "training: 9 batch 374 batch_loss: 0.11534622311592102\n",
      "training: 9 batch 375 batch_loss: 0.11163660883903503\n",
      "training: 9 batch 376 batch_loss: 0.11122527718544006\n",
      "training: 9 batch 377 batch_loss: 0.11546117067337036\n",
      "training: 9 batch 378 batch_loss: 0.11249309778213501\n",
      "training: 9 batch 379 batch_loss: 0.11691927909851074\n",
      "training: 9 batch 380 batch_loss: 0.11436262726783752\n",
      "training: 9 batch 381 batch_loss: 0.11571666598320007\n",
      "training: 9 batch 382 batch_loss: 0.11466741561889648\n",
      "training: 9 batch 383 batch_loss: 0.11141601204872131\n",
      "training: 9 batch 384 batch_loss: 0.11362937092781067\n",
      "training: 9 batch 385 batch_loss: 0.11351972818374634\n",
      "training: 9 batch 386 batch_loss: 0.11244988441467285\n",
      "training: 9 batch 387 batch_loss: 0.11212721467018127\n",
      "training: 9 batch 388 batch_loss: 0.10970175266265869\n",
      "training: 9 batch 389 batch_loss: 0.11291289329528809\n",
      "training: 9 batch 390 batch_loss: 0.11495712399482727\n",
      "training: 9 batch 391 batch_loss: 0.11328765749931335\n",
      "training: 9 batch 392 batch_loss: 0.11473757028579712\n",
      "training: 9 batch 393 batch_loss: 0.1114417314529419\n",
      "training: 9 batch 394 batch_loss: 0.11218798160552979\n",
      "training: 9 batch 395 batch_loss: 0.11399760842323303\n",
      "training: 9 batch 396 batch_loss: 0.11279541254043579\n",
      "training: 9 batch 397 batch_loss: 0.11271458864212036\n",
      "training: 9 batch 398 batch_loss: 0.11714565753936768\n",
      "training: 9 batch 399 batch_loss: 0.11316829919815063\n",
      "training: 9 batch 400 batch_loss: 0.11115479469299316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 9 batch 401 batch_loss: 0.11129885911941528\n",
      "training: 9 batch 402 batch_loss: 0.1110067069530487\n",
      "training: 9 batch 403 batch_loss: 0.1148095428943634\n",
      "training: 9 batch 404 batch_loss: 0.11207443475723267\n",
      "training: 9 batch 405 batch_loss: 0.11223804950714111\n",
      "training: 9 batch 406 batch_loss: 0.1163831353187561\n",
      "training: 9 batch 407 batch_loss: 0.11348363757133484\n",
      "training: 9 batch 408 batch_loss: 0.1146891713142395\n",
      "training: 9 batch 409 batch_loss: 0.11334502696990967\n",
      "training: 9 batch 410 batch_loss: 0.11503985524177551\n",
      "training: 9 batch 411 batch_loss: 0.11357828974723816\n",
      "training: 9 batch 412 batch_loss: 0.10914254188537598\n",
      "training: 9 batch 413 batch_loss: 0.11624211072921753\n",
      "training: 9 batch 414 batch_loss: 0.11413872241973877\n",
      "training: 9 batch 415 batch_loss: 0.11272668838500977\n",
      "training: 9 batch 416 batch_loss: 0.11559733748435974\n",
      "training: 9 batch 417 batch_loss: 0.11341840028762817\n",
      "training: 9 batch 418 batch_loss: 0.11230742931365967\n",
      "training: 9 batch 419 batch_loss: 0.11464247107505798\n",
      "training: 9 batch 420 batch_loss: 0.11319702863693237\n",
      "training: 9 batch 421 batch_loss: 0.11663097143173218\n",
      "training: 9 batch 422 batch_loss: 0.11206114292144775\n",
      "training: 9 batch 423 batch_loss: 0.11458611488342285\n",
      "training: 9 batch 424 batch_loss: 0.11336129903793335\n",
      "training: 9 batch 425 batch_loss: 0.11670854687690735\n",
      "training: 9 batch 426 batch_loss: 0.11152586340904236\n",
      "training: 9 batch 427 batch_loss: 0.11470359563827515\n",
      "training: 9 batch 428 batch_loss: 0.11292344331741333\n",
      "training: 9 batch 429 batch_loss: 0.11241388320922852\n",
      "training: 9 batch 430 batch_loss: 0.11329907178878784\n",
      "training: 9 batch 431 batch_loss: 0.11282038688659668\n",
      "training: 9 batch 432 batch_loss: 0.11570790410041809\n",
      "training: 9 batch 433 batch_loss: 0.11422362923622131\n",
      "training: 9 batch 434 batch_loss: 0.11371776461601257\n",
      "training: 9 batch 435 batch_loss: 0.11424621939659119\n",
      "training: 9 batch 436 batch_loss: 0.11355021595954895\n",
      "training: 9 batch 437 batch_loss: 0.11665022373199463\n",
      "training: 9 batch 438 batch_loss: 0.11305344104766846\n",
      "training: 9 batch 439 batch_loss: 0.11458843946456909\n",
      "training: 9 batch 440 batch_loss: 0.11425697803497314\n",
      "training: 9 batch 441 batch_loss: 0.11827221512794495\n",
      "training: 9 batch 442 batch_loss: 0.11367997527122498\n",
      "training: 9 batch 443 batch_loss: 0.11600431799888611\n",
      "training: 9 batch 444 batch_loss: 0.11499890685081482\n",
      "training: 9 batch 445 batch_loss: 0.11187133193016052\n",
      "training: 9 batch 446 batch_loss: 0.11225873231887817\n",
      "training: 9 batch 447 batch_loss: 0.10994783043861389\n",
      "training: 9 batch 448 batch_loss: 0.1134098470211029\n",
      "training: 9 batch 449 batch_loss: 0.11409813165664673\n",
      "training: 9 batch 450 batch_loss: 0.11220479011535645\n",
      "training: 9 batch 451 batch_loss: 0.11271604895591736\n",
      "training: 9 batch 452 batch_loss: 0.11196652054786682\n",
      "training: 9 batch 453 batch_loss: 0.11654448509216309\n",
      "training: 9 batch 454 batch_loss: 0.11436045169830322\n",
      "training: 9 batch 455 batch_loss: 0.11087930202484131\n",
      "training: 9 batch 456 batch_loss: 0.11594131588935852\n",
      "training: 9 batch 457 batch_loss: 0.11339318752288818\n",
      "training: 9 batch 458 batch_loss: 0.11044847965240479\n",
      "training: 9 batch 459 batch_loss: 0.11300814151763916\n",
      "training: 9 batch 460 batch_loss: 0.11309084296226501\n",
      "training: 9 batch 461 batch_loss: 0.11391162872314453\n",
      "training: 9 batch 462 batch_loss: 0.1151609718799591\n",
      "training: 9 batch 463 batch_loss: 0.1168026328086853\n",
      "training: 9 batch 464 batch_loss: 0.11545932292938232\n",
      "training: 9 batch 465 batch_loss: 0.11300569772720337\n",
      "training: 9 batch 466 batch_loss: 0.11149123311042786\n",
      "training: 9 batch 467 batch_loss: 0.1155928373336792\n",
      "training: 9 batch 468 batch_loss: 0.11562201380729675\n",
      "training: 9 batch 469 batch_loss: 0.1137734055519104\n",
      "training: 9 batch 470 batch_loss: 0.11620402336120605\n",
      "training: 9 batch 471 batch_loss: 0.11289671063423157\n",
      "training: 9 batch 472 batch_loss: 0.11315491795539856\n",
      "training: 9 batch 473 batch_loss: 0.11394000053405762\n",
      "training: 9 batch 474 batch_loss: 0.113421231508255\n",
      "training: 9 batch 475 batch_loss: 0.11378225684165955\n",
      "training: 9 batch 476 batch_loss: 0.11318764090538025\n",
      "training: 9 batch 477 batch_loss: 0.1130286157131195\n",
      "training: 9 batch 478 batch_loss: 0.11341029405593872\n",
      "training: 9 batch 479 batch_loss: 0.1145365834236145\n",
      "training: 9 batch 480 batch_loss: 0.11481991410255432\n",
      "training: 9 batch 481 batch_loss: 0.11091053485870361\n",
      "training: 9 batch 482 batch_loss: 0.11697530746459961\n",
      "training: 9 batch 483 batch_loss: 0.1130492091178894\n",
      "training: 9 batch 484 batch_loss: 0.11465710401535034\n",
      "training: 9 batch 485 batch_loss: 0.11434739828109741\n",
      "training: 9 batch 486 batch_loss: 0.11374276876449585\n",
      "training: 9 batch 487 batch_loss: 0.11501163244247437\n",
      "training: 9 batch 488 batch_loss: 0.11483946442604065\n",
      "training: 9 batch 489 batch_loss: 0.1110185980796814\n",
      "training: 9 batch 490 batch_loss: 0.11334389448165894\n",
      "training: 9 batch 491 batch_loss: 0.11393630504608154\n",
      "training: 9 batch 492 batch_loss: 0.11697423458099365\n",
      "training: 9 batch 493 batch_loss: 0.1136273741722107\n",
      "training: 9 batch 494 batch_loss: 0.11259439587593079\n",
      "training: 9 batch 495 batch_loss: 0.11526945233345032\n",
      "training: 9 batch 496 batch_loss: 0.11151149868965149\n",
      "training: 9 batch 497 batch_loss: 0.11207953095436096\n",
      "training: 9 batch 498 batch_loss: 0.11382028460502625\n",
      "training: 9 batch 499 batch_loss: 0.11689558625221252\n",
      "training: 9 batch 500 batch_loss: 0.11267191171646118\n",
      "training: 9 batch 501 batch_loss: 0.11354130506515503\n",
      "training: 9 batch 502 batch_loss: 0.11612564325332642\n",
      "training: 9 batch 503 batch_loss: 0.11371767520904541\n",
      "training: 9 batch 504 batch_loss: 0.11491531133651733\n",
      "training: 9 batch 505 batch_loss: 0.11295568943023682\n",
      "training: 9 batch 506 batch_loss: 0.11515748500823975\n",
      "training: 9 batch 507 batch_loss: 0.11397102475166321\n",
      "training: 9 batch 508 batch_loss: 0.1126440167427063\n",
      "training: 9 batch 509 batch_loss: 0.116976797580719\n",
      "training: 9 batch 510 batch_loss: 0.1145223081111908\n",
      "training: 9 batch 511 batch_loss: 0.1147216260433197\n",
      "training: 9 batch 512 batch_loss: 0.11477243900299072\n",
      "training: 9 batch 513 batch_loss: 0.11406600475311279\n",
      "training: 9 batch 514 batch_loss: 0.11652690172195435\n",
      "training: 9 batch 515 batch_loss: 0.11273366212844849\n",
      "training: 9 batch 516 batch_loss: 0.11390995979309082\n",
      "training: 9 batch 517 batch_loss: 0.11087331175804138\n",
      "training: 9 batch 518 batch_loss: 0.11525285243988037\n",
      "training: 9 batch 519 batch_loss: 0.11698755621910095\n",
      "training: 9 batch 520 batch_loss: 0.11379539966583252\n",
      "training: 9 batch 521 batch_loss: 0.11564013361930847\n",
      "training: 9 batch 522 batch_loss: 0.11467131972312927\n",
      "training: 9 batch 523 batch_loss: 0.1150040328502655\n",
      "training: 9 batch 524 batch_loss: 0.11339497566223145\n",
      "training: 9 batch 525 batch_loss: 0.11454847455024719\n",
      "training: 9 batch 526 batch_loss: 0.11348950862884521\n",
      "training: 9 batch 527 batch_loss: 0.11312994360923767\n",
      "training: 9 batch 528 batch_loss: 0.11487442255020142\n",
      "training: 9 batch 529 batch_loss: 0.11248180270195007\n",
      "training: 9 batch 530 batch_loss: 0.11420449614524841\n",
      "training: 9 batch 531 batch_loss: 0.11410161852836609\n",
      "training: 9 batch 532 batch_loss: 0.11349573731422424\n",
      "training: 9 batch 533 batch_loss: 0.11332741379737854\n",
      "training: 9 batch 534 batch_loss: 0.11214655637741089\n",
      "training: 9 batch 535 batch_loss: 0.11399483680725098\n",
      "training: 9 batch 536 batch_loss: 0.1124357283115387\n",
      "training: 9 batch 537 batch_loss: 0.11243647336959839\n",
      "training: 9 batch 538 batch_loss: 0.11656758189201355\n",
      "training: 9 batch 539 batch_loss: 0.11530187726020813\n",
      "training: 9 batch 540 batch_loss: 0.11564511060714722\n",
      "training: 9 batch 541 batch_loss: 0.11065438389778137\n",
      "training: 9 batch 542 batch_loss: 0.11537450551986694\n",
      "training: 9 batch 543 batch_loss: 0.11148890852928162\n",
      "training: 9 batch 544 batch_loss: 0.11704373359680176\n",
      "training: 9 batch 545 batch_loss: 0.11282125115394592\n",
      "training: 9 batch 546 batch_loss: 0.11575523018836975\n",
      "training: 9 batch 547 batch_loss: 0.11302095651626587\n",
      "training: 9 batch 548 batch_loss: 0.11580219864845276\n",
      "training: 9 batch 549 batch_loss: 0.116273432970047\n",
      "training: 9 batch 550 batch_loss: 0.11244595050811768\n",
      "training: 9 batch 551 batch_loss: 0.11478692293167114\n",
      "training: 9 batch 552 batch_loss: 0.113210529088974\n",
      "training: 9 batch 553 batch_loss: 0.11363869905471802\n",
      "training: 9 batch 554 batch_loss: 0.1136343777179718\n",
      "training: 9 batch 555 batch_loss: 0.11871778964996338\n",
      "training: 9 batch 556 batch_loss: 0.11437380313873291\n",
      "training: 9 batch 557 batch_loss: 0.11304157972335815\n",
      "training: 9 batch 558 batch_loss: 0.11443042755126953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 9 batch 559 batch_loss: 0.11360490322113037\n",
      "training: 9 batch 560 batch_loss: 0.11270129680633545\n",
      "training: 9 batch 561 batch_loss: 0.11355984210968018\n",
      "training: 9 batch 562 batch_loss: 0.11449506878852844\n",
      "training: 9 batch 563 batch_loss: 0.11488428711891174\n",
      "training: 9 batch 564 batch_loss: 0.11536699533462524\n",
      "training: 9 batch 565 batch_loss: 0.1160149872303009\n",
      "training: 9 batch 566 batch_loss: 0.11495232582092285\n",
      "training: 9 batch 567 batch_loss: 0.11661580204963684\n",
      "training: 9 batch 568 batch_loss: 0.11555510759353638\n",
      "training: 9 batch 569 batch_loss: 0.11454463005065918\n",
      "training: 9 batch 570 batch_loss: 0.11052539944648743\n",
      "training: 9 batch 571 batch_loss: 0.11611652374267578\n",
      "training: 9 batch 572 batch_loss: 0.11446282267570496\n",
      "training: 9 batch 573 batch_loss: 0.1152399480342865\n",
      "training: 9 batch 574 batch_loss: 0.11513134837150574\n",
      "training: 9 batch 575 batch_loss: 0.11579245328903198\n",
      "training: 9 batch 576 batch_loss: 0.11673027276992798\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 9, Hit Ratio:0.023211733784598575 | Precision:0.03424751793964416 | Recall:0.04923816458410927 | NDCG:0.04271160985304105\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 10 batch 0 batch_loss: 0.11073952913284302\n",
      "training: 10 batch 1 batch_loss: 0.11252978444099426\n",
      "training: 10 batch 2 batch_loss: 0.11092662811279297\n",
      "training: 10 batch 3 batch_loss: 0.11014217138290405\n",
      "training: 10 batch 4 batch_loss: 0.11134511232376099\n",
      "training: 10 batch 5 batch_loss: 0.1114274263381958\n",
      "training: 10 batch 6 batch_loss: 0.11349350214004517\n",
      "training: 10 batch 7 batch_loss: 0.11224225163459778\n",
      "training: 10 batch 8 batch_loss: 0.10891491174697876\n",
      "training: 10 batch 9 batch_loss: 0.11247259378433228\n",
      "training: 10 batch 10 batch_loss: 0.11100643873214722\n",
      "training: 10 batch 11 batch_loss: 0.11308249831199646\n",
      "training: 10 batch 12 batch_loss: 0.11385178565979004\n",
      "training: 10 batch 13 batch_loss: 0.1128789484500885\n",
      "training: 10 batch 14 batch_loss: 0.11471894383430481\n",
      "training: 10 batch 15 batch_loss: 0.11611953377723694\n",
      "training: 10 batch 16 batch_loss: 0.11249977350234985\n",
      "training: 10 batch 17 batch_loss: 0.11497598886489868\n",
      "training: 10 batch 18 batch_loss: 0.11170244216918945\n",
      "training: 10 batch 19 batch_loss: 0.11510679125785828\n",
      "training: 10 batch 20 batch_loss: 0.11358153820037842\n",
      "training: 10 batch 21 batch_loss: 0.11265543103218079\n",
      "training: 10 batch 22 batch_loss: 0.11416453123092651\n",
      "training: 10 batch 23 batch_loss: 0.11290779709815979\n",
      "training: 10 batch 24 batch_loss: 0.1169390082359314\n",
      "training: 10 batch 25 batch_loss: 0.11102813482284546\n",
      "training: 10 batch 26 batch_loss: 0.11201420426368713\n",
      "training: 10 batch 27 batch_loss: 0.11454862356185913\n",
      "training: 10 batch 28 batch_loss: 0.11285045742988586\n",
      "training: 10 batch 29 batch_loss: 0.11039647459983826\n",
      "training: 10 batch 30 batch_loss: 0.11112791299819946\n",
      "training: 10 batch 31 batch_loss: 0.11022883653640747\n",
      "training: 10 batch 32 batch_loss: 0.11437800526618958\n",
      "training: 10 batch 33 batch_loss: 0.11310091614723206\n",
      "training: 10 batch 34 batch_loss: 0.11009439826011658\n",
      "training: 10 batch 35 batch_loss: 0.11389496922492981\n",
      "training: 10 batch 36 batch_loss: 0.11600512266159058\n",
      "training: 10 batch 37 batch_loss: 0.11460673809051514\n",
      "training: 10 batch 38 batch_loss: 0.11505448818206787\n",
      "training: 10 batch 39 batch_loss: 0.11296001076698303\n",
      "training: 10 batch 40 batch_loss: 0.11160203814506531\n",
      "training: 10 batch 41 batch_loss: 0.11347931623458862\n",
      "training: 10 batch 42 batch_loss: 0.11341673135757446\n",
      "training: 10 batch 43 batch_loss: 0.1180582046508789\n",
      "training: 10 batch 44 batch_loss: 0.11560609936714172\n",
      "training: 10 batch 45 batch_loss: 0.11375397443771362\n",
      "training: 10 batch 46 batch_loss: 0.10779425501823425\n",
      "training: 10 batch 47 batch_loss: 0.11404034495353699\n",
      "training: 10 batch 48 batch_loss: 0.11310815811157227\n",
      "training: 10 batch 49 batch_loss: 0.11544159054756165\n",
      "training: 10 batch 50 batch_loss: 0.11392062902450562\n",
      "training: 10 batch 51 batch_loss: 0.1156652569770813\n",
      "training: 10 batch 52 batch_loss: 0.11337891221046448\n",
      "training: 10 batch 53 batch_loss: 0.11456173658370972\n",
      "training: 10 batch 54 batch_loss: 0.11247846484184265\n",
      "training: 10 batch 55 batch_loss: 0.11145198345184326\n",
      "training: 10 batch 56 batch_loss: 0.11047142744064331\n",
      "training: 10 batch 57 batch_loss: 0.11297932267189026\n",
      "training: 10 batch 58 batch_loss: 0.11431261897087097\n",
      "training: 10 batch 59 batch_loss: 0.11131179332733154\n",
      "training: 10 batch 60 batch_loss: 0.11266478896141052\n",
      "training: 10 batch 61 batch_loss: 0.11331045627593994\n",
      "training: 10 batch 62 batch_loss: 0.11072090268135071\n",
      "training: 10 batch 63 batch_loss: 0.11570581793785095\n",
      "training: 10 batch 64 batch_loss: 0.11382615566253662\n",
      "training: 10 batch 65 batch_loss: 0.1128447949886322\n",
      "training: 10 batch 66 batch_loss: 0.11410817503929138\n",
      "training: 10 batch 67 batch_loss: 0.11169707775115967\n",
      "training: 10 batch 68 batch_loss: 0.11728692054748535\n",
      "training: 10 batch 69 batch_loss: 0.11407378315925598\n",
      "training: 10 batch 70 batch_loss: 0.11242404580116272\n",
      "training: 10 batch 71 batch_loss: 0.11265769600868225\n",
      "training: 10 batch 72 batch_loss: 0.11222118139266968\n",
      "training: 10 batch 73 batch_loss: 0.11302196979522705\n",
      "training: 10 batch 74 batch_loss: 0.1143636703491211\n",
      "training: 10 batch 75 batch_loss: 0.11404818296432495\n",
      "training: 10 batch 76 batch_loss: 0.11422893404960632\n",
      "training: 10 batch 77 batch_loss: 0.11104804277420044\n",
      "training: 10 batch 78 batch_loss: 0.11365878582000732\n",
      "training: 10 batch 79 batch_loss: 0.1122383177280426\n",
      "training: 10 batch 80 batch_loss: 0.11428457498550415\n",
      "training: 10 batch 81 batch_loss: 0.11007717251777649\n",
      "training: 10 batch 82 batch_loss: 0.11313536763191223\n",
      "training: 10 batch 83 batch_loss: 0.1138211190700531\n",
      "training: 10 batch 84 batch_loss: 0.1107761561870575\n",
      "training: 10 batch 85 batch_loss: 0.11246749758720398\n",
      "training: 10 batch 86 batch_loss: 0.1162189245223999\n",
      "training: 10 batch 87 batch_loss: 0.113827645778656\n",
      "training: 10 batch 88 batch_loss: 0.1131315529346466\n",
      "training: 10 batch 89 batch_loss: 0.11399400234222412\n",
      "training: 10 batch 90 batch_loss: 0.11403614282608032\n",
      "training: 10 batch 91 batch_loss: 0.11344653367996216\n",
      "training: 10 batch 92 batch_loss: 0.11327269673347473\n",
      "training: 10 batch 93 batch_loss: 0.11586138606071472\n",
      "training: 10 batch 94 batch_loss: 0.11305248737335205\n",
      "training: 10 batch 95 batch_loss: 0.11485400795936584\n",
      "training: 10 batch 96 batch_loss: 0.112460196018219\n",
      "training: 10 batch 97 batch_loss: 0.11625280976295471\n",
      "training: 10 batch 98 batch_loss: 0.11002463102340698\n",
      "training: 10 batch 99 batch_loss: 0.1148260235786438\n",
      "training: 10 batch 100 batch_loss: 0.1157209575176239\n",
      "training: 10 batch 101 batch_loss: 0.11347854137420654\n",
      "training: 10 batch 102 batch_loss: 0.11288416385650635\n",
      "training: 10 batch 103 batch_loss: 0.11266392469406128\n",
      "training: 10 batch 104 batch_loss: 0.11468875408172607\n",
      "training: 10 batch 105 batch_loss: 0.11786586046218872\n",
      "training: 10 batch 106 batch_loss: 0.11198797821998596\n",
      "training: 10 batch 107 batch_loss: 0.11416754126548767\n",
      "training: 10 batch 108 batch_loss: 0.11232829093933105\n",
      "training: 10 batch 109 batch_loss: 0.11491873860359192\n",
      "training: 10 batch 110 batch_loss: 0.11113321781158447\n",
      "training: 10 batch 111 batch_loss: 0.11833435297012329\n",
      "training: 10 batch 112 batch_loss: 0.11199072003364563\n",
      "training: 10 batch 113 batch_loss: 0.11435455083847046\n",
      "training: 10 batch 114 batch_loss: 0.11126834154129028\n",
      "training: 10 batch 115 batch_loss: 0.11495164036750793\n",
      "training: 10 batch 116 batch_loss: 0.1124352216720581\n",
      "training: 10 batch 117 batch_loss: 0.11494624614715576\n",
      "training: 10 batch 118 batch_loss: 0.11961084604263306\n",
      "training: 10 batch 119 batch_loss: 0.1136561930179596\n",
      "training: 10 batch 120 batch_loss: 0.11461099982261658\n",
      "training: 10 batch 121 batch_loss: 0.11352196335792542\n",
      "training: 10 batch 122 batch_loss: 0.1168631911277771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 10 batch 123 batch_loss: 0.11248216032981873\n",
      "training: 10 batch 124 batch_loss: 0.11484074592590332\n",
      "training: 10 batch 125 batch_loss: 0.11628258228302002\n",
      "training: 10 batch 126 batch_loss: 0.11188161373138428\n",
      "training: 10 batch 127 batch_loss: 0.11438450217247009\n",
      "training: 10 batch 128 batch_loss: 0.11366835236549377\n",
      "training: 10 batch 129 batch_loss: 0.11327612400054932\n",
      "training: 10 batch 130 batch_loss: 0.11196863651275635\n",
      "training: 10 batch 131 batch_loss: 0.11585003137588501\n",
      "training: 10 batch 132 batch_loss: 0.1123010516166687\n",
      "training: 10 batch 133 batch_loss: 0.11666223406791687\n",
      "training: 10 batch 134 batch_loss: 0.11131089925765991\n",
      "training: 10 batch 135 batch_loss: 0.11159691214561462\n",
      "training: 10 batch 136 batch_loss: 0.11274546384811401\n",
      "training: 10 batch 137 batch_loss: 0.11349785327911377\n",
      "training: 10 batch 138 batch_loss: 0.1115666925907135\n",
      "training: 10 batch 139 batch_loss: 0.11538848280906677\n",
      "training: 10 batch 140 batch_loss: 0.1167733371257782\n",
      "training: 10 batch 141 batch_loss: 0.11652085185050964\n",
      "training: 10 batch 142 batch_loss: 0.11433133482933044\n",
      "training: 10 batch 143 batch_loss: 0.1144292950630188\n",
      "training: 10 batch 144 batch_loss: 0.1156291663646698\n",
      "training: 10 batch 145 batch_loss: 0.1155690848827362\n",
      "training: 10 batch 146 batch_loss: 0.1161763072013855\n",
      "training: 10 batch 147 batch_loss: 0.11474400758743286\n",
      "training: 10 batch 148 batch_loss: 0.1126793622970581\n",
      "training: 10 batch 149 batch_loss: 0.11338245868682861\n",
      "training: 10 batch 150 batch_loss: 0.1115843653678894\n",
      "training: 10 batch 151 batch_loss: 0.11133459210395813\n",
      "training: 10 batch 152 batch_loss: 0.114206463098526\n",
      "training: 10 batch 153 batch_loss: 0.11720332503318787\n",
      "training: 10 batch 154 batch_loss: 0.11711099743843079\n",
      "training: 10 batch 155 batch_loss: 0.112399160861969\n",
      "training: 10 batch 156 batch_loss: 0.11201447248458862\n",
      "training: 10 batch 157 batch_loss: 0.11329194903373718\n",
      "training: 10 batch 158 batch_loss: 0.11659032106399536\n",
      "training: 10 batch 159 batch_loss: 0.11570528149604797\n",
      "training: 10 batch 160 batch_loss: 0.11473774909973145\n",
      "training: 10 batch 161 batch_loss: 0.11009788513183594\n",
      "training: 10 batch 162 batch_loss: 0.11221474409103394\n",
      "training: 10 batch 163 batch_loss: 0.11624753475189209\n",
      "training: 10 batch 164 batch_loss: 0.11238929629325867\n",
      "training: 10 batch 165 batch_loss: 0.11588168144226074\n",
      "training: 10 batch 166 batch_loss: 0.1129101812839508\n",
      "training: 10 batch 167 batch_loss: 0.11493384838104248\n",
      "training: 10 batch 168 batch_loss: 0.11274999380111694\n",
      "training: 10 batch 169 batch_loss: 0.11405491828918457\n",
      "training: 10 batch 170 batch_loss: 0.11488819122314453\n",
      "training: 10 batch 171 batch_loss: 0.11607354879379272\n",
      "training: 10 batch 172 batch_loss: 0.11382275819778442\n",
      "training: 10 batch 173 batch_loss: 0.11581042408943176\n",
      "training: 10 batch 174 batch_loss: 0.11366388201713562\n",
      "training: 10 batch 175 batch_loss: 0.11283552646636963\n",
      "training: 10 batch 176 batch_loss: 0.11500221490859985\n",
      "training: 10 batch 177 batch_loss: 0.1175667941570282\n",
      "training: 10 batch 178 batch_loss: 0.11428093910217285\n",
      "training: 10 batch 179 batch_loss: 0.11428588628768921\n",
      "training: 10 batch 180 batch_loss: 0.11840593814849854\n",
      "training: 10 batch 181 batch_loss: 0.11288389563560486\n",
      "training: 10 batch 182 batch_loss: 0.1157454252243042\n",
      "training: 10 batch 183 batch_loss: 0.11809000372886658\n",
      "training: 10 batch 184 batch_loss: 0.11521565914154053\n",
      "training: 10 batch 185 batch_loss: 0.113726407289505\n",
      "training: 10 batch 186 batch_loss: 0.11250415444374084\n",
      "training: 10 batch 187 batch_loss: 0.11660057306289673\n",
      "training: 10 batch 188 batch_loss: 0.1156274676322937\n",
      "training: 10 batch 189 batch_loss: 0.11544391512870789\n",
      "training: 10 batch 190 batch_loss: 0.11565658450126648\n",
      "training: 10 batch 191 batch_loss: 0.1161094605922699\n",
      "training: 10 batch 192 batch_loss: 0.11569494009017944\n",
      "training: 10 batch 193 batch_loss: 0.11583760380744934\n",
      "training: 10 batch 194 batch_loss: 0.11506187915802002\n",
      "training: 10 batch 195 batch_loss: 0.11643907427787781\n",
      "training: 10 batch 196 batch_loss: 0.11484360694885254\n",
      "training: 10 batch 197 batch_loss: 0.11424136161804199\n",
      "training: 10 batch 198 batch_loss: 0.11617845296859741\n",
      "training: 10 batch 199 batch_loss: 0.11471900343894958\n",
      "training: 10 batch 200 batch_loss: 0.11881208419799805\n",
      "training: 10 batch 201 batch_loss: 0.11036458611488342\n",
      "training: 10 batch 202 batch_loss: 0.11477184295654297\n",
      "training: 10 batch 203 batch_loss: 0.11647853255271912\n",
      "training: 10 batch 204 batch_loss: 0.11700782179832458\n",
      "training: 10 batch 205 batch_loss: 0.11382737755775452\n",
      "training: 10 batch 206 batch_loss: 0.11690443754196167\n",
      "training: 10 batch 207 batch_loss: 0.11530104279518127\n",
      "training: 10 batch 208 batch_loss: 0.11037009954452515\n",
      "training: 10 batch 209 batch_loss: 0.11296054720878601\n",
      "training: 10 batch 210 batch_loss: 0.11501342058181763\n",
      "training: 10 batch 211 batch_loss: 0.1146308183670044\n",
      "training: 10 batch 212 batch_loss: 0.11722245812416077\n",
      "training: 10 batch 213 batch_loss: 0.11807364225387573\n",
      "training: 10 batch 214 batch_loss: 0.11545556783676147\n",
      "training: 10 batch 215 batch_loss: 0.11614936590194702\n",
      "training: 10 batch 216 batch_loss: 0.11602598428726196\n",
      "training: 10 batch 217 batch_loss: 0.11468103528022766\n",
      "training: 10 batch 218 batch_loss: 0.11588194966316223\n",
      "training: 10 batch 219 batch_loss: 0.114818274974823\n",
      "training: 10 batch 220 batch_loss: 0.11411026120185852\n",
      "training: 10 batch 221 batch_loss: 0.11620765924453735\n",
      "training: 10 batch 222 batch_loss: 0.11699381470680237\n",
      "training: 10 batch 223 batch_loss: 0.114844411611557\n",
      "training: 10 batch 224 batch_loss: 0.11097896099090576\n",
      "training: 10 batch 225 batch_loss: 0.11627897620201111\n",
      "training: 10 batch 226 batch_loss: 0.1123650074005127\n",
      "training: 10 batch 227 batch_loss: 0.11507436633110046\n",
      "training: 10 batch 228 batch_loss: 0.11458587646484375\n",
      "training: 10 batch 229 batch_loss: 0.11776852607727051\n",
      "training: 10 batch 230 batch_loss: 0.11474165320396423\n",
      "training: 10 batch 231 batch_loss: 0.11418884992599487\n",
      "training: 10 batch 232 batch_loss: 0.11248889565467834\n",
      "training: 10 batch 233 batch_loss: 0.11648017168045044\n",
      "training: 10 batch 234 batch_loss: 0.11745777726173401\n",
      "training: 10 batch 235 batch_loss: 0.11678871512413025\n",
      "training: 10 batch 236 batch_loss: 0.11491841077804565\n",
      "training: 10 batch 237 batch_loss: 0.11552339792251587\n",
      "training: 10 batch 238 batch_loss: 0.11608380079269409\n",
      "training: 10 batch 239 batch_loss: 0.11553183197975159\n",
      "training: 10 batch 240 batch_loss: 0.11353185772895813\n",
      "training: 10 batch 241 batch_loss: 0.11697301268577576\n",
      "training: 10 batch 242 batch_loss: 0.11591282486915588\n",
      "training: 10 batch 243 batch_loss: 0.11343944072723389\n",
      "training: 10 batch 244 batch_loss: 0.11401587724685669\n",
      "training: 10 batch 245 batch_loss: 0.11861163377761841\n",
      "training: 10 batch 246 batch_loss: 0.11629581451416016\n",
      "training: 10 batch 247 batch_loss: 0.11883324384689331\n",
      "training: 10 batch 248 batch_loss: 0.11586043238639832\n",
      "training: 10 batch 249 batch_loss: 0.116437166929245\n",
      "training: 10 batch 250 batch_loss: 0.11640235781669617\n",
      "training: 10 batch 251 batch_loss: 0.11302760243415833\n",
      "training: 10 batch 252 batch_loss: 0.11831578612327576\n",
      "training: 10 batch 253 batch_loss: 0.1151018738746643\n",
      "training: 10 batch 254 batch_loss: 0.11452972888946533\n",
      "training: 10 batch 255 batch_loss: 0.11642271280288696\n",
      "training: 10 batch 256 batch_loss: 0.11542028188705444\n",
      "training: 10 batch 257 batch_loss: 0.11655822396278381\n",
      "training: 10 batch 258 batch_loss: 0.11317610740661621\n",
      "training: 10 batch 259 batch_loss: 0.11392039060592651\n",
      "training: 10 batch 260 batch_loss: 0.11461734771728516\n",
      "training: 10 batch 261 batch_loss: 0.11577332019805908\n",
      "training: 10 batch 262 batch_loss: 0.11607927083969116\n",
      "training: 10 batch 263 batch_loss: 0.11747971177101135\n",
      "training: 10 batch 264 batch_loss: 0.11816513538360596\n",
      "training: 10 batch 265 batch_loss: 0.11483785510063171\n",
      "training: 10 batch 266 batch_loss: 0.11224755644798279\n",
      "training: 10 batch 267 batch_loss: 0.11578521132469177\n",
      "training: 10 batch 268 batch_loss: 0.11689287424087524\n",
      "training: 10 batch 269 batch_loss: 0.11588191986083984\n",
      "training: 10 batch 270 batch_loss: 0.11660042405128479\n",
      "training: 10 batch 271 batch_loss: 0.11504599452018738\n",
      "training: 10 batch 272 batch_loss: 0.116922527551651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 10 batch 273 batch_loss: 0.11533927917480469\n",
      "training: 10 batch 274 batch_loss: 0.11531054973602295\n",
      "training: 10 batch 275 batch_loss: 0.11889797449111938\n",
      "training: 10 batch 276 batch_loss: 0.11508727073669434\n",
      "training: 10 batch 277 batch_loss: 0.11587026715278625\n",
      "training: 10 batch 278 batch_loss: 0.11272326111793518\n",
      "training: 10 batch 279 batch_loss: 0.11474111676216125\n",
      "training: 10 batch 280 batch_loss: 0.11216223239898682\n",
      "training: 10 batch 281 batch_loss: 0.1158219575881958\n",
      "training: 10 batch 282 batch_loss: 0.11698848009109497\n",
      "training: 10 batch 283 batch_loss: 0.11410447955131531\n",
      "training: 10 batch 284 batch_loss: 0.1146090030670166\n",
      "training: 10 batch 285 batch_loss: 0.1167774498462677\n",
      "training: 10 batch 286 batch_loss: 0.11355727910995483\n",
      "training: 10 batch 287 batch_loss: 0.11359888315200806\n",
      "training: 10 batch 288 batch_loss: 0.11230435967445374\n",
      "training: 10 batch 289 batch_loss: 0.11554527282714844\n",
      "training: 10 batch 290 batch_loss: 0.11852535605430603\n",
      "training: 10 batch 291 batch_loss: 0.11543804407119751\n",
      "training: 10 batch 292 batch_loss: 0.11609917879104614\n",
      "training: 10 batch 293 batch_loss: 0.11641457676887512\n",
      "training: 10 batch 294 batch_loss: 0.11689701676368713\n",
      "training: 10 batch 295 batch_loss: 0.11729413270950317\n",
      "training: 10 batch 296 batch_loss: 0.11498105525970459\n",
      "training: 10 batch 297 batch_loss: 0.11512663960456848\n",
      "training: 10 batch 298 batch_loss: 0.11535438895225525\n",
      "training: 10 batch 299 batch_loss: 0.11523029208183289\n",
      "training: 10 batch 300 batch_loss: 0.11518862843513489\n",
      "training: 10 batch 301 batch_loss: 0.11612683534622192\n",
      "training: 10 batch 302 batch_loss: 0.11555999517440796\n",
      "training: 10 batch 303 batch_loss: 0.11564600467681885\n",
      "training: 10 batch 304 batch_loss: 0.11614876985549927\n",
      "training: 10 batch 305 batch_loss: 0.11540985107421875\n",
      "training: 10 batch 306 batch_loss: 0.11482477188110352\n",
      "training: 10 batch 307 batch_loss: 0.11743581295013428\n",
      "training: 10 batch 308 batch_loss: 0.11545833945274353\n",
      "training: 10 batch 309 batch_loss: 0.11337873339653015\n",
      "training: 10 batch 310 batch_loss: 0.11479610204696655\n",
      "training: 10 batch 311 batch_loss: 0.11222723126411438\n",
      "training: 10 batch 312 batch_loss: 0.11621004343032837\n",
      "training: 10 batch 313 batch_loss: 0.11614817380905151\n",
      "training: 10 batch 314 batch_loss: 0.1149781346321106\n",
      "training: 10 batch 315 batch_loss: 0.11547037959098816\n",
      "training: 10 batch 316 batch_loss: 0.11635619401931763\n",
      "training: 10 batch 317 batch_loss: 0.11638864874839783\n",
      "training: 10 batch 318 batch_loss: 0.11383965611457825\n",
      "training: 10 batch 319 batch_loss: 0.11665025353431702\n",
      "training: 10 batch 320 batch_loss: 0.11442631483078003\n",
      "training: 10 batch 321 batch_loss: 0.1181083619594574\n",
      "training: 10 batch 322 batch_loss: 0.11619508266448975\n",
      "training: 10 batch 323 batch_loss: 0.11699375510215759\n",
      "training: 10 batch 324 batch_loss: 0.11011630296707153\n",
      "training: 10 batch 325 batch_loss: 0.11637476086616516\n",
      "training: 10 batch 326 batch_loss: 0.1141391396522522\n",
      "training: 10 batch 327 batch_loss: 0.11432868242263794\n",
      "training: 10 batch 328 batch_loss: 0.1138961911201477\n",
      "training: 10 batch 329 batch_loss: 0.11703473329544067\n",
      "training: 10 batch 330 batch_loss: 0.11238616704940796\n",
      "training: 10 batch 331 batch_loss: 0.1170288622379303\n",
      "training: 10 batch 332 batch_loss: 0.11490407586097717\n",
      "training: 10 batch 333 batch_loss: 0.11523658037185669\n",
      "training: 10 batch 334 batch_loss: 0.1166028380393982\n",
      "training: 10 batch 335 batch_loss: 0.117439866065979\n",
      "training: 10 batch 336 batch_loss: 0.11526334285736084\n",
      "training: 10 batch 337 batch_loss: 0.11714199185371399\n",
      "training: 10 batch 338 batch_loss: 0.1146385669708252\n",
      "training: 10 batch 339 batch_loss: 0.11488622426986694\n",
      "training: 10 batch 340 batch_loss: 0.11399009823799133\n",
      "training: 10 batch 341 batch_loss: 0.11225444078445435\n",
      "training: 10 batch 342 batch_loss: 0.11528581380844116\n",
      "training: 10 batch 343 batch_loss: 0.11507466435432434\n",
      "training: 10 batch 344 batch_loss: 0.1153726577758789\n",
      "training: 10 batch 345 batch_loss: 0.1165628731250763\n",
      "training: 10 batch 346 batch_loss: 0.11495965719223022\n",
      "training: 10 batch 347 batch_loss: 0.11286848783493042\n",
      "training: 10 batch 348 batch_loss: 0.11811500787734985\n",
      "training: 10 batch 349 batch_loss: 0.11873885989189148\n",
      "training: 10 batch 350 batch_loss: 0.11765503883361816\n",
      "training: 10 batch 351 batch_loss: 0.11281576752662659\n",
      "training: 10 batch 352 batch_loss: 0.11668550968170166\n",
      "training: 10 batch 353 batch_loss: 0.11779728531837463\n",
      "training: 10 batch 354 batch_loss: 0.11750775575637817\n",
      "training: 10 batch 355 batch_loss: 0.11622753739356995\n",
      "training: 10 batch 356 batch_loss: 0.11528348922729492\n",
      "training: 10 batch 357 batch_loss: 0.11479437351226807\n",
      "training: 10 batch 358 batch_loss: 0.11545184254646301\n",
      "training: 10 batch 359 batch_loss: 0.11698144674301147\n",
      "training: 10 batch 360 batch_loss: 0.11653971672058105\n",
      "training: 10 batch 361 batch_loss: 0.11526614427566528\n",
      "training: 10 batch 362 batch_loss: 0.11605715751647949\n",
      "training: 10 batch 363 batch_loss: 0.11557763814926147\n",
      "training: 10 batch 364 batch_loss: 0.1158553957939148\n",
      "training: 10 batch 365 batch_loss: 0.11875784397125244\n",
      "training: 10 batch 366 batch_loss: 0.1141979992389679\n",
      "training: 10 batch 367 batch_loss: 0.11619028449058533\n",
      "training: 10 batch 368 batch_loss: 0.11732447147369385\n",
      "training: 10 batch 369 batch_loss: 0.11500188708305359\n",
      "training: 10 batch 370 batch_loss: 0.11582106351852417\n",
      "training: 10 batch 371 batch_loss: 0.11652365326881409\n",
      "training: 10 batch 372 batch_loss: 0.11633414030075073\n",
      "training: 10 batch 373 batch_loss: 0.1146727204322815\n",
      "training: 10 batch 374 batch_loss: 0.1133188009262085\n",
      "training: 10 batch 375 batch_loss: 0.11858156323432922\n",
      "training: 10 batch 376 batch_loss: 0.11773276329040527\n",
      "training: 10 batch 377 batch_loss: 0.11955294013023376\n",
      "training: 10 batch 378 batch_loss: 0.1157500147819519\n",
      "training: 10 batch 379 batch_loss: 0.11716797947883606\n",
      "training: 10 batch 380 batch_loss: 0.11548146605491638\n",
      "training: 10 batch 381 batch_loss: 0.11249017715454102\n",
      "training: 10 batch 382 batch_loss: 0.11460444331169128\n",
      "training: 10 batch 383 batch_loss: 0.11839991807937622\n",
      "training: 10 batch 384 batch_loss: 0.1143994927406311\n",
      "training: 10 batch 385 batch_loss: 0.1166943907737732\n",
      "training: 10 batch 386 batch_loss: 0.1145247220993042\n",
      "training: 10 batch 387 batch_loss: 0.11899319291114807\n",
      "training: 10 batch 388 batch_loss: 0.11803114414215088\n",
      "training: 10 batch 389 batch_loss: 0.1168505847454071\n",
      "training: 10 batch 390 batch_loss: 0.11493924260139465\n",
      "training: 10 batch 391 batch_loss: 0.11556833982467651\n",
      "training: 10 batch 392 batch_loss: 0.11669573187828064\n",
      "training: 10 batch 393 batch_loss: 0.11623647809028625\n",
      "training: 10 batch 394 batch_loss: 0.1180073618888855\n",
      "training: 10 batch 395 batch_loss: 0.11659026145935059\n",
      "training: 10 batch 396 batch_loss: 0.11588427424430847\n",
      "training: 10 batch 397 batch_loss: 0.11357918381690979\n",
      "training: 10 batch 398 batch_loss: 0.11673501133918762\n",
      "training: 10 batch 399 batch_loss: 0.1153392493724823\n",
      "training: 10 batch 400 batch_loss: 0.1159040629863739\n",
      "training: 10 batch 401 batch_loss: 0.11705607175827026\n",
      "training: 10 batch 402 batch_loss: 0.11391854286193848\n",
      "training: 10 batch 403 batch_loss: 0.11551380157470703\n",
      "training: 10 batch 404 batch_loss: 0.11682683229446411\n",
      "training: 10 batch 405 batch_loss: 0.11517739295959473\n",
      "training: 10 batch 406 batch_loss: 0.11743897199630737\n",
      "training: 10 batch 407 batch_loss: 0.11819708347320557\n",
      "training: 10 batch 408 batch_loss: 0.11703523993492126\n",
      "training: 10 batch 409 batch_loss: 0.11612409353256226\n",
      "training: 10 batch 410 batch_loss: 0.11516526341438293\n",
      "training: 10 batch 411 batch_loss: 0.11698007583618164\n",
      "training: 10 batch 412 batch_loss: 0.11611083149909973\n",
      "training: 10 batch 413 batch_loss: 0.115945965051651\n",
      "training: 10 batch 414 batch_loss: 0.11755529046058655\n",
      "training: 10 batch 415 batch_loss: 0.11450859904289246\n",
      "training: 10 batch 416 batch_loss: 0.11694806814193726\n",
      "training: 10 batch 417 batch_loss: 0.11652743816375732\n",
      "training: 10 batch 418 batch_loss: 0.11264732480049133\n",
      "training: 10 batch 419 batch_loss: 0.11778491735458374\n",
      "training: 10 batch 420 batch_loss: 0.11548981070518494\n",
      "training: 10 batch 421 batch_loss: 0.11604142189025879\n",
      "training: 10 batch 422 batch_loss: 0.11619406938552856\n",
      "training: 10 batch 423 batch_loss: 0.11679267883300781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 10 batch 424 batch_loss: 0.11925473809242249\n",
      "training: 10 batch 425 batch_loss: 0.11560437083244324\n",
      "training: 10 batch 426 batch_loss: 0.11614289879798889\n",
      "training: 10 batch 427 batch_loss: 0.11175978183746338\n",
      "training: 10 batch 428 batch_loss: 0.11801642179489136\n",
      "training: 10 batch 429 batch_loss: 0.11666586995124817\n",
      "training: 10 batch 430 batch_loss: 0.11396682262420654\n",
      "training: 10 batch 431 batch_loss: 0.11678728461265564\n",
      "training: 10 batch 432 batch_loss: 0.1148129403591156\n",
      "training: 10 batch 433 batch_loss: 0.1160556972026825\n",
      "training: 10 batch 434 batch_loss: 0.11534643173217773\n",
      "training: 10 batch 435 batch_loss: 0.11681205034255981\n",
      "training: 10 batch 436 batch_loss: 0.11553817987442017\n",
      "training: 10 batch 437 batch_loss: 0.11591261625289917\n",
      "training: 10 batch 438 batch_loss: 0.11889535188674927\n",
      "training: 10 batch 439 batch_loss: 0.116112619638443\n",
      "training: 10 batch 440 batch_loss: 0.11744654178619385\n",
      "training: 10 batch 441 batch_loss: 0.11324098706245422\n",
      "training: 10 batch 442 batch_loss: 0.11639776825904846\n",
      "training: 10 batch 443 batch_loss: 0.11587274074554443\n",
      "training: 10 batch 444 batch_loss: 0.11566808819770813\n",
      "training: 10 batch 445 batch_loss: 0.11835718154907227\n",
      "training: 10 batch 446 batch_loss: 0.1176442801952362\n",
      "training: 10 batch 447 batch_loss: 0.1162988543510437\n",
      "training: 10 batch 448 batch_loss: 0.1171715259552002\n",
      "training: 10 batch 449 batch_loss: 0.11966082453727722\n",
      "training: 10 batch 450 batch_loss: 0.11550137400627136\n",
      "training: 10 batch 451 batch_loss: 0.11326178908348083\n",
      "training: 10 batch 452 batch_loss: 0.1159573495388031\n",
      "training: 10 batch 453 batch_loss: 0.11752694845199585\n",
      "training: 10 batch 454 batch_loss: 0.11641418933868408\n",
      "training: 10 batch 455 batch_loss: 0.11293792724609375\n",
      "training: 10 batch 456 batch_loss: 0.1164216697216034\n",
      "training: 10 batch 457 batch_loss: 0.11801347136497498\n",
      "training: 10 batch 458 batch_loss: 0.1130937933921814\n",
      "training: 10 batch 459 batch_loss: 0.11671954393386841\n",
      "training: 10 batch 460 batch_loss: 0.1163092851638794\n",
      "training: 10 batch 461 batch_loss: 0.11803558468818665\n",
      "training: 10 batch 462 batch_loss: 0.11963549256324768\n",
      "training: 10 batch 463 batch_loss: 0.11562234163284302\n",
      "training: 10 batch 464 batch_loss: 0.11276304721832275\n",
      "training: 10 batch 465 batch_loss: 0.11851945519447327\n",
      "training: 10 batch 466 batch_loss: 0.11468777060508728\n",
      "training: 10 batch 467 batch_loss: 0.11901718378067017\n",
      "training: 10 batch 468 batch_loss: 0.11497741937637329\n",
      "training: 10 batch 469 batch_loss: 0.11633720993995667\n",
      "training: 10 batch 470 batch_loss: 0.11817595362663269\n",
      "training: 10 batch 471 batch_loss: 0.11389878392219543\n",
      "training: 10 batch 472 batch_loss: 0.1154589056968689\n",
      "training: 10 batch 473 batch_loss: 0.11718288064002991\n",
      "training: 10 batch 474 batch_loss: 0.11920619010925293\n",
      "training: 10 batch 475 batch_loss: 0.11705935001373291\n",
      "training: 10 batch 476 batch_loss: 0.1169719398021698\n",
      "training: 10 batch 477 batch_loss: 0.1162651777267456\n",
      "training: 10 batch 478 batch_loss: 0.1171489953994751\n",
      "training: 10 batch 479 batch_loss: 0.11495983600616455\n",
      "training: 10 batch 480 batch_loss: 0.11637610197067261\n",
      "training: 10 batch 481 batch_loss: 0.11760726571083069\n",
      "training: 10 batch 482 batch_loss: 0.11465516686439514\n",
      "training: 10 batch 483 batch_loss: 0.11665058135986328\n",
      "training: 10 batch 484 batch_loss: 0.11714869737625122\n",
      "training: 10 batch 485 batch_loss: 0.11796236038208008\n",
      "training: 10 batch 486 batch_loss: 0.11236345767974854\n",
      "training: 10 batch 487 batch_loss: 0.11669489741325378\n",
      "training: 10 batch 488 batch_loss: 0.11720120906829834\n",
      "training: 10 batch 489 batch_loss: 0.11693450808525085\n",
      "training: 10 batch 490 batch_loss: 0.11690139770507812\n",
      "training: 10 batch 491 batch_loss: 0.11759072542190552\n",
      "training: 10 batch 492 batch_loss: 0.11876317858695984\n",
      "training: 10 batch 493 batch_loss: 0.11852109432220459\n",
      "training: 10 batch 494 batch_loss: 0.11593866348266602\n",
      "training: 10 batch 495 batch_loss: 0.11976125836372375\n",
      "training: 10 batch 496 batch_loss: 0.11807546019554138\n",
      "training: 10 batch 497 batch_loss: 0.11761218309402466\n",
      "training: 10 batch 498 batch_loss: 0.11439463496208191\n",
      "training: 10 batch 499 batch_loss: 0.11533308029174805\n",
      "training: 10 batch 500 batch_loss: 0.11677053570747375\n",
      "training: 10 batch 501 batch_loss: 0.11708468198776245\n",
      "training: 10 batch 502 batch_loss: 0.115719735622406\n",
      "training: 10 batch 503 batch_loss: 0.11611700057983398\n",
      "training: 10 batch 504 batch_loss: 0.11503630876541138\n",
      "training: 10 batch 505 batch_loss: 0.11402788758277893\n",
      "training: 10 batch 506 batch_loss: 0.11684706807136536\n",
      "training: 10 batch 507 batch_loss: 0.11940586566925049\n",
      "training: 10 batch 508 batch_loss: 0.11963841319084167\n",
      "training: 10 batch 509 batch_loss: 0.1178838312625885\n",
      "training: 10 batch 510 batch_loss: 0.11868637800216675\n",
      "training: 10 batch 511 batch_loss: 0.11753946542739868\n",
      "training: 10 batch 512 batch_loss: 0.11663898825645447\n",
      "training: 10 batch 513 batch_loss: 0.11385715007781982\n",
      "training: 10 batch 514 batch_loss: 0.11602485179901123\n",
      "training: 10 batch 515 batch_loss: 0.11658990383148193\n",
      "training: 10 batch 516 batch_loss: 0.11539137363433838\n",
      "training: 10 batch 517 batch_loss: 0.11973312497138977\n",
      "training: 10 batch 518 batch_loss: 0.11820879578590393\n",
      "training: 10 batch 519 batch_loss: 0.1165061891078949\n",
      "training: 10 batch 520 batch_loss: 0.1162809431552887\n",
      "training: 10 batch 521 batch_loss: 0.11993810534477234\n",
      "training: 10 batch 522 batch_loss: 0.11664986610412598\n",
      "training: 10 batch 523 batch_loss: 0.11802220344543457\n",
      "training: 10 batch 524 batch_loss: 0.11867448687553406\n",
      "training: 10 batch 525 batch_loss: 0.1183566153049469\n",
      "training: 10 batch 526 batch_loss: 0.11679357290267944\n",
      "training: 10 batch 527 batch_loss: 0.12027934193611145\n",
      "training: 10 batch 528 batch_loss: 0.11751756072044373\n",
      "training: 10 batch 529 batch_loss: 0.11680442094802856\n",
      "training: 10 batch 530 batch_loss: 0.11689212918281555\n",
      "training: 10 batch 531 batch_loss: 0.11684277653694153\n",
      "training: 10 batch 532 batch_loss: 0.11490285396575928\n",
      "training: 10 batch 533 batch_loss: 0.1181023120880127\n",
      "training: 10 batch 534 batch_loss: 0.11730533838272095\n",
      "training: 10 batch 535 batch_loss: 0.11733326315879822\n",
      "training: 10 batch 536 batch_loss: 0.1182820200920105\n",
      "training: 10 batch 537 batch_loss: 0.11797744035720825\n",
      "training: 10 batch 538 batch_loss: 0.11755812168121338\n",
      "training: 10 batch 539 batch_loss: 0.1152336597442627\n",
      "training: 10 batch 540 batch_loss: 0.11367985606193542\n",
      "training: 10 batch 541 batch_loss: 0.11566171050071716\n",
      "training: 10 batch 542 batch_loss: 0.11960160732269287\n",
      "training: 10 batch 543 batch_loss: 0.11728042364120483\n",
      "training: 10 batch 544 batch_loss: 0.11621510982513428\n",
      "training: 10 batch 545 batch_loss: 0.11748218536376953\n",
      "training: 10 batch 546 batch_loss: 0.11693218350410461\n",
      "training: 10 batch 547 batch_loss: 0.11913919448852539\n",
      "training: 10 batch 548 batch_loss: 0.11663186550140381\n",
      "training: 10 batch 549 batch_loss: 0.11636453866958618\n",
      "training: 10 batch 550 batch_loss: 0.11562654376029968\n",
      "training: 10 batch 551 batch_loss: 0.1167856752872467\n",
      "training: 10 batch 552 batch_loss: 0.11804217100143433\n",
      "training: 10 batch 553 batch_loss: 0.11951285600662231\n",
      "training: 10 batch 554 batch_loss: 0.11820361018180847\n",
      "training: 10 batch 555 batch_loss: 0.11547538638114929\n",
      "training: 10 batch 556 batch_loss: 0.11733764410018921\n",
      "training: 10 batch 557 batch_loss: 0.11660045385360718\n",
      "training: 10 batch 558 batch_loss: 0.11669263243675232\n",
      "training: 10 batch 559 batch_loss: 0.1177138090133667\n",
      "training: 10 batch 560 batch_loss: 0.11532744765281677\n",
      "training: 10 batch 561 batch_loss: 0.11797508597373962\n",
      "training: 10 batch 562 batch_loss: 0.11645525693893433\n",
      "training: 10 batch 563 batch_loss: 0.11543843150138855\n",
      "training: 10 batch 564 batch_loss: 0.11644977331161499\n",
      "training: 10 batch 565 batch_loss: 0.11787998676300049\n",
      "training: 10 batch 566 batch_loss: 0.11620217561721802\n",
      "training: 10 batch 567 batch_loss: 0.11861997842788696\n",
      "training: 10 batch 568 batch_loss: 0.11680519580841064\n",
      "training: 10 batch 569 batch_loss: 0.11747744679450989\n",
      "training: 10 batch 570 batch_loss: 0.11862978339195251\n",
      "training: 10 batch 571 batch_loss: 0.1192166805267334\n",
      "training: 10 batch 572 batch_loss: 0.11680406332015991\n",
      "training: 10 batch 573 batch_loss: 0.11346906423568726\n",
      "training: 10 batch 574 batch_loss: 0.11718934774398804\n",
      "training: 10 batch 575 batch_loss: 0.11722856760025024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 10 batch 576 batch_loss: 0.11718913912773132\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 10, Hit Ratio:0.023328325443964383 | Precision:0.034419541924702646 | Recall:0.04954037634225624 | NDCG:0.04316720329280064\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 11 batch 0 batch_loss: 0.11486753821372986\n",
      "training: 11 batch 1 batch_loss: 0.1134442687034607\n",
      "training: 11 batch 2 batch_loss: 0.11305558681488037\n",
      "training: 11 batch 3 batch_loss: 0.11559849977493286\n",
      "training: 11 batch 4 batch_loss: 0.11474043130874634\n",
      "training: 11 batch 5 batch_loss: 0.11705374717712402\n",
      "training: 11 batch 6 batch_loss: 0.1143256425857544\n",
      "training: 11 batch 7 batch_loss: 0.1131991446018219\n",
      "training: 11 batch 8 batch_loss: 0.11812609434127808\n",
      "training: 11 batch 9 batch_loss: 0.11399316787719727\n",
      "training: 11 batch 10 batch_loss: 0.11424911022186279\n",
      "training: 11 batch 11 batch_loss: 0.11505460739135742\n",
      "training: 11 batch 12 batch_loss: 0.11476334929466248\n",
      "training: 11 batch 13 batch_loss: 0.11527788639068604\n",
      "training: 11 batch 14 batch_loss: 0.1166485846042633\n",
      "training: 11 batch 15 batch_loss: 0.11821290850639343\n",
      "training: 11 batch 16 batch_loss: 0.11736369132995605\n",
      "training: 11 batch 17 batch_loss: 0.1161552369594574\n",
      "training: 11 batch 18 batch_loss: 0.11526143550872803\n",
      "training: 11 batch 19 batch_loss: 0.11627709865570068\n",
      "training: 11 batch 20 batch_loss: 0.11710914969444275\n",
      "training: 11 batch 21 batch_loss: 0.11292898654937744\n",
      "training: 11 batch 22 batch_loss: 0.11421579122543335\n",
      "training: 11 batch 23 batch_loss: 0.1125994622707367\n",
      "training: 11 batch 24 batch_loss: 0.11675280332565308\n",
      "training: 11 batch 25 batch_loss: 0.11565065383911133\n",
      "training: 11 batch 26 batch_loss: 0.11880424618721008\n",
      "training: 11 batch 27 batch_loss: 0.11717617511749268\n",
      "training: 11 batch 28 batch_loss: 0.11252665519714355\n",
      "training: 11 batch 29 batch_loss: 0.11770880222320557\n",
      "training: 11 batch 30 batch_loss: 0.11612612009048462\n",
      "training: 11 batch 31 batch_loss: 0.1159180998802185\n",
      "training: 11 batch 32 batch_loss: 0.113689124584198\n",
      "training: 11 batch 33 batch_loss: 0.11109587550163269\n",
      "training: 11 batch 34 batch_loss: 0.11460322141647339\n",
      "training: 11 batch 35 batch_loss: 0.11601835489273071\n",
      "training: 11 batch 36 batch_loss: 0.11342716217041016\n",
      "training: 11 batch 37 batch_loss: 0.11536577343940735\n",
      "training: 11 batch 38 batch_loss: 0.11457931995391846\n",
      "training: 11 batch 39 batch_loss: 0.11649107933044434\n",
      "training: 11 batch 40 batch_loss: 0.11612367630004883\n",
      "training: 11 batch 41 batch_loss: 0.11602216958999634\n",
      "training: 11 batch 42 batch_loss: 0.11558365821838379\n",
      "training: 11 batch 43 batch_loss: 0.11807247996330261\n",
      "training: 11 batch 44 batch_loss: 0.11568495631217957\n",
      "training: 11 batch 45 batch_loss: 0.11392652988433838\n",
      "training: 11 batch 46 batch_loss: 0.11768141388893127\n",
      "training: 11 batch 47 batch_loss: 0.11384356021881104\n",
      "training: 11 batch 48 batch_loss: 0.11662358045578003\n",
      "training: 11 batch 49 batch_loss: 0.11484718322753906\n",
      "training: 11 batch 50 batch_loss: 0.11645719408988953\n",
      "training: 11 batch 51 batch_loss: 0.11622750759124756\n",
      "training: 11 batch 52 batch_loss: 0.11637571454048157\n",
      "training: 11 batch 53 batch_loss: 0.12037837505340576\n",
      "training: 11 batch 54 batch_loss: 0.11531674861907959\n",
      "training: 11 batch 55 batch_loss: 0.11282011866569519\n",
      "training: 11 batch 56 batch_loss: 0.11763551831245422\n",
      "training: 11 batch 57 batch_loss: 0.11655884981155396\n",
      "training: 11 batch 58 batch_loss: 0.11661618947982788\n",
      "training: 11 batch 59 batch_loss: 0.1152862012386322\n",
      "training: 11 batch 60 batch_loss: 0.11937171220779419\n",
      "training: 11 batch 61 batch_loss: 0.11694633960723877\n",
      "training: 11 batch 62 batch_loss: 0.11785674095153809\n",
      "training: 11 batch 63 batch_loss: 0.11451196670532227\n",
      "training: 11 batch 64 batch_loss: 0.11708131432533264\n",
      "training: 11 batch 65 batch_loss: 0.11608725786209106\n",
      "training: 11 batch 66 batch_loss: 0.11188438534736633\n",
      "training: 11 batch 67 batch_loss: 0.1196104884147644\n",
      "training: 11 batch 68 batch_loss: 0.11606520414352417\n",
      "training: 11 batch 69 batch_loss: 0.11781546473503113\n",
      "training: 11 batch 70 batch_loss: 0.11408978700637817\n",
      "training: 11 batch 71 batch_loss: 0.11848437786102295\n",
      "training: 11 batch 72 batch_loss: 0.11636656522750854\n",
      "training: 11 batch 73 batch_loss: 0.11639025807380676\n",
      "training: 11 batch 74 batch_loss: 0.11664247512817383\n",
      "training: 11 batch 75 batch_loss: 0.11916452646255493\n",
      "training: 11 batch 76 batch_loss: 0.11721742153167725\n",
      "training: 11 batch 77 batch_loss: 0.11901569366455078\n",
      "training: 11 batch 78 batch_loss: 0.11728617548942566\n",
      "training: 11 batch 79 batch_loss: 0.11814025044441223\n",
      "training: 11 batch 80 batch_loss: 0.11632904410362244\n",
      "training: 11 batch 81 batch_loss: 0.11472508311271667\n",
      "training: 11 batch 82 batch_loss: 0.11397603154182434\n",
      "training: 11 batch 83 batch_loss: 0.11702656745910645\n",
      "training: 11 batch 84 batch_loss: 0.11937195062637329\n",
      "training: 11 batch 85 batch_loss: 0.11561211943626404\n",
      "training: 11 batch 86 batch_loss: 0.1144753098487854\n",
      "training: 11 batch 87 batch_loss: 0.11752727627754211\n",
      "training: 11 batch 88 batch_loss: 0.11537036299705505\n",
      "training: 11 batch 89 batch_loss: 0.11416059732437134\n",
      "training: 11 batch 90 batch_loss: 0.11789685487747192\n",
      "training: 11 batch 91 batch_loss: 0.11682718992233276\n",
      "training: 11 batch 92 batch_loss: 0.11412036418914795\n",
      "training: 11 batch 93 batch_loss: 0.11683270335197449\n",
      "training: 11 batch 94 batch_loss: 0.11575120687484741\n",
      "training: 11 batch 95 batch_loss: 0.11699974536895752\n",
      "training: 11 batch 96 batch_loss: 0.11565154790878296\n",
      "training: 11 batch 97 batch_loss: 0.11519971489906311\n",
      "training: 11 batch 98 batch_loss: 0.11738565564155579\n",
      "training: 11 batch 99 batch_loss: 0.11741238832473755\n",
      "training: 11 batch 100 batch_loss: 0.11627745628356934\n",
      "training: 11 batch 101 batch_loss: 0.11534509062767029\n",
      "training: 11 batch 102 batch_loss: 0.1165703535079956\n",
      "training: 11 batch 103 batch_loss: 0.11692026257514954\n",
      "training: 11 batch 104 batch_loss: 0.1186302900314331\n",
      "training: 11 batch 105 batch_loss: 0.11662536859512329\n",
      "training: 11 batch 106 batch_loss: 0.1169668436050415\n",
      "training: 11 batch 107 batch_loss: 0.11457368731498718\n",
      "training: 11 batch 108 batch_loss: 0.11614087224006653\n",
      "training: 11 batch 109 batch_loss: 0.11535805463790894\n",
      "training: 11 batch 110 batch_loss: 0.11565759778022766\n",
      "training: 11 batch 111 batch_loss: 0.1170177161693573\n",
      "training: 11 batch 112 batch_loss: 0.11315333843231201\n",
      "training: 11 batch 113 batch_loss: 0.11838430166244507\n",
      "training: 11 batch 114 batch_loss: 0.11457321047782898\n",
      "training: 11 batch 115 batch_loss: 0.11810529232025146\n",
      "training: 11 batch 116 batch_loss: 0.11405476927757263\n",
      "training: 11 batch 117 batch_loss: 0.1144862174987793\n",
      "training: 11 batch 118 batch_loss: 0.12035727500915527\n",
      "training: 11 batch 119 batch_loss: 0.1171715259552002\n",
      "training: 11 batch 120 batch_loss: 0.1176430881023407\n",
      "training: 11 batch 121 batch_loss: 0.11964768171310425\n",
      "training: 11 batch 122 batch_loss: 0.11776632070541382\n",
      "training: 11 batch 123 batch_loss: 0.11549845337867737\n",
      "training: 11 batch 124 batch_loss: 0.11422482132911682\n",
      "training: 11 batch 125 batch_loss: 0.12040096521377563\n",
      "training: 11 batch 126 batch_loss: 0.11545562744140625\n",
      "training: 11 batch 127 batch_loss: 0.11675453186035156\n",
      "training: 11 batch 128 batch_loss: 0.11663854122161865\n",
      "training: 11 batch 129 batch_loss: 0.11667844653129578\n",
      "training: 11 batch 130 batch_loss: 0.11908513307571411\n",
      "training: 11 batch 131 batch_loss: 0.11814025044441223\n",
      "training: 11 batch 132 batch_loss: 0.11565396189689636\n",
      "training: 11 batch 133 batch_loss: 0.11522674560546875\n",
      "training: 11 batch 134 batch_loss: 0.1184014081954956\n",
      "training: 11 batch 135 batch_loss: 0.11814257502555847\n",
      "training: 11 batch 136 batch_loss: 0.11616706848144531\n",
      "training: 11 batch 137 batch_loss: 0.11778789758682251\n",
      "training: 11 batch 138 batch_loss: 0.11721405386924744\n",
      "training: 11 batch 139 batch_loss: 0.1177293062210083\n",
      "training: 11 batch 140 batch_loss: 0.11673775315284729\n",
      "training: 11 batch 141 batch_loss: 0.12034621834754944\n",
      "training: 11 batch 142 batch_loss: 0.11397388577461243\n",
      "training: 11 batch 143 batch_loss: 0.11677414178848267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 11 batch 144 batch_loss: 0.11812317371368408\n",
      "training: 11 batch 145 batch_loss: 0.11587396264076233\n",
      "training: 11 batch 146 batch_loss: 0.1163269579410553\n",
      "training: 11 batch 147 batch_loss: 0.11749142408370972\n",
      "training: 11 batch 148 batch_loss: 0.1160479485988617\n",
      "training: 11 batch 149 batch_loss: 0.11654096841812134\n",
      "training: 11 batch 150 batch_loss: 0.11647585034370422\n",
      "training: 11 batch 151 batch_loss: 0.11557775735855103\n",
      "training: 11 batch 152 batch_loss: 0.11780351400375366\n",
      "training: 11 batch 153 batch_loss: 0.11850374937057495\n",
      "training: 11 batch 154 batch_loss: 0.11588788032531738\n",
      "training: 11 batch 155 batch_loss: 0.11984044313430786\n",
      "training: 11 batch 156 batch_loss: 0.11755797266960144\n",
      "training: 11 batch 157 batch_loss: 0.11591300368309021\n",
      "training: 11 batch 158 batch_loss: 0.11661002039909363\n",
      "training: 11 batch 159 batch_loss: 0.11730477213859558\n",
      "training: 11 batch 160 batch_loss: 0.12041822075843811\n",
      "training: 11 batch 161 batch_loss: 0.11760839819908142\n",
      "training: 11 batch 162 batch_loss: 0.11448720097541809\n",
      "training: 11 batch 163 batch_loss: 0.11851900815963745\n",
      "training: 11 batch 164 batch_loss: 0.11802995204925537\n",
      "training: 11 batch 165 batch_loss: 0.1207512617111206\n",
      "training: 11 batch 166 batch_loss: 0.11863285303115845\n",
      "training: 11 batch 167 batch_loss: 0.11759117245674133\n",
      "training: 11 batch 168 batch_loss: 0.11914902925491333\n",
      "training: 11 batch 169 batch_loss: 0.11948123574256897\n",
      "training: 11 batch 170 batch_loss: 0.11590456962585449\n",
      "training: 11 batch 171 batch_loss: 0.11620786786079407\n",
      "training: 11 batch 172 batch_loss: 0.11634230613708496\n",
      "training: 11 batch 173 batch_loss: 0.11514291167259216\n",
      "training: 11 batch 174 batch_loss: 0.11567533016204834\n",
      "training: 11 batch 175 batch_loss: 0.12122541666030884\n",
      "training: 11 batch 176 batch_loss: 0.11666214466094971\n",
      "training: 11 batch 177 batch_loss: 0.11758315563201904\n",
      "training: 11 batch 178 batch_loss: 0.11775285005569458\n",
      "training: 11 batch 179 batch_loss: 0.11765742301940918\n",
      "training: 11 batch 180 batch_loss: 0.1178087592124939\n",
      "training: 11 batch 181 batch_loss: 0.11973387002944946\n",
      "training: 11 batch 182 batch_loss: 0.11950677633285522\n",
      "training: 11 batch 183 batch_loss: 0.11776000261306763\n",
      "training: 11 batch 184 batch_loss: 0.11835086345672607\n",
      "training: 11 batch 185 batch_loss: 0.11901277303695679\n",
      "training: 11 batch 186 batch_loss: 0.11839485168457031\n",
      "training: 11 batch 187 batch_loss: 0.11457690596580505\n",
      "training: 11 batch 188 batch_loss: 0.11843937635421753\n",
      "training: 11 batch 189 batch_loss: 0.11920663714408875\n",
      "training: 11 batch 190 batch_loss: 0.11539116501808167\n",
      "training: 11 batch 191 batch_loss: 0.11905854940414429\n",
      "training: 11 batch 192 batch_loss: 0.11831733584403992\n",
      "training: 11 batch 193 batch_loss: 0.12046730518341064\n",
      "training: 11 batch 194 batch_loss: 0.11885547637939453\n",
      "training: 11 batch 195 batch_loss: 0.11737671494483948\n",
      "training: 11 batch 196 batch_loss: 0.11613413691520691\n",
      "training: 11 batch 197 batch_loss: 0.11668610572814941\n",
      "training: 11 batch 198 batch_loss: 0.11956292390823364\n",
      "training: 11 batch 199 batch_loss: 0.11635470390319824\n",
      "training: 11 batch 200 batch_loss: 0.11592850089073181\n",
      "training: 11 batch 201 batch_loss: 0.11724153161048889\n",
      "training: 11 batch 202 batch_loss: 0.11651834845542908\n",
      "training: 11 batch 203 batch_loss: 0.11953619122505188\n",
      "training: 11 batch 204 batch_loss: 0.12029114365577698\n",
      "training: 11 batch 205 batch_loss: 0.1185435950756073\n",
      "training: 11 batch 206 batch_loss: 0.11666399240493774\n",
      "training: 11 batch 207 batch_loss: 0.11727496981620789\n",
      "training: 11 batch 208 batch_loss: 0.11411279439926147\n",
      "training: 11 batch 209 batch_loss: 0.11664700508117676\n",
      "training: 11 batch 210 batch_loss: 0.11688113212585449\n",
      "training: 11 batch 211 batch_loss: 0.11635002493858337\n",
      "training: 11 batch 212 batch_loss: 0.11720594763755798\n",
      "training: 11 batch 213 batch_loss: 0.11926040053367615\n",
      "training: 11 batch 214 batch_loss: 0.11540418863296509\n",
      "training: 11 batch 215 batch_loss: 0.11846363544464111\n",
      "training: 11 batch 216 batch_loss: 0.11583039164543152\n",
      "training: 11 batch 217 batch_loss: 0.1183333694934845\n",
      "training: 11 batch 218 batch_loss: 0.11939513683319092\n",
      "training: 11 batch 219 batch_loss: 0.11775490641593933\n",
      "training: 11 batch 220 batch_loss: 0.11795037984848022\n",
      "training: 11 batch 221 batch_loss: 0.11871421337127686\n",
      "training: 11 batch 222 batch_loss: 0.11479219794273376\n",
      "training: 11 batch 223 batch_loss: 0.11602133512496948\n",
      "training: 11 batch 224 batch_loss: 0.12010759115219116\n",
      "training: 11 batch 225 batch_loss: 0.11766713857650757\n",
      "training: 11 batch 226 batch_loss: 0.1215900182723999\n",
      "training: 11 batch 227 batch_loss: 0.1174420416355133\n",
      "training: 11 batch 228 batch_loss: 0.11974135041236877\n",
      "training: 11 batch 229 batch_loss: 0.1188206672668457\n",
      "training: 11 batch 230 batch_loss: 0.11854183673858643\n",
      "training: 11 batch 231 batch_loss: 0.1166858971118927\n",
      "training: 11 batch 232 batch_loss: 0.11611509323120117\n",
      "training: 11 batch 233 batch_loss: 0.11537489295005798\n",
      "training: 11 batch 234 batch_loss: 0.11664354801177979\n",
      "training: 11 batch 235 batch_loss: 0.11683213710784912\n",
      "training: 11 batch 236 batch_loss: 0.11747604608535767\n",
      "training: 11 batch 237 batch_loss: 0.1185038685798645\n",
      "training: 11 batch 238 batch_loss: 0.11851248145103455\n",
      "training: 11 batch 239 batch_loss: 0.11824211478233337\n",
      "training: 11 batch 240 batch_loss: 0.11615753173828125\n",
      "training: 11 batch 241 batch_loss: 0.11938297748565674\n",
      "training: 11 batch 242 batch_loss: 0.1162685751914978\n",
      "training: 11 batch 243 batch_loss: 0.11698910593986511\n",
      "training: 11 batch 244 batch_loss: 0.11969062685966492\n",
      "training: 11 batch 245 batch_loss: 0.11671710014343262\n",
      "training: 11 batch 246 batch_loss: 0.11651161313056946\n",
      "training: 11 batch 247 batch_loss: 0.11479508876800537\n",
      "training: 11 batch 248 batch_loss: 0.11906787753105164\n",
      "training: 11 batch 249 batch_loss: 0.11544150114059448\n",
      "training: 11 batch 250 batch_loss: 0.11983317136764526\n",
      "training: 11 batch 251 batch_loss: 0.11672267317771912\n",
      "training: 11 batch 252 batch_loss: 0.11685717105865479\n",
      "training: 11 batch 253 batch_loss: 0.12206721305847168\n",
      "training: 11 batch 254 batch_loss: 0.11676466464996338\n",
      "training: 11 batch 255 batch_loss: 0.11901801824569702\n",
      "training: 11 batch 256 batch_loss: 0.11729592084884644\n",
      "training: 11 batch 257 batch_loss: 0.12094902992248535\n",
      "training: 11 batch 258 batch_loss: 0.11587396264076233\n",
      "training: 11 batch 259 batch_loss: 0.11761277914047241\n",
      "training: 11 batch 260 batch_loss: 0.11790516972541809\n",
      "training: 11 batch 261 batch_loss: 0.11590331792831421\n",
      "training: 11 batch 262 batch_loss: 0.1175103485584259\n",
      "training: 11 batch 263 batch_loss: 0.1190555989742279\n",
      "training: 11 batch 264 batch_loss: 0.11651527881622314\n",
      "training: 11 batch 265 batch_loss: 0.1177026629447937\n",
      "training: 11 batch 266 batch_loss: 0.11956089735031128\n",
      "training: 11 batch 267 batch_loss: 0.11794289946556091\n",
      "training: 11 batch 268 batch_loss: 0.11669737100601196\n",
      "training: 11 batch 269 batch_loss: 0.11998459696769714\n",
      "training: 11 batch 270 batch_loss: 0.11637181043624878\n",
      "training: 11 batch 271 batch_loss: 0.11693453788757324\n",
      "training: 11 batch 272 batch_loss: 0.1173277497291565\n",
      "training: 11 batch 273 batch_loss: 0.11795958876609802\n",
      "training: 11 batch 274 batch_loss: 0.11826887726783752\n",
      "training: 11 batch 275 batch_loss: 0.11828264594078064\n",
      "training: 11 batch 276 batch_loss: 0.1177586019039154\n",
      "training: 11 batch 277 batch_loss: 0.11640670895576477\n",
      "training: 11 batch 278 batch_loss: 0.11988210678100586\n",
      "training: 11 batch 279 batch_loss: 0.11679834127426147\n",
      "training: 11 batch 280 batch_loss: 0.11757439374923706\n",
      "training: 11 batch 281 batch_loss: 0.11986804008483887\n",
      "training: 11 batch 282 batch_loss: 0.11673250794410706\n",
      "training: 11 batch 283 batch_loss: 0.1174502968788147\n",
      "training: 11 batch 284 batch_loss: 0.11954247951507568\n",
      "training: 11 batch 285 batch_loss: 0.11720967292785645\n",
      "training: 11 batch 286 batch_loss: 0.11505773663520813\n",
      "training: 11 batch 287 batch_loss: 0.11851009726524353\n",
      "training: 11 batch 288 batch_loss: 0.11892446875572205\n",
      "training: 11 batch 289 batch_loss: 0.12055990099906921\n",
      "training: 11 batch 290 batch_loss: 0.11893412470817566\n",
      "training: 11 batch 291 batch_loss: 0.1211974024772644\n",
      "training: 11 batch 292 batch_loss: 0.11768004298210144\n",
      "training: 11 batch 293 batch_loss: 0.11509376764297485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 11 batch 294 batch_loss: 0.12349697947502136\n",
      "training: 11 batch 295 batch_loss: 0.11683759093284607\n",
      "training: 11 batch 296 batch_loss: 0.11684590578079224\n",
      "training: 11 batch 297 batch_loss: 0.11572223901748657\n",
      "training: 11 batch 298 batch_loss: 0.11706534028053284\n",
      "training: 11 batch 299 batch_loss: 0.11845278739929199\n",
      "training: 11 batch 300 batch_loss: 0.11947709321975708\n",
      "training: 11 batch 301 batch_loss: 0.1175791323184967\n",
      "training: 11 batch 302 batch_loss: 0.11583808064460754\n",
      "training: 11 batch 303 batch_loss: 0.11900931596755981\n",
      "training: 11 batch 304 batch_loss: 0.11475250124931335\n",
      "training: 11 batch 305 batch_loss: 0.11695441603660583\n",
      "training: 11 batch 306 batch_loss: 0.11879119277000427\n",
      "training: 11 batch 307 batch_loss: 0.1196778416633606\n",
      "training: 11 batch 308 batch_loss: 0.12016165256500244\n",
      "training: 11 batch 309 batch_loss: 0.12120822072029114\n",
      "training: 11 batch 310 batch_loss: 0.11552536487579346\n",
      "training: 11 batch 311 batch_loss: 0.1170794665813446\n",
      "training: 11 batch 312 batch_loss: 0.1172005832195282\n",
      "training: 11 batch 313 batch_loss: 0.11884164810180664\n",
      "training: 11 batch 314 batch_loss: 0.1167333722114563\n",
      "training: 11 batch 315 batch_loss: 0.11774381995201111\n",
      "training: 11 batch 316 batch_loss: 0.11681756377220154\n",
      "training: 11 batch 317 batch_loss: 0.11925864219665527\n",
      "training: 11 batch 318 batch_loss: 0.11658927798271179\n",
      "training: 11 batch 319 batch_loss: 0.11793690919876099\n",
      "training: 11 batch 320 batch_loss: 0.1185295581817627\n",
      "training: 11 batch 321 batch_loss: 0.11623075604438782\n",
      "training: 11 batch 322 batch_loss: 0.12100693583488464\n",
      "training: 11 batch 323 batch_loss: 0.1151672899723053\n",
      "training: 11 batch 324 batch_loss: 0.11626029014587402\n",
      "training: 11 batch 325 batch_loss: 0.11685669422149658\n",
      "training: 11 batch 326 batch_loss: 0.11531546711921692\n",
      "training: 11 batch 327 batch_loss: 0.11783361434936523\n",
      "training: 11 batch 328 batch_loss: 0.12017467617988586\n",
      "training: 11 batch 329 batch_loss: 0.11562150716781616\n",
      "training: 11 batch 330 batch_loss: 0.12030065059661865\n",
      "training: 11 batch 331 batch_loss: 0.11580055952072144\n",
      "training: 11 batch 332 batch_loss: 0.11987414956092834\n",
      "training: 11 batch 333 batch_loss: 0.12004798650741577\n",
      "training: 11 batch 334 batch_loss: 0.11993950605392456\n",
      "training: 11 batch 335 batch_loss: 0.11950507760047913\n",
      "training: 11 batch 336 batch_loss: 0.11899998784065247\n",
      "training: 11 batch 337 batch_loss: 0.12053078413009644\n",
      "training: 11 batch 338 batch_loss: 0.11539074778556824\n",
      "training: 11 batch 339 batch_loss: 0.12010166049003601\n",
      "training: 11 batch 340 batch_loss: 0.11945116519927979\n",
      "training: 11 batch 341 batch_loss: 0.11687290668487549\n",
      "training: 11 batch 342 batch_loss: 0.11526983976364136\n",
      "training: 11 batch 343 batch_loss: 0.1198340654373169\n",
      "training: 11 batch 344 batch_loss: 0.12127652764320374\n",
      "training: 11 batch 345 batch_loss: 0.1196662187576294\n",
      "training: 11 batch 346 batch_loss: 0.12062528729438782\n",
      "training: 11 batch 347 batch_loss: 0.11745250225067139\n",
      "training: 11 batch 348 batch_loss: 0.11765444278717041\n",
      "training: 11 batch 349 batch_loss: 0.11942249536514282\n",
      "training: 11 batch 350 batch_loss: 0.12213009595870972\n",
      "training: 11 batch 351 batch_loss: 0.11646595597267151\n",
      "training: 11 batch 352 batch_loss: 0.11750072240829468\n",
      "training: 11 batch 353 batch_loss: 0.11988011002540588\n",
      "training: 11 batch 354 batch_loss: 0.11982840299606323\n",
      "training: 11 batch 355 batch_loss: 0.11599418520927429\n",
      "training: 11 batch 356 batch_loss: 0.12079280614852905\n",
      "training: 11 batch 357 batch_loss: 0.12001526355743408\n",
      "training: 11 batch 358 batch_loss: 0.1159115731716156\n",
      "training: 11 batch 359 batch_loss: 0.11849495768547058\n",
      "training: 11 batch 360 batch_loss: 0.1186957061290741\n",
      "training: 11 batch 361 batch_loss: 0.12016719579696655\n",
      "training: 11 batch 362 batch_loss: 0.12059620022773743\n",
      "training: 11 batch 363 batch_loss: 0.11782962083816528\n",
      "training: 11 batch 364 batch_loss: 0.11899790167808533\n",
      "training: 11 batch 365 batch_loss: 0.11863243579864502\n",
      "training: 11 batch 366 batch_loss: 0.11789634823799133\n",
      "training: 11 batch 367 batch_loss: 0.11736950278282166\n",
      "training: 11 batch 368 batch_loss: 0.11887010931968689\n",
      "training: 11 batch 369 batch_loss: 0.1185213029384613\n",
      "training: 11 batch 370 batch_loss: 0.11911934614181519\n",
      "training: 11 batch 371 batch_loss: 0.11667418479919434\n",
      "training: 11 batch 372 batch_loss: 0.11796563863754272\n",
      "training: 11 batch 373 batch_loss: 0.12091949582099915\n",
      "training: 11 batch 374 batch_loss: 0.11908727884292603\n",
      "training: 11 batch 375 batch_loss: 0.11555474996566772\n",
      "training: 11 batch 376 batch_loss: 0.11894688010215759\n",
      "training: 11 batch 377 batch_loss: 0.11954352259635925\n",
      "training: 11 batch 378 batch_loss: 0.1189579963684082\n",
      "training: 11 batch 379 batch_loss: 0.11709123849868774\n",
      "training: 11 batch 380 batch_loss: 0.11876919865608215\n",
      "training: 11 batch 381 batch_loss: 0.12147173285484314\n",
      "training: 11 batch 382 batch_loss: 0.11908748745918274\n",
      "training: 11 batch 383 batch_loss: 0.11878544092178345\n",
      "training: 11 batch 384 batch_loss: 0.11943870782852173\n",
      "training: 11 batch 385 batch_loss: 0.11966574192047119\n",
      "training: 11 batch 386 batch_loss: 0.11679604649543762\n",
      "training: 11 batch 387 batch_loss: 0.11821633577346802\n",
      "training: 11 batch 388 batch_loss: 0.11549004912376404\n",
      "training: 11 batch 389 batch_loss: 0.11822283267974854\n",
      "training: 11 batch 390 batch_loss: 0.11957308650016785\n",
      "training: 11 batch 391 batch_loss: 0.1194959282875061\n",
      "training: 11 batch 392 batch_loss: 0.11912310123443604\n",
      "training: 11 batch 393 batch_loss: 0.12074524164199829\n",
      "training: 11 batch 394 batch_loss: 0.11885461211204529\n",
      "training: 11 batch 395 batch_loss: 0.11644110083580017\n",
      "training: 11 batch 396 batch_loss: 0.1180504560470581\n",
      "training: 11 batch 397 batch_loss: 0.1189039945602417\n",
      "training: 11 batch 398 batch_loss: 0.11882948875427246\n",
      "training: 11 batch 399 batch_loss: 0.11776980757713318\n",
      "training: 11 batch 400 batch_loss: 0.11932629346847534\n",
      "training: 11 batch 401 batch_loss: 0.118040531873703\n",
      "training: 11 batch 402 batch_loss: 0.1202700138092041\n",
      "training: 11 batch 403 batch_loss: 0.12020614743232727\n",
      "training: 11 batch 404 batch_loss: 0.11908984184265137\n",
      "training: 11 batch 405 batch_loss: 0.11848148703575134\n",
      "training: 11 batch 406 batch_loss: 0.11895996332168579\n",
      "training: 11 batch 407 batch_loss: 0.11829155683517456\n",
      "training: 11 batch 408 batch_loss: 0.12027245759963989\n",
      "training: 11 batch 409 batch_loss: 0.12040919065475464\n",
      "training: 11 batch 410 batch_loss: 0.12173479795455933\n",
      "training: 11 batch 411 batch_loss: 0.11914640665054321\n",
      "training: 11 batch 412 batch_loss: 0.11569294333457947\n",
      "training: 11 batch 413 batch_loss: 0.11942684650421143\n",
      "training: 11 batch 414 batch_loss: 0.11663544178009033\n",
      "training: 11 batch 415 batch_loss: 0.11768227815628052\n",
      "training: 11 batch 416 batch_loss: 0.11910000443458557\n",
      "training: 11 batch 417 batch_loss: 0.11870521306991577\n",
      "training: 11 batch 418 batch_loss: 0.11752104759216309\n",
      "training: 11 batch 419 batch_loss: 0.11934593319892883\n",
      "training: 11 batch 420 batch_loss: 0.11839509010314941\n",
      "training: 11 batch 421 batch_loss: 0.119530588388443\n",
      "training: 11 batch 422 batch_loss: 0.11888495087623596\n",
      "training: 11 batch 423 batch_loss: 0.1176915168762207\n",
      "training: 11 batch 424 batch_loss: 0.11993959546089172\n",
      "training: 11 batch 425 batch_loss: 0.1226426362991333\n",
      "training: 11 batch 426 batch_loss: 0.12037968635559082\n",
      "training: 11 batch 427 batch_loss: 0.1188996434211731\n",
      "training: 11 batch 428 batch_loss: 0.11936086416244507\n",
      "training: 11 batch 429 batch_loss: 0.11846485733985901\n",
      "training: 11 batch 430 batch_loss: 0.12102574110031128\n",
      "training: 11 batch 431 batch_loss: 0.11956396698951721\n",
      "training: 11 batch 432 batch_loss: 0.11783349514007568\n",
      "training: 11 batch 433 batch_loss: 0.11933964490890503\n",
      "training: 11 batch 434 batch_loss: 0.12140247225761414\n",
      "training: 11 batch 435 batch_loss: 0.11799311637878418\n",
      "training: 11 batch 436 batch_loss: 0.12038975954055786\n",
      "training: 11 batch 437 batch_loss: 0.1184241771697998\n",
      "training: 11 batch 438 batch_loss: 0.11924472451210022\n",
      "training: 11 batch 439 batch_loss: 0.11679431796073914\n",
      "training: 11 batch 440 batch_loss: 0.1190909743309021\n",
      "training: 11 batch 441 batch_loss: 0.11976128816604614\n",
      "training: 11 batch 442 batch_loss: 0.12230846285820007\n",
      "training: 11 batch 443 batch_loss: 0.12098035216331482\n",
      "training: 11 batch 444 batch_loss: 0.11769330501556396\n",
      "training: 11 batch 445 batch_loss: 0.11789712309837341\n",
      "training: 11 batch 446 batch_loss: 0.12180054187774658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 11 batch 447 batch_loss: 0.11662065982818604\n",
      "training: 11 batch 448 batch_loss: 0.11774924397468567\n",
      "training: 11 batch 449 batch_loss: 0.12013468146324158\n",
      "training: 11 batch 450 batch_loss: 0.11834308505058289\n",
      "training: 11 batch 451 batch_loss: 0.11979013681411743\n",
      "training: 11 batch 452 batch_loss: 0.11801466345787048\n",
      "training: 11 batch 453 batch_loss: 0.11946830153465271\n",
      "training: 11 batch 454 batch_loss: 0.11959272623062134\n",
      "training: 11 batch 455 batch_loss: 0.12038683891296387\n",
      "training: 11 batch 456 batch_loss: 0.12154954671859741\n",
      "training: 11 batch 457 batch_loss: 0.11900162696838379\n",
      "training: 11 batch 458 batch_loss: 0.11804524064064026\n",
      "training: 11 batch 459 batch_loss: 0.11863723397254944\n",
      "training: 11 batch 460 batch_loss: 0.11757433414459229\n",
      "training: 11 batch 461 batch_loss: 0.11902672052383423\n",
      "training: 11 batch 462 batch_loss: 0.11717844009399414\n",
      "training: 11 batch 463 batch_loss: 0.12168106436729431\n",
      "training: 11 batch 464 batch_loss: 0.11468729376792908\n",
      "training: 11 batch 465 batch_loss: 0.12312436103820801\n",
      "training: 11 batch 466 batch_loss: 0.11574682593345642\n",
      "training: 11 batch 467 batch_loss: 0.11836227774620056\n",
      "training: 11 batch 468 batch_loss: 0.11991843581199646\n",
      "training: 11 batch 469 batch_loss: 0.12175735831260681\n",
      "training: 11 batch 470 batch_loss: 0.11971428990364075\n",
      "training: 11 batch 471 batch_loss: 0.11997583508491516\n",
      "training: 11 batch 472 batch_loss: 0.12168854475021362\n",
      "training: 11 batch 473 batch_loss: 0.12144505977630615\n",
      "training: 11 batch 474 batch_loss: 0.12341845035552979\n",
      "training: 11 batch 475 batch_loss: 0.11812764406204224\n",
      "training: 11 batch 476 batch_loss: 0.11947157979011536\n",
      "training: 11 batch 477 batch_loss: 0.12006288766860962\n",
      "training: 11 batch 478 batch_loss: 0.11759746074676514\n",
      "training: 11 batch 479 batch_loss: 0.11955511569976807\n",
      "training: 11 batch 480 batch_loss: 0.12195533514022827\n",
      "training: 11 batch 481 batch_loss: 0.12162351608276367\n",
      "training: 11 batch 482 batch_loss: 0.11946675181388855\n",
      "training: 11 batch 483 batch_loss: 0.11832678318023682\n",
      "training: 11 batch 484 batch_loss: 0.12000134587287903\n",
      "training: 11 batch 485 batch_loss: 0.1179194450378418\n",
      "training: 11 batch 486 batch_loss: 0.11947649717330933\n",
      "training: 11 batch 487 batch_loss: 0.11508354544639587\n",
      "training: 11 batch 488 batch_loss: 0.12173110246658325\n",
      "training: 11 batch 489 batch_loss: 0.1195092499256134\n",
      "training: 11 batch 490 batch_loss: 0.11843574047088623\n",
      "training: 11 batch 491 batch_loss: 0.11821094155311584\n",
      "training: 11 batch 492 batch_loss: 0.12287241220474243\n",
      "training: 11 batch 493 batch_loss: 0.11877846717834473\n",
      "training: 11 batch 494 batch_loss: 0.12222698330879211\n",
      "training: 11 batch 495 batch_loss: 0.12068656086921692\n",
      "training: 11 batch 496 batch_loss: 0.11644867062568665\n",
      "training: 11 batch 497 batch_loss: 0.11926642060279846\n",
      "training: 11 batch 498 batch_loss: 0.11859390139579773\n",
      "training: 11 batch 499 batch_loss: 0.11964219808578491\n",
      "training: 11 batch 500 batch_loss: 0.11990293860435486\n",
      "training: 11 batch 501 batch_loss: 0.12129276990890503\n",
      "training: 11 batch 502 batch_loss: 0.11924290657043457\n",
      "training: 11 batch 503 batch_loss: 0.1173119843006134\n",
      "training: 11 batch 504 batch_loss: 0.11966767907142639\n",
      "training: 11 batch 505 batch_loss: 0.11925068497657776\n",
      "training: 11 batch 506 batch_loss: 0.12190315127372742\n",
      "training: 11 batch 507 batch_loss: 0.12126460671424866\n",
      "training: 11 batch 508 batch_loss: 0.11678370833396912\n",
      "training: 11 batch 509 batch_loss: 0.11897778511047363\n",
      "training: 11 batch 510 batch_loss: 0.11921605467796326\n",
      "training: 11 batch 511 batch_loss: 0.12245309352874756\n",
      "training: 11 batch 512 batch_loss: 0.11590659618377686\n",
      "training: 11 batch 513 batch_loss: 0.11881858110427856\n",
      "training: 11 batch 514 batch_loss: 0.11806687712669373\n",
      "training: 11 batch 515 batch_loss: 0.11501181125640869\n",
      "training: 11 batch 516 batch_loss: 0.11571294069290161\n",
      "training: 11 batch 517 batch_loss: 0.11935833096504211\n",
      "training: 11 batch 518 batch_loss: 0.11907517910003662\n",
      "training: 11 batch 519 batch_loss: 0.11604386568069458\n",
      "training: 11 batch 520 batch_loss: 0.12147021293640137\n",
      "training: 11 batch 521 batch_loss: 0.12009409070014954\n",
      "training: 11 batch 522 batch_loss: 0.11856180429458618\n",
      "training: 11 batch 523 batch_loss: 0.12345126271247864\n",
      "training: 11 batch 524 batch_loss: 0.11751437187194824\n",
      "training: 11 batch 525 batch_loss: 0.11912843585014343\n",
      "training: 11 batch 526 batch_loss: 0.11898532509803772\n",
      "training: 11 batch 527 batch_loss: 0.11977699398994446\n",
      "training: 11 batch 528 batch_loss: 0.11615452170372009\n",
      "training: 11 batch 529 batch_loss: 0.11788010597229004\n",
      "training: 11 batch 530 batch_loss: 0.1219470202922821\n",
      "training: 11 batch 531 batch_loss: 0.12063813209533691\n",
      "training: 11 batch 532 batch_loss: 0.12064054608345032\n",
      "training: 11 batch 533 batch_loss: 0.11952856183052063\n",
      "training: 11 batch 534 batch_loss: 0.1178213357925415\n",
      "training: 11 batch 535 batch_loss: 0.11782103776931763\n",
      "training: 11 batch 536 batch_loss: 0.12121456861495972\n",
      "training: 11 batch 537 batch_loss: 0.1183948814868927\n",
      "training: 11 batch 538 batch_loss: 0.12022140622138977\n",
      "training: 11 batch 539 batch_loss: 0.11826437711715698\n",
      "training: 11 batch 540 batch_loss: 0.11984673142433167\n",
      "training: 11 batch 541 batch_loss: 0.12018704414367676\n",
      "training: 11 batch 542 batch_loss: 0.11754435300827026\n",
      "training: 11 batch 543 batch_loss: 0.11948367953300476\n",
      "training: 11 batch 544 batch_loss: 0.12076401710510254\n",
      "training: 11 batch 545 batch_loss: 0.11982792615890503\n",
      "training: 11 batch 546 batch_loss: 0.1227496862411499\n",
      "training: 11 batch 547 batch_loss: 0.12183913588523865\n",
      "training: 11 batch 548 batch_loss: 0.1195809543132782\n",
      "training: 11 batch 549 batch_loss: 0.11729174852371216\n",
      "training: 11 batch 550 batch_loss: 0.11901074647903442\n",
      "training: 11 batch 551 batch_loss: 0.1208355724811554\n",
      "training: 11 batch 552 batch_loss: 0.11574298143386841\n",
      "training: 11 batch 553 batch_loss: 0.11811226606369019\n",
      "training: 11 batch 554 batch_loss: 0.11655396223068237\n",
      "training: 11 batch 555 batch_loss: 0.12378713488578796\n",
      "training: 11 batch 556 batch_loss: 0.12057596445083618\n",
      "training: 11 batch 557 batch_loss: 0.1209607720375061\n",
      "training: 11 batch 558 batch_loss: 0.11976063251495361\n",
      "training: 11 batch 559 batch_loss: 0.11691367626190186\n",
      "training: 11 batch 560 batch_loss: 0.11740532517433167\n",
      "training: 11 batch 561 batch_loss: 0.12244823575019836\n",
      "training: 11 batch 562 batch_loss: 0.1206110417842865\n",
      "training: 11 batch 563 batch_loss: 0.11969828605651855\n",
      "training: 11 batch 564 batch_loss: 0.12220817804336548\n",
      "training: 11 batch 565 batch_loss: 0.12211108207702637\n",
      "training: 11 batch 566 batch_loss: 0.12348431348800659\n",
      "training: 11 batch 567 batch_loss: 0.12119975686073303\n",
      "training: 11 batch 568 batch_loss: 0.12021094560623169\n",
      "training: 11 batch 569 batch_loss: 0.12037360668182373\n",
      "training: 11 batch 570 batch_loss: 0.1200135350227356\n",
      "training: 11 batch 571 batch_loss: 0.12189942598342896\n",
      "training: 11 batch 572 batch_loss: 0.11927318572998047\n",
      "training: 11 batch 573 batch_loss: 0.12178397178649902\n",
      "training: 11 batch 574 batch_loss: 0.12112477421760559\n",
      "training: 11 batch 575 batch_loss: 0.11899518966674805\n",
      "training: 11 batch 576 batch_loss: 0.11879342794418335\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 11, Hit Ratio:0.023791360891160022 | Precision:0.03510272289393493 | Recall:0.049883893280369594 | NDCG:0.044082680113747844\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 12 batch 0 batch_loss: 0.11491227149963379\n",
      "training: 12 batch 1 batch_loss: 0.11606991291046143\n",
      "training: 12 batch 2 batch_loss: 0.11951538920402527\n",
      "training: 12 batch 3 batch_loss: 0.116385817527771\n",
      "training: 12 batch 4 batch_loss: 0.11832588911056519\n",
      "training: 12 batch 5 batch_loss: 0.1202104389667511\n",
      "training: 12 batch 6 batch_loss: 0.11802941560745239\n",
      "training: 12 batch 7 batch_loss: 0.12042093276977539\n",
      "training: 12 batch 8 batch_loss: 0.11709403991699219\n",
      "training: 12 batch 9 batch_loss: 0.11808878183364868\n",
      "training: 12 batch 10 batch_loss: 0.11845263838768005\n",
      "training: 12 batch 11 batch_loss: 0.11889973282814026\n",
      "training: 12 batch 12 batch_loss: 0.12109705805778503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 12 batch 13 batch_loss: 0.11999017000198364\n",
      "training: 12 batch 14 batch_loss: 0.11823010444641113\n",
      "training: 12 batch 15 batch_loss: 0.1187252402305603\n",
      "training: 12 batch 16 batch_loss: 0.11821377277374268\n",
      "training: 12 batch 17 batch_loss: 0.11584004759788513\n",
      "training: 12 batch 18 batch_loss: 0.11589804291725159\n",
      "training: 12 batch 19 batch_loss: 0.11874198913574219\n",
      "training: 12 batch 20 batch_loss: 0.11745032668113708\n",
      "training: 12 batch 21 batch_loss: 0.1187470555305481\n",
      "training: 12 batch 22 batch_loss: 0.11760380864143372\n",
      "training: 12 batch 23 batch_loss: 0.11873334646224976\n",
      "training: 12 batch 24 batch_loss: 0.11901527643203735\n",
      "training: 12 batch 25 batch_loss: 0.1188003420829773\n",
      "training: 12 batch 26 batch_loss: 0.11913114786148071\n",
      "training: 12 batch 27 batch_loss: 0.11965951323509216\n",
      "training: 12 batch 28 batch_loss: 0.11960843205451965\n",
      "training: 12 batch 29 batch_loss: 0.11808964610099792\n",
      "training: 12 batch 30 batch_loss: 0.11934280395507812\n",
      "training: 12 batch 31 batch_loss: 0.11805868148803711\n",
      "training: 12 batch 32 batch_loss: 0.11818045377731323\n",
      "training: 12 batch 33 batch_loss: 0.11796614527702332\n",
      "training: 12 batch 34 batch_loss: 0.11417409777641296\n",
      "training: 12 batch 35 batch_loss: 0.11609095335006714\n",
      "training: 12 batch 36 batch_loss: 0.1181197464466095\n",
      "training: 12 batch 37 batch_loss: 0.11836940050125122\n",
      "training: 12 batch 38 batch_loss: 0.116370290517807\n",
      "training: 12 batch 39 batch_loss: 0.1183561384677887\n",
      "training: 12 batch 40 batch_loss: 0.11865341663360596\n",
      "training: 12 batch 41 batch_loss: 0.11993873119354248\n",
      "training: 12 batch 42 batch_loss: 0.12151148915290833\n",
      "training: 12 batch 43 batch_loss: 0.11949083209037781\n",
      "training: 12 batch 44 batch_loss: 0.11479398608207703\n",
      "training: 12 batch 45 batch_loss: 0.11980080604553223\n",
      "training: 12 batch 46 batch_loss: 0.1189517080783844\n",
      "training: 12 batch 47 batch_loss: 0.11457601189613342\n",
      "training: 12 batch 48 batch_loss: 0.11938709020614624\n",
      "training: 12 batch 49 batch_loss: 0.12070009112358093\n",
      "training: 12 batch 50 batch_loss: 0.11853334307670593\n",
      "training: 12 batch 51 batch_loss: 0.11807781457901001\n",
      "training: 12 batch 52 batch_loss: 0.11766302585601807\n",
      "training: 12 batch 53 batch_loss: 0.11691868305206299\n",
      "training: 12 batch 54 batch_loss: 0.11912229657173157\n",
      "training: 12 batch 55 batch_loss: 0.12195438146591187\n",
      "training: 12 batch 56 batch_loss: 0.12023547291755676\n",
      "training: 12 batch 57 batch_loss: 0.11930885910987854\n",
      "training: 12 batch 58 batch_loss: 0.11766612529754639\n",
      "training: 12 batch 59 batch_loss: 0.11810797452926636\n",
      "training: 12 batch 60 batch_loss: 0.11789131164550781\n",
      "training: 12 batch 61 batch_loss: 0.11953997611999512\n",
      "training: 12 batch 62 batch_loss: 0.11776506900787354\n",
      "training: 12 batch 63 batch_loss: 0.11674940586090088\n",
      "training: 12 batch 64 batch_loss: 0.11767229437828064\n",
      "training: 12 batch 65 batch_loss: 0.12118184566497803\n",
      "training: 12 batch 66 batch_loss: 0.12114861607551575\n",
      "training: 12 batch 67 batch_loss: 0.1196625828742981\n",
      "training: 12 batch 68 batch_loss: 0.11589115858078003\n",
      "training: 12 batch 69 batch_loss: 0.1201736330986023\n",
      "training: 12 batch 70 batch_loss: 0.12027126550674438\n",
      "training: 12 batch 71 batch_loss: 0.11648449301719666\n",
      "training: 12 batch 72 batch_loss: 0.11808198690414429\n",
      "training: 12 batch 73 batch_loss: 0.1210552453994751\n",
      "training: 12 batch 74 batch_loss: 0.11934193968772888\n",
      "training: 12 batch 75 batch_loss: 0.12114086747169495\n",
      "training: 12 batch 76 batch_loss: 0.12383443117141724\n",
      "training: 12 batch 77 batch_loss: 0.12070834636688232\n",
      "training: 12 batch 78 batch_loss: 0.11861461400985718\n",
      "training: 12 batch 79 batch_loss: 0.11970272660255432\n",
      "training: 12 batch 80 batch_loss: 0.11764770746231079\n",
      "training: 12 batch 81 batch_loss: 0.11933380365371704\n",
      "training: 12 batch 82 batch_loss: 0.11652934551239014\n",
      "training: 12 batch 83 batch_loss: 0.117327481508255\n",
      "training: 12 batch 84 batch_loss: 0.11927449703216553\n",
      "training: 12 batch 85 batch_loss: 0.11697643995285034\n",
      "training: 12 batch 86 batch_loss: 0.11857104301452637\n",
      "training: 12 batch 87 batch_loss: 0.11794877052307129\n",
      "training: 12 batch 88 batch_loss: 0.11820435523986816\n",
      "training: 12 batch 89 batch_loss: 0.11806190013885498\n",
      "training: 12 batch 90 batch_loss: 0.11960095167160034\n",
      "training: 12 batch 91 batch_loss: 0.11757320165634155\n",
      "training: 12 batch 92 batch_loss: 0.11910301446914673\n",
      "training: 12 batch 93 batch_loss: 0.11972594261169434\n",
      "training: 12 batch 94 batch_loss: 0.12145641446113586\n",
      "training: 12 batch 95 batch_loss: 0.12020856142044067\n",
      "training: 12 batch 96 batch_loss: 0.11648735404014587\n",
      "training: 12 batch 97 batch_loss: 0.11841505765914917\n",
      "training: 12 batch 98 batch_loss: 0.12033119797706604\n",
      "training: 12 batch 99 batch_loss: 0.11825260519981384\n",
      "training: 12 batch 100 batch_loss: 0.11838552355766296\n",
      "training: 12 batch 101 batch_loss: 0.11736434698104858\n",
      "training: 12 batch 102 batch_loss: 0.11840781569480896\n",
      "training: 12 batch 103 batch_loss: 0.12057614326477051\n",
      "training: 12 batch 104 batch_loss: 0.12105289101600647\n",
      "training: 12 batch 105 batch_loss: 0.11885476112365723\n",
      "training: 12 batch 106 batch_loss: 0.11952334642410278\n",
      "training: 12 batch 107 batch_loss: 0.11898228526115417\n",
      "training: 12 batch 108 batch_loss: 0.11644729971885681\n",
      "training: 12 batch 109 batch_loss: 0.12061819434165955\n",
      "training: 12 batch 110 batch_loss: 0.12296420335769653\n",
      "training: 12 batch 111 batch_loss: 0.11988383531570435\n",
      "training: 12 batch 112 batch_loss: 0.11736953258514404\n",
      "training: 12 batch 113 batch_loss: 0.11731034517288208\n",
      "training: 12 batch 114 batch_loss: 0.11810976266860962\n",
      "training: 12 batch 115 batch_loss: 0.11737778782844543\n",
      "training: 12 batch 116 batch_loss: 0.12101060152053833\n",
      "training: 12 batch 117 batch_loss: 0.11783751845359802\n",
      "training: 12 batch 118 batch_loss: 0.11974623799324036\n",
      "training: 12 batch 119 batch_loss: 0.12113896012306213\n",
      "training: 12 batch 120 batch_loss: 0.11605298519134521\n",
      "training: 12 batch 121 batch_loss: 0.1182299256324768\n",
      "training: 12 batch 122 batch_loss: 0.1173461377620697\n",
      "training: 12 batch 123 batch_loss: 0.11830201745033264\n",
      "training: 12 batch 124 batch_loss: 0.11946260929107666\n",
      "training: 12 batch 125 batch_loss: 0.1216731071472168\n",
      "training: 12 batch 126 batch_loss: 0.11983495950698853\n",
      "training: 12 batch 127 batch_loss: 0.12030890583992004\n",
      "training: 12 batch 128 batch_loss: 0.11975133419036865\n",
      "training: 12 batch 129 batch_loss: 0.120078444480896\n",
      "training: 12 batch 130 batch_loss: 0.1221388578414917\n",
      "training: 12 batch 131 batch_loss: 0.12236589193344116\n",
      "training: 12 batch 132 batch_loss: 0.11784940958023071\n",
      "training: 12 batch 133 batch_loss: 0.11882299184799194\n",
      "training: 12 batch 134 batch_loss: 0.11884969472885132\n",
      "training: 12 batch 135 batch_loss: 0.1220148503780365\n",
      "training: 12 batch 136 batch_loss: 0.1213037371635437\n",
      "training: 12 batch 137 batch_loss: 0.1184961199760437\n",
      "training: 12 batch 138 batch_loss: 0.11562541127204895\n",
      "training: 12 batch 139 batch_loss: 0.1168757975101471\n",
      "training: 12 batch 140 batch_loss: 0.11553812026977539\n",
      "training: 12 batch 141 batch_loss: 0.12216359376907349\n",
      "training: 12 batch 142 batch_loss: 0.11685606837272644\n",
      "training: 12 batch 143 batch_loss: 0.11642685532569885\n",
      "training: 12 batch 144 batch_loss: 0.1185767650604248\n",
      "training: 12 batch 145 batch_loss: 0.11865049600601196\n",
      "training: 12 batch 146 batch_loss: 0.12031298875808716\n",
      "training: 12 batch 147 batch_loss: 0.11854714155197144\n",
      "training: 12 batch 148 batch_loss: 0.1176295280456543\n",
      "training: 12 batch 149 batch_loss: 0.12057989835739136\n",
      "training: 12 batch 150 batch_loss: 0.11634743213653564\n",
      "training: 12 batch 151 batch_loss: 0.11970186233520508\n",
      "training: 12 batch 152 batch_loss: 0.11926952004432678\n",
      "training: 12 batch 153 batch_loss: 0.1180199384689331\n",
      "training: 12 batch 154 batch_loss: 0.12052810192108154\n",
      "training: 12 batch 155 batch_loss: 0.11995857954025269\n",
      "training: 12 batch 156 batch_loss: 0.11741659045219421\n",
      "training: 12 batch 157 batch_loss: 0.12071582674980164\n",
      "training: 12 batch 158 batch_loss: 0.11792129278182983\n",
      "training: 12 batch 159 batch_loss: 0.1208752989768982\n",
      "training: 12 batch 160 batch_loss: 0.11944088339805603\n",
      "training: 12 batch 161 batch_loss: 0.12334123253822327\n",
      "training: 12 batch 162 batch_loss: 0.11829346418380737\n",
      "training: 12 batch 163 batch_loss: 0.1205839216709137\n",
      "training: 12 batch 164 batch_loss: 0.1234675943851471\n",
      "training: 12 batch 165 batch_loss: 0.11905330419540405\n",
      "training: 12 batch 166 batch_loss: 0.11745312809944153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 12 batch 167 batch_loss: 0.11836805939674377\n",
      "training: 12 batch 168 batch_loss: 0.12043410539627075\n",
      "training: 12 batch 169 batch_loss: 0.11998745799064636\n",
      "training: 12 batch 170 batch_loss: 0.11936652660369873\n",
      "training: 12 batch 171 batch_loss: 0.12128740549087524\n",
      "training: 12 batch 172 batch_loss: 0.12112632393836975\n",
      "training: 12 batch 173 batch_loss: 0.12034672498703003\n",
      "training: 12 batch 174 batch_loss: 0.1175418496131897\n",
      "training: 12 batch 175 batch_loss: 0.11990845203399658\n",
      "training: 12 batch 176 batch_loss: 0.12053090333938599\n",
      "training: 12 batch 177 batch_loss: 0.12133851647377014\n",
      "training: 12 batch 178 batch_loss: 0.11802265048027039\n",
      "training: 12 batch 179 batch_loss: 0.11852353811264038\n",
      "training: 12 batch 180 batch_loss: 0.11817619204521179\n",
      "training: 12 batch 181 batch_loss: 0.12194287776947021\n",
      "training: 12 batch 182 batch_loss: 0.11691731214523315\n",
      "training: 12 batch 183 batch_loss: 0.11784908175468445\n",
      "training: 12 batch 184 batch_loss: 0.1202000081539154\n",
      "training: 12 batch 185 batch_loss: 0.11619254946708679\n",
      "training: 12 batch 186 batch_loss: 0.12016028165817261\n",
      "training: 12 batch 187 batch_loss: 0.12086915969848633\n",
      "training: 12 batch 188 batch_loss: 0.119874507188797\n",
      "training: 12 batch 189 batch_loss: 0.11941561102867126\n",
      "training: 12 batch 190 batch_loss: 0.11992481350898743\n",
      "training: 12 batch 191 batch_loss: 0.11945432424545288\n",
      "training: 12 batch 192 batch_loss: 0.11998337507247925\n",
      "training: 12 batch 193 batch_loss: 0.1170332133769989\n",
      "training: 12 batch 194 batch_loss: 0.11784297227859497\n",
      "training: 12 batch 195 batch_loss: 0.11995428800582886\n",
      "training: 12 batch 196 batch_loss: 0.12028181552886963\n",
      "training: 12 batch 197 batch_loss: 0.12196055054664612\n",
      "training: 12 batch 198 batch_loss: 0.1202506422996521\n",
      "training: 12 batch 199 batch_loss: 0.1226431131362915\n",
      "training: 12 batch 200 batch_loss: 0.12349334359169006\n",
      "training: 12 batch 201 batch_loss: 0.12138998508453369\n",
      "training: 12 batch 202 batch_loss: 0.11884438991546631\n",
      "training: 12 batch 203 batch_loss: 0.12124502658843994\n",
      "training: 12 batch 204 batch_loss: 0.12091833353042603\n",
      "training: 12 batch 205 batch_loss: 0.12216880917549133\n",
      "training: 12 batch 206 batch_loss: 0.12068888545036316\n",
      "training: 12 batch 207 batch_loss: 0.11781936883926392\n",
      "training: 12 batch 208 batch_loss: 0.12225854396820068\n",
      "training: 12 batch 209 batch_loss: 0.12092471122741699\n",
      "training: 12 batch 210 batch_loss: 0.11920511722564697\n",
      "training: 12 batch 211 batch_loss: 0.12150797247886658\n",
      "training: 12 batch 212 batch_loss: 0.11719211935997009\n",
      "training: 12 batch 213 batch_loss: 0.1207023561000824\n",
      "training: 12 batch 214 batch_loss: 0.11743190884590149\n",
      "training: 12 batch 215 batch_loss: 0.12033253908157349\n",
      "training: 12 batch 216 batch_loss: 0.11670589447021484\n",
      "training: 12 batch 217 batch_loss: 0.12129777669906616\n",
      "training: 12 batch 218 batch_loss: 0.12054497003555298\n",
      "training: 12 batch 219 batch_loss: 0.12435516715049744\n",
      "training: 12 batch 220 batch_loss: 0.11919522285461426\n",
      "training: 12 batch 221 batch_loss: 0.11989524960517883\n",
      "training: 12 batch 222 batch_loss: 0.11917078495025635\n",
      "training: 12 batch 223 batch_loss: 0.1221066415309906\n",
      "training: 12 batch 224 batch_loss: 0.11969113349914551\n",
      "training: 12 batch 225 batch_loss: 0.11964216828346252\n",
      "training: 12 batch 226 batch_loss: 0.11997920274734497\n",
      "training: 12 batch 227 batch_loss: 0.11905404925346375\n",
      "training: 12 batch 228 batch_loss: 0.11780962347984314\n",
      "training: 12 batch 229 batch_loss: 0.11934193968772888\n",
      "training: 12 batch 230 batch_loss: 0.12068408727645874\n",
      "training: 12 batch 231 batch_loss: 0.12196150422096252\n",
      "training: 12 batch 232 batch_loss: 0.1196625828742981\n",
      "training: 12 batch 233 batch_loss: 0.11933231353759766\n",
      "training: 12 batch 234 batch_loss: 0.1191878616809845\n",
      "training: 12 batch 235 batch_loss: 0.1215313971042633\n",
      "training: 12 batch 236 batch_loss: 0.11790475249290466\n",
      "training: 12 batch 237 batch_loss: 0.12047752737998962\n",
      "training: 12 batch 238 batch_loss: 0.12033870816230774\n",
      "training: 12 batch 239 batch_loss: 0.1184883713722229\n",
      "training: 12 batch 240 batch_loss: 0.12092012166976929\n",
      "training: 12 batch 241 batch_loss: 0.12296345829963684\n",
      "training: 12 batch 242 batch_loss: 0.118956059217453\n",
      "training: 12 batch 243 batch_loss: 0.11932826042175293\n",
      "training: 12 batch 244 batch_loss: 0.11815270781517029\n",
      "training: 12 batch 245 batch_loss: 0.1204308271408081\n",
      "training: 12 batch 246 batch_loss: 0.11999383568763733\n",
      "training: 12 batch 247 batch_loss: 0.12170612812042236\n",
      "training: 12 batch 248 batch_loss: 0.12285584211349487\n",
      "training: 12 batch 249 batch_loss: 0.11901038885116577\n",
      "training: 12 batch 250 batch_loss: 0.12225919961929321\n",
      "training: 12 batch 251 batch_loss: 0.11907988786697388\n",
      "training: 12 batch 252 batch_loss: 0.12281501293182373\n",
      "training: 12 batch 253 batch_loss: 0.12035220861434937\n",
      "training: 12 batch 254 batch_loss: 0.12125879526138306\n",
      "training: 12 batch 255 batch_loss: 0.12056586146354675\n",
      "training: 12 batch 256 batch_loss: 0.12301450967788696\n",
      "training: 12 batch 257 batch_loss: 0.11850333213806152\n",
      "training: 12 batch 258 batch_loss: 0.11875259876251221\n",
      "training: 12 batch 259 batch_loss: 0.12154048681259155\n",
      "training: 12 batch 260 batch_loss: 0.12070348858833313\n",
      "training: 12 batch 261 batch_loss: 0.11912280321121216\n",
      "training: 12 batch 262 batch_loss: 0.12160322070121765\n",
      "training: 12 batch 263 batch_loss: 0.12284412980079651\n",
      "training: 12 batch 264 batch_loss: 0.12204205989837646\n",
      "training: 12 batch 265 batch_loss: 0.11951148509979248\n",
      "training: 12 batch 266 batch_loss: 0.12046617269515991\n",
      "training: 12 batch 267 batch_loss: 0.12193310260772705\n",
      "training: 12 batch 268 batch_loss: 0.12293490767478943\n",
      "training: 12 batch 269 batch_loss: 0.12145987153053284\n",
      "training: 12 batch 270 batch_loss: 0.12166708707809448\n",
      "training: 12 batch 271 batch_loss: 0.12323158979415894\n",
      "training: 12 batch 272 batch_loss: 0.12158706784248352\n",
      "training: 12 batch 273 batch_loss: 0.1217266321182251\n",
      "training: 12 batch 274 batch_loss: 0.11993467807769775\n",
      "training: 12 batch 275 batch_loss: 0.12107163667678833\n",
      "training: 12 batch 276 batch_loss: 0.11908823251724243\n",
      "training: 12 batch 277 batch_loss: 0.12109225988388062\n",
      "training: 12 batch 278 batch_loss: 0.12111839652061462\n",
      "training: 12 batch 279 batch_loss: 0.11845743656158447\n",
      "training: 12 batch 280 batch_loss: 0.12118172645568848\n",
      "training: 12 batch 281 batch_loss: 0.11715474724769592\n",
      "training: 12 batch 282 batch_loss: 0.12002265453338623\n",
      "training: 12 batch 283 batch_loss: 0.11965936422348022\n",
      "training: 12 batch 284 batch_loss: 0.12382590770721436\n",
      "training: 12 batch 285 batch_loss: 0.11976537108421326\n",
      "training: 12 batch 286 batch_loss: 0.12325945496559143\n",
      "training: 12 batch 287 batch_loss: 0.12049362063407898\n",
      "training: 12 batch 288 batch_loss: 0.11858248710632324\n",
      "training: 12 batch 289 batch_loss: 0.12213414907455444\n",
      "training: 12 batch 290 batch_loss: 0.12050575017929077\n",
      "training: 12 batch 291 batch_loss: 0.11722299456596375\n",
      "training: 12 batch 292 batch_loss: 0.11947068572044373\n",
      "training: 12 batch 293 batch_loss: 0.12207889556884766\n",
      "training: 12 batch 294 batch_loss: 0.11999154090881348\n",
      "training: 12 batch 295 batch_loss: 0.12134730815887451\n",
      "training: 12 batch 296 batch_loss: 0.12001138925552368\n",
      "training: 12 batch 297 batch_loss: 0.12196633219718933\n",
      "training: 12 batch 298 batch_loss: 0.12217652797698975\n",
      "training: 12 batch 299 batch_loss: 0.119434654712677\n",
      "training: 12 batch 300 batch_loss: 0.12316614389419556\n",
      "training: 12 batch 301 batch_loss: 0.1215122640132904\n",
      "training: 12 batch 302 batch_loss: 0.1179848313331604\n",
      "training: 12 batch 303 batch_loss: 0.1251225471496582\n",
      "training: 12 batch 304 batch_loss: 0.1199098527431488\n",
      "training: 12 batch 305 batch_loss: 0.12193191051483154\n",
      "training: 12 batch 306 batch_loss: 0.12192106246948242\n",
      "training: 12 batch 307 batch_loss: 0.11923742294311523\n",
      "training: 12 batch 308 batch_loss: 0.1175992488861084\n",
      "training: 12 batch 309 batch_loss: 0.12031081318855286\n",
      "training: 12 batch 310 batch_loss: 0.12107765674591064\n",
      "training: 12 batch 311 batch_loss: 0.12138441205024719\n",
      "training: 12 batch 312 batch_loss: 0.1213146448135376\n",
      "training: 12 batch 313 batch_loss: 0.12013906240463257\n",
      "training: 12 batch 314 batch_loss: 0.12330353260040283\n",
      "training: 12 batch 315 batch_loss: 0.12328875064849854\n",
      "training: 12 batch 316 batch_loss: 0.1197388768196106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 12 batch 317 batch_loss: 0.11695289611816406\n",
      "training: 12 batch 318 batch_loss: 0.12292027473449707\n",
      "training: 12 batch 319 batch_loss: 0.12207388877868652\n",
      "training: 12 batch 320 batch_loss: 0.12012311816215515\n",
      "training: 12 batch 321 batch_loss: 0.11993122100830078\n",
      "training: 12 batch 322 batch_loss: 0.11862826347351074\n",
      "training: 12 batch 323 batch_loss: 0.12166965007781982\n",
      "training: 12 batch 324 batch_loss: 0.12316614389419556\n",
      "training: 12 batch 325 batch_loss: 0.12054651975631714\n",
      "training: 12 batch 326 batch_loss: 0.11947789788246155\n",
      "training: 12 batch 327 batch_loss: 0.12119472026824951\n",
      "training: 12 batch 328 batch_loss: 0.1223520040512085\n",
      "training: 12 batch 329 batch_loss: 0.12109577655792236\n",
      "training: 12 batch 330 batch_loss: 0.1196306049823761\n",
      "training: 12 batch 331 batch_loss: 0.1217067539691925\n",
      "training: 12 batch 332 batch_loss: 0.11914968490600586\n",
      "training: 12 batch 333 batch_loss: 0.12078705430030823\n",
      "training: 12 batch 334 batch_loss: 0.12139397859573364\n",
      "training: 12 batch 335 batch_loss: 0.12087929248809814\n",
      "training: 12 batch 336 batch_loss: 0.120587557554245\n",
      "training: 12 batch 337 batch_loss: 0.11935257911682129\n",
      "training: 12 batch 338 batch_loss: 0.12396648526191711\n",
      "training: 12 batch 339 batch_loss: 0.12027847766876221\n",
      "training: 12 batch 340 batch_loss: 0.12229600548744202\n",
      "training: 12 batch 341 batch_loss: 0.12235584855079651\n",
      "training: 12 batch 342 batch_loss: 0.11916819214820862\n",
      "training: 12 batch 343 batch_loss: 0.12344902753829956\n",
      "training: 12 batch 344 batch_loss: 0.12130945920944214\n",
      "training: 12 batch 345 batch_loss: 0.11800602078437805\n",
      "training: 12 batch 346 batch_loss: 0.1185309886932373\n",
      "training: 12 batch 347 batch_loss: 0.11951887607574463\n",
      "training: 12 batch 348 batch_loss: 0.1191941499710083\n",
      "training: 12 batch 349 batch_loss: 0.11968359351158142\n",
      "training: 12 batch 350 batch_loss: 0.12087702751159668\n",
      "training: 12 batch 351 batch_loss: 0.12044364213943481\n",
      "training: 12 batch 352 batch_loss: 0.12114143371582031\n",
      "training: 12 batch 353 batch_loss: 0.12374523282051086\n",
      "training: 12 batch 354 batch_loss: 0.12378451228141785\n",
      "training: 12 batch 355 batch_loss: 0.12205475568771362\n",
      "training: 12 batch 356 batch_loss: 0.12190243601799011\n",
      "training: 12 batch 357 batch_loss: 0.12329155206680298\n",
      "training: 12 batch 358 batch_loss: 0.11904504895210266\n",
      "training: 12 batch 359 batch_loss: 0.1222500205039978\n",
      "training: 12 batch 360 batch_loss: 0.12225735187530518\n",
      "training: 12 batch 361 batch_loss: 0.12291455268859863\n",
      "training: 12 batch 362 batch_loss: 0.11892527341842651\n",
      "training: 12 batch 363 batch_loss: 0.12142056226730347\n",
      "training: 12 batch 364 batch_loss: 0.12157905101776123\n",
      "training: 12 batch 365 batch_loss: 0.12377095222473145\n",
      "training: 12 batch 366 batch_loss: 0.12235042452812195\n",
      "training: 12 batch 367 batch_loss: 0.11987724900245667\n",
      "training: 12 batch 368 batch_loss: 0.11977273225784302\n",
      "training: 12 batch 369 batch_loss: 0.12150028347969055\n",
      "training: 12 batch 370 batch_loss: 0.12214505672454834\n",
      "training: 12 batch 371 batch_loss: 0.12182563543319702\n",
      "training: 12 batch 372 batch_loss: 0.12216112017631531\n",
      "training: 12 batch 373 batch_loss: 0.120047926902771\n",
      "training: 12 batch 374 batch_loss: 0.11930423974990845\n",
      "training: 12 batch 375 batch_loss: 0.12141221761703491\n",
      "training: 12 batch 376 batch_loss: 0.11921429634094238\n",
      "training: 12 batch 377 batch_loss: 0.11870378255844116\n",
      "training: 12 batch 378 batch_loss: 0.1210874617099762\n",
      "training: 12 batch 379 batch_loss: 0.12399640679359436\n",
      "training: 12 batch 380 batch_loss: 0.1217828094959259\n",
      "training: 12 batch 381 batch_loss: 0.12167805433273315\n",
      "training: 12 batch 382 batch_loss: 0.12135878205299377\n",
      "training: 12 batch 383 batch_loss: 0.12148267030715942\n",
      "training: 12 batch 384 batch_loss: 0.12063807249069214\n",
      "training: 12 batch 385 batch_loss: 0.12438404560089111\n",
      "training: 12 batch 386 batch_loss: 0.12336388230323792\n",
      "training: 12 batch 387 batch_loss: 0.12237504124641418\n",
      "training: 12 batch 388 batch_loss: 0.1260325312614441\n",
      "training: 12 batch 389 batch_loss: 0.11958149075508118\n",
      "training: 12 batch 390 batch_loss: 0.12345913052558899\n",
      "training: 12 batch 391 batch_loss: 0.12296167016029358\n",
      "training: 12 batch 392 batch_loss: 0.11983239650726318\n",
      "training: 12 batch 393 batch_loss: 0.12437036633491516\n",
      "training: 12 batch 394 batch_loss: 0.12212687730789185\n",
      "training: 12 batch 395 batch_loss: 0.11975926160812378\n",
      "training: 12 batch 396 batch_loss: 0.12067410349845886\n",
      "training: 12 batch 397 batch_loss: 0.11910516023635864\n",
      "training: 12 batch 398 batch_loss: 0.12007588148117065\n",
      "training: 12 batch 399 batch_loss: 0.12433773279190063\n",
      "training: 12 batch 400 batch_loss: 0.1226930022239685\n",
      "training: 12 batch 401 batch_loss: 0.1221466064453125\n",
      "training: 12 batch 402 batch_loss: 0.12245231866836548\n",
      "training: 12 batch 403 batch_loss: 0.11992371082305908\n",
      "training: 12 batch 404 batch_loss: 0.12204277515411377\n",
      "training: 12 batch 405 batch_loss: 0.12339925765991211\n",
      "training: 12 batch 406 batch_loss: 0.12546011805534363\n",
      "training: 12 batch 407 batch_loss: 0.12314385175704956\n",
      "training: 12 batch 408 batch_loss: 0.11940503120422363\n",
      "training: 12 batch 409 batch_loss: 0.1208985447883606\n",
      "training: 12 batch 410 batch_loss: 0.12225872278213501\n",
      "training: 12 batch 411 batch_loss: 0.12142476439476013\n",
      "training: 12 batch 412 batch_loss: 0.1224440336227417\n",
      "training: 12 batch 413 batch_loss: 0.12014394998550415\n",
      "training: 12 batch 414 batch_loss: 0.12268096208572388\n",
      "training: 12 batch 415 batch_loss: 0.12068095803260803\n",
      "training: 12 batch 416 batch_loss: 0.12269407510757446\n",
      "training: 12 batch 417 batch_loss: 0.12255355715751648\n",
      "training: 12 batch 418 batch_loss: 0.1237395703792572\n",
      "training: 12 batch 419 batch_loss: 0.1216680109500885\n",
      "training: 12 batch 420 batch_loss: 0.11834320425987244\n",
      "training: 12 batch 421 batch_loss: 0.12402769923210144\n",
      "training: 12 batch 422 batch_loss: 0.12436622381210327\n",
      "training: 12 batch 423 batch_loss: 0.12376099824905396\n",
      "training: 12 batch 424 batch_loss: 0.1198580265045166\n",
      "training: 12 batch 425 batch_loss: 0.1237679123878479\n",
      "training: 12 batch 426 batch_loss: 0.12283524870872498\n",
      "training: 12 batch 427 batch_loss: 0.12235385179519653\n",
      "training: 12 batch 428 batch_loss: 0.12266108393669128\n",
      "training: 12 batch 429 batch_loss: 0.12026405334472656\n",
      "training: 12 batch 430 batch_loss: 0.1262873113155365\n",
      "training: 12 batch 431 batch_loss: 0.12527179718017578\n",
      "training: 12 batch 432 batch_loss: 0.11964890360832214\n",
      "training: 12 batch 433 batch_loss: 0.11906066536903381\n",
      "training: 12 batch 434 batch_loss: 0.12396681308746338\n",
      "training: 12 batch 435 batch_loss: 0.12357074022293091\n",
      "training: 12 batch 436 batch_loss: 0.12088340520858765\n",
      "training: 12 batch 437 batch_loss: 0.12184673547744751\n",
      "training: 12 batch 438 batch_loss: 0.12172874808311462\n",
      "training: 12 batch 439 batch_loss: 0.12294742465019226\n",
      "training: 12 batch 440 batch_loss: 0.12184417247772217\n",
      "training: 12 batch 441 batch_loss: 0.12043535709381104\n",
      "training: 12 batch 442 batch_loss: 0.12185457348823547\n",
      "training: 12 batch 443 batch_loss: 0.12118110060691833\n",
      "training: 12 batch 444 batch_loss: 0.12095272541046143\n",
      "training: 12 batch 445 batch_loss: 0.12068861722946167\n",
      "training: 12 batch 446 batch_loss: 0.12352314591407776\n",
      "training: 12 batch 447 batch_loss: 0.12178829312324524\n",
      "training: 12 batch 448 batch_loss: 0.12399613857269287\n",
      "training: 12 batch 449 batch_loss: 0.12324613332748413\n",
      "training: 12 batch 450 batch_loss: 0.12301221489906311\n",
      "training: 12 batch 451 batch_loss: 0.1230839192867279\n",
      "training: 12 batch 452 batch_loss: 0.12336915731430054\n",
      "training: 12 batch 453 batch_loss: 0.12234273552894592\n",
      "training: 12 batch 454 batch_loss: 0.12130913138389587\n",
      "training: 12 batch 455 batch_loss: 0.12155121564865112\n",
      "training: 12 batch 456 batch_loss: 0.12025487422943115\n",
      "training: 12 batch 457 batch_loss: 0.12393525242805481\n",
      "training: 12 batch 458 batch_loss: 0.11955034732818604\n",
      "training: 12 batch 459 batch_loss: 0.12097734212875366\n",
      "training: 12 batch 460 batch_loss: 0.12186414003372192\n",
      "training: 12 batch 461 batch_loss: 0.1223602294921875\n",
      "training: 12 batch 462 batch_loss: 0.12510424852371216\n",
      "training: 12 batch 463 batch_loss: 0.12074759602546692\n",
      "training: 12 batch 464 batch_loss: 0.12250763177871704\n",
      "training: 12 batch 465 batch_loss: 0.12158605456352234\n",
      "training: 12 batch 466 batch_loss: 0.12307795882225037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 12 batch 467 batch_loss: 0.12273216247558594\n",
      "training: 12 batch 468 batch_loss: 0.12214049696922302\n",
      "training: 12 batch 469 batch_loss: 0.11938104033470154\n",
      "training: 12 batch 470 batch_loss: 0.12082704901695251\n",
      "training: 12 batch 471 batch_loss: 0.12012964487075806\n",
      "training: 12 batch 472 batch_loss: 0.12164786458015442\n",
      "training: 12 batch 473 batch_loss: 0.12047779560089111\n",
      "training: 12 batch 474 batch_loss: 0.121673583984375\n",
      "training: 12 batch 475 batch_loss: 0.11981728672981262\n",
      "training: 12 batch 476 batch_loss: 0.12314942479133606\n",
      "training: 12 batch 477 batch_loss: 0.12322467565536499\n",
      "training: 12 batch 478 batch_loss: 0.12048301100730896\n",
      "training: 12 batch 479 batch_loss: 0.12015655636787415\n",
      "training: 12 batch 480 batch_loss: 0.12099042534828186\n",
      "training: 12 batch 481 batch_loss: 0.12362799048423767\n",
      "training: 12 batch 482 batch_loss: 0.12038850784301758\n",
      "training: 12 batch 483 batch_loss: 0.11987251043319702\n",
      "training: 12 batch 484 batch_loss: 0.1200890839099884\n",
      "training: 12 batch 485 batch_loss: 0.11978307366371155\n",
      "training: 12 batch 486 batch_loss: 0.11990886926651001\n",
      "training: 12 batch 487 batch_loss: 0.12294164299964905\n",
      "training: 12 batch 488 batch_loss: 0.12076812982559204\n",
      "training: 12 batch 489 batch_loss: 0.12252086400985718\n",
      "training: 12 batch 490 batch_loss: 0.12279197573661804\n",
      "training: 12 batch 491 batch_loss: 0.12284785509109497\n",
      "training: 12 batch 492 batch_loss: 0.12698274850845337\n",
      "training: 12 batch 493 batch_loss: 0.12043485045433044\n",
      "training: 12 batch 494 batch_loss: 0.12112480401992798\n",
      "training: 12 batch 495 batch_loss: 0.12476581335067749\n",
      "training: 12 batch 496 batch_loss: 0.12127238512039185\n",
      "training: 12 batch 497 batch_loss: 0.12342482805252075\n",
      "training: 12 batch 498 batch_loss: 0.12434959411621094\n",
      "training: 12 batch 499 batch_loss: 0.12236878275871277\n",
      "training: 12 batch 500 batch_loss: 0.12226003408432007\n",
      "training: 12 batch 501 batch_loss: 0.12339094281196594\n",
      "training: 12 batch 502 batch_loss: 0.12374648451805115\n",
      "training: 12 batch 503 batch_loss: 0.12270545959472656\n",
      "training: 12 batch 504 batch_loss: 0.11958789825439453\n",
      "training: 12 batch 505 batch_loss: 0.1175566017627716\n",
      "training: 12 batch 506 batch_loss: 0.12132114171981812\n",
      "training: 12 batch 507 batch_loss: 0.12126070261001587\n",
      "training: 12 batch 508 batch_loss: 0.12391185760498047\n",
      "training: 12 batch 509 batch_loss: 0.12698853015899658\n",
      "training: 12 batch 510 batch_loss: 0.12089544534683228\n",
      "training: 12 batch 511 batch_loss: 0.12223213911056519\n",
      "training: 12 batch 512 batch_loss: 0.12233144044876099\n",
      "training: 12 batch 513 batch_loss: 0.12096506357192993\n",
      "training: 12 batch 514 batch_loss: 0.12219485640525818\n",
      "training: 12 batch 515 batch_loss: 0.1251114308834076\n",
      "training: 12 batch 516 batch_loss: 0.12521064281463623\n",
      "training: 12 batch 517 batch_loss: 0.12236624956130981\n",
      "training: 12 batch 518 batch_loss: 0.1202591061592102\n",
      "training: 12 batch 519 batch_loss: 0.12212347984313965\n",
      "training: 12 batch 520 batch_loss: 0.122822105884552\n",
      "training: 12 batch 521 batch_loss: 0.12227946519851685\n",
      "training: 12 batch 522 batch_loss: 0.12185612320899963\n",
      "training: 12 batch 523 batch_loss: 0.11992579698562622\n",
      "training: 12 batch 524 batch_loss: 0.11953598260879517\n",
      "training: 12 batch 525 batch_loss: 0.12417364120483398\n",
      "training: 12 batch 526 batch_loss: 0.12262958288192749\n",
      "training: 12 batch 527 batch_loss: 0.12200641632080078\n",
      "training: 12 batch 528 batch_loss: 0.12305927276611328\n",
      "training: 12 batch 529 batch_loss: 0.1225423514842987\n",
      "training: 12 batch 530 batch_loss: 0.1251101791858673\n",
      "training: 12 batch 531 batch_loss: 0.12116813659667969\n",
      "training: 12 batch 532 batch_loss: 0.12282514572143555\n",
      "training: 12 batch 533 batch_loss: 0.12231561541557312\n",
      "training: 12 batch 534 batch_loss: 0.12292230129241943\n",
      "training: 12 batch 535 batch_loss: 0.12124133110046387\n",
      "training: 12 batch 536 batch_loss: 0.12350359559059143\n",
      "training: 12 batch 537 batch_loss: 0.1232978105545044\n",
      "training: 12 batch 538 batch_loss: 0.1258077621459961\n",
      "training: 12 batch 539 batch_loss: 0.12142428755760193\n",
      "training: 12 batch 540 batch_loss: 0.12287428975105286\n",
      "training: 12 batch 541 batch_loss: 0.12273222208023071\n",
      "training: 12 batch 542 batch_loss: 0.12301617860794067\n",
      "training: 12 batch 543 batch_loss: 0.12201574444770813\n",
      "training: 12 batch 544 batch_loss: 0.12460222840309143\n",
      "training: 12 batch 545 batch_loss: 0.11802774667739868\n",
      "training: 12 batch 546 batch_loss: 0.11885315179824829\n",
      "training: 12 batch 547 batch_loss: 0.12211108207702637\n",
      "training: 12 batch 548 batch_loss: 0.11934980750083923\n",
      "training: 12 batch 549 batch_loss: 0.11949986219406128\n",
      "training: 12 batch 550 batch_loss: 0.12161064147949219\n",
      "training: 12 batch 551 batch_loss: 0.12269830703735352\n",
      "training: 12 batch 552 batch_loss: 0.121540367603302\n",
      "training: 12 batch 553 batch_loss: 0.12150642275810242\n",
      "training: 12 batch 554 batch_loss: 0.12061741948127747\n",
      "training: 12 batch 555 batch_loss: 0.12344014644622803\n",
      "training: 12 batch 556 batch_loss: 0.12631860375404358\n",
      "training: 12 batch 557 batch_loss: 0.12117338180541992\n",
      "training: 12 batch 558 batch_loss: 0.12306314706802368\n",
      "training: 12 batch 559 batch_loss: 0.12352541089057922\n",
      "training: 12 batch 560 batch_loss: 0.12048006057739258\n",
      "training: 12 batch 561 batch_loss: 0.1230798065662384\n",
      "training: 12 batch 562 batch_loss: 0.12038424611091614\n",
      "training: 12 batch 563 batch_loss: 0.12273907661437988\n",
      "training: 12 batch 564 batch_loss: 0.12121319770812988\n",
      "training: 12 batch 565 batch_loss: 0.12285488843917847\n",
      "training: 12 batch 566 batch_loss: 0.12112632393836975\n",
      "training: 12 batch 567 batch_loss: 0.12238931655883789\n",
      "training: 12 batch 568 batch_loss: 0.12008315324783325\n",
      "training: 12 batch 569 batch_loss: 0.12111595273017883\n",
      "training: 12 batch 570 batch_loss: 0.12004703283309937\n",
      "training: 12 batch 571 batch_loss: 0.12108102440834045\n",
      "training: 12 batch 572 batch_loss: 0.12428182363510132\n",
      "training: 12 batch 573 batch_loss: 0.12448841333389282\n",
      "training: 12 batch 574 batch_loss: 0.12179648876190186\n",
      "training: 12 batch 575 batch_loss: 0.1232084333896637\n",
      "training: 12 batch 576 batch_loss: 0.1213880181312561\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 12, Hit Ratio:0.02384132874517394 | Precision:0.035176447458959995 | Recall:0.05019485603236961 | NDCG:0.04420267292543814\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 13 batch 0 batch_loss: 0.11971881985664368\n",
      "training: 13 batch 1 batch_loss: 0.12024110555648804\n",
      "training: 13 batch 2 batch_loss: 0.12252408266067505\n",
      "training: 13 batch 3 batch_loss: 0.12174567580223083\n",
      "training: 13 batch 4 batch_loss: 0.1228182315826416\n",
      "training: 13 batch 5 batch_loss: 0.11901769042015076\n",
      "training: 13 batch 6 batch_loss: 0.12141978740692139\n",
      "training: 13 batch 7 batch_loss: 0.1228228509426117\n",
      "training: 13 batch 8 batch_loss: 0.11984452605247498\n",
      "training: 13 batch 9 batch_loss: 0.12211975455284119\n",
      "training: 13 batch 10 batch_loss: 0.11944583058357239\n",
      "training: 13 batch 11 batch_loss: 0.12077543139457703\n",
      "training: 13 batch 12 batch_loss: 0.12142148613929749\n",
      "training: 13 batch 13 batch_loss: 0.11772102117538452\n",
      "training: 13 batch 14 batch_loss: 0.12214997410774231\n",
      "training: 13 batch 15 batch_loss: 0.12325537204742432\n",
      "training: 13 batch 16 batch_loss: 0.11741268634796143\n",
      "training: 13 batch 17 batch_loss: 0.12142497301101685\n",
      "training: 13 batch 18 batch_loss: 0.11735910177230835\n",
      "training: 13 batch 19 batch_loss: 0.12423360347747803\n",
      "training: 13 batch 20 batch_loss: 0.12014889717102051\n",
      "training: 13 batch 21 batch_loss: 0.11946558952331543\n",
      "training: 13 batch 22 batch_loss: 0.1240682601928711\n",
      "training: 13 batch 23 batch_loss: 0.1199396550655365\n",
      "training: 13 batch 24 batch_loss: 0.12264609336853027\n",
      "training: 13 batch 25 batch_loss: 0.12230649590492249\n",
      "training: 13 batch 26 batch_loss: 0.12099748849868774\n",
      "training: 13 batch 27 batch_loss: 0.12071287631988525\n",
      "training: 13 batch 28 batch_loss: 0.11744382977485657\n",
      "training: 13 batch 29 batch_loss: 0.12113165855407715\n",
      "training: 13 batch 30 batch_loss: 0.12013980746269226\n",
      "training: 13 batch 31 batch_loss: 0.12196683883666992\n",
      "training: 13 batch 32 batch_loss: 0.12073272466659546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 13 batch 33 batch_loss: 0.119932621717453\n",
      "training: 13 batch 34 batch_loss: 0.12128269672393799\n",
      "training: 13 batch 35 batch_loss: 0.11805561184883118\n",
      "training: 13 batch 36 batch_loss: 0.12134185433387756\n",
      "training: 13 batch 37 batch_loss: 0.12121766805648804\n",
      "training: 13 batch 38 batch_loss: 0.12240225076675415\n",
      "training: 13 batch 39 batch_loss: 0.12111854553222656\n",
      "training: 13 batch 40 batch_loss: 0.11964151263237\n",
      "training: 13 batch 41 batch_loss: 0.12042105197906494\n",
      "training: 13 batch 42 batch_loss: 0.12339207530021667\n",
      "training: 13 batch 43 batch_loss: 0.11997857689857483\n",
      "training: 13 batch 44 batch_loss: 0.11979871988296509\n",
      "training: 13 batch 45 batch_loss: 0.12082874774932861\n",
      "training: 13 batch 46 batch_loss: 0.11770087480545044\n",
      "training: 13 batch 47 batch_loss: 0.12315288186073303\n",
      "training: 13 batch 48 batch_loss: 0.11891540884971619\n",
      "training: 13 batch 49 batch_loss: 0.11741992831230164\n",
      "training: 13 batch 50 batch_loss: 0.11902490258216858\n",
      "training: 13 batch 51 batch_loss: 0.11876979470252991\n",
      "training: 13 batch 52 batch_loss: 0.12186014652252197\n",
      "training: 13 batch 53 batch_loss: 0.12222650647163391\n",
      "training: 13 batch 54 batch_loss: 0.1218523383140564\n",
      "training: 13 batch 55 batch_loss: 0.11809796094894409\n",
      "training: 13 batch 56 batch_loss: 0.12117519974708557\n",
      "training: 13 batch 57 batch_loss: 0.12232112884521484\n",
      "training: 13 batch 58 batch_loss: 0.1219714879989624\n",
      "training: 13 batch 59 batch_loss: 0.12053751945495605\n",
      "training: 13 batch 60 batch_loss: 0.12259075045585632\n",
      "training: 13 batch 61 batch_loss: 0.12419626116752625\n",
      "training: 13 batch 62 batch_loss: 0.12113910913467407\n",
      "training: 13 batch 63 batch_loss: 0.12339192628860474\n",
      "training: 13 batch 64 batch_loss: 0.12416130304336548\n",
      "training: 13 batch 65 batch_loss: 0.12112671136856079\n",
      "training: 13 batch 66 batch_loss: 0.11798349022865295\n",
      "training: 13 batch 67 batch_loss: 0.12135279178619385\n",
      "training: 13 batch 68 batch_loss: 0.12417688965797424\n",
      "training: 13 batch 69 batch_loss: 0.12132522463798523\n",
      "training: 13 batch 70 batch_loss: 0.12195628881454468\n",
      "training: 13 batch 71 batch_loss: 0.1189022958278656\n",
      "training: 13 batch 72 batch_loss: 0.12055948376655579\n",
      "training: 13 batch 73 batch_loss: 0.12163937091827393\n",
      "training: 13 batch 74 batch_loss: 0.12570038437843323\n",
      "training: 13 batch 75 batch_loss: 0.11846223473548889\n",
      "training: 13 batch 76 batch_loss: 0.12115070223808289\n",
      "training: 13 batch 77 batch_loss: 0.11868500709533691\n",
      "training: 13 batch 78 batch_loss: 0.12140563130378723\n",
      "training: 13 batch 79 batch_loss: 0.1250266432762146\n",
      "training: 13 batch 80 batch_loss: 0.12228462100028992\n",
      "training: 13 batch 81 batch_loss: 0.12362584471702576\n",
      "training: 13 batch 82 batch_loss: 0.1196245551109314\n",
      "training: 13 batch 83 batch_loss: 0.11977049708366394\n",
      "training: 13 batch 84 batch_loss: 0.1220589280128479\n",
      "training: 13 batch 85 batch_loss: 0.1233372688293457\n",
      "training: 13 batch 86 batch_loss: 0.12045988440513611\n",
      "training: 13 batch 87 batch_loss: 0.12184217572212219\n",
      "training: 13 batch 88 batch_loss: 0.12217706441879272\n",
      "training: 13 batch 89 batch_loss: 0.12440907955169678\n",
      "training: 13 batch 90 batch_loss: 0.12328118085861206\n",
      "training: 13 batch 91 batch_loss: 0.1186271607875824\n",
      "training: 13 batch 92 batch_loss: 0.11966380476951599\n",
      "training: 13 batch 93 batch_loss: 0.11910289525985718\n",
      "training: 13 batch 94 batch_loss: 0.12293195724487305\n",
      "training: 13 batch 95 batch_loss: 0.12263184785842896\n",
      "training: 13 batch 96 batch_loss: 0.12140846252441406\n",
      "training: 13 batch 97 batch_loss: 0.1212872862815857\n",
      "training: 13 batch 98 batch_loss: 0.1230090856552124\n",
      "training: 13 batch 99 batch_loss: 0.12036570906639099\n",
      "training: 13 batch 100 batch_loss: 0.12054204940795898\n",
      "training: 13 batch 101 batch_loss: 0.12415879964828491\n",
      "training: 13 batch 102 batch_loss: 0.12122657895088196\n",
      "training: 13 batch 103 batch_loss: 0.12128090858459473\n",
      "training: 13 batch 104 batch_loss: 0.12013751268386841\n",
      "training: 13 batch 105 batch_loss: 0.1224871277809143\n",
      "training: 13 batch 106 batch_loss: 0.12168464064598083\n",
      "training: 13 batch 107 batch_loss: 0.12346771359443665\n",
      "training: 13 batch 108 batch_loss: 0.11983779072761536\n",
      "training: 13 batch 109 batch_loss: 0.12130221724510193\n",
      "training: 13 batch 110 batch_loss: 0.12111049890518188\n",
      "training: 13 batch 111 batch_loss: 0.12174370884895325\n",
      "training: 13 batch 112 batch_loss: 0.12121972441673279\n",
      "training: 13 batch 113 batch_loss: 0.12205946445465088\n",
      "training: 13 batch 114 batch_loss: 0.12355229258537292\n",
      "training: 13 batch 115 batch_loss: 0.12108823657035828\n",
      "training: 13 batch 116 batch_loss: 0.11776253581047058\n",
      "training: 13 batch 117 batch_loss: 0.12074950337409973\n",
      "training: 13 batch 118 batch_loss: 0.12293609976768494\n",
      "training: 13 batch 119 batch_loss: 0.1203126311302185\n",
      "training: 13 batch 120 batch_loss: 0.12078675627708435\n",
      "training: 13 batch 121 batch_loss: 0.12147599458694458\n",
      "training: 13 batch 122 batch_loss: 0.12158209085464478\n",
      "training: 13 batch 123 batch_loss: 0.12251865863800049\n",
      "training: 13 batch 124 batch_loss: 0.12242051959037781\n",
      "training: 13 batch 125 batch_loss: 0.12496304512023926\n",
      "training: 13 batch 126 batch_loss: 0.1222282350063324\n",
      "training: 13 batch 127 batch_loss: 0.12024039030075073\n",
      "training: 13 batch 128 batch_loss: 0.11881130933761597\n",
      "training: 13 batch 129 batch_loss: 0.12114500999450684\n",
      "training: 13 batch 130 batch_loss: 0.12444829940795898\n",
      "training: 13 batch 131 batch_loss: 0.12301298975944519\n",
      "training: 13 batch 132 batch_loss: 0.11865556240081787\n",
      "training: 13 batch 133 batch_loss: 0.11873012781143188\n",
      "training: 13 batch 134 batch_loss: 0.11925011873245239\n",
      "training: 13 batch 135 batch_loss: 0.12276577949523926\n",
      "training: 13 batch 136 batch_loss: 0.12241235375404358\n",
      "training: 13 batch 137 batch_loss: 0.12006533145904541\n",
      "training: 13 batch 138 batch_loss: 0.11930352449417114\n",
      "training: 13 batch 139 batch_loss: 0.12034878134727478\n",
      "training: 13 batch 140 batch_loss: 0.12382906675338745\n",
      "training: 13 batch 141 batch_loss: 0.12295106053352356\n",
      "training: 13 batch 142 batch_loss: 0.12194409966468811\n",
      "training: 13 batch 143 batch_loss: 0.12266197800636292\n",
      "training: 13 batch 144 batch_loss: 0.1227942407131195\n",
      "training: 13 batch 145 batch_loss: 0.12325358390808105\n",
      "training: 13 batch 146 batch_loss: 0.12518930435180664\n",
      "training: 13 batch 147 batch_loss: 0.12598955631256104\n",
      "training: 13 batch 148 batch_loss: 0.1232631504535675\n",
      "training: 13 batch 149 batch_loss: 0.12233492732048035\n",
      "training: 13 batch 150 batch_loss: 0.12217363715171814\n",
      "training: 13 batch 151 batch_loss: 0.12236768007278442\n",
      "training: 13 batch 152 batch_loss: 0.1240517795085907\n",
      "training: 13 batch 153 batch_loss: 0.1229221522808075\n",
      "training: 13 batch 154 batch_loss: 0.12321022152900696\n",
      "training: 13 batch 155 batch_loss: 0.12262439727783203\n",
      "training: 13 batch 156 batch_loss: 0.12104427814483643\n",
      "training: 13 batch 157 batch_loss: 0.121744304895401\n",
      "training: 13 batch 158 batch_loss: 0.11961883306503296\n",
      "training: 13 batch 159 batch_loss: 0.12422046065330505\n",
      "training: 13 batch 160 batch_loss: 0.12024682760238647\n",
      "training: 13 batch 161 batch_loss: 0.12398788332939148\n",
      "training: 13 batch 162 batch_loss: 0.1247326135635376\n",
      "training: 13 batch 163 batch_loss: 0.12230914831161499\n",
      "training: 13 batch 164 batch_loss: 0.12205246090888977\n",
      "training: 13 batch 165 batch_loss: 0.12209513783454895\n",
      "training: 13 batch 166 batch_loss: 0.12136828899383545\n",
      "training: 13 batch 167 batch_loss: 0.12345907092094421\n",
      "training: 13 batch 168 batch_loss: 0.11869475245475769\n",
      "training: 13 batch 169 batch_loss: 0.12156754732131958\n",
      "training: 13 batch 170 batch_loss: 0.12508511543273926\n",
      "training: 13 batch 171 batch_loss: 0.12125217914581299\n",
      "training: 13 batch 172 batch_loss: 0.12303140759468079\n",
      "training: 13 batch 173 batch_loss: 0.12402456998825073\n",
      "training: 13 batch 174 batch_loss: 0.123757004737854\n",
      "training: 13 batch 175 batch_loss: 0.12411743402481079\n",
      "training: 13 batch 176 batch_loss: 0.12167418003082275\n",
      "training: 13 batch 177 batch_loss: 0.12187448143959045\n",
      "training: 13 batch 178 batch_loss: 0.12037613987922668\n",
      "training: 13 batch 179 batch_loss: 0.12286403775215149\n",
      "training: 13 batch 180 batch_loss: 0.12105998396873474\n",
      "training: 13 batch 181 batch_loss: 0.12445634603500366\n",
      "training: 13 batch 182 batch_loss: 0.12338680028915405\n",
      "training: 13 batch 183 batch_loss: 0.1203334629535675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 13 batch 184 batch_loss: 0.1231609582901001\n",
      "training: 13 batch 185 batch_loss: 0.1240377426147461\n",
      "training: 13 batch 186 batch_loss: 0.12448933720588684\n",
      "training: 13 batch 187 batch_loss: 0.12132805585861206\n",
      "training: 13 batch 188 batch_loss: 0.12172549962997437\n",
      "training: 13 batch 189 batch_loss: 0.12099453806877136\n",
      "training: 13 batch 190 batch_loss: 0.12262183427810669\n",
      "training: 13 batch 191 batch_loss: 0.12337452173233032\n",
      "training: 13 batch 192 batch_loss: 0.11872583627700806\n",
      "training: 13 batch 193 batch_loss: 0.12139236927032471\n",
      "training: 13 batch 194 batch_loss: 0.1247914731502533\n",
      "training: 13 batch 195 batch_loss: 0.12323057651519775\n",
      "training: 13 batch 196 batch_loss: 0.12109026312828064\n",
      "training: 13 batch 197 batch_loss: 0.12427103519439697\n",
      "training: 13 batch 198 batch_loss: 0.1211388111114502\n",
      "training: 13 batch 199 batch_loss: 0.12204456329345703\n",
      "training: 13 batch 200 batch_loss: 0.12359392642974854\n",
      "training: 13 batch 201 batch_loss: 0.12387645244598389\n",
      "training: 13 batch 202 batch_loss: 0.12478649616241455\n",
      "training: 13 batch 203 batch_loss: 0.12318152189254761\n",
      "training: 13 batch 204 batch_loss: 0.12168282270431519\n",
      "training: 13 batch 205 batch_loss: 0.12288296222686768\n",
      "training: 13 batch 206 batch_loss: 0.12376740574836731\n",
      "training: 13 batch 207 batch_loss: 0.12141317129135132\n",
      "training: 13 batch 208 batch_loss: 0.1232709288597107\n",
      "training: 13 batch 209 batch_loss: 0.12485134601593018\n",
      "training: 13 batch 210 batch_loss: 0.12173536419868469\n",
      "training: 13 batch 211 batch_loss: 0.12038138508796692\n",
      "training: 13 batch 212 batch_loss: 0.12100011110305786\n",
      "training: 13 batch 213 batch_loss: 0.12015622854232788\n",
      "training: 13 batch 214 batch_loss: 0.12333950400352478\n",
      "training: 13 batch 215 batch_loss: 0.12380272150039673\n",
      "training: 13 batch 216 batch_loss: 0.12180554866790771\n",
      "training: 13 batch 217 batch_loss: 0.12497127056121826\n",
      "training: 13 batch 218 batch_loss: 0.1225118637084961\n",
      "training: 13 batch 219 batch_loss: 0.1231515109539032\n",
      "training: 13 batch 220 batch_loss: 0.12220779061317444\n",
      "training: 13 batch 221 batch_loss: 0.1238631010055542\n",
      "training: 13 batch 222 batch_loss: 0.1235170066356659\n",
      "training: 13 batch 223 batch_loss: 0.12074410915374756\n",
      "training: 13 batch 224 batch_loss: 0.12225082516670227\n",
      "training: 13 batch 225 batch_loss: 0.12412646412849426\n",
      "training: 13 batch 226 batch_loss: 0.12076893448829651\n",
      "training: 13 batch 227 batch_loss: 0.12370365858078003\n",
      "training: 13 batch 228 batch_loss: 0.12547579407691956\n",
      "training: 13 batch 229 batch_loss: 0.1234862208366394\n",
      "training: 13 batch 230 batch_loss: 0.1228581964969635\n",
      "training: 13 batch 231 batch_loss: 0.1224743127822876\n",
      "training: 13 batch 232 batch_loss: 0.12359726428985596\n",
      "training: 13 batch 233 batch_loss: 0.12471640110015869\n",
      "training: 13 batch 234 batch_loss: 0.12315219640731812\n",
      "training: 13 batch 235 batch_loss: 0.12195256352424622\n",
      "training: 13 batch 236 batch_loss: 0.12337574362754822\n",
      "training: 13 batch 237 batch_loss: 0.12170985341072083\n",
      "training: 13 batch 238 batch_loss: 0.12304717302322388\n",
      "training: 13 batch 239 batch_loss: 0.12391495704650879\n",
      "training: 13 batch 240 batch_loss: 0.12133806943893433\n",
      "training: 13 batch 241 batch_loss: 0.12427753210067749\n",
      "training: 13 batch 242 batch_loss: 0.12503769993782043\n",
      "training: 13 batch 243 batch_loss: 0.12190550565719604\n",
      "training: 13 batch 244 batch_loss: 0.12075448036193848\n",
      "training: 13 batch 245 batch_loss: 0.12410938739776611\n",
      "training: 13 batch 246 batch_loss: 0.12176573276519775\n",
      "training: 13 batch 247 batch_loss: 0.1255505383014679\n",
      "training: 13 batch 248 batch_loss: 0.12377762794494629\n",
      "training: 13 batch 249 batch_loss: 0.12098440527915955\n",
      "training: 13 batch 250 batch_loss: 0.12172698974609375\n",
      "training: 13 batch 251 batch_loss: 0.12216135859489441\n",
      "training: 13 batch 252 batch_loss: 0.12369358539581299\n",
      "training: 13 batch 253 batch_loss: 0.12228655815124512\n",
      "training: 13 batch 254 batch_loss: 0.12346124649047852\n",
      "training: 13 batch 255 batch_loss: 0.12421810626983643\n",
      "training: 13 batch 256 batch_loss: 0.12178364396095276\n",
      "training: 13 batch 257 batch_loss: 0.12769079208374023\n",
      "training: 13 batch 258 batch_loss: 0.12478935718536377\n",
      "training: 13 batch 259 batch_loss: 0.1238829493522644\n",
      "training: 13 batch 260 batch_loss: 0.12213245034217834\n",
      "training: 13 batch 261 batch_loss: 0.12165528535842896\n",
      "training: 13 batch 262 batch_loss: 0.1230747401714325\n",
      "training: 13 batch 263 batch_loss: 0.12297844886779785\n",
      "training: 13 batch 264 batch_loss: 0.12573754787445068\n",
      "training: 13 batch 265 batch_loss: 0.12240386009216309\n",
      "training: 13 batch 266 batch_loss: 0.12473317980766296\n",
      "training: 13 batch 267 batch_loss: 0.12367609143257141\n",
      "training: 13 batch 268 batch_loss: 0.123058021068573\n",
      "training: 13 batch 269 batch_loss: 0.12393397092819214\n",
      "training: 13 batch 270 batch_loss: 0.12181156873703003\n",
      "training: 13 batch 271 batch_loss: 0.12722724676132202\n",
      "training: 13 batch 272 batch_loss: 0.12429577112197876\n",
      "training: 13 batch 273 batch_loss: 0.12399420142173767\n",
      "training: 13 batch 274 batch_loss: 0.12223044037818909\n",
      "training: 13 batch 275 batch_loss: 0.12029430270195007\n",
      "training: 13 batch 276 batch_loss: 0.12378120422363281\n",
      "training: 13 batch 277 batch_loss: 0.12137073278427124\n",
      "training: 13 batch 278 batch_loss: 0.1231408417224884\n",
      "training: 13 batch 279 batch_loss: 0.1202613115310669\n",
      "training: 13 batch 280 batch_loss: 0.12116062641143799\n",
      "training: 13 batch 281 batch_loss: 0.12364205718040466\n",
      "training: 13 batch 282 batch_loss: 0.12375098466873169\n",
      "training: 13 batch 283 batch_loss: 0.12262547016143799\n",
      "training: 13 batch 284 batch_loss: 0.12384119629859924\n",
      "training: 13 batch 285 batch_loss: 0.12200912833213806\n",
      "training: 13 batch 286 batch_loss: 0.12348708510398865\n",
      "training: 13 batch 287 batch_loss: 0.12382900714874268\n",
      "training: 13 batch 288 batch_loss: 0.12386414408683777\n",
      "training: 13 batch 289 batch_loss: 0.11982288956642151\n",
      "training: 13 batch 290 batch_loss: 0.12403392791748047\n",
      "training: 13 batch 291 batch_loss: 0.11964097619056702\n",
      "training: 13 batch 292 batch_loss: 0.12128973007202148\n",
      "training: 13 batch 293 batch_loss: 0.126003235578537\n",
      "training: 13 batch 294 batch_loss: 0.12203890085220337\n",
      "training: 13 batch 295 batch_loss: 0.12294867634773254\n",
      "training: 13 batch 296 batch_loss: 0.12699800729751587\n",
      "training: 13 batch 297 batch_loss: 0.11886489391326904\n",
      "training: 13 batch 298 batch_loss: 0.12462544441223145\n",
      "training: 13 batch 299 batch_loss: 0.12250751256942749\n",
      "training: 13 batch 300 batch_loss: 0.12025564908981323\n",
      "training: 13 batch 301 batch_loss: 0.12098044157028198\n",
      "training: 13 batch 302 batch_loss: 0.12488287687301636\n",
      "training: 13 batch 303 batch_loss: 0.12186777591705322\n",
      "training: 13 batch 304 batch_loss: 0.12351444363594055\n",
      "training: 13 batch 305 batch_loss: 0.1253819465637207\n",
      "training: 13 batch 306 batch_loss: 0.1221265196800232\n",
      "training: 13 batch 307 batch_loss: 0.12559765577316284\n",
      "training: 13 batch 308 batch_loss: 0.12246090173721313\n",
      "training: 13 batch 309 batch_loss: 0.12346798181533813\n",
      "training: 13 batch 310 batch_loss: 0.1208900511264801\n",
      "training: 13 batch 311 batch_loss: 0.12620636820793152\n",
      "training: 13 batch 312 batch_loss: 0.12527811527252197\n",
      "training: 13 batch 313 batch_loss: 0.1233864426612854\n",
      "training: 13 batch 314 batch_loss: 0.12618061900138855\n",
      "training: 13 batch 315 batch_loss: 0.12391769886016846\n",
      "training: 13 batch 316 batch_loss: 0.1240643858909607\n",
      "training: 13 batch 317 batch_loss: 0.12405288219451904\n",
      "training: 13 batch 318 batch_loss: 0.12284508347511292\n",
      "training: 13 batch 319 batch_loss: 0.12159961462020874\n",
      "training: 13 batch 320 batch_loss: 0.12495869398117065\n",
      "training: 13 batch 321 batch_loss: 0.1257023811340332\n",
      "training: 13 batch 322 batch_loss: 0.12358850240707397\n",
      "training: 13 batch 323 batch_loss: 0.12354481220245361\n",
      "training: 13 batch 324 batch_loss: 0.12391650676727295\n",
      "training: 13 batch 325 batch_loss: 0.12175163626670837\n",
      "training: 13 batch 326 batch_loss: 0.12216648459434509\n",
      "training: 13 batch 327 batch_loss: 0.1235211193561554\n",
      "training: 13 batch 328 batch_loss: 0.1230018138885498\n",
      "training: 13 batch 329 batch_loss: 0.12209707498550415\n",
      "training: 13 batch 330 batch_loss: 0.12510767579078674\n",
      "training: 13 batch 331 batch_loss: 0.12078344821929932\n",
      "training: 13 batch 332 batch_loss: 0.12100636959075928\n",
      "training: 13 batch 333 batch_loss: 0.1257624328136444\n",
      "training: 13 batch 334 batch_loss: 0.12135270237922668\n",
      "training: 13 batch 335 batch_loss: 0.1226969063282013\n",
      "training: 13 batch 336 batch_loss: 0.12468564510345459\n",
      "training: 13 batch 337 batch_loss: 0.12392804026603699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 13 batch 338 batch_loss: 0.12105640769004822\n",
      "training: 13 batch 339 batch_loss: 0.12468567490577698\n",
      "training: 13 batch 340 batch_loss: 0.12215769290924072\n",
      "training: 13 batch 341 batch_loss: 0.1228259801864624\n",
      "training: 13 batch 342 batch_loss: 0.12521860003471375\n",
      "training: 13 batch 343 batch_loss: 0.1265212595462799\n",
      "training: 13 batch 344 batch_loss: 0.12318438291549683\n",
      "training: 13 batch 345 batch_loss: 0.12360268831253052\n",
      "training: 13 batch 346 batch_loss: 0.12581193447113037\n",
      "training: 13 batch 347 batch_loss: 0.12050285935401917\n",
      "training: 13 batch 348 batch_loss: 0.12220442295074463\n",
      "training: 13 batch 349 batch_loss: 0.1257408857345581\n",
      "training: 13 batch 350 batch_loss: 0.12202146649360657\n",
      "training: 13 batch 351 batch_loss: 0.1256619691848755\n",
      "training: 13 batch 352 batch_loss: 0.122275710105896\n",
      "training: 13 batch 353 batch_loss: 0.12598422169685364\n",
      "training: 13 batch 354 batch_loss: 0.1250256597995758\n",
      "training: 13 batch 355 batch_loss: 0.12033525109291077\n",
      "training: 13 batch 356 batch_loss: 0.12541034817695618\n",
      "training: 13 batch 357 batch_loss: 0.1258803904056549\n",
      "training: 13 batch 358 batch_loss: 0.12229487299919128\n",
      "training: 13 batch 359 batch_loss: 0.1253454089164734\n",
      "training: 13 batch 360 batch_loss: 0.12121778726577759\n",
      "training: 13 batch 361 batch_loss: 0.12368005514144897\n",
      "training: 13 batch 362 batch_loss: 0.12491631507873535\n",
      "training: 13 batch 363 batch_loss: 0.12191715836524963\n",
      "training: 13 batch 364 batch_loss: 0.12217459082603455\n",
      "training: 13 batch 365 batch_loss: 0.12394681572914124\n",
      "training: 13 batch 366 batch_loss: 0.12398329377174377\n",
      "training: 13 batch 367 batch_loss: 0.12190166115760803\n",
      "training: 13 batch 368 batch_loss: 0.12523570656776428\n",
      "training: 13 batch 369 batch_loss: 0.12460577487945557\n",
      "training: 13 batch 370 batch_loss: 0.12517690658569336\n",
      "training: 13 batch 371 batch_loss: 0.12147465348243713\n",
      "training: 13 batch 372 batch_loss: 0.12784495949745178\n",
      "training: 13 batch 373 batch_loss: 0.12251400947570801\n",
      "training: 13 batch 374 batch_loss: 0.1238231360912323\n",
      "training: 13 batch 375 batch_loss: 0.12440016865730286\n",
      "training: 13 batch 376 batch_loss: 0.12055355310440063\n",
      "training: 13 batch 377 batch_loss: 0.1231502890586853\n",
      "training: 13 batch 378 batch_loss: 0.12403008341789246\n",
      "training: 13 batch 379 batch_loss: 0.12515822052955627\n",
      "training: 13 batch 380 batch_loss: 0.12558647990226746\n",
      "training: 13 batch 381 batch_loss: 0.12629812955856323\n",
      "training: 13 batch 382 batch_loss: 0.12257274985313416\n",
      "training: 13 batch 383 batch_loss: 0.12457254528999329\n",
      "training: 13 batch 384 batch_loss: 0.12440875172615051\n",
      "training: 13 batch 385 batch_loss: 0.12246590852737427\n",
      "training: 13 batch 386 batch_loss: 0.12355789542198181\n",
      "training: 13 batch 387 batch_loss: 0.12223660945892334\n",
      "training: 13 batch 388 batch_loss: 0.1238815188407898\n",
      "training: 13 batch 389 batch_loss: 0.1236221194267273\n",
      "training: 13 batch 390 batch_loss: 0.12336888909339905\n",
      "training: 13 batch 391 batch_loss: 0.12278646230697632\n",
      "training: 13 batch 392 batch_loss: 0.12227952480316162\n",
      "training: 13 batch 393 batch_loss: 0.12638115882873535\n",
      "training: 13 batch 394 batch_loss: 0.12394198775291443\n",
      "training: 13 batch 395 batch_loss: 0.12504324316978455\n",
      "training: 13 batch 396 batch_loss: 0.1218360960483551\n",
      "training: 13 batch 397 batch_loss: 0.12193861603736877\n",
      "training: 13 batch 398 batch_loss: 0.12522351741790771\n",
      "training: 13 batch 399 batch_loss: 0.1225523054599762\n",
      "training: 13 batch 400 batch_loss: 0.12354737520217896\n",
      "training: 13 batch 401 batch_loss: 0.12300950288772583\n",
      "training: 13 batch 402 batch_loss: 0.12677043676376343\n",
      "training: 13 batch 403 batch_loss: 0.12443822622299194\n",
      "training: 13 batch 404 batch_loss: 0.12412452697753906\n",
      "training: 13 batch 405 batch_loss: 0.12548166513442993\n",
      "training: 13 batch 406 batch_loss: 0.12451609969139099\n",
      "training: 13 batch 407 batch_loss: 0.12359404563903809\n",
      "training: 13 batch 408 batch_loss: 0.12177956104278564\n",
      "training: 13 batch 409 batch_loss: 0.12527155876159668\n",
      "training: 13 batch 410 batch_loss: 0.12204304337501526\n",
      "training: 13 batch 411 batch_loss: 0.12593019008636475\n",
      "training: 13 batch 412 batch_loss: 0.12607640027999878\n",
      "training: 13 batch 413 batch_loss: 0.12344583868980408\n",
      "training: 13 batch 414 batch_loss: 0.1272885501384735\n",
      "training: 13 batch 415 batch_loss: 0.12492328882217407\n",
      "training: 13 batch 416 batch_loss: 0.12619692087173462\n",
      "training: 13 batch 417 batch_loss: 0.12542062997817993\n",
      "training: 13 batch 418 batch_loss: 0.12429609894752502\n",
      "training: 13 batch 419 batch_loss: 0.12310606241226196\n",
      "training: 13 batch 420 batch_loss: 0.12704190611839294\n",
      "training: 13 batch 421 batch_loss: 0.12121346592903137\n",
      "training: 13 batch 422 batch_loss: 0.12378820776939392\n",
      "training: 13 batch 423 batch_loss: 0.12488451600074768\n",
      "training: 13 batch 424 batch_loss: 0.12192302942276001\n",
      "training: 13 batch 425 batch_loss: 0.12022820115089417\n",
      "training: 13 batch 426 batch_loss: 0.12791410088539124\n",
      "training: 13 batch 427 batch_loss: 0.12232062220573425\n",
      "training: 13 batch 428 batch_loss: 0.12447965145111084\n",
      "training: 13 batch 429 batch_loss: 0.12781041860580444\n",
      "training: 13 batch 430 batch_loss: 0.12255460023880005\n",
      "training: 13 batch 431 batch_loss: 0.12260907888412476\n",
      "training: 13 batch 432 batch_loss: 0.12296450138092041\n",
      "training: 13 batch 433 batch_loss: 0.1257995367050171\n",
      "training: 13 batch 434 batch_loss: 0.1244983971118927\n",
      "training: 13 batch 435 batch_loss: 0.12285533547401428\n",
      "training: 13 batch 436 batch_loss: 0.1258814036846161\n",
      "training: 13 batch 437 batch_loss: 0.1251744031906128\n",
      "training: 13 batch 438 batch_loss: 0.12456458806991577\n",
      "training: 13 batch 439 batch_loss: 0.12608155608177185\n",
      "training: 13 batch 440 batch_loss: 0.12561872601509094\n",
      "training: 13 batch 441 batch_loss: 0.12322577834129333\n",
      "training: 13 batch 442 batch_loss: 0.12154394388198853\n",
      "training: 13 batch 443 batch_loss: 0.12238171696662903\n",
      "training: 13 batch 444 batch_loss: 0.12419116497039795\n",
      "training: 13 batch 445 batch_loss: 0.12377697229385376\n",
      "training: 13 batch 446 batch_loss: 0.125607430934906\n",
      "training: 13 batch 447 batch_loss: 0.12355902791023254\n",
      "training: 13 batch 448 batch_loss: 0.12470516562461853\n",
      "training: 13 batch 449 batch_loss: 0.12583407759666443\n",
      "training: 13 batch 450 batch_loss: 0.12030982971191406\n",
      "training: 13 batch 451 batch_loss: 0.12639188766479492\n",
      "training: 13 batch 452 batch_loss: 0.12346974015235901\n",
      "training: 13 batch 453 batch_loss: 0.12664556503295898\n",
      "training: 13 batch 454 batch_loss: 0.12341523170471191\n",
      "training: 13 batch 455 batch_loss: 0.1250484585762024\n",
      "training: 13 batch 456 batch_loss: 0.12602102756500244\n",
      "training: 13 batch 457 batch_loss: 0.12290188670158386\n",
      "training: 13 batch 458 batch_loss: 0.12327653169631958\n",
      "training: 13 batch 459 batch_loss: 0.12542816996574402\n",
      "training: 13 batch 460 batch_loss: 0.13038945198059082\n",
      "training: 13 batch 461 batch_loss: 0.12414783239364624\n",
      "training: 13 batch 462 batch_loss: 0.12329894304275513\n",
      "training: 13 batch 463 batch_loss: 0.12377595901489258\n",
      "training: 13 batch 464 batch_loss: 0.12391194701194763\n",
      "training: 13 batch 465 batch_loss: 0.12129402160644531\n",
      "training: 13 batch 466 batch_loss: 0.12785854935646057\n",
      "training: 13 batch 467 batch_loss: 0.1277330219745636\n",
      "training: 13 batch 468 batch_loss: 0.1254882514476776\n",
      "training: 13 batch 469 batch_loss: 0.1268150806427002\n",
      "training: 13 batch 470 batch_loss: 0.12507539987564087\n",
      "training: 13 batch 471 batch_loss: 0.12418225407600403\n",
      "training: 13 batch 472 batch_loss: 0.12718814611434937\n",
      "training: 13 batch 473 batch_loss: 0.12419292330741882\n",
      "training: 13 batch 474 batch_loss: 0.12448114156723022\n",
      "training: 13 batch 475 batch_loss: 0.11863777041435242\n",
      "training: 13 batch 476 batch_loss: 0.12465411424636841\n",
      "training: 13 batch 477 batch_loss: 0.12438106536865234\n",
      "training: 13 batch 478 batch_loss: 0.12407609820365906\n",
      "training: 13 batch 479 batch_loss: 0.12723377346992493\n",
      "training: 13 batch 480 batch_loss: 0.12447258830070496\n",
      "training: 13 batch 481 batch_loss: 0.12266018986701965\n",
      "training: 13 batch 482 batch_loss: 0.12627175450325012\n",
      "training: 13 batch 483 batch_loss: 0.12635669112205505\n",
      "training: 13 batch 484 batch_loss: 0.12286427617073059\n",
      "training: 13 batch 485 batch_loss: 0.12548613548278809\n",
      "training: 13 batch 486 batch_loss: 0.12146127223968506\n",
      "training: 13 batch 487 batch_loss: 0.12618905305862427\n",
      "training: 13 batch 488 batch_loss: 0.12398496270179749\n",
      "training: 13 batch 489 batch_loss: 0.12705793976783752\n",
      "training: 13 batch 490 batch_loss: 0.12191912531852722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 13 batch 491 batch_loss: 0.12382382154464722\n",
      "training: 13 batch 492 batch_loss: 0.12501144409179688\n",
      "training: 13 batch 493 batch_loss: 0.12415587902069092\n",
      "training: 13 batch 494 batch_loss: 0.12350410223007202\n",
      "training: 13 batch 495 batch_loss: 0.12269115447998047\n",
      "training: 13 batch 496 batch_loss: 0.12390351295471191\n",
      "training: 13 batch 497 batch_loss: 0.12153398990631104\n",
      "training: 13 batch 498 batch_loss: 0.12263387441635132\n",
      "training: 13 batch 499 batch_loss: 0.12368559837341309\n",
      "training: 13 batch 500 batch_loss: 0.12777695059776306\n",
      "training: 13 batch 501 batch_loss: 0.1218641996383667\n",
      "training: 13 batch 502 batch_loss: 0.12379655241966248\n",
      "training: 13 batch 503 batch_loss: 0.1232072114944458\n",
      "training: 13 batch 504 batch_loss: 0.1219501793384552\n",
      "training: 13 batch 505 batch_loss: 0.1260598599910736\n",
      "training: 13 batch 506 batch_loss: 0.12409219145774841\n",
      "training: 13 batch 507 batch_loss: 0.12557458877563477\n",
      "training: 13 batch 508 batch_loss: 0.1232638955116272\n",
      "training: 13 batch 509 batch_loss: 0.125334233045578\n",
      "training: 13 batch 510 batch_loss: 0.1250196397304535\n",
      "training: 13 batch 511 batch_loss: 0.12353330850601196\n",
      "training: 13 batch 512 batch_loss: 0.12528350949287415\n",
      "training: 13 batch 513 batch_loss: 0.12377512454986572\n",
      "training: 13 batch 514 batch_loss: 0.12518316507339478\n",
      "training: 13 batch 515 batch_loss: 0.12305334210395813\n",
      "training: 13 batch 516 batch_loss: 0.12498694658279419\n",
      "training: 13 batch 517 batch_loss: 0.12544801831245422\n",
      "training: 13 batch 518 batch_loss: 0.12448358535766602\n",
      "training: 13 batch 519 batch_loss: 0.12495109438896179\n",
      "training: 13 batch 520 batch_loss: 0.12200546264648438\n",
      "training: 13 batch 521 batch_loss: 0.12621870636940002\n",
      "training: 13 batch 522 batch_loss: 0.1237846314907074\n",
      "training: 13 batch 523 batch_loss: 0.12445220351219177\n",
      "training: 13 batch 524 batch_loss: 0.12212243676185608\n",
      "training: 13 batch 525 batch_loss: 0.12258413434028625\n",
      "training: 13 batch 526 batch_loss: 0.12261509895324707\n",
      "training: 13 batch 527 batch_loss: 0.12648680806159973\n",
      "training: 13 batch 528 batch_loss: 0.1256048083305359\n",
      "training: 13 batch 529 batch_loss: 0.1257745921611786\n",
      "training: 13 batch 530 batch_loss: 0.12317129969596863\n",
      "training: 13 batch 531 batch_loss: 0.12527644634246826\n",
      "training: 13 batch 532 batch_loss: 0.12549874186515808\n",
      "training: 13 batch 533 batch_loss: 0.12489530444145203\n",
      "training: 13 batch 534 batch_loss: 0.12546831369400024\n",
      "training: 13 batch 535 batch_loss: 0.1218445897102356\n",
      "training: 13 batch 536 batch_loss: 0.12625595927238464\n",
      "training: 13 batch 537 batch_loss: 0.1288856565952301\n",
      "training: 13 batch 538 batch_loss: 0.12288394570350647\n",
      "training: 13 batch 539 batch_loss: 0.1266111433506012\n",
      "training: 13 batch 540 batch_loss: 0.12131214141845703\n",
      "training: 13 batch 541 batch_loss: 0.12477719783782959\n",
      "training: 13 batch 542 batch_loss: 0.12429699301719666\n",
      "training: 13 batch 543 batch_loss: 0.12305751442909241\n",
      "training: 13 batch 544 batch_loss: 0.12591570615768433\n",
      "training: 13 batch 545 batch_loss: 0.12510699033737183\n",
      "training: 13 batch 546 batch_loss: 0.12601995468139648\n",
      "training: 13 batch 547 batch_loss: 0.12705156207084656\n",
      "training: 13 batch 548 batch_loss: 0.12259811162948608\n",
      "training: 13 batch 549 batch_loss: 0.12540680170059204\n",
      "training: 13 batch 550 batch_loss: 0.1253451406955719\n",
      "training: 13 batch 551 batch_loss: 0.12569445371627808\n",
      "training: 13 batch 552 batch_loss: 0.12555265426635742\n",
      "training: 13 batch 553 batch_loss: 0.12529069185256958\n",
      "training: 13 batch 554 batch_loss: 0.1263432502746582\n",
      "training: 13 batch 555 batch_loss: 0.12170058488845825\n",
      "training: 13 batch 556 batch_loss: 0.12510401010513306\n",
      "training: 13 batch 557 batch_loss: 0.12483954429626465\n",
      "training: 13 batch 558 batch_loss: 0.12449139356613159\n",
      "training: 13 batch 559 batch_loss: 0.12497735023498535\n",
      "training: 13 batch 560 batch_loss: 0.12903422117233276\n",
      "training: 13 batch 561 batch_loss: 0.1257075071334839\n",
      "training: 13 batch 562 batch_loss: 0.12701940536499023\n",
      "training: 13 batch 563 batch_loss: 0.12708121538162231\n",
      "training: 13 batch 564 batch_loss: 0.12599492073059082\n",
      "training: 13 batch 565 batch_loss: 0.1210138201713562\n",
      "training: 13 batch 566 batch_loss: 0.12680616974830627\n",
      "training: 13 batch 567 batch_loss: 0.12215545773506165\n",
      "training: 13 batch 568 batch_loss: 0.12567293643951416\n",
      "training: 13 batch 569 batch_loss: 0.12463408708572388\n",
      "training: 13 batch 570 batch_loss: 0.12643203139305115\n",
      "training: 13 batch 571 batch_loss: 0.12414208054542542\n",
      "training: 13 batch 572 batch_loss: 0.1281471848487854\n",
      "training: 13 batch 573 batch_loss: 0.12646400928497314\n",
      "training: 13 batch 574 batch_loss: 0.12406295537948608\n",
      "training: 13 batch 575 batch_loss: 0.12324094772338867\n",
      "training: 13 batch 576 batch_loss: 0.12711870670318604\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 13, Hit Ratio:0.024317688953439952 | Precision:0.035879288312198956 | Recall:0.05069817292514133 | NDCG:0.04532079136911031\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 14 batch 0 batch_loss: 0.12387347221374512\n",
      "training: 14 batch 1 batch_loss: 0.12209773063659668\n",
      "training: 14 batch 2 batch_loss: 0.12340724468231201\n",
      "training: 14 batch 3 batch_loss: 0.12038993835449219\n",
      "training: 14 batch 4 batch_loss: 0.12311267852783203\n",
      "training: 14 batch 5 batch_loss: 0.12165603041648865\n",
      "training: 14 batch 6 batch_loss: 0.12390926480293274\n",
      "training: 14 batch 7 batch_loss: 0.12376981973648071\n",
      "training: 14 batch 8 batch_loss: 0.12285047769546509\n",
      "training: 14 batch 9 batch_loss: 0.12326624989509583\n",
      "training: 14 batch 10 batch_loss: 0.12598764896392822\n",
      "training: 14 batch 11 batch_loss: 0.12306207418441772\n",
      "training: 14 batch 12 batch_loss: 0.12275832891464233\n",
      "training: 14 batch 13 batch_loss: 0.12368792295455933\n",
      "training: 14 batch 14 batch_loss: 0.124073326587677\n",
      "training: 14 batch 15 batch_loss: 0.12084627151489258\n",
      "training: 14 batch 16 batch_loss: 0.12264671921730042\n",
      "training: 14 batch 17 batch_loss: 0.12151488661766052\n",
      "training: 14 batch 18 batch_loss: 0.12537235021591187\n",
      "training: 14 batch 19 batch_loss: 0.12051776051521301\n",
      "training: 14 batch 20 batch_loss: 0.12331810593605042\n",
      "training: 14 batch 21 batch_loss: 0.12426215410232544\n",
      "training: 14 batch 22 batch_loss: 0.12358281016349792\n",
      "training: 14 batch 23 batch_loss: 0.12443506717681885\n",
      "training: 14 batch 24 batch_loss: 0.12272530794143677\n",
      "training: 14 batch 25 batch_loss: 0.12376940250396729\n",
      "training: 14 batch 26 batch_loss: 0.12578481435775757\n",
      "training: 14 batch 27 batch_loss: 0.12433701753616333\n",
      "training: 14 batch 28 batch_loss: 0.1250060498714447\n",
      "training: 14 batch 29 batch_loss: 0.1223246157169342\n",
      "training: 14 batch 30 batch_loss: 0.12593397498130798\n",
      "training: 14 batch 31 batch_loss: 0.12343141436576843\n",
      "training: 14 batch 32 batch_loss: 0.12641802430152893\n",
      "training: 14 batch 33 batch_loss: 0.12294167280197144\n",
      "training: 14 batch 34 batch_loss: 0.12358736991882324\n",
      "training: 14 batch 35 batch_loss: 0.12506887316703796\n",
      "training: 14 batch 36 batch_loss: 0.12055540084838867\n",
      "training: 14 batch 37 batch_loss: 0.12675777077674866\n",
      "training: 14 batch 38 batch_loss: 0.1255492866039276\n",
      "training: 14 batch 39 batch_loss: 0.1197921633720398\n",
      "training: 14 batch 40 batch_loss: 0.12474161386489868\n",
      "training: 14 batch 41 batch_loss: 0.12488171458244324\n",
      "training: 14 batch 42 batch_loss: 0.12359088659286499\n",
      "training: 14 batch 43 batch_loss: 0.12240812182426453\n",
      "training: 14 batch 44 batch_loss: 0.12506043910980225\n",
      "training: 14 batch 45 batch_loss: 0.1240997314453125\n",
      "training: 14 batch 46 batch_loss: 0.12392163276672363\n",
      "training: 14 batch 47 batch_loss: 0.12381035089492798\n",
      "training: 14 batch 48 batch_loss: 0.12731832265853882\n",
      "training: 14 batch 49 batch_loss: 0.1250234842300415\n",
      "training: 14 batch 50 batch_loss: 0.12463220953941345\n",
      "training: 14 batch 51 batch_loss: 0.12735223770141602\n",
      "training: 14 batch 52 batch_loss: 0.12361395359039307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 14 batch 53 batch_loss: 0.12458354234695435\n",
      "training: 14 batch 54 batch_loss: 0.12269848585128784\n",
      "training: 14 batch 55 batch_loss: 0.12360700964927673\n",
      "training: 14 batch 56 batch_loss: 0.12664452195167542\n",
      "training: 14 batch 57 batch_loss: 0.12314110994338989\n",
      "training: 14 batch 58 batch_loss: 0.12454661726951599\n",
      "training: 14 batch 59 batch_loss: 0.12433278560638428\n",
      "training: 14 batch 60 batch_loss: 0.12364417314529419\n",
      "training: 14 batch 61 batch_loss: 0.1265765130519867\n",
      "training: 14 batch 62 batch_loss: 0.12125536799430847\n",
      "training: 14 batch 63 batch_loss: 0.12311697006225586\n",
      "training: 14 batch 64 batch_loss: 0.12556242942810059\n",
      "training: 14 batch 65 batch_loss: 0.12583470344543457\n",
      "training: 14 batch 66 batch_loss: 0.12459856271743774\n",
      "training: 14 batch 67 batch_loss: 0.12391144037246704\n",
      "training: 14 batch 68 batch_loss: 0.12281540036201477\n",
      "training: 14 batch 69 batch_loss: 0.12447154521942139\n",
      "training: 14 batch 70 batch_loss: 0.12514212727546692\n",
      "training: 14 batch 71 batch_loss: 0.1205303966999054\n",
      "training: 14 batch 72 batch_loss: 0.12306928634643555\n",
      "training: 14 batch 73 batch_loss: 0.12421971559524536\n",
      "training: 14 batch 74 batch_loss: 0.12497597932815552\n",
      "training: 14 batch 75 batch_loss: 0.1231052577495575\n",
      "training: 14 batch 76 batch_loss: 0.1234874427318573\n",
      "training: 14 batch 77 batch_loss: 0.12262755632400513\n",
      "training: 14 batch 78 batch_loss: 0.12310951948165894\n",
      "training: 14 batch 79 batch_loss: 0.12126463651657104\n",
      "training: 14 batch 80 batch_loss: 0.1234198808670044\n",
      "training: 14 batch 81 batch_loss: 0.12030008435249329\n",
      "training: 14 batch 82 batch_loss: 0.12566465139389038\n",
      "training: 14 batch 83 batch_loss: 0.12289810180664062\n",
      "training: 14 batch 84 batch_loss: 0.1268722414970398\n",
      "training: 14 batch 85 batch_loss: 0.12403115630149841\n",
      "training: 14 batch 86 batch_loss: 0.12409701943397522\n",
      "training: 14 batch 87 batch_loss: 0.12420853972434998\n",
      "training: 14 batch 88 batch_loss: 0.12592437863349915\n",
      "training: 14 batch 89 batch_loss: 0.12463387846946716\n",
      "training: 14 batch 90 batch_loss: 0.1211162805557251\n",
      "training: 14 batch 91 batch_loss: 0.1243448555469513\n",
      "training: 14 batch 92 batch_loss: 0.12508854269981384\n",
      "training: 14 batch 93 batch_loss: 0.12585484981536865\n",
      "training: 14 batch 94 batch_loss: 0.12557554244995117\n",
      "training: 14 batch 95 batch_loss: 0.12408477067947388\n",
      "training: 14 batch 96 batch_loss: 0.12495094537734985\n",
      "training: 14 batch 97 batch_loss: 0.12529480457305908\n",
      "training: 14 batch 98 batch_loss: 0.12593680620193481\n",
      "training: 14 batch 99 batch_loss: 0.12145090103149414\n",
      "training: 14 batch 100 batch_loss: 0.12607601284980774\n",
      "training: 14 batch 101 batch_loss: 0.12688744068145752\n",
      "training: 14 batch 102 batch_loss: 0.1230517029762268\n",
      "training: 14 batch 103 batch_loss: 0.1267867088317871\n",
      "training: 14 batch 104 batch_loss: 0.12449795007705688\n",
      "training: 14 batch 105 batch_loss: 0.12573125958442688\n",
      "training: 14 batch 106 batch_loss: 0.12532493472099304\n",
      "training: 14 batch 107 batch_loss: 0.12573498487472534\n",
      "training: 14 batch 108 batch_loss: 0.12111222743988037\n",
      "training: 14 batch 109 batch_loss: 0.12371096014976501\n",
      "training: 14 batch 110 batch_loss: 0.12341296672821045\n",
      "training: 14 batch 111 batch_loss: 0.12598451972007751\n",
      "training: 14 batch 112 batch_loss: 0.12395051121711731\n",
      "training: 14 batch 113 batch_loss: 0.12250486016273499\n",
      "training: 14 batch 114 batch_loss: 0.12624499201774597\n",
      "training: 14 batch 115 batch_loss: 0.12371885776519775\n",
      "training: 14 batch 116 batch_loss: 0.12867772579193115\n",
      "training: 14 batch 117 batch_loss: 0.12679749727249146\n",
      "training: 14 batch 118 batch_loss: 0.12200707197189331\n",
      "training: 14 batch 119 batch_loss: 0.12455755472183228\n",
      "training: 14 batch 120 batch_loss: 0.12544941902160645\n",
      "training: 14 batch 121 batch_loss: 0.12362059950828552\n",
      "training: 14 batch 122 batch_loss: 0.12321603298187256\n",
      "training: 14 batch 123 batch_loss: 0.1257997751235962\n",
      "training: 14 batch 124 batch_loss: 0.12251824140548706\n",
      "training: 14 batch 125 batch_loss: 0.12568354606628418\n",
      "training: 14 batch 126 batch_loss: 0.12485811114311218\n",
      "training: 14 batch 127 batch_loss: 0.12547361850738525\n",
      "training: 14 batch 128 batch_loss: 0.12538990378379822\n",
      "training: 14 batch 129 batch_loss: 0.12667804956436157\n",
      "training: 14 batch 130 batch_loss: 0.12513068318367004\n",
      "training: 14 batch 131 batch_loss: 0.12550151348114014\n",
      "training: 14 batch 132 batch_loss: 0.12542593479156494\n",
      "training: 14 batch 133 batch_loss: 0.12366920709609985\n",
      "training: 14 batch 134 batch_loss: 0.12220355868339539\n",
      "training: 14 batch 135 batch_loss: 0.12680748105049133\n",
      "training: 14 batch 136 batch_loss: 0.12378746271133423\n",
      "training: 14 batch 137 batch_loss: 0.12492352724075317\n",
      "training: 14 batch 138 batch_loss: 0.12370401620864868\n",
      "training: 14 batch 139 batch_loss: 0.12305641174316406\n",
      "training: 14 batch 140 batch_loss: 0.12417757511138916\n",
      "training: 14 batch 141 batch_loss: 0.12415376305580139\n",
      "training: 14 batch 142 batch_loss: 0.12629032135009766\n",
      "training: 14 batch 143 batch_loss: 0.12612700462341309\n",
      "training: 14 batch 144 batch_loss: 0.12399071455001831\n",
      "training: 14 batch 145 batch_loss: 0.12629228830337524\n",
      "training: 14 batch 146 batch_loss: 0.12324073910713196\n",
      "training: 14 batch 147 batch_loss: 0.12741196155548096\n",
      "training: 14 batch 148 batch_loss: 0.12221196293830872\n",
      "training: 14 batch 149 batch_loss: 0.12295320630073547\n",
      "training: 14 batch 150 batch_loss: 0.1268143355846405\n",
      "training: 14 batch 151 batch_loss: 0.12292367219924927\n",
      "training: 14 batch 152 batch_loss: 0.12464159727096558\n",
      "training: 14 batch 153 batch_loss: 0.12716099619865417\n",
      "training: 14 batch 154 batch_loss: 0.1257096529006958\n",
      "training: 14 batch 155 batch_loss: 0.125951886177063\n",
      "training: 14 batch 156 batch_loss: 0.12116849422454834\n",
      "training: 14 batch 157 batch_loss: 0.12311741709709167\n",
      "training: 14 batch 158 batch_loss: 0.12518513202667236\n",
      "training: 14 batch 159 batch_loss: 0.12648361921310425\n",
      "training: 14 batch 160 batch_loss: 0.12232571840286255\n",
      "training: 14 batch 161 batch_loss: 0.12598413228988647\n",
      "training: 14 batch 162 batch_loss: 0.1240641176700592\n",
      "training: 14 batch 163 batch_loss: 0.13073652982711792\n",
      "training: 14 batch 164 batch_loss: 0.1212729811668396\n",
      "training: 14 batch 165 batch_loss: 0.12814074754714966\n",
      "training: 14 batch 166 batch_loss: 0.12668031454086304\n",
      "training: 14 batch 167 batch_loss: 0.1218813955783844\n",
      "training: 14 batch 168 batch_loss: 0.12555459141731262\n",
      "training: 14 batch 169 batch_loss: 0.12379109859466553\n",
      "training: 14 batch 170 batch_loss: 0.1233181357383728\n",
      "training: 14 batch 171 batch_loss: 0.1230514645576477\n",
      "training: 14 batch 172 batch_loss: 0.12581691145896912\n",
      "training: 14 batch 173 batch_loss: 0.12563985586166382\n",
      "training: 14 batch 174 batch_loss: 0.12261784076690674\n",
      "training: 14 batch 175 batch_loss: 0.12571612000465393\n",
      "training: 14 batch 176 batch_loss: 0.12549620866775513\n",
      "training: 14 batch 177 batch_loss: 0.12282049655914307\n",
      "training: 14 batch 178 batch_loss: 0.12471398711204529\n",
      "training: 14 batch 179 batch_loss: 0.12260681390762329\n",
      "training: 14 batch 180 batch_loss: 0.12438571453094482\n",
      "training: 14 batch 181 batch_loss: 0.12316727638244629\n",
      "training: 14 batch 182 batch_loss: 0.12641632556915283\n",
      "training: 14 batch 183 batch_loss: 0.12468737363815308\n",
      "training: 14 batch 184 batch_loss: 0.12912213802337646\n",
      "training: 14 batch 185 batch_loss: 0.12734556198120117\n",
      "training: 14 batch 186 batch_loss: 0.1257827877998352\n",
      "training: 14 batch 187 batch_loss: 0.12442028522491455\n",
      "training: 14 batch 188 batch_loss: 0.1241145133972168\n",
      "training: 14 batch 189 batch_loss: 0.12515297532081604\n",
      "training: 14 batch 190 batch_loss: 0.12378904223442078\n",
      "training: 14 batch 191 batch_loss: 0.1249469518661499\n",
      "training: 14 batch 192 batch_loss: 0.12203392386436462\n",
      "training: 14 batch 193 batch_loss: 0.12264925241470337\n",
      "training: 14 batch 194 batch_loss: 0.12314054369926453\n",
      "training: 14 batch 195 batch_loss: 0.12222254276275635\n",
      "training: 14 batch 196 batch_loss: 0.12710565328598022\n",
      "training: 14 batch 197 batch_loss: 0.12418913841247559\n",
      "training: 14 batch 198 batch_loss: 0.12327799201011658\n",
      "training: 14 batch 199 batch_loss: 0.12418675422668457\n",
      "training: 14 batch 200 batch_loss: 0.1218823492527008\n",
      "training: 14 batch 201 batch_loss: 0.13008731603622437\n",
      "training: 14 batch 202 batch_loss: 0.12654513120651245\n",
      "training: 14 batch 203 batch_loss: 0.12478861212730408\n",
      "training: 14 batch 204 batch_loss: 0.12405264377593994\n",
      "training: 14 batch 205 batch_loss: 0.12284523248672485\n",
      "training: 14 batch 206 batch_loss: 0.12475943565368652\n",
      "training: 14 batch 207 batch_loss: 0.12457218766212463\n",
      "training: 14 batch 208 batch_loss: 0.12522006034851074\n",
      "training: 14 batch 209 batch_loss: 0.1268891990184784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 14 batch 210 batch_loss: 0.12711161375045776\n",
      "training: 14 batch 211 batch_loss: 0.1265603005886078\n",
      "training: 14 batch 212 batch_loss: 0.12643003463745117\n",
      "training: 14 batch 213 batch_loss: 0.12620490789413452\n",
      "training: 14 batch 214 batch_loss: 0.12273228168487549\n",
      "training: 14 batch 215 batch_loss: 0.12480476498603821\n",
      "training: 14 batch 216 batch_loss: 0.12791034579277039\n",
      "training: 14 batch 217 batch_loss: 0.1238347589969635\n",
      "training: 14 batch 218 batch_loss: 0.12684422731399536\n",
      "training: 14 batch 219 batch_loss: 0.12745201587677002\n",
      "training: 14 batch 220 batch_loss: 0.1256471574306488\n",
      "training: 14 batch 221 batch_loss: 0.12494933605194092\n",
      "training: 14 batch 222 batch_loss: 0.12540864944458008\n",
      "training: 14 batch 223 batch_loss: 0.12387681007385254\n",
      "training: 14 batch 224 batch_loss: 0.12587016820907593\n",
      "training: 14 batch 225 batch_loss: 0.12407588958740234\n",
      "training: 14 batch 226 batch_loss: 0.12433990836143494\n",
      "training: 14 batch 227 batch_loss: 0.12285161018371582\n",
      "training: 14 batch 228 batch_loss: 0.12618982791900635\n",
      "training: 14 batch 229 batch_loss: 0.12667638063430786\n",
      "training: 14 batch 230 batch_loss: 0.12723040580749512\n",
      "training: 14 batch 231 batch_loss: 0.12384673953056335\n",
      "training: 14 batch 232 batch_loss: 0.12503907084465027\n",
      "training: 14 batch 233 batch_loss: 0.126320481300354\n",
      "training: 14 batch 234 batch_loss: 0.12519118189811707\n",
      "training: 14 batch 235 batch_loss: 0.1267796754837036\n",
      "training: 14 batch 236 batch_loss: 0.12651664018630981\n",
      "training: 14 batch 237 batch_loss: 0.12396475672721863\n",
      "training: 14 batch 238 batch_loss: 0.12804418802261353\n",
      "training: 14 batch 239 batch_loss: 0.1265554428100586\n",
      "training: 14 batch 240 batch_loss: 0.12752389907836914\n",
      "training: 14 batch 241 batch_loss: 0.12731871008872986\n",
      "training: 14 batch 242 batch_loss: 0.1266235113143921\n",
      "training: 14 batch 243 batch_loss: 0.12532171607017517\n",
      "training: 14 batch 244 batch_loss: 0.12620922923088074\n",
      "training: 14 batch 245 batch_loss: 0.12638399004936218\n",
      "training: 14 batch 246 batch_loss: 0.12322142720222473\n",
      "training: 14 batch 247 batch_loss: 0.12597540020942688\n",
      "training: 14 batch 248 batch_loss: 0.12594732642173767\n",
      "training: 14 batch 249 batch_loss: 0.12654069066047668\n",
      "training: 14 batch 250 batch_loss: 0.12861230969429016\n",
      "training: 14 batch 251 batch_loss: 0.1259947419166565\n",
      "training: 14 batch 252 batch_loss: 0.1257222294807434\n",
      "training: 14 batch 253 batch_loss: 0.1265297532081604\n",
      "training: 14 batch 254 batch_loss: 0.12544465065002441\n",
      "training: 14 batch 255 batch_loss: 0.12523311376571655\n",
      "training: 14 batch 256 batch_loss: 0.1261853277683258\n",
      "training: 14 batch 257 batch_loss: 0.12436112761497498\n",
      "training: 14 batch 258 batch_loss: 0.12603658437728882\n",
      "training: 14 batch 259 batch_loss: 0.12546765804290771\n",
      "training: 14 batch 260 batch_loss: 0.12571066617965698\n",
      "training: 14 batch 261 batch_loss: 0.1268954575061798\n",
      "training: 14 batch 262 batch_loss: 0.1262245774269104\n",
      "training: 14 batch 263 batch_loss: 0.12520012259483337\n",
      "training: 14 batch 264 batch_loss: 0.12333667278289795\n",
      "training: 14 batch 265 batch_loss: 0.12298852205276489\n",
      "training: 14 batch 266 batch_loss: 0.12358054518699646\n",
      "training: 14 batch 267 batch_loss: 0.12695279717445374\n",
      "training: 14 batch 268 batch_loss: 0.12740558385849\n",
      "training: 14 batch 269 batch_loss: 0.12701353430747986\n",
      "training: 14 batch 270 batch_loss: 0.1243395209312439\n",
      "training: 14 batch 271 batch_loss: 0.12619465589523315\n",
      "training: 14 batch 272 batch_loss: 0.12343582510948181\n",
      "training: 14 batch 273 batch_loss: 0.12462925910949707\n",
      "training: 14 batch 274 batch_loss: 0.12431013584136963\n",
      "training: 14 batch 275 batch_loss: 0.12709945440292358\n",
      "training: 14 batch 276 batch_loss: 0.12625545263290405\n",
      "training: 14 batch 277 batch_loss: 0.1276835799217224\n",
      "training: 14 batch 278 batch_loss: 0.1268119215965271\n",
      "training: 14 batch 279 batch_loss: 0.12634596228599548\n",
      "training: 14 batch 280 batch_loss: 0.1275762915611267\n",
      "training: 14 batch 281 batch_loss: 0.12222450971603394\n",
      "training: 14 batch 282 batch_loss: 0.12611550092697144\n",
      "training: 14 batch 283 batch_loss: 0.12414282560348511\n",
      "training: 14 batch 284 batch_loss: 0.1256846785545349\n",
      "training: 14 batch 285 batch_loss: 0.1273655891418457\n",
      "training: 14 batch 286 batch_loss: 0.1275724172592163\n",
      "training: 14 batch 287 batch_loss: 0.12442794442176819\n",
      "training: 14 batch 288 batch_loss: 0.12176135182380676\n",
      "training: 14 batch 289 batch_loss: 0.12521567940711975\n",
      "training: 14 batch 290 batch_loss: 0.12815678119659424\n",
      "training: 14 batch 291 batch_loss: 0.12802836298942566\n",
      "training: 14 batch 292 batch_loss: 0.1287647783756256\n",
      "training: 14 batch 293 batch_loss: 0.1274113655090332\n",
      "training: 14 batch 294 batch_loss: 0.12442022562026978\n",
      "training: 14 batch 295 batch_loss: 0.12509021162986755\n",
      "training: 14 batch 296 batch_loss: 0.12832677364349365\n",
      "training: 14 batch 297 batch_loss: 0.12692302465438843\n",
      "training: 14 batch 298 batch_loss: 0.1261034607887268\n",
      "training: 14 batch 299 batch_loss: 0.12570106983184814\n",
      "training: 14 batch 300 batch_loss: 0.1257956624031067\n",
      "training: 14 batch 301 batch_loss: 0.1257285475730896\n",
      "training: 14 batch 302 batch_loss: 0.12395405769348145\n",
      "training: 14 batch 303 batch_loss: 0.1239260733127594\n",
      "training: 14 batch 304 batch_loss: 0.12646785378456116\n",
      "training: 14 batch 305 batch_loss: 0.12878772616386414\n",
      "training: 14 batch 306 batch_loss: 0.12610876560211182\n",
      "training: 14 batch 307 batch_loss: 0.1255626082420349\n",
      "training: 14 batch 308 batch_loss: 0.12546658515930176\n",
      "training: 14 batch 309 batch_loss: 0.1270427107810974\n",
      "training: 14 batch 310 batch_loss: 0.12996089458465576\n",
      "training: 14 batch 311 batch_loss: 0.1263238489627838\n",
      "training: 14 batch 312 batch_loss: 0.12346482276916504\n",
      "training: 14 batch 313 batch_loss: 0.12436419725418091\n",
      "training: 14 batch 314 batch_loss: 0.12803098559379578\n",
      "training: 14 batch 315 batch_loss: 0.12497496604919434\n",
      "training: 14 batch 316 batch_loss: 0.12713801860809326\n",
      "training: 14 batch 317 batch_loss: 0.12419894337654114\n",
      "training: 14 batch 318 batch_loss: 0.12558817863464355\n",
      "training: 14 batch 319 batch_loss: 0.1261746883392334\n",
      "training: 14 batch 320 batch_loss: 0.12261182069778442\n",
      "training: 14 batch 321 batch_loss: 0.12533146142959595\n",
      "training: 14 batch 322 batch_loss: 0.13161572813987732\n",
      "training: 14 batch 323 batch_loss: 0.12427276372909546\n",
      "training: 14 batch 324 batch_loss: 0.12385475635528564\n",
      "training: 14 batch 325 batch_loss: 0.12613272666931152\n",
      "training: 14 batch 326 batch_loss: 0.12607571482658386\n",
      "training: 14 batch 327 batch_loss: 0.12202125787734985\n",
      "training: 14 batch 328 batch_loss: 0.12775743007659912\n",
      "training: 14 batch 329 batch_loss: 0.12393879890441895\n",
      "training: 14 batch 330 batch_loss: 0.12526041269302368\n",
      "training: 14 batch 331 batch_loss: 0.12239411473274231\n",
      "training: 14 batch 332 batch_loss: 0.13003790378570557\n",
      "training: 14 batch 333 batch_loss: 0.12592771649360657\n",
      "training: 14 batch 334 batch_loss: 0.12616786360740662\n",
      "training: 14 batch 335 batch_loss: 0.1284182369709015\n",
      "training: 14 batch 336 batch_loss: 0.12502792477607727\n",
      "training: 14 batch 337 batch_loss: 0.1288527250289917\n",
      "training: 14 batch 338 batch_loss: 0.1220552921295166\n",
      "training: 14 batch 339 batch_loss: 0.12812146544456482\n",
      "training: 14 batch 340 batch_loss: 0.12648248672485352\n",
      "training: 14 batch 341 batch_loss: 0.12672990560531616\n",
      "training: 14 batch 342 batch_loss: 0.12720564007759094\n",
      "training: 14 batch 343 batch_loss: 0.12460881471633911\n",
      "training: 14 batch 344 batch_loss: 0.1254967451095581\n",
      "training: 14 batch 345 batch_loss: 0.12393590807914734\n",
      "training: 14 batch 346 batch_loss: 0.12861943244934082\n",
      "training: 14 batch 347 batch_loss: 0.12694823741912842\n",
      "training: 14 batch 348 batch_loss: 0.12632572650909424\n",
      "training: 14 batch 349 batch_loss: 0.12554892897605896\n",
      "training: 14 batch 350 batch_loss: 0.12724757194519043\n",
      "training: 14 batch 351 batch_loss: 0.12289059162139893\n",
      "training: 14 batch 352 batch_loss: 0.12708130478858948\n",
      "training: 14 batch 353 batch_loss: 0.12585490942001343\n",
      "training: 14 batch 354 batch_loss: 0.12712332606315613\n",
      "training: 14 batch 355 batch_loss: 0.12512102723121643\n",
      "training: 14 batch 356 batch_loss: 0.1254408359527588\n",
      "training: 14 batch 357 batch_loss: 0.12449225783348083\n",
      "training: 14 batch 358 batch_loss: 0.12751075625419617\n",
      "training: 14 batch 359 batch_loss: 0.1255563199520111\n",
      "training: 14 batch 360 batch_loss: 0.12653735280036926\n",
      "training: 14 batch 361 batch_loss: 0.12770754098892212\n",
      "training: 14 batch 362 batch_loss: 0.12782883644104004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 14 batch 363 batch_loss: 0.12693679332733154\n",
      "training: 14 batch 364 batch_loss: 0.12222999334335327\n",
      "training: 14 batch 365 batch_loss: 0.12663859128952026\n",
      "training: 14 batch 366 batch_loss: 0.12933778762817383\n",
      "training: 14 batch 367 batch_loss: 0.12669268250465393\n",
      "training: 14 batch 368 batch_loss: 0.12747275829315186\n",
      "training: 14 batch 369 batch_loss: 0.12460553646087646\n",
      "training: 14 batch 370 batch_loss: 0.12713542580604553\n",
      "training: 14 batch 371 batch_loss: 0.12426480650901794\n",
      "training: 14 batch 372 batch_loss: 0.1281718611717224\n",
      "training: 14 batch 373 batch_loss: 0.1255308985710144\n",
      "training: 14 batch 374 batch_loss: 0.12838414311408997\n",
      "training: 14 batch 375 batch_loss: 0.1284620761871338\n",
      "training: 14 batch 376 batch_loss: 0.12761104106903076\n",
      "training: 14 batch 377 batch_loss: 0.1264011263847351\n",
      "training: 14 batch 378 batch_loss: 0.12726500630378723\n",
      "training: 14 batch 379 batch_loss: 0.12514972686767578\n",
      "training: 14 batch 380 batch_loss: 0.12830811738967896\n",
      "training: 14 batch 381 batch_loss: 0.1256338655948639\n",
      "training: 14 batch 382 batch_loss: 0.12772643566131592\n",
      "training: 14 batch 383 batch_loss: 0.12595054507255554\n",
      "training: 14 batch 384 batch_loss: 0.1272510290145874\n",
      "training: 14 batch 385 batch_loss: 0.1228199303150177\n",
      "training: 14 batch 386 batch_loss: 0.12382259964942932\n",
      "training: 14 batch 387 batch_loss: 0.12472748756408691\n",
      "training: 14 batch 388 batch_loss: 0.12599852681159973\n",
      "training: 14 batch 389 batch_loss: 0.12890413403511047\n",
      "training: 14 batch 390 batch_loss: 0.12692347168922424\n",
      "training: 14 batch 391 batch_loss: 0.12680533528327942\n",
      "training: 14 batch 392 batch_loss: 0.12706312537193298\n",
      "training: 14 batch 393 batch_loss: 0.12564519047737122\n",
      "training: 14 batch 394 batch_loss: 0.12707751989364624\n",
      "training: 14 batch 395 batch_loss: 0.12543398141860962\n",
      "training: 14 batch 396 batch_loss: 0.12868091464042664\n",
      "training: 14 batch 397 batch_loss: 0.12667495012283325\n",
      "training: 14 batch 398 batch_loss: 0.12810707092285156\n",
      "training: 14 batch 399 batch_loss: 0.12774616479873657\n",
      "training: 14 batch 400 batch_loss: 0.12607690691947937\n",
      "training: 14 batch 401 batch_loss: 0.12594002485275269\n",
      "training: 14 batch 402 batch_loss: 0.12590241432189941\n",
      "training: 14 batch 403 batch_loss: 0.12803542613983154\n",
      "training: 14 batch 404 batch_loss: 0.1263197660446167\n",
      "training: 14 batch 405 batch_loss: 0.12764972448349\n",
      "training: 14 batch 406 batch_loss: 0.1272728443145752\n",
      "training: 14 batch 407 batch_loss: 0.12558415532112122\n",
      "training: 14 batch 408 batch_loss: 0.12534436583518982\n",
      "training: 14 batch 409 batch_loss: 0.12818241119384766\n",
      "training: 14 batch 410 batch_loss: 0.12505820393562317\n",
      "training: 14 batch 411 batch_loss: 0.12942364811897278\n",
      "training: 14 batch 412 batch_loss: 0.12831953167915344\n",
      "training: 14 batch 413 batch_loss: 0.12435701489448547\n",
      "training: 14 batch 414 batch_loss: 0.1248517632484436\n",
      "training: 14 batch 415 batch_loss: 0.12670230865478516\n",
      "training: 14 batch 416 batch_loss: 0.12533283233642578\n",
      "training: 14 batch 417 batch_loss: 0.1277097761631012\n",
      "training: 14 batch 418 batch_loss: 0.12649935483932495\n",
      "training: 14 batch 419 batch_loss: 0.12639376521110535\n",
      "training: 14 batch 420 batch_loss: 0.12828665971755981\n",
      "training: 14 batch 421 batch_loss: 0.12631827592849731\n",
      "training: 14 batch 422 batch_loss: 0.1258401870727539\n",
      "training: 14 batch 423 batch_loss: 0.1271042823791504\n",
      "training: 14 batch 424 batch_loss: 0.12668964266777039\n",
      "training: 14 batch 425 batch_loss: 0.1265210509300232\n",
      "training: 14 batch 426 batch_loss: 0.12531191110610962\n",
      "training: 14 batch 427 batch_loss: 0.12861394882202148\n",
      "training: 14 batch 428 batch_loss: 0.1268332302570343\n",
      "training: 14 batch 429 batch_loss: 0.12682437896728516\n",
      "training: 14 batch 430 batch_loss: 0.12553641200065613\n",
      "training: 14 batch 431 batch_loss: 0.12491384148597717\n",
      "training: 14 batch 432 batch_loss: 0.12778761982917786\n",
      "training: 14 batch 433 batch_loss: 0.12526777386665344\n",
      "training: 14 batch 434 batch_loss: 0.12510472536087036\n",
      "training: 14 batch 435 batch_loss: 0.12569525837898254\n",
      "training: 14 batch 436 batch_loss: 0.12693026661872864\n",
      "training: 14 batch 437 batch_loss: 0.12806221842765808\n",
      "training: 14 batch 438 batch_loss: 0.12744268774986267\n",
      "training: 14 batch 439 batch_loss: 0.12612882256507874\n",
      "training: 14 batch 440 batch_loss: 0.1263396441936493\n",
      "training: 14 batch 441 batch_loss: 0.1258913278579712\n",
      "training: 14 batch 442 batch_loss: 0.12665775418281555\n",
      "training: 14 batch 443 batch_loss: 0.1273803412914276\n",
      "training: 14 batch 444 batch_loss: 0.12980559468269348\n",
      "training: 14 batch 445 batch_loss: 0.12737643718719482\n",
      "training: 14 batch 446 batch_loss: 0.12439531087875366\n",
      "training: 14 batch 447 batch_loss: 0.1250806450843811\n",
      "training: 14 batch 448 batch_loss: 0.12479174137115479\n",
      "training: 14 batch 449 batch_loss: 0.12908777594566345\n",
      "training: 14 batch 450 batch_loss: 0.1255107820034027\n",
      "training: 14 batch 451 batch_loss: 0.12514716386795044\n",
      "training: 14 batch 452 batch_loss: 0.12631338834762573\n",
      "training: 14 batch 453 batch_loss: 0.12739524245262146\n",
      "training: 14 batch 454 batch_loss: 0.12498155236244202\n",
      "training: 14 batch 455 batch_loss: 0.12413644790649414\n",
      "training: 14 batch 456 batch_loss: 0.13013198971748352\n",
      "training: 14 batch 457 batch_loss: 0.13051345944404602\n",
      "training: 14 batch 458 batch_loss: 0.1285657286643982\n",
      "training: 14 batch 459 batch_loss: 0.12616196274757385\n",
      "training: 14 batch 460 batch_loss: 0.12631365656852722\n",
      "training: 14 batch 461 batch_loss: 0.12825927138328552\n",
      "training: 14 batch 462 batch_loss: 0.12524566054344177\n",
      "training: 14 batch 463 batch_loss: 0.12990450859069824\n",
      "training: 14 batch 464 batch_loss: 0.12580353021621704\n",
      "training: 14 batch 465 batch_loss: 0.12720006704330444\n",
      "training: 14 batch 466 batch_loss: 0.1281169056892395\n",
      "training: 14 batch 467 batch_loss: 0.12660571932792664\n",
      "training: 14 batch 468 batch_loss: 0.1277952790260315\n",
      "training: 14 batch 469 batch_loss: 0.1275399625301361\n",
      "training: 14 batch 470 batch_loss: 0.12778407335281372\n",
      "training: 14 batch 471 batch_loss: 0.124915212392807\n",
      "training: 14 batch 472 batch_loss: 0.12654834985733032\n",
      "training: 14 batch 473 batch_loss: 0.1246793270111084\n",
      "training: 14 batch 474 batch_loss: 0.13013902306556702\n",
      "training: 14 batch 475 batch_loss: 0.12803572416305542\n",
      "training: 14 batch 476 batch_loss: 0.12606817483901978\n",
      "training: 14 batch 477 batch_loss: 0.1292525827884674\n",
      "training: 14 batch 478 batch_loss: 0.12643036246299744\n",
      "training: 14 batch 479 batch_loss: 0.12723475694656372\n",
      "training: 14 batch 480 batch_loss: 0.12536928057670593\n",
      "training: 14 batch 481 batch_loss: 0.12464827299118042\n",
      "training: 14 batch 482 batch_loss: 0.12712299823760986\n",
      "training: 14 batch 483 batch_loss: 0.1252342164516449\n",
      "training: 14 batch 484 batch_loss: 0.1289360225200653\n",
      "training: 14 batch 485 batch_loss: 0.12424460053443909\n",
      "training: 14 batch 486 batch_loss: 0.12727653980255127\n",
      "training: 14 batch 487 batch_loss: 0.13000991940498352\n",
      "training: 14 batch 488 batch_loss: 0.12622979283332825\n",
      "training: 14 batch 489 batch_loss: 0.12408480048179626\n",
      "training: 14 batch 490 batch_loss: 0.12484392523765564\n",
      "training: 14 batch 491 batch_loss: 0.12850770354270935\n",
      "training: 14 batch 492 batch_loss: 0.12764382362365723\n",
      "training: 14 batch 493 batch_loss: 0.12615445256233215\n",
      "training: 14 batch 494 batch_loss: 0.1255548894405365\n",
      "training: 14 batch 495 batch_loss: 0.12996205687522888\n",
      "training: 14 batch 496 batch_loss: 0.12797033786773682\n",
      "training: 14 batch 497 batch_loss: 0.12743902206420898\n",
      "training: 14 batch 498 batch_loss: 0.12668001651763916\n",
      "training: 14 batch 499 batch_loss: 0.12688255310058594\n",
      "training: 14 batch 500 batch_loss: 0.12553921341896057\n",
      "training: 14 batch 501 batch_loss: 0.12802636623382568\n",
      "training: 14 batch 502 batch_loss: 0.12476855516433716\n",
      "training: 14 batch 503 batch_loss: 0.12817126512527466\n",
      "training: 14 batch 504 batch_loss: 0.12674418091773987\n",
      "training: 14 batch 505 batch_loss: 0.1277550756931305\n",
      "training: 14 batch 506 batch_loss: 0.12525847554206848\n",
      "training: 14 batch 507 batch_loss: 0.1271914839744568\n",
      "training: 14 batch 508 batch_loss: 0.12588998675346375\n",
      "training: 14 batch 509 batch_loss: 0.126202791929245\n",
      "training: 14 batch 510 batch_loss: 0.12553519010543823\n",
      "training: 14 batch 511 batch_loss: 0.12609025835990906\n",
      "training: 14 batch 512 batch_loss: 0.12919414043426514\n",
      "training: 14 batch 513 batch_loss: 0.12831327319145203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 14 batch 514 batch_loss: 0.12614214420318604\n",
      "training: 14 batch 515 batch_loss: 0.1282060146331787\n",
      "training: 14 batch 516 batch_loss: 0.1286352574825287\n",
      "training: 14 batch 517 batch_loss: 0.12743541598320007\n",
      "training: 14 batch 518 batch_loss: 0.1288774013519287\n",
      "training: 14 batch 519 batch_loss: 0.12756508588790894\n",
      "training: 14 batch 520 batch_loss: 0.13172531127929688\n",
      "training: 14 batch 521 batch_loss: 0.128892183303833\n",
      "training: 14 batch 522 batch_loss: 0.12581917643547058\n",
      "training: 14 batch 523 batch_loss: 0.12721574306488037\n",
      "training: 14 batch 524 batch_loss: 0.12716993689537048\n",
      "training: 14 batch 525 batch_loss: 0.12715762853622437\n",
      "training: 14 batch 526 batch_loss: 0.12799981236457825\n",
      "training: 14 batch 527 batch_loss: 0.12418335676193237\n",
      "training: 14 batch 528 batch_loss: 0.12752863764762878\n",
      "training: 14 batch 529 batch_loss: 0.12606781721115112\n",
      "training: 14 batch 530 batch_loss: 0.12438300251960754\n",
      "training: 14 batch 531 batch_loss: 0.1236887276172638\n",
      "training: 14 batch 532 batch_loss: 0.1279754638671875\n",
      "training: 14 batch 533 batch_loss: 0.12739971280097961\n",
      "training: 14 batch 534 batch_loss: 0.12906771898269653\n",
      "training: 14 batch 535 batch_loss: 0.1267557144165039\n",
      "training: 14 batch 536 batch_loss: 0.13263237476348877\n",
      "training: 14 batch 537 batch_loss: 0.12688425183296204\n",
      "training: 14 batch 538 batch_loss: 0.12736618518829346\n",
      "training: 14 batch 539 batch_loss: 0.12625956535339355\n",
      "training: 14 batch 540 batch_loss: 0.12520667910575867\n",
      "training: 14 batch 541 batch_loss: 0.12872853875160217\n",
      "training: 14 batch 542 batch_loss: 0.13161560893058777\n",
      "training: 14 batch 543 batch_loss: 0.12603551149368286\n",
      "training: 14 batch 544 batch_loss: 0.1265440583229065\n",
      "training: 14 batch 545 batch_loss: 0.12463358044624329\n",
      "training: 14 batch 546 batch_loss: 0.12820422649383545\n",
      "training: 14 batch 547 batch_loss: 0.12646162509918213\n",
      "training: 14 batch 548 batch_loss: 0.12845435738563538\n",
      "training: 14 batch 549 batch_loss: 0.12761443853378296\n",
      "training: 14 batch 550 batch_loss: 0.12364298105239868\n",
      "training: 14 batch 551 batch_loss: 0.1265055239200592\n",
      "training: 14 batch 552 batch_loss: 0.12651830911636353\n",
      "training: 14 batch 553 batch_loss: 0.12485671043395996\n",
      "training: 14 batch 554 batch_loss: 0.12800490856170654\n",
      "training: 14 batch 555 batch_loss: 0.12955421209335327\n",
      "training: 14 batch 556 batch_loss: 0.1279384195804596\n",
      "training: 14 batch 557 batch_loss: 0.1274573802947998\n",
      "training: 14 batch 558 batch_loss: 0.12815412878990173\n",
      "training: 14 batch 559 batch_loss: 0.12815693020820618\n",
      "training: 14 batch 560 batch_loss: 0.12752756476402283\n",
      "training: 14 batch 561 batch_loss: 0.12856066226959229\n",
      "training: 14 batch 562 batch_loss: 0.12913480401039124\n",
      "training: 14 batch 563 batch_loss: 0.13251584768295288\n",
      "training: 14 batch 564 batch_loss: 0.12712052464485168\n",
      "training: 14 batch 565 batch_loss: 0.12605023384094238\n",
      "training: 14 batch 566 batch_loss: 0.12429195642471313\n",
      "training: 14 batch 567 batch_loss: 0.1291247010231018\n",
      "training: 14 batch 568 batch_loss: 0.1277305781841278\n",
      "training: 14 batch 569 batch_loss: 0.12639516592025757\n",
      "training: 14 batch 570 batch_loss: 0.12710940837860107\n",
      "training: 14 batch 571 batch_loss: 0.12802568078041077\n",
      "training: 14 batch 572 batch_loss: 0.12832942605018616\n",
      "training: 14 batch 573 batch_loss: 0.1286858320236206\n",
      "training: 14 batch 574 batch_loss: 0.1278100609779358\n",
      "training: 14 batch 575 batch_loss: 0.1278153359889984\n",
      "training: 14 batch 576 batch_loss: 0.12591314315795898\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 14, Hit Ratio:0.024564197033241947 | Precision:0.03624299616632262 | Recall:0.05112081610672387 | NDCG:0.04583044983370174\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 15 batch 0 batch_loss: 0.12690412998199463\n",
      "training: 15 batch 1 batch_loss: 0.12652593851089478\n",
      "training: 15 batch 2 batch_loss: 0.12456801533699036\n",
      "training: 15 batch 3 batch_loss: 0.12615841627120972\n",
      "training: 15 batch 4 batch_loss: 0.1288527548313141\n",
      "training: 15 batch 5 batch_loss: 0.127567857503891\n",
      "training: 15 batch 6 batch_loss: 0.1286289393901825\n",
      "training: 15 batch 7 batch_loss: 0.12667715549468994\n",
      "training: 15 batch 8 batch_loss: 0.1290625035762787\n",
      "training: 15 batch 9 batch_loss: 0.125947505235672\n",
      "training: 15 batch 10 batch_loss: 0.126524418592453\n",
      "training: 15 batch 11 batch_loss: 0.1268506646156311\n",
      "training: 15 batch 12 batch_loss: 0.12543395161628723\n",
      "training: 15 batch 13 batch_loss: 0.12369954586029053\n",
      "training: 15 batch 14 batch_loss: 0.12364417314529419\n",
      "training: 15 batch 15 batch_loss: 0.1265491247177124\n",
      "training: 15 batch 16 batch_loss: 0.12083831429481506\n",
      "training: 15 batch 17 batch_loss: 0.12689003348350525\n",
      "training: 15 batch 18 batch_loss: 0.1310957968235016\n",
      "training: 15 batch 19 batch_loss: 0.12688523530960083\n",
      "training: 15 batch 20 batch_loss: 0.12383601069450378\n",
      "training: 15 batch 21 batch_loss: 0.12293466925621033\n",
      "training: 15 batch 22 batch_loss: 0.12889006733894348\n",
      "training: 15 batch 23 batch_loss: 0.1261226236820221\n",
      "training: 15 batch 24 batch_loss: 0.12774094939231873\n",
      "training: 15 batch 25 batch_loss: 0.12649470567703247\n",
      "training: 15 batch 26 batch_loss: 0.12571850419044495\n",
      "training: 15 batch 27 batch_loss: 0.12366312742233276\n",
      "training: 15 batch 28 batch_loss: 0.12323129177093506\n",
      "training: 15 batch 29 batch_loss: 0.1265506148338318\n",
      "training: 15 batch 30 batch_loss: 0.12646260857582092\n",
      "training: 15 batch 31 batch_loss: 0.12628313899040222\n",
      "training: 15 batch 32 batch_loss: 0.12602224946022034\n",
      "training: 15 batch 33 batch_loss: 0.12756550312042236\n",
      "training: 15 batch 34 batch_loss: 0.12925809621810913\n",
      "training: 15 batch 35 batch_loss: 0.12539097666740417\n",
      "training: 15 batch 36 batch_loss: 0.12675750255584717\n",
      "training: 15 batch 37 batch_loss: 0.12607979774475098\n",
      "training: 15 batch 38 batch_loss: 0.129060298204422\n",
      "training: 15 batch 39 batch_loss: 0.1244513988494873\n",
      "training: 15 batch 40 batch_loss: 0.12786322832107544\n",
      "training: 15 batch 41 batch_loss: 0.12793850898742676\n",
      "training: 15 batch 42 batch_loss: 0.12550729513168335\n",
      "training: 15 batch 43 batch_loss: 0.12878736853599548\n",
      "training: 15 batch 44 batch_loss: 0.12353548407554626\n",
      "training: 15 batch 45 batch_loss: 0.12793242931365967\n",
      "training: 15 batch 46 batch_loss: 0.12792116403579712\n",
      "training: 15 batch 47 batch_loss: 0.12528115510940552\n",
      "training: 15 batch 48 batch_loss: 0.12324365973472595\n",
      "training: 15 batch 49 batch_loss: 0.1284511685371399\n",
      "training: 15 batch 50 batch_loss: 0.12617933750152588\n",
      "training: 15 batch 51 batch_loss: 0.12855184078216553\n",
      "training: 15 batch 52 batch_loss: 0.1299695074558258\n",
      "training: 15 batch 53 batch_loss: 0.12184140086174011\n",
      "training: 15 batch 54 batch_loss: 0.12996190786361694\n",
      "training: 15 batch 55 batch_loss: 0.12509310245513916\n",
      "training: 15 batch 56 batch_loss: 0.1266096830368042\n",
      "training: 15 batch 57 batch_loss: 0.12457114458084106\n",
      "training: 15 batch 58 batch_loss: 0.12614014744758606\n",
      "training: 15 batch 59 batch_loss: 0.12503290176391602\n",
      "training: 15 batch 60 batch_loss: 0.12745574116706848\n",
      "training: 15 batch 61 batch_loss: 0.1281026005744934\n",
      "training: 15 batch 62 batch_loss: 0.1250477433204651\n",
      "training: 15 batch 63 batch_loss: 0.12478020787239075\n",
      "training: 15 batch 64 batch_loss: 0.12742167711257935\n",
      "training: 15 batch 65 batch_loss: 0.1258280873298645\n",
      "training: 15 batch 66 batch_loss: 0.12385186553001404\n",
      "training: 15 batch 67 batch_loss: 0.12561804056167603\n",
      "training: 15 batch 68 batch_loss: 0.12560653686523438\n",
      "training: 15 batch 69 batch_loss: 0.12843939661979675\n",
      "training: 15 batch 70 batch_loss: 0.12597531080245972\n",
      "training: 15 batch 71 batch_loss: 0.12665453553199768\n",
      "training: 15 batch 72 batch_loss: 0.12610474228858948\n",
      "training: 15 batch 73 batch_loss: 0.12750881910324097\n",
      "training: 15 batch 74 batch_loss: 0.12577125430107117\n",
      "training: 15 batch 75 batch_loss: 0.1270684003829956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 15 batch 76 batch_loss: 0.1271793246269226\n",
      "training: 15 batch 77 batch_loss: 0.12243789434432983\n",
      "training: 15 batch 78 batch_loss: 0.12538743019104004\n",
      "training: 15 batch 79 batch_loss: 0.12698987126350403\n",
      "training: 15 batch 80 batch_loss: 0.12678390741348267\n",
      "training: 15 batch 81 batch_loss: 0.12675291299819946\n",
      "training: 15 batch 82 batch_loss: 0.12634938955307007\n",
      "training: 15 batch 83 batch_loss: 0.12889710068702698\n",
      "training: 15 batch 84 batch_loss: 0.12689617276191711\n",
      "training: 15 batch 85 batch_loss: 0.12682339549064636\n",
      "training: 15 batch 86 batch_loss: 0.12513619661331177\n",
      "training: 15 batch 87 batch_loss: 0.12768959999084473\n",
      "training: 15 batch 88 batch_loss: 0.12532204389572144\n",
      "training: 15 batch 89 batch_loss: 0.12520545721054077\n",
      "training: 15 batch 90 batch_loss: 0.12827211618423462\n",
      "training: 15 batch 91 batch_loss: 0.12486428022384644\n",
      "training: 15 batch 92 batch_loss: 0.12556606531143188\n",
      "training: 15 batch 93 batch_loss: 0.125380277633667\n",
      "training: 15 batch 94 batch_loss: 0.12572118639945984\n",
      "training: 15 batch 95 batch_loss: 0.1278860867023468\n",
      "training: 15 batch 96 batch_loss: 0.12804335355758667\n",
      "training: 15 batch 97 batch_loss: 0.12437191605567932\n",
      "training: 15 batch 98 batch_loss: 0.12932464480400085\n",
      "training: 15 batch 99 batch_loss: 0.12222239375114441\n",
      "training: 15 batch 100 batch_loss: 0.13170123100280762\n",
      "training: 15 batch 101 batch_loss: 0.12775003910064697\n",
      "training: 15 batch 102 batch_loss: 0.12618911266326904\n",
      "training: 15 batch 103 batch_loss: 0.12684112787246704\n",
      "training: 15 batch 104 batch_loss: 0.12560847401618958\n",
      "training: 15 batch 105 batch_loss: 0.1274597942829132\n",
      "training: 15 batch 106 batch_loss: 0.12658804655075073\n",
      "training: 15 batch 107 batch_loss: 0.12839403748512268\n",
      "training: 15 batch 108 batch_loss: 0.12715917825698853\n",
      "training: 15 batch 109 batch_loss: 0.12813812494277954\n",
      "training: 15 batch 110 batch_loss: 0.12483859062194824\n",
      "training: 15 batch 111 batch_loss: 0.12391847372055054\n",
      "training: 15 batch 112 batch_loss: 0.12701240181922913\n",
      "training: 15 batch 113 batch_loss: 0.1279674470424652\n",
      "training: 15 batch 114 batch_loss: 0.12855592370033264\n",
      "training: 15 batch 115 batch_loss: 0.12790995836257935\n",
      "training: 15 batch 116 batch_loss: 0.12800627946853638\n",
      "training: 15 batch 117 batch_loss: 0.12753883004188538\n",
      "training: 15 batch 118 batch_loss: 0.12609469890594482\n",
      "training: 15 batch 119 batch_loss: 0.125567227602005\n",
      "training: 15 batch 120 batch_loss: 0.1296692192554474\n",
      "training: 15 batch 121 batch_loss: 0.12608182430267334\n",
      "training: 15 batch 122 batch_loss: 0.12826791405677795\n",
      "training: 15 batch 123 batch_loss: 0.128989577293396\n",
      "training: 15 batch 124 batch_loss: 0.1264002025127411\n",
      "training: 15 batch 125 batch_loss: 0.12730711698532104\n",
      "training: 15 batch 126 batch_loss: 0.12687432765960693\n",
      "training: 15 batch 127 batch_loss: 0.12863940000534058\n",
      "training: 15 batch 128 batch_loss: 0.12683454155921936\n",
      "training: 15 batch 129 batch_loss: 0.12781238555908203\n",
      "training: 15 batch 130 batch_loss: 0.12748420238494873\n",
      "training: 15 batch 131 batch_loss: 0.12268388271331787\n",
      "training: 15 batch 132 batch_loss: 0.12742763757705688\n",
      "training: 15 batch 133 batch_loss: 0.12442353367805481\n",
      "training: 15 batch 134 batch_loss: 0.12945392727851868\n",
      "training: 15 batch 135 batch_loss: 0.12691658735275269\n",
      "training: 15 batch 136 batch_loss: 0.12809360027313232\n",
      "training: 15 batch 137 batch_loss: 0.12523919343948364\n",
      "training: 15 batch 138 batch_loss: 0.12683671712875366\n",
      "training: 15 batch 139 batch_loss: 0.1270768940448761\n",
      "training: 15 batch 140 batch_loss: 0.1264767050743103\n",
      "training: 15 batch 141 batch_loss: 0.12632903456687927\n",
      "training: 15 batch 142 batch_loss: 0.12560757994651794\n",
      "training: 15 batch 143 batch_loss: 0.1282362937927246\n",
      "training: 15 batch 144 batch_loss: 0.1292218565940857\n",
      "training: 15 batch 145 batch_loss: 0.1243031919002533\n",
      "training: 15 batch 146 batch_loss: 0.12634694576263428\n",
      "training: 15 batch 147 batch_loss: 0.12688741087913513\n",
      "training: 15 batch 148 batch_loss: 0.12586385011672974\n",
      "training: 15 batch 149 batch_loss: 0.12955299019813538\n",
      "training: 15 batch 150 batch_loss: 0.12737277150154114\n",
      "training: 15 batch 151 batch_loss: 0.13090893626213074\n",
      "training: 15 batch 152 batch_loss: 0.12556535005569458\n",
      "training: 15 batch 153 batch_loss: 0.1272437572479248\n",
      "training: 15 batch 154 batch_loss: 0.12903735041618347\n",
      "training: 15 batch 155 batch_loss: 0.12861162424087524\n",
      "training: 15 batch 156 batch_loss: 0.12899479269981384\n",
      "training: 15 batch 157 batch_loss: 0.1252688765525818\n",
      "training: 15 batch 158 batch_loss: 0.13008010387420654\n",
      "training: 15 batch 159 batch_loss: 0.12703201174736023\n",
      "training: 15 batch 160 batch_loss: 0.12952464818954468\n",
      "training: 15 batch 161 batch_loss: 0.12601757049560547\n",
      "training: 15 batch 162 batch_loss: 0.12588298320770264\n",
      "training: 15 batch 163 batch_loss: 0.12719079852104187\n",
      "training: 15 batch 164 batch_loss: 0.12692740559577942\n",
      "training: 15 batch 165 batch_loss: 0.12867683172225952\n",
      "training: 15 batch 166 batch_loss: 0.12953561544418335\n",
      "training: 15 batch 167 batch_loss: 0.1273409128189087\n",
      "training: 15 batch 168 batch_loss: 0.1291830837726593\n",
      "training: 15 batch 169 batch_loss: 0.12667879462242126\n",
      "training: 15 batch 170 batch_loss: 0.12627243995666504\n",
      "training: 15 batch 171 batch_loss: 0.1277058720588684\n",
      "training: 15 batch 172 batch_loss: 0.1251392364501953\n",
      "training: 15 batch 173 batch_loss: 0.12900081276893616\n",
      "training: 15 batch 174 batch_loss: 0.1277921199798584\n",
      "training: 15 batch 175 batch_loss: 0.12773045897483826\n",
      "training: 15 batch 176 batch_loss: 0.12941625714302063\n",
      "training: 15 batch 177 batch_loss: 0.12911173701286316\n",
      "training: 15 batch 178 batch_loss: 0.1263149082660675\n",
      "training: 15 batch 179 batch_loss: 0.1283409297466278\n",
      "training: 15 batch 180 batch_loss: 0.1284026801586151\n",
      "training: 15 batch 181 batch_loss: 0.1270291805267334\n",
      "training: 15 batch 182 batch_loss: 0.12678372859954834\n",
      "training: 15 batch 183 batch_loss: 0.1276351809501648\n",
      "training: 15 batch 184 batch_loss: 0.12399977445602417\n",
      "training: 15 batch 185 batch_loss: 0.12917104363441467\n",
      "training: 15 batch 186 batch_loss: 0.12722602486610413\n",
      "training: 15 batch 187 batch_loss: 0.12713128328323364\n",
      "training: 15 batch 188 batch_loss: 0.1275755763053894\n",
      "training: 15 batch 189 batch_loss: 0.1272478699684143\n",
      "training: 15 batch 190 batch_loss: 0.13310086727142334\n",
      "training: 15 batch 191 batch_loss: 0.12627702951431274\n",
      "training: 15 batch 192 batch_loss: 0.12726563215255737\n",
      "training: 15 batch 193 batch_loss: 0.12684571743011475\n",
      "training: 15 batch 194 batch_loss: 0.126611590385437\n",
      "training: 15 batch 195 batch_loss: 0.1295558512210846\n",
      "training: 15 batch 196 batch_loss: 0.12485116720199585\n",
      "training: 15 batch 197 batch_loss: 0.12757453322410583\n",
      "training: 15 batch 198 batch_loss: 0.12909144163131714\n",
      "training: 15 batch 199 batch_loss: 0.12762844562530518\n",
      "training: 15 batch 200 batch_loss: 0.12775146961212158\n",
      "training: 15 batch 201 batch_loss: 0.1293586790561676\n",
      "training: 15 batch 202 batch_loss: 0.128545880317688\n",
      "training: 15 batch 203 batch_loss: 0.12684425711631775\n",
      "training: 15 batch 204 batch_loss: 0.12680524587631226\n",
      "training: 15 batch 205 batch_loss: 0.1242227554321289\n",
      "training: 15 batch 206 batch_loss: 0.12468302249908447\n",
      "training: 15 batch 207 batch_loss: 0.13003095984458923\n",
      "training: 15 batch 208 batch_loss: 0.12805712223052979\n",
      "training: 15 batch 209 batch_loss: 0.126469224691391\n",
      "training: 15 batch 210 batch_loss: 0.12669014930725098\n",
      "training: 15 batch 211 batch_loss: 0.12621620297431946\n",
      "training: 15 batch 212 batch_loss: 0.1280597448348999\n",
      "training: 15 batch 213 batch_loss: 0.12806817889213562\n",
      "training: 15 batch 214 batch_loss: 0.12822112441062927\n",
      "training: 15 batch 215 batch_loss: 0.12690886855125427\n",
      "training: 15 batch 216 batch_loss: 0.12789994478225708\n",
      "training: 15 batch 217 batch_loss: 0.12545722723007202\n",
      "training: 15 batch 218 batch_loss: 0.1277346909046173\n",
      "training: 15 batch 219 batch_loss: 0.12685182690620422\n",
      "training: 15 batch 220 batch_loss: 0.1281302273273468\n",
      "training: 15 batch 221 batch_loss: 0.12604957818984985\n",
      "training: 15 batch 222 batch_loss: 0.1263585090637207\n",
      "training: 15 batch 223 batch_loss: 0.12800368666648865\n",
      "training: 15 batch 224 batch_loss: 0.12594005465507507\n",
      "training: 15 batch 225 batch_loss: 0.12859532237052917\n",
      "training: 15 batch 226 batch_loss: 0.12925320863723755\n",
      "training: 15 batch 227 batch_loss: 0.12746518850326538\n",
      "training: 15 batch 228 batch_loss: 0.12796610593795776\n",
      "training: 15 batch 229 batch_loss: 0.1274433135986328\n",
      "training: 15 batch 230 batch_loss: 0.12668901681900024\n",
      "training: 15 batch 231 batch_loss: 0.12826180458068848\n",
      "training: 15 batch 232 batch_loss: 0.1301226019859314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 15 batch 233 batch_loss: 0.12943261861801147\n",
      "training: 15 batch 234 batch_loss: 0.1282031536102295\n",
      "training: 15 batch 235 batch_loss: 0.12673500180244446\n",
      "training: 15 batch 236 batch_loss: 0.12745720148086548\n",
      "training: 15 batch 237 batch_loss: 0.1262998878955841\n",
      "training: 15 batch 238 batch_loss: 0.1287079155445099\n",
      "training: 15 batch 239 batch_loss: 0.1270585060119629\n",
      "training: 15 batch 240 batch_loss: 0.12853103876113892\n",
      "training: 15 batch 241 batch_loss: 0.12832102179527283\n",
      "training: 15 batch 242 batch_loss: 0.12404593825340271\n",
      "training: 15 batch 243 batch_loss: 0.12789011001586914\n",
      "training: 15 batch 244 batch_loss: 0.12697115540504456\n",
      "training: 15 batch 245 batch_loss: 0.13009411096572876\n",
      "training: 15 batch 246 batch_loss: 0.1267993152141571\n",
      "training: 15 batch 247 batch_loss: 0.12883368134498596\n",
      "training: 15 batch 248 batch_loss: 0.13016915321350098\n",
      "training: 15 batch 249 batch_loss: 0.12444519996643066\n",
      "training: 15 batch 250 batch_loss: 0.12977319955825806\n",
      "training: 15 batch 251 batch_loss: 0.12586051225662231\n",
      "training: 15 batch 252 batch_loss: 0.13194060325622559\n",
      "training: 15 batch 253 batch_loss: 0.12669485807418823\n",
      "training: 15 batch 254 batch_loss: 0.12764453887939453\n",
      "training: 15 batch 255 batch_loss: 0.13179883360862732\n",
      "training: 15 batch 256 batch_loss: 0.1247665286064148\n",
      "training: 15 batch 257 batch_loss: 0.1279345452785492\n",
      "training: 15 batch 258 batch_loss: 0.1293361485004425\n",
      "training: 15 batch 259 batch_loss: 0.13240349292755127\n",
      "training: 15 batch 260 batch_loss: 0.12820756435394287\n",
      "training: 15 batch 261 batch_loss: 0.1278805434703827\n",
      "training: 15 batch 262 batch_loss: 0.12884753942489624\n",
      "training: 15 batch 263 batch_loss: 0.12992984056472778\n",
      "training: 15 batch 264 batch_loss: 0.12360572814941406\n",
      "training: 15 batch 265 batch_loss: 0.12791723012924194\n",
      "training: 15 batch 266 batch_loss: 0.12596866488456726\n",
      "training: 15 batch 267 batch_loss: 0.12662503123283386\n",
      "training: 15 batch 268 batch_loss: 0.12605664134025574\n",
      "training: 15 batch 269 batch_loss: 0.12753331661224365\n",
      "training: 15 batch 270 batch_loss: 0.12908294796943665\n",
      "training: 15 batch 271 batch_loss: 0.13069909811019897\n",
      "training: 15 batch 272 batch_loss: 0.12717732787132263\n",
      "training: 15 batch 273 batch_loss: 0.12522771954536438\n",
      "training: 15 batch 274 batch_loss: 0.12877899408340454\n",
      "training: 15 batch 275 batch_loss: 0.13182333111763\n",
      "training: 15 batch 276 batch_loss: 0.12895417213439941\n",
      "training: 15 batch 277 batch_loss: 0.1286746859550476\n",
      "training: 15 batch 278 batch_loss: 0.12971332669258118\n",
      "training: 15 batch 279 batch_loss: 0.12840181589126587\n",
      "training: 15 batch 280 batch_loss: 0.12733885645866394\n",
      "training: 15 batch 281 batch_loss: 0.12841752171516418\n",
      "training: 15 batch 282 batch_loss: 0.12970122694969177\n",
      "training: 15 batch 283 batch_loss: 0.12977197766304016\n",
      "training: 15 batch 284 batch_loss: 0.1288500428199768\n",
      "training: 15 batch 285 batch_loss: 0.12752562761306763\n",
      "training: 15 batch 286 batch_loss: 0.13110363483428955\n",
      "training: 15 batch 287 batch_loss: 0.13025906682014465\n",
      "training: 15 batch 288 batch_loss: 0.13152286410331726\n",
      "training: 15 batch 289 batch_loss: 0.12743085622787476\n",
      "training: 15 batch 290 batch_loss: 0.12637361884117126\n",
      "training: 15 batch 291 batch_loss: 0.13026899099349976\n",
      "training: 15 batch 292 batch_loss: 0.12858283519744873\n",
      "training: 15 batch 293 batch_loss: 0.12968426942825317\n",
      "training: 15 batch 294 batch_loss: 0.13103950023651123\n",
      "training: 15 batch 295 batch_loss: 0.12819421291351318\n",
      "training: 15 batch 296 batch_loss: 0.13076800107955933\n",
      "training: 15 batch 297 batch_loss: 0.12976548075675964\n",
      "training: 15 batch 298 batch_loss: 0.12561514973640442\n",
      "training: 15 batch 299 batch_loss: 0.13112443685531616\n",
      "training: 15 batch 300 batch_loss: 0.12860015034675598\n",
      "training: 15 batch 301 batch_loss: 0.1284327507019043\n",
      "training: 15 batch 302 batch_loss: 0.1299189329147339\n",
      "training: 15 batch 303 batch_loss: 0.12998747825622559\n",
      "training: 15 batch 304 batch_loss: 0.12640070915222168\n",
      "training: 15 batch 305 batch_loss: 0.12800049781799316\n",
      "training: 15 batch 306 batch_loss: 0.12578317523002625\n",
      "training: 15 batch 307 batch_loss: 0.1289655864238739\n",
      "training: 15 batch 308 batch_loss: 0.12573814392089844\n",
      "training: 15 batch 309 batch_loss: 0.12874948978424072\n",
      "training: 15 batch 310 batch_loss: 0.12906676530838013\n",
      "training: 15 batch 311 batch_loss: 0.12755030393600464\n",
      "training: 15 batch 312 batch_loss: 0.12875470519065857\n",
      "training: 15 batch 313 batch_loss: 0.12702447175979614\n",
      "training: 15 batch 314 batch_loss: 0.12951382994651794\n",
      "training: 15 batch 315 batch_loss: 0.12890732288360596\n",
      "training: 15 batch 316 batch_loss: 0.1280580461025238\n",
      "training: 15 batch 317 batch_loss: 0.12689507007598877\n",
      "training: 15 batch 318 batch_loss: 0.12712246179580688\n",
      "training: 15 batch 319 batch_loss: 0.12982437014579773\n",
      "training: 15 batch 320 batch_loss: 0.12999176979064941\n",
      "training: 15 batch 321 batch_loss: 0.1280350685119629\n",
      "training: 15 batch 322 batch_loss: 0.12904304265975952\n",
      "training: 15 batch 323 batch_loss: 0.1281680464744568\n",
      "training: 15 batch 324 batch_loss: 0.12696346640586853\n",
      "training: 15 batch 325 batch_loss: 0.13005295395851135\n",
      "training: 15 batch 326 batch_loss: 0.13144686818122864\n",
      "training: 15 batch 327 batch_loss: 0.1266678273677826\n",
      "training: 15 batch 328 batch_loss: 0.1311277449131012\n",
      "training: 15 batch 329 batch_loss: 0.1266160011291504\n",
      "training: 15 batch 330 batch_loss: 0.13322526216506958\n",
      "training: 15 batch 331 batch_loss: 0.1297762393951416\n",
      "training: 15 batch 332 batch_loss: 0.1267516016960144\n",
      "training: 15 batch 333 batch_loss: 0.1270594596862793\n",
      "training: 15 batch 334 batch_loss: 0.1292392909526825\n",
      "training: 15 batch 335 batch_loss: 0.12839674949645996\n",
      "training: 15 batch 336 batch_loss: 0.12603595852851868\n",
      "training: 15 batch 337 batch_loss: 0.1294029951095581\n",
      "training: 15 batch 338 batch_loss: 0.12804624438285828\n",
      "training: 15 batch 339 batch_loss: 0.12685665488243103\n",
      "training: 15 batch 340 batch_loss: 0.12731343507766724\n",
      "training: 15 batch 341 batch_loss: 0.12868338823318481\n",
      "training: 15 batch 342 batch_loss: 0.1278875768184662\n",
      "training: 15 batch 343 batch_loss: 0.12761405110359192\n",
      "training: 15 batch 344 batch_loss: 0.131921648979187\n",
      "training: 15 batch 345 batch_loss: 0.1277352273464203\n",
      "training: 15 batch 346 batch_loss: 0.12900793552398682\n",
      "training: 15 batch 347 batch_loss: 0.12874850630760193\n",
      "training: 15 batch 348 batch_loss: 0.13018330931663513\n",
      "training: 15 batch 349 batch_loss: 0.13222041726112366\n",
      "training: 15 batch 350 batch_loss: 0.13045912981033325\n",
      "training: 15 batch 351 batch_loss: 0.12838596105575562\n",
      "training: 15 batch 352 batch_loss: 0.13060063123703003\n",
      "training: 15 batch 353 batch_loss: 0.13087376952171326\n",
      "training: 15 batch 354 batch_loss: 0.12778186798095703\n",
      "training: 15 batch 355 batch_loss: 0.12939772009849548\n",
      "training: 15 batch 356 batch_loss: 0.12578600645065308\n",
      "training: 15 batch 357 batch_loss: 0.12768936157226562\n",
      "training: 15 batch 358 batch_loss: 0.12885096669197083\n",
      "training: 15 batch 359 batch_loss: 0.13130849599838257\n",
      "training: 15 batch 360 batch_loss: 0.12979620695114136\n",
      "training: 15 batch 361 batch_loss: 0.12936103343963623\n",
      "training: 15 batch 362 batch_loss: 0.1287986934185028\n",
      "training: 15 batch 363 batch_loss: 0.1283082365989685\n",
      "training: 15 batch 364 batch_loss: 0.13079586625099182\n",
      "training: 15 batch 365 batch_loss: 0.12861859798431396\n",
      "training: 15 batch 366 batch_loss: 0.12773668766021729\n",
      "training: 15 batch 367 batch_loss: 0.12840262055397034\n",
      "training: 15 batch 368 batch_loss: 0.1303694248199463\n",
      "training: 15 batch 369 batch_loss: 0.1266368329524994\n",
      "training: 15 batch 370 batch_loss: 0.12832003831863403\n",
      "training: 15 batch 371 batch_loss: 0.12933415174484253\n",
      "training: 15 batch 372 batch_loss: 0.12950360774993896\n",
      "training: 15 batch 373 batch_loss: 0.13433438539505005\n",
      "training: 15 batch 374 batch_loss: 0.12658804655075073\n",
      "training: 15 batch 375 batch_loss: 0.12995409965515137\n",
      "training: 15 batch 376 batch_loss: 0.1310654878616333\n",
      "training: 15 batch 377 batch_loss: 0.1296454668045044\n",
      "training: 15 batch 378 batch_loss: 0.1290193796157837\n",
      "training: 15 batch 379 batch_loss: 0.13031122088432312\n",
      "training: 15 batch 380 batch_loss: 0.13302144408226013\n",
      "training: 15 batch 381 batch_loss: 0.12694573402404785\n",
      "training: 15 batch 382 batch_loss: 0.12883776426315308\n",
      "training: 15 batch 383 batch_loss: 0.13071995973587036\n",
      "training: 15 batch 384 batch_loss: 0.13125833868980408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 15 batch 385 batch_loss: 0.13149741291999817\n",
      "training: 15 batch 386 batch_loss: 0.12826266884803772\n",
      "training: 15 batch 387 batch_loss: 0.12835481762886047\n",
      "training: 15 batch 388 batch_loss: 0.1267184615135193\n",
      "training: 15 batch 389 batch_loss: 0.1300109326839447\n",
      "training: 15 batch 390 batch_loss: 0.12662452459335327\n",
      "training: 15 batch 391 batch_loss: 0.12797337770462036\n",
      "training: 15 batch 392 batch_loss: 0.12868136167526245\n",
      "training: 15 batch 393 batch_loss: 0.12482023239135742\n",
      "training: 15 batch 394 batch_loss: 0.13089409470558167\n",
      "training: 15 batch 395 batch_loss: 0.12951961159706116\n",
      "training: 15 batch 396 batch_loss: 0.12613481283187866\n",
      "training: 15 batch 397 batch_loss: 0.12747681140899658\n",
      "training: 15 batch 398 batch_loss: 0.12886133790016174\n",
      "training: 15 batch 399 batch_loss: 0.13021624088287354\n",
      "training: 15 batch 400 batch_loss: 0.12971025705337524\n",
      "training: 15 batch 401 batch_loss: 0.1283203661441803\n",
      "training: 15 batch 402 batch_loss: 0.1312829554080963\n",
      "training: 15 batch 403 batch_loss: 0.12662476301193237\n",
      "training: 15 batch 404 batch_loss: 0.12951284646987915\n",
      "training: 15 batch 405 batch_loss: 0.1274876892566681\n",
      "training: 15 batch 406 batch_loss: 0.12892204523086548\n",
      "training: 15 batch 407 batch_loss: 0.129006028175354\n",
      "training: 15 batch 408 batch_loss: 0.13233420252799988\n",
      "training: 15 batch 409 batch_loss: 0.13088524341583252\n",
      "training: 15 batch 410 batch_loss: 0.13132157921791077\n",
      "training: 15 batch 411 batch_loss: 0.13022667169570923\n",
      "training: 15 batch 412 batch_loss: 0.13015156984329224\n",
      "training: 15 batch 413 batch_loss: 0.12844908237457275\n",
      "training: 15 batch 414 batch_loss: 0.13068795204162598\n",
      "training: 15 batch 415 batch_loss: 0.1306021511554718\n",
      "training: 15 batch 416 batch_loss: 0.12964868545532227\n",
      "training: 15 batch 417 batch_loss: 0.12605983018875122\n",
      "training: 15 batch 418 batch_loss: 0.12983232736587524\n",
      "training: 15 batch 419 batch_loss: 0.13027465343475342\n",
      "training: 15 batch 420 batch_loss: 0.128294438123703\n",
      "training: 15 batch 421 batch_loss: 0.1277308166027069\n",
      "training: 15 batch 422 batch_loss: 0.1275339126586914\n",
      "training: 15 batch 423 batch_loss: 0.12929654121398926\n",
      "training: 15 batch 424 batch_loss: 0.12425464391708374\n",
      "training: 15 batch 425 batch_loss: 0.12482056021690369\n",
      "training: 15 batch 426 batch_loss: 0.12926393747329712\n",
      "training: 15 batch 427 batch_loss: 0.13247519731521606\n",
      "training: 15 batch 428 batch_loss: 0.12987351417541504\n",
      "training: 15 batch 429 batch_loss: 0.12969058752059937\n",
      "training: 15 batch 430 batch_loss: 0.12713909149169922\n",
      "training: 15 batch 431 batch_loss: 0.1289394199848175\n",
      "training: 15 batch 432 batch_loss: 0.12920862436294556\n",
      "training: 15 batch 433 batch_loss: 0.1323634684085846\n",
      "training: 15 batch 434 batch_loss: 0.13119038939476013\n",
      "training: 15 batch 435 batch_loss: 0.12930408120155334\n",
      "training: 15 batch 436 batch_loss: 0.1306997537612915\n",
      "training: 15 batch 437 batch_loss: 0.13140010833740234\n",
      "training: 15 batch 438 batch_loss: 0.1295795440673828\n",
      "training: 15 batch 439 batch_loss: 0.12983691692352295\n",
      "training: 15 batch 440 batch_loss: 0.12860321998596191\n",
      "training: 15 batch 441 batch_loss: 0.12831532955169678\n",
      "training: 15 batch 442 batch_loss: 0.13238868117332458\n",
      "training: 15 batch 443 batch_loss: 0.13009792566299438\n",
      "training: 15 batch 444 batch_loss: 0.12967801094055176\n",
      "training: 15 batch 445 batch_loss: 0.12931033968925476\n",
      "training: 15 batch 446 batch_loss: 0.13272523880004883\n",
      "training: 15 batch 447 batch_loss: 0.13402223587036133\n",
      "training: 15 batch 448 batch_loss: 0.1306583285331726\n",
      "training: 15 batch 449 batch_loss: 0.13000276684761047\n",
      "training: 15 batch 450 batch_loss: 0.12702715396881104\n",
      "training: 15 batch 451 batch_loss: 0.12853962182998657\n",
      "training: 15 batch 452 batch_loss: 0.12545812129974365\n",
      "training: 15 batch 453 batch_loss: 0.13235896825790405\n",
      "training: 15 batch 454 batch_loss: 0.1310557723045349\n",
      "training: 15 batch 455 batch_loss: 0.12986207008361816\n",
      "training: 15 batch 456 batch_loss: 0.1300782561302185\n",
      "training: 15 batch 457 batch_loss: 0.13157746195793152\n",
      "training: 15 batch 458 batch_loss: 0.1313769817352295\n",
      "training: 15 batch 459 batch_loss: 0.1295931339263916\n",
      "training: 15 batch 460 batch_loss: 0.12799251079559326\n",
      "training: 15 batch 461 batch_loss: 0.12791979312896729\n",
      "training: 15 batch 462 batch_loss: 0.12830579280853271\n",
      "training: 15 batch 463 batch_loss: 0.12667670845985413\n",
      "training: 15 batch 464 batch_loss: 0.12797755002975464\n",
      "training: 15 batch 465 batch_loss: 0.1290796399116516\n",
      "training: 15 batch 466 batch_loss: 0.13033485412597656\n",
      "training: 15 batch 467 batch_loss: 0.12903934717178345\n",
      "training: 15 batch 468 batch_loss: 0.13336509466171265\n",
      "training: 15 batch 469 batch_loss: 0.13110485672950745\n",
      "training: 15 batch 470 batch_loss: 0.12982410192489624\n",
      "training: 15 batch 471 batch_loss: 0.12869006395339966\n",
      "training: 15 batch 472 batch_loss: 0.12818226218223572\n",
      "training: 15 batch 473 batch_loss: 0.12880176305770874\n",
      "training: 15 batch 474 batch_loss: 0.129336416721344\n",
      "training: 15 batch 475 batch_loss: 0.12899363040924072\n",
      "training: 15 batch 476 batch_loss: 0.12920081615447998\n",
      "training: 15 batch 477 batch_loss: 0.13006237149238586\n",
      "training: 15 batch 478 batch_loss: 0.12865465879440308\n",
      "training: 15 batch 479 batch_loss: 0.1273212730884552\n",
      "training: 15 batch 480 batch_loss: 0.1316814422607422\n",
      "training: 15 batch 481 batch_loss: 0.1293100118637085\n",
      "training: 15 batch 482 batch_loss: 0.13114598393440247\n",
      "training: 15 batch 483 batch_loss: 0.12688839435577393\n",
      "training: 15 batch 484 batch_loss: 0.13072174787521362\n",
      "training: 15 batch 485 batch_loss: 0.1304592490196228\n",
      "training: 15 batch 486 batch_loss: 0.13246679306030273\n",
      "training: 15 batch 487 batch_loss: 0.13110417127609253\n",
      "training: 15 batch 488 batch_loss: 0.133234441280365\n",
      "training: 15 batch 489 batch_loss: 0.12743201851844788\n",
      "training: 15 batch 490 batch_loss: 0.1291123628616333\n",
      "training: 15 batch 491 batch_loss: 0.13164186477661133\n",
      "training: 15 batch 492 batch_loss: 0.12735188007354736\n",
      "training: 15 batch 493 batch_loss: 0.1316845417022705\n",
      "training: 15 batch 494 batch_loss: 0.13079139590263367\n",
      "training: 15 batch 495 batch_loss: 0.12812557816505432\n",
      "training: 15 batch 496 batch_loss: 0.12815189361572266\n",
      "training: 15 batch 497 batch_loss: 0.1281750202178955\n",
      "training: 15 batch 498 batch_loss: 0.13142603635787964\n",
      "training: 15 batch 499 batch_loss: 0.13001108169555664\n",
      "training: 15 batch 500 batch_loss: 0.12604886293411255\n",
      "training: 15 batch 501 batch_loss: 0.1325514316558838\n",
      "training: 15 batch 502 batch_loss: 0.1302279233932495\n",
      "training: 15 batch 503 batch_loss: 0.12824052572250366\n",
      "training: 15 batch 504 batch_loss: 0.13111266493797302\n",
      "training: 15 batch 505 batch_loss: 0.13170412182807922\n",
      "training: 15 batch 506 batch_loss: 0.12748771905899048\n",
      "training: 15 batch 507 batch_loss: 0.12958502769470215\n",
      "training: 15 batch 508 batch_loss: 0.12965130805969238\n",
      "training: 15 batch 509 batch_loss: 0.12835615873336792\n",
      "training: 15 batch 510 batch_loss: 0.12928634881973267\n",
      "training: 15 batch 511 batch_loss: 0.12913113832473755\n",
      "training: 15 batch 512 batch_loss: 0.13009753823280334\n",
      "training: 15 batch 513 batch_loss: 0.13153842091560364\n",
      "training: 15 batch 514 batch_loss: 0.12973931431770325\n",
      "training: 15 batch 515 batch_loss: 0.12950116395950317\n",
      "training: 15 batch 516 batch_loss: 0.13109555840492249\n",
      "training: 15 batch 517 batch_loss: 0.12970399856567383\n",
      "training: 15 batch 518 batch_loss: 0.13128161430358887\n",
      "training: 15 batch 519 batch_loss: 0.12915366888046265\n",
      "training: 15 batch 520 batch_loss: 0.1327412724494934\n",
      "training: 15 batch 521 batch_loss: 0.12888777256011963\n",
      "training: 15 batch 522 batch_loss: 0.1296381950378418\n",
      "training: 15 batch 523 batch_loss: 0.12910419702529907\n",
      "training: 15 batch 524 batch_loss: 0.12856435775756836\n",
      "training: 15 batch 525 batch_loss: 0.13027170300483704\n",
      "training: 15 batch 526 batch_loss: 0.12896040081977844\n",
      "training: 15 batch 527 batch_loss: 0.12978553771972656\n",
      "training: 15 batch 528 batch_loss: 0.12941035628318787\n",
      "training: 15 batch 529 batch_loss: 0.13284417986869812\n",
      "training: 15 batch 530 batch_loss: 0.13197314739227295\n",
      "training: 15 batch 531 batch_loss: 0.12640413641929626\n",
      "training: 15 batch 532 batch_loss: 0.13090434670448303\n",
      "training: 15 batch 533 batch_loss: 0.13407588005065918\n",
      "training: 15 batch 534 batch_loss: 0.12939032912254333\n",
      "training: 15 batch 535 batch_loss: 0.13343536853790283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 15 batch 536 batch_loss: 0.12795281410217285\n",
      "training: 15 batch 537 batch_loss: 0.13334310054779053\n",
      "training: 15 batch 538 batch_loss: 0.12957113981246948\n",
      "training: 15 batch 539 batch_loss: 0.12876230478286743\n",
      "training: 15 batch 540 batch_loss: 0.12870895862579346\n",
      "training: 15 batch 541 batch_loss: 0.13170599937438965\n",
      "training: 15 batch 542 batch_loss: 0.13287103176116943\n",
      "training: 15 batch 543 batch_loss: 0.1280064582824707\n",
      "training: 15 batch 544 batch_loss: 0.13126307725906372\n",
      "training: 15 batch 545 batch_loss: 0.13198721408843994\n",
      "training: 15 batch 546 batch_loss: 0.12735652923583984\n",
      "training: 15 batch 547 batch_loss: 0.12560561299324036\n",
      "training: 15 batch 548 batch_loss: 0.12782761454582214\n",
      "training: 15 batch 549 batch_loss: 0.130428284406662\n",
      "training: 15 batch 550 batch_loss: 0.13170787692070007\n",
      "training: 15 batch 551 batch_loss: 0.13084423542022705\n",
      "training: 15 batch 552 batch_loss: 0.12940368056297302\n",
      "training: 15 batch 553 batch_loss: 0.13021785020828247\n",
      "training: 15 batch 554 batch_loss: 0.12824314832687378\n",
      "training: 15 batch 555 batch_loss: 0.13087555766105652\n",
      "training: 15 batch 556 batch_loss: 0.13038024306297302\n",
      "training: 15 batch 557 batch_loss: 0.13023650646209717\n",
      "training: 15 batch 558 batch_loss: 0.12807786464691162\n",
      "training: 15 batch 559 batch_loss: 0.13518673181533813\n",
      "training: 15 batch 560 batch_loss: 0.12970411777496338\n",
      "training: 15 batch 561 batch_loss: 0.12936201691627502\n",
      "training: 15 batch 562 batch_loss: 0.13050445914268494\n",
      "training: 15 batch 563 batch_loss: 0.13018584251403809\n",
      "training: 15 batch 564 batch_loss: 0.12929633259773254\n",
      "training: 15 batch 565 batch_loss: 0.13147792220115662\n",
      "training: 15 batch 566 batch_loss: 0.12793034315109253\n",
      "training: 15 batch 567 batch_loss: 0.12702396512031555\n",
      "training: 15 batch 568 batch_loss: 0.1301509141921997\n",
      "training: 15 batch 569 batch_loss: 0.1328868865966797\n",
      "training: 15 batch 570 batch_loss: 0.13112032413482666\n",
      "training: 15 batch 571 batch_loss: 0.13365113735198975\n",
      "training: 15 batch 572 batch_loss: 0.13115763664245605\n",
      "training: 15 batch 573 batch_loss: 0.1289316713809967\n",
      "training: 15 batch 574 batch_loss: 0.1284618079662323\n",
      "training: 15 batch 575 batch_loss: 0.12964564561843872\n",
      "training: 15 batch 576 batch_loss: 0.13167646527290344\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 15, Hit Ratio:0.02499392057776164 | Precision:0.03687702742553819 | Recall:0.052006444888971 | NDCG:0.04676843643138022\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 16 batch 0 batch_loss: 0.12805938720703125\n",
      "training: 16 batch 1 batch_loss: 0.12794601917266846\n",
      "training: 16 batch 2 batch_loss: 0.12880849838256836\n",
      "training: 16 batch 3 batch_loss: 0.12664461135864258\n",
      "training: 16 batch 4 batch_loss: 0.13112014532089233\n",
      "training: 16 batch 5 batch_loss: 0.12935003638267517\n",
      "training: 16 batch 6 batch_loss: 0.1289159059524536\n",
      "training: 16 batch 7 batch_loss: 0.12628352642059326\n",
      "training: 16 batch 8 batch_loss: 0.12785303592681885\n",
      "training: 16 batch 9 batch_loss: 0.1251503825187683\n",
      "training: 16 batch 10 batch_loss: 0.12863624095916748\n",
      "training: 16 batch 11 batch_loss: 0.12880456447601318\n",
      "training: 16 batch 12 batch_loss: 0.12897592782974243\n",
      "training: 16 batch 13 batch_loss: 0.1279202699661255\n",
      "training: 16 batch 14 batch_loss: 0.12825635075569153\n",
      "training: 16 batch 15 batch_loss: 0.13174653053283691\n",
      "training: 16 batch 16 batch_loss: 0.12680840492248535\n",
      "training: 16 batch 17 batch_loss: 0.12838399410247803\n",
      "training: 16 batch 18 batch_loss: 0.1270463466644287\n",
      "training: 16 batch 19 batch_loss: 0.127058207988739\n",
      "training: 16 batch 20 batch_loss: 0.1291094422340393\n",
      "training: 16 batch 21 batch_loss: 0.13006895780563354\n",
      "training: 16 batch 22 batch_loss: 0.1289999783039093\n",
      "training: 16 batch 23 batch_loss: 0.13134467601776123\n",
      "training: 16 batch 24 batch_loss: 0.12737515568733215\n",
      "training: 16 batch 25 batch_loss: 0.1273389458656311\n",
      "training: 16 batch 26 batch_loss: 0.1282004415988922\n",
      "training: 16 batch 27 batch_loss: 0.12642598152160645\n",
      "training: 16 batch 28 batch_loss: 0.1323457658290863\n",
      "training: 16 batch 29 batch_loss: 0.12750676274299622\n",
      "training: 16 batch 30 batch_loss: 0.13008785247802734\n",
      "training: 16 batch 31 batch_loss: 0.12837505340576172\n",
      "training: 16 batch 32 batch_loss: 0.12779611349105835\n",
      "training: 16 batch 33 batch_loss: 0.12768185138702393\n",
      "training: 16 batch 34 batch_loss: 0.12804582715034485\n",
      "training: 16 batch 35 batch_loss: 0.13214319944381714\n",
      "training: 16 batch 36 batch_loss: 0.1301971673965454\n",
      "training: 16 batch 37 batch_loss: 0.12987610697746277\n",
      "training: 16 batch 38 batch_loss: 0.12530755996704102\n",
      "training: 16 batch 39 batch_loss: 0.12634485960006714\n",
      "training: 16 batch 40 batch_loss: 0.12834352254867554\n",
      "training: 16 batch 41 batch_loss: 0.12782832980155945\n",
      "training: 16 batch 42 batch_loss: 0.13084274530410767\n",
      "training: 16 batch 43 batch_loss: 0.12824797630310059\n",
      "training: 16 batch 44 batch_loss: 0.12581345438957214\n",
      "training: 16 batch 45 batch_loss: 0.12889912724494934\n",
      "training: 16 batch 46 batch_loss: 0.12927263975143433\n",
      "training: 16 batch 47 batch_loss: 0.12896275520324707\n",
      "training: 16 batch 48 batch_loss: 0.12820720672607422\n",
      "training: 16 batch 49 batch_loss: 0.13062062859535217\n",
      "training: 16 batch 50 batch_loss: 0.12835854291915894\n",
      "training: 16 batch 51 batch_loss: 0.1275174617767334\n",
      "training: 16 batch 52 batch_loss: 0.127814382314682\n",
      "training: 16 batch 53 batch_loss: 0.12711995840072632\n",
      "training: 16 batch 54 batch_loss: 0.12923794984817505\n",
      "training: 16 batch 55 batch_loss: 0.1296224594116211\n",
      "training: 16 batch 56 batch_loss: 0.1288764476776123\n",
      "training: 16 batch 57 batch_loss: 0.12957096099853516\n",
      "training: 16 batch 58 batch_loss: 0.13035792112350464\n",
      "training: 16 batch 59 batch_loss: 0.1280057728290558\n",
      "training: 16 batch 60 batch_loss: 0.13055574893951416\n",
      "training: 16 batch 61 batch_loss: 0.12915486097335815\n",
      "training: 16 batch 62 batch_loss: 0.12955379486083984\n",
      "training: 16 batch 63 batch_loss: 0.1305311620235443\n",
      "training: 16 batch 64 batch_loss: 0.13265946507453918\n",
      "training: 16 batch 65 batch_loss: 0.13033407926559448\n",
      "training: 16 batch 66 batch_loss: 0.1294037103652954\n",
      "training: 16 batch 67 batch_loss: 0.1285291612148285\n",
      "training: 16 batch 68 batch_loss: 0.1277191936969757\n",
      "training: 16 batch 69 batch_loss: 0.12972155213356018\n",
      "training: 16 batch 70 batch_loss: 0.12426599860191345\n",
      "training: 16 batch 71 batch_loss: 0.12620937824249268\n",
      "training: 16 batch 72 batch_loss: 0.13000530004501343\n",
      "training: 16 batch 73 batch_loss: 0.12953105568885803\n",
      "training: 16 batch 74 batch_loss: 0.12948131561279297\n",
      "training: 16 batch 75 batch_loss: 0.12824207544326782\n",
      "training: 16 batch 76 batch_loss: 0.13107508420944214\n",
      "training: 16 batch 77 batch_loss: 0.13222727179527283\n",
      "training: 16 batch 78 batch_loss: 0.1262797713279724\n",
      "training: 16 batch 79 batch_loss: 0.12836313247680664\n",
      "training: 16 batch 80 batch_loss: 0.12677699327468872\n",
      "training: 16 batch 81 batch_loss: 0.1302945613861084\n",
      "training: 16 batch 82 batch_loss: 0.126389741897583\n",
      "training: 16 batch 83 batch_loss: 0.12937217950820923\n",
      "training: 16 batch 84 batch_loss: 0.13322129845619202\n",
      "training: 16 batch 85 batch_loss: 0.12987187504768372\n",
      "training: 16 batch 86 batch_loss: 0.12754300236701965\n",
      "training: 16 batch 87 batch_loss: 0.13113951683044434\n",
      "training: 16 batch 88 batch_loss: 0.12937533855438232\n",
      "training: 16 batch 89 batch_loss: 0.1288595199584961\n",
      "training: 16 batch 90 batch_loss: 0.13116684556007385\n",
      "training: 16 batch 91 batch_loss: 0.12894928455352783\n",
      "training: 16 batch 92 batch_loss: 0.1297476887702942\n",
      "training: 16 batch 93 batch_loss: 0.13420897722244263\n",
      "training: 16 batch 94 batch_loss: 0.1286342442035675\n",
      "training: 16 batch 95 batch_loss: 0.1301511824131012\n",
      "training: 16 batch 96 batch_loss: 0.12965404987335205\n",
      "training: 16 batch 97 batch_loss: 0.1282137632369995\n",
      "training: 16 batch 98 batch_loss: 0.13300734758377075\n",
      "training: 16 batch 99 batch_loss: 0.13132327795028687\n",
      "training: 16 batch 100 batch_loss: 0.1322312355041504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 16 batch 101 batch_loss: 0.13179928064346313\n",
      "training: 16 batch 102 batch_loss: 0.13186132907867432\n",
      "training: 16 batch 103 batch_loss: 0.1312236785888672\n",
      "training: 16 batch 104 batch_loss: 0.13100633025169373\n",
      "training: 16 batch 105 batch_loss: 0.12786638736724854\n",
      "training: 16 batch 106 batch_loss: 0.13029950857162476\n",
      "training: 16 batch 107 batch_loss: 0.13212788105010986\n",
      "training: 16 batch 108 batch_loss: 0.1290333867073059\n",
      "training: 16 batch 109 batch_loss: 0.1319933831691742\n",
      "training: 16 batch 110 batch_loss: 0.1298498809337616\n",
      "training: 16 batch 111 batch_loss: 0.12985512614250183\n",
      "training: 16 batch 112 batch_loss: 0.13027030229568481\n",
      "training: 16 batch 113 batch_loss: 0.12886673212051392\n",
      "training: 16 batch 114 batch_loss: 0.13041910529136658\n",
      "training: 16 batch 115 batch_loss: 0.13079702854156494\n",
      "training: 16 batch 116 batch_loss: 0.12992870807647705\n",
      "training: 16 batch 117 batch_loss: 0.13226819038391113\n",
      "training: 16 batch 118 batch_loss: 0.1297561526298523\n",
      "training: 16 batch 119 batch_loss: 0.12817338109016418\n",
      "training: 16 batch 120 batch_loss: 0.13123217225074768\n",
      "training: 16 batch 121 batch_loss: 0.13161411881446838\n",
      "training: 16 batch 122 batch_loss: 0.12976977229118347\n",
      "training: 16 batch 123 batch_loss: 0.1291174590587616\n",
      "training: 16 batch 124 batch_loss: 0.13007879257202148\n",
      "training: 16 batch 125 batch_loss: 0.12964791059494019\n",
      "training: 16 batch 126 batch_loss: 0.1301354467868805\n",
      "training: 16 batch 127 batch_loss: 0.12727737426757812\n",
      "training: 16 batch 128 batch_loss: 0.1280515491962433\n",
      "training: 16 batch 129 batch_loss: 0.12956425547599792\n",
      "training: 16 batch 130 batch_loss: 0.12891611456871033\n",
      "training: 16 batch 131 batch_loss: 0.13246583938598633\n",
      "training: 16 batch 132 batch_loss: 0.13015085458755493\n",
      "training: 16 batch 133 batch_loss: 0.1303011178970337\n",
      "training: 16 batch 134 batch_loss: 0.13199350237846375\n",
      "training: 16 batch 135 batch_loss: 0.1306421160697937\n",
      "training: 16 batch 136 batch_loss: 0.12534981966018677\n",
      "training: 16 batch 137 batch_loss: 0.12835228443145752\n",
      "training: 16 batch 138 batch_loss: 0.12957808375358582\n",
      "training: 16 batch 139 batch_loss: 0.1317482590675354\n",
      "training: 16 batch 140 batch_loss: 0.13008445501327515\n",
      "training: 16 batch 141 batch_loss: 0.12785741686820984\n",
      "training: 16 batch 142 batch_loss: 0.1293296217918396\n",
      "training: 16 batch 143 batch_loss: 0.1299726366996765\n",
      "training: 16 batch 144 batch_loss: 0.1311793029308319\n",
      "training: 16 batch 145 batch_loss: 0.13164260983467102\n",
      "training: 16 batch 146 batch_loss: 0.12977904081344604\n",
      "training: 16 batch 147 batch_loss: 0.12928679585456848\n",
      "training: 16 batch 148 batch_loss: 0.13172835111618042\n",
      "training: 16 batch 149 batch_loss: 0.1284860074520111\n",
      "training: 16 batch 150 batch_loss: 0.13151893019676208\n",
      "training: 16 batch 151 batch_loss: 0.12936991453170776\n",
      "training: 16 batch 152 batch_loss: 0.12981253862380981\n",
      "training: 16 batch 153 batch_loss: 0.13127344846725464\n",
      "training: 16 batch 154 batch_loss: 0.13283231854438782\n",
      "training: 16 batch 155 batch_loss: 0.13425970077514648\n",
      "training: 16 batch 156 batch_loss: 0.12961643934249878\n",
      "training: 16 batch 157 batch_loss: 0.13117164373397827\n",
      "training: 16 batch 158 batch_loss: 0.13159555196762085\n",
      "training: 16 batch 159 batch_loss: 0.12991613149642944\n",
      "training: 16 batch 160 batch_loss: 0.12963402271270752\n",
      "training: 16 batch 161 batch_loss: 0.1300387680530548\n",
      "training: 16 batch 162 batch_loss: 0.12636590003967285\n",
      "training: 16 batch 163 batch_loss: 0.13101139664649963\n",
      "training: 16 batch 164 batch_loss: 0.13066884875297546\n",
      "training: 16 batch 165 batch_loss: 0.12961894273757935\n",
      "training: 16 batch 166 batch_loss: 0.13168761134147644\n",
      "training: 16 batch 167 batch_loss: 0.12738734483718872\n",
      "training: 16 batch 168 batch_loss: 0.12927547097206116\n",
      "training: 16 batch 169 batch_loss: 0.12836092710494995\n",
      "training: 16 batch 170 batch_loss: 0.13017690181732178\n",
      "training: 16 batch 171 batch_loss: 0.13026979565620422\n",
      "training: 16 batch 172 batch_loss: 0.13184204697608948\n",
      "training: 16 batch 173 batch_loss: 0.13081300258636475\n",
      "training: 16 batch 174 batch_loss: 0.12949514389038086\n",
      "training: 16 batch 175 batch_loss: 0.13398224115371704\n",
      "training: 16 batch 176 batch_loss: 0.13256514072418213\n",
      "training: 16 batch 177 batch_loss: 0.130784809589386\n",
      "training: 16 batch 178 batch_loss: 0.12854468822479248\n",
      "training: 16 batch 179 batch_loss: 0.13043564558029175\n",
      "training: 16 batch 180 batch_loss: 0.12810373306274414\n",
      "training: 16 batch 181 batch_loss: 0.13320544362068176\n",
      "training: 16 batch 182 batch_loss: 0.12980255484580994\n",
      "training: 16 batch 183 batch_loss: 0.13215011358261108\n",
      "training: 16 batch 184 batch_loss: 0.1299789845943451\n",
      "training: 16 batch 185 batch_loss: 0.1284610629081726\n",
      "training: 16 batch 186 batch_loss: 0.13287395238876343\n",
      "training: 16 batch 187 batch_loss: 0.13315162062644958\n",
      "training: 16 batch 188 batch_loss: 0.13122069835662842\n",
      "training: 16 batch 189 batch_loss: 0.12968191504478455\n",
      "training: 16 batch 190 batch_loss: 0.13219356536865234\n",
      "training: 16 batch 191 batch_loss: 0.13208067417144775\n",
      "training: 16 batch 192 batch_loss: 0.1294827163219452\n",
      "training: 16 batch 193 batch_loss: 0.13032647967338562\n",
      "training: 16 batch 194 batch_loss: 0.12960079312324524\n",
      "training: 16 batch 195 batch_loss: 0.12986230850219727\n",
      "training: 16 batch 196 batch_loss: 0.1317938268184662\n",
      "training: 16 batch 197 batch_loss: 0.13101980090141296\n",
      "training: 16 batch 198 batch_loss: 0.13050466775894165\n",
      "training: 16 batch 199 batch_loss: 0.133101224899292\n",
      "training: 16 batch 200 batch_loss: 0.12949207425117493\n",
      "training: 16 batch 201 batch_loss: 0.13015326857566833\n",
      "training: 16 batch 202 batch_loss: 0.13136711716651917\n",
      "training: 16 batch 203 batch_loss: 0.13188210129737854\n",
      "training: 16 batch 204 batch_loss: 0.13082680106163025\n",
      "training: 16 batch 205 batch_loss: 0.13258421421051025\n",
      "training: 16 batch 206 batch_loss: 0.13003423810005188\n",
      "training: 16 batch 207 batch_loss: 0.1284017562866211\n",
      "training: 16 batch 208 batch_loss: 0.1276930868625641\n",
      "training: 16 batch 209 batch_loss: 0.12835049629211426\n",
      "training: 16 batch 210 batch_loss: 0.13068324327468872\n",
      "training: 16 batch 211 batch_loss: 0.1300048530101776\n",
      "training: 16 batch 212 batch_loss: 0.1307721734046936\n",
      "training: 16 batch 213 batch_loss: 0.13159996271133423\n",
      "training: 16 batch 214 batch_loss: 0.12891998887062073\n",
      "training: 16 batch 215 batch_loss: 0.1312653124332428\n",
      "training: 16 batch 216 batch_loss: 0.1309203803539276\n",
      "training: 16 batch 217 batch_loss: 0.1301891803741455\n",
      "training: 16 batch 218 batch_loss: 0.1295466423034668\n",
      "training: 16 batch 219 batch_loss: 0.13041484355926514\n",
      "training: 16 batch 220 batch_loss: 0.1287146508693695\n",
      "training: 16 batch 221 batch_loss: 0.12723255157470703\n",
      "training: 16 batch 222 batch_loss: 0.13043412566184998\n",
      "training: 16 batch 223 batch_loss: 0.1312544345855713\n",
      "training: 16 batch 224 batch_loss: 0.12792927026748657\n",
      "training: 16 batch 225 batch_loss: 0.13131526112556458\n",
      "training: 16 batch 226 batch_loss: 0.1307210624217987\n",
      "training: 16 batch 227 batch_loss: 0.1280595064163208\n",
      "training: 16 batch 228 batch_loss: 0.13119447231292725\n",
      "training: 16 batch 229 batch_loss: 0.12885451316833496\n",
      "training: 16 batch 230 batch_loss: 0.13068410754203796\n",
      "training: 16 batch 231 batch_loss: 0.12889328598976135\n",
      "training: 16 batch 232 batch_loss: 0.12903791666030884\n",
      "training: 16 batch 233 batch_loss: 0.12675252556800842\n",
      "training: 16 batch 234 batch_loss: 0.12892568111419678\n",
      "training: 16 batch 235 batch_loss: 0.13090336322784424\n",
      "training: 16 batch 236 batch_loss: 0.13196635246276855\n",
      "training: 16 batch 237 batch_loss: 0.13049888610839844\n",
      "training: 16 batch 238 batch_loss: 0.13049530982971191\n",
      "training: 16 batch 239 batch_loss: 0.13320103287696838\n",
      "training: 16 batch 240 batch_loss: 0.12952792644500732\n",
      "training: 16 batch 241 batch_loss: 0.13571462035179138\n",
      "training: 16 batch 242 batch_loss: 0.1275925636291504\n",
      "training: 16 batch 243 batch_loss: 0.12963074445724487\n",
      "training: 16 batch 244 batch_loss: 0.1309952735900879\n",
      "training: 16 batch 245 batch_loss: 0.130803644657135\n",
      "training: 16 batch 246 batch_loss: 0.12774711847305298\n",
      "training: 16 batch 247 batch_loss: 0.13111311197280884\n",
      "training: 16 batch 248 batch_loss: 0.12861156463623047\n",
      "training: 16 batch 249 batch_loss: 0.13011515140533447\n",
      "training: 16 batch 250 batch_loss: 0.1295257806777954\n",
      "training: 16 batch 251 batch_loss: 0.13267609477043152\n",
      "training: 16 batch 252 batch_loss: 0.1326756775379181\n",
      "training: 16 batch 253 batch_loss: 0.13018083572387695\n",
      "training: 16 batch 254 batch_loss: 0.13354149460792542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 16 batch 255 batch_loss: 0.13012030720710754\n",
      "training: 16 batch 256 batch_loss: 0.1334328055381775\n",
      "training: 16 batch 257 batch_loss: 0.13112744688987732\n",
      "training: 16 batch 258 batch_loss: 0.12868547439575195\n",
      "training: 16 batch 259 batch_loss: 0.1280907392501831\n",
      "training: 16 batch 260 batch_loss: 0.13155412673950195\n",
      "training: 16 batch 261 batch_loss: 0.13248178362846375\n",
      "training: 16 batch 262 batch_loss: 0.13108614087104797\n",
      "training: 16 batch 263 batch_loss: 0.13018658757209778\n",
      "training: 16 batch 264 batch_loss: 0.1311514675617218\n",
      "training: 16 batch 265 batch_loss: 0.1346260905265808\n",
      "training: 16 batch 266 batch_loss: 0.12871885299682617\n",
      "training: 16 batch 267 batch_loss: 0.13191267848014832\n",
      "training: 16 batch 268 batch_loss: 0.13404831290245056\n",
      "training: 16 batch 269 batch_loss: 0.13091415166854858\n",
      "training: 16 batch 270 batch_loss: 0.1304549276828766\n",
      "training: 16 batch 271 batch_loss: 0.13171729445457458\n",
      "training: 16 batch 272 batch_loss: 0.12875226140022278\n",
      "training: 16 batch 273 batch_loss: 0.1309516727924347\n",
      "training: 16 batch 274 batch_loss: 0.13283124566078186\n",
      "training: 16 batch 275 batch_loss: 0.12970587611198425\n",
      "training: 16 batch 276 batch_loss: 0.13358882069587708\n",
      "training: 16 batch 277 batch_loss: 0.13291946053504944\n",
      "training: 16 batch 278 batch_loss: 0.12870365381240845\n",
      "training: 16 batch 279 batch_loss: 0.12995067238807678\n",
      "training: 16 batch 280 batch_loss: 0.12893030047416687\n",
      "training: 16 batch 281 batch_loss: 0.12852567434310913\n",
      "training: 16 batch 282 batch_loss: 0.13156136870384216\n",
      "training: 16 batch 283 batch_loss: 0.13011124730110168\n",
      "training: 16 batch 284 batch_loss: 0.13035249710083008\n",
      "training: 16 batch 285 batch_loss: 0.12993016839027405\n",
      "training: 16 batch 286 batch_loss: 0.12935400009155273\n",
      "training: 16 batch 287 batch_loss: 0.1301615834236145\n",
      "training: 16 batch 288 batch_loss: 0.13175112009048462\n",
      "training: 16 batch 289 batch_loss: 0.13134190440177917\n",
      "training: 16 batch 290 batch_loss: 0.1316518783569336\n",
      "training: 16 batch 291 batch_loss: 0.13199198246002197\n",
      "training: 16 batch 292 batch_loss: 0.1308903992176056\n",
      "training: 16 batch 293 batch_loss: 0.1288149654865265\n",
      "training: 16 batch 294 batch_loss: 0.12988731265068054\n",
      "training: 16 batch 295 batch_loss: 0.1334105134010315\n",
      "training: 16 batch 296 batch_loss: 0.12866073846817017\n",
      "training: 16 batch 297 batch_loss: 0.12942257523536682\n",
      "training: 16 batch 298 batch_loss: 0.1328224539756775\n",
      "training: 16 batch 299 batch_loss: 0.12836402654647827\n",
      "training: 16 batch 300 batch_loss: 0.13062375783920288\n",
      "training: 16 batch 301 batch_loss: 0.12948006391525269\n",
      "training: 16 batch 302 batch_loss: 0.13242340087890625\n",
      "training: 16 batch 303 batch_loss: 0.1320568025112152\n",
      "training: 16 batch 304 batch_loss: 0.13229086995124817\n",
      "training: 16 batch 305 batch_loss: 0.13237744569778442\n",
      "training: 16 batch 306 batch_loss: 0.13255494832992554\n",
      "training: 16 batch 307 batch_loss: 0.13040593266487122\n",
      "training: 16 batch 308 batch_loss: 0.13298919796943665\n",
      "training: 16 batch 309 batch_loss: 0.13143685460090637\n",
      "training: 16 batch 310 batch_loss: 0.13305795192718506\n",
      "training: 16 batch 311 batch_loss: 0.1307719349861145\n",
      "training: 16 batch 312 batch_loss: 0.13023626804351807\n",
      "training: 16 batch 313 batch_loss: 0.13215655088424683\n",
      "training: 16 batch 314 batch_loss: 0.12943759560585022\n",
      "training: 16 batch 315 batch_loss: 0.13503006100654602\n",
      "training: 16 batch 316 batch_loss: 0.13028061389923096\n",
      "training: 16 batch 317 batch_loss: 0.13246139883995056\n",
      "training: 16 batch 318 batch_loss: 0.13143879175186157\n",
      "training: 16 batch 319 batch_loss: 0.131294846534729\n",
      "training: 16 batch 320 batch_loss: 0.13096806406974792\n",
      "training: 16 batch 321 batch_loss: 0.13471883535385132\n",
      "training: 16 batch 322 batch_loss: 0.13122880458831787\n",
      "training: 16 batch 323 batch_loss: 0.13253659009933472\n",
      "training: 16 batch 324 batch_loss: 0.12984266877174377\n",
      "training: 16 batch 325 batch_loss: 0.13211026787757874\n",
      "training: 16 batch 326 batch_loss: 0.1296198070049286\n",
      "training: 16 batch 327 batch_loss: 0.12907782196998596\n",
      "training: 16 batch 328 batch_loss: 0.12979400157928467\n",
      "training: 16 batch 329 batch_loss: 0.12935057282447815\n",
      "training: 16 batch 330 batch_loss: 0.13027703762054443\n",
      "training: 16 batch 331 batch_loss: 0.13283231854438782\n",
      "training: 16 batch 332 batch_loss: 0.1321747899055481\n",
      "training: 16 batch 333 batch_loss: 0.1315380334854126\n",
      "training: 16 batch 334 batch_loss: 0.1274053156375885\n",
      "training: 16 batch 335 batch_loss: 0.13314998149871826\n",
      "training: 16 batch 336 batch_loss: 0.12970685958862305\n",
      "training: 16 batch 337 batch_loss: 0.13134106993675232\n",
      "training: 16 batch 338 batch_loss: 0.13132107257843018\n",
      "training: 16 batch 339 batch_loss: 0.12952777743339539\n",
      "training: 16 batch 340 batch_loss: 0.13425764441490173\n",
      "training: 16 batch 341 batch_loss: 0.13358008861541748\n",
      "training: 16 batch 342 batch_loss: 0.13076770305633545\n",
      "training: 16 batch 343 batch_loss: 0.13004055619239807\n",
      "training: 16 batch 344 batch_loss: 0.13202348351478577\n",
      "training: 16 batch 345 batch_loss: 0.13323551416397095\n",
      "training: 16 batch 346 batch_loss: 0.13038045167922974\n",
      "training: 16 batch 347 batch_loss: 0.12916648387908936\n",
      "training: 16 batch 348 batch_loss: 0.12779107689857483\n",
      "training: 16 batch 349 batch_loss: 0.1304827630519867\n",
      "training: 16 batch 350 batch_loss: 0.13121896982192993\n",
      "training: 16 batch 351 batch_loss: 0.13057968020439148\n",
      "training: 16 batch 352 batch_loss: 0.1328815519809723\n",
      "training: 16 batch 353 batch_loss: 0.13137224316596985\n",
      "training: 16 batch 354 batch_loss: 0.13128921389579773\n",
      "training: 16 batch 355 batch_loss: 0.12865501642227173\n",
      "training: 16 batch 356 batch_loss: 0.1297324299812317\n",
      "training: 16 batch 357 batch_loss: 0.1325661838054657\n",
      "training: 16 batch 358 batch_loss: 0.13430255651474\n",
      "training: 16 batch 359 batch_loss: 0.1304982602596283\n",
      "training: 16 batch 360 batch_loss: 0.13350307941436768\n",
      "training: 16 batch 361 batch_loss: 0.12918999791145325\n",
      "training: 16 batch 362 batch_loss: 0.13186079263687134\n",
      "training: 16 batch 363 batch_loss: 0.13002079725265503\n",
      "training: 16 batch 364 batch_loss: 0.133858323097229\n",
      "training: 16 batch 365 batch_loss: 0.13254383206367493\n",
      "training: 16 batch 366 batch_loss: 0.13098689913749695\n",
      "training: 16 batch 367 batch_loss: 0.1328810155391693\n",
      "training: 16 batch 368 batch_loss: 0.1287451982498169\n",
      "training: 16 batch 369 batch_loss: 0.12863197922706604\n",
      "training: 16 batch 370 batch_loss: 0.13157618045806885\n",
      "training: 16 batch 371 batch_loss: 0.13098767399787903\n",
      "training: 16 batch 372 batch_loss: 0.13208547234535217\n",
      "training: 16 batch 373 batch_loss: 0.12673017382621765\n",
      "training: 16 batch 374 batch_loss: 0.12974244356155396\n",
      "training: 16 batch 375 batch_loss: 0.12994077801704407\n",
      "training: 16 batch 376 batch_loss: 0.12992408871650696\n",
      "training: 16 batch 377 batch_loss: 0.13241443037986755\n",
      "training: 16 batch 378 batch_loss: 0.12798598408699036\n",
      "training: 16 batch 379 batch_loss: 0.13121187686920166\n",
      "training: 16 batch 380 batch_loss: 0.13222122192382812\n",
      "training: 16 batch 381 batch_loss: 0.13280647993087769\n",
      "training: 16 batch 382 batch_loss: 0.13013333082199097\n",
      "training: 16 batch 383 batch_loss: 0.1303291916847229\n",
      "training: 16 batch 384 batch_loss: 0.1330375373363495\n",
      "training: 16 batch 385 batch_loss: 0.12946158647537231\n",
      "training: 16 batch 386 batch_loss: 0.13164377212524414\n",
      "training: 16 batch 387 batch_loss: 0.12925177812576294\n",
      "training: 16 batch 388 batch_loss: 0.13180667161941528\n",
      "training: 16 batch 389 batch_loss: 0.1327543556690216\n",
      "training: 16 batch 390 batch_loss: 0.13127678632736206\n",
      "training: 16 batch 391 batch_loss: 0.1341908872127533\n",
      "training: 16 batch 392 batch_loss: 0.13381341099739075\n",
      "training: 16 batch 393 batch_loss: 0.13026997447013855\n",
      "training: 16 batch 394 batch_loss: 0.13223189115524292\n",
      "training: 16 batch 395 batch_loss: 0.13235029578208923\n",
      "training: 16 batch 396 batch_loss: 0.13008970022201538\n",
      "training: 16 batch 397 batch_loss: 0.1303287148475647\n",
      "training: 16 batch 398 batch_loss: 0.1303551197052002\n",
      "training: 16 batch 399 batch_loss: 0.13066959381103516\n",
      "training: 16 batch 400 batch_loss: 0.13020354509353638\n",
      "training: 16 batch 401 batch_loss: 0.13418906927108765\n",
      "training: 16 batch 402 batch_loss: 0.13194730877876282\n",
      "training: 16 batch 403 batch_loss: 0.131386399269104\n",
      "training: 16 batch 404 batch_loss: 0.1342485547065735\n",
      "training: 16 batch 405 batch_loss: 0.1323569416999817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 16 batch 406 batch_loss: 0.12938973307609558\n",
      "training: 16 batch 407 batch_loss: 0.1331043243408203\n",
      "training: 16 batch 408 batch_loss: 0.1295386254787445\n",
      "training: 16 batch 409 batch_loss: 0.13275641202926636\n",
      "training: 16 batch 410 batch_loss: 0.12873733043670654\n",
      "training: 16 batch 411 batch_loss: 0.12894052267074585\n",
      "training: 16 batch 412 batch_loss: 0.13408759236335754\n",
      "training: 16 batch 413 batch_loss: 0.13072258234024048\n",
      "training: 16 batch 414 batch_loss: 0.12920135259628296\n",
      "training: 16 batch 415 batch_loss: 0.13263541460037231\n",
      "training: 16 batch 416 batch_loss: 0.13150101900100708\n",
      "training: 16 batch 417 batch_loss: 0.1326696276664734\n",
      "training: 16 batch 418 batch_loss: 0.1323980987071991\n",
      "training: 16 batch 419 batch_loss: 0.1294674277305603\n",
      "training: 16 batch 420 batch_loss: 0.13231095671653748\n",
      "training: 16 batch 421 batch_loss: 0.13297122716903687\n",
      "training: 16 batch 422 batch_loss: 0.1308739185333252\n",
      "training: 16 batch 423 batch_loss: 0.13300830125808716\n",
      "training: 16 batch 424 batch_loss: 0.13042506575584412\n",
      "training: 16 batch 425 batch_loss: 0.13298213481903076\n",
      "training: 16 batch 426 batch_loss: 0.13134843111038208\n",
      "training: 16 batch 427 batch_loss: 0.13145196437835693\n",
      "training: 16 batch 428 batch_loss: 0.13023489713668823\n",
      "training: 16 batch 429 batch_loss: 0.13111534714698792\n",
      "training: 16 batch 430 batch_loss: 0.1344931721687317\n",
      "training: 16 batch 431 batch_loss: 0.13122937083244324\n",
      "training: 16 batch 432 batch_loss: 0.13264593482017517\n",
      "training: 16 batch 433 batch_loss: 0.13367661833763123\n",
      "training: 16 batch 434 batch_loss: 0.13401451706886292\n",
      "training: 16 batch 435 batch_loss: 0.13287249207496643\n",
      "training: 16 batch 436 batch_loss: 0.1317957043647766\n",
      "training: 16 batch 437 batch_loss: 0.13166150450706482\n",
      "training: 16 batch 438 batch_loss: 0.1311863362789154\n",
      "training: 16 batch 439 batch_loss: 0.12824910879135132\n",
      "training: 16 batch 440 batch_loss: 0.13403630256652832\n",
      "training: 16 batch 441 batch_loss: 0.12902981042861938\n",
      "training: 16 batch 442 batch_loss: 0.13127470016479492\n",
      "training: 16 batch 443 batch_loss: 0.13135966658592224\n",
      "training: 16 batch 444 batch_loss: 0.13221314549446106\n",
      "training: 16 batch 445 batch_loss: 0.13141536712646484\n",
      "training: 16 batch 446 batch_loss: 0.13298720121383667\n",
      "training: 16 batch 447 batch_loss: 0.12972113490104675\n",
      "training: 16 batch 448 batch_loss: 0.13282877206802368\n",
      "training: 16 batch 449 batch_loss: 0.13264194130897522\n",
      "training: 16 batch 450 batch_loss: 0.13230523467063904\n",
      "training: 16 batch 451 batch_loss: 0.13048022985458374\n",
      "training: 16 batch 452 batch_loss: 0.13361158967018127\n",
      "training: 16 batch 453 batch_loss: 0.1325119435787201\n",
      "training: 16 batch 454 batch_loss: 0.13414406776428223\n",
      "training: 16 batch 455 batch_loss: 0.13412514328956604\n",
      "training: 16 batch 456 batch_loss: 0.13095718622207642\n",
      "training: 16 batch 457 batch_loss: 0.13297808170318604\n",
      "training: 16 batch 458 batch_loss: 0.13049307465553284\n",
      "training: 16 batch 459 batch_loss: 0.13082456588745117\n",
      "training: 16 batch 460 batch_loss: 0.13158577680587769\n",
      "training: 16 batch 461 batch_loss: 0.13006165623664856\n",
      "training: 16 batch 462 batch_loss: 0.1323486864566803\n",
      "training: 16 batch 463 batch_loss: 0.13095849752426147\n",
      "training: 16 batch 464 batch_loss: 0.13478496670722961\n",
      "training: 16 batch 465 batch_loss: 0.12984463572502136\n",
      "training: 16 batch 466 batch_loss: 0.12949839234352112\n",
      "training: 16 batch 467 batch_loss: 0.1322098970413208\n",
      "training: 16 batch 468 batch_loss: 0.13126784563064575\n",
      "training: 16 batch 469 batch_loss: 0.13210022449493408\n",
      "training: 16 batch 470 batch_loss: 0.1325928270816803\n",
      "training: 16 batch 471 batch_loss: 0.1305750608444214\n",
      "training: 16 batch 472 batch_loss: 0.13367187976837158\n",
      "training: 16 batch 473 batch_loss: 0.13074752688407898\n",
      "training: 16 batch 474 batch_loss: 0.13262158632278442\n",
      "training: 16 batch 475 batch_loss: 0.1306542158126831\n",
      "training: 16 batch 476 batch_loss: 0.12919512391090393\n",
      "training: 16 batch 477 batch_loss: 0.13245293498039246\n",
      "training: 16 batch 478 batch_loss: 0.13499611616134644\n",
      "training: 16 batch 479 batch_loss: 0.13288584351539612\n",
      "training: 16 batch 480 batch_loss: 0.1313728392124176\n",
      "training: 16 batch 481 batch_loss: 0.13281318545341492\n",
      "training: 16 batch 482 batch_loss: 0.13297587633132935\n",
      "training: 16 batch 483 batch_loss: 0.13296958804130554\n",
      "training: 16 batch 484 batch_loss: 0.13373810052871704\n",
      "training: 16 batch 485 batch_loss: 0.13337555527687073\n",
      "training: 16 batch 486 batch_loss: 0.13091614842414856\n",
      "training: 16 batch 487 batch_loss: 0.1343596875667572\n",
      "training: 16 batch 488 batch_loss: 0.13070982694625854\n",
      "training: 16 batch 489 batch_loss: 0.1315605640411377\n",
      "training: 16 batch 490 batch_loss: 0.13348591327667236\n",
      "training: 16 batch 491 batch_loss: 0.1330316960811615\n",
      "training: 16 batch 492 batch_loss: 0.1302514672279358\n",
      "training: 16 batch 493 batch_loss: 0.13305583596229553\n",
      "training: 16 batch 494 batch_loss: 0.12865746021270752\n",
      "training: 16 batch 495 batch_loss: 0.12898588180541992\n",
      "training: 16 batch 496 batch_loss: 0.13263139128684998\n",
      "training: 16 batch 497 batch_loss: 0.13261187076568604\n",
      "training: 16 batch 498 batch_loss: 0.1288391351699829\n",
      "training: 16 batch 499 batch_loss: 0.13343128561973572\n",
      "training: 16 batch 500 batch_loss: 0.13239425420761108\n",
      "training: 16 batch 501 batch_loss: 0.13447481393814087\n",
      "training: 16 batch 502 batch_loss: 0.13653883337974548\n",
      "training: 16 batch 503 batch_loss: 0.1328030824661255\n",
      "training: 16 batch 504 batch_loss: 0.13029706478118896\n",
      "training: 16 batch 505 batch_loss: 0.13201415538787842\n",
      "training: 16 batch 506 batch_loss: 0.13288608193397522\n",
      "training: 16 batch 507 batch_loss: 0.13466084003448486\n",
      "training: 16 batch 508 batch_loss: 0.13482648134231567\n",
      "training: 16 batch 509 batch_loss: 0.13046115636825562\n",
      "training: 16 batch 510 batch_loss: 0.13393419981002808\n",
      "training: 16 batch 511 batch_loss: 0.1306016445159912\n",
      "training: 16 batch 512 batch_loss: 0.13576969504356384\n",
      "training: 16 batch 513 batch_loss: 0.13365745544433594\n",
      "training: 16 batch 514 batch_loss: 0.13297003507614136\n",
      "training: 16 batch 515 batch_loss: 0.1346868872642517\n",
      "training: 16 batch 516 batch_loss: 0.13225311040878296\n",
      "training: 16 batch 517 batch_loss: 0.13612356781959534\n",
      "training: 16 batch 518 batch_loss: 0.13353845477104187\n",
      "training: 16 batch 519 batch_loss: 0.13137945532798767\n",
      "training: 16 batch 520 batch_loss: 0.13132140040397644\n",
      "training: 16 batch 521 batch_loss: 0.13604483008384705\n",
      "training: 16 batch 522 batch_loss: 0.132320374250412\n",
      "training: 16 batch 523 batch_loss: 0.1316758692264557\n",
      "training: 16 batch 524 batch_loss: 0.1308036744594574\n",
      "training: 16 batch 525 batch_loss: 0.13278210163116455\n",
      "training: 16 batch 526 batch_loss: 0.134170800447464\n",
      "training: 16 batch 527 batch_loss: 0.13212281465530396\n",
      "training: 16 batch 528 batch_loss: 0.13167604804039001\n",
      "training: 16 batch 529 batch_loss: 0.13309341669082642\n",
      "training: 16 batch 530 batch_loss: 0.13198590278625488\n",
      "training: 16 batch 531 batch_loss: 0.13386672735214233\n",
      "training: 16 batch 532 batch_loss: 0.13255739212036133\n",
      "training: 16 batch 533 batch_loss: 0.13534855842590332\n",
      "training: 16 batch 534 batch_loss: 0.1318475902080536\n",
      "training: 16 batch 535 batch_loss: 0.1328827142715454\n",
      "training: 16 batch 536 batch_loss: 0.13280627131462097\n",
      "training: 16 batch 537 batch_loss: 0.13261574506759644\n",
      "training: 16 batch 538 batch_loss: 0.13253521919250488\n",
      "training: 16 batch 539 batch_loss: 0.1335475742816925\n",
      "training: 16 batch 540 batch_loss: 0.13138961791992188\n",
      "training: 16 batch 541 batch_loss: 0.13247334957122803\n",
      "training: 16 batch 542 batch_loss: 0.13389712572097778\n",
      "training: 16 batch 543 batch_loss: 0.1305481195449829\n",
      "training: 16 batch 544 batch_loss: 0.13219201564788818\n",
      "training: 16 batch 545 batch_loss: 0.1337946653366089\n",
      "training: 16 batch 546 batch_loss: 0.13362038135528564\n",
      "training: 16 batch 547 batch_loss: 0.13028401136398315\n",
      "training: 16 batch 548 batch_loss: 0.1351596713066101\n",
      "training: 16 batch 549 batch_loss: 0.13131141662597656\n",
      "training: 16 batch 550 batch_loss: 0.13061851263046265\n",
      "training: 16 batch 551 batch_loss: 0.1311667561531067\n",
      "training: 16 batch 552 batch_loss: 0.13422095775604248\n",
      "training: 16 batch 553 batch_loss: 0.1314277946949005\n",
      "training: 16 batch 554 batch_loss: 0.13410374522209167\n",
      "training: 16 batch 555 batch_loss: 0.1341928243637085\n",
      "training: 16 batch 556 batch_loss: 0.1351444125175476\n",
      "training: 16 batch 557 batch_loss: 0.13072073459625244\n",
      "training: 16 batch 558 batch_loss: 0.13141536712646484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 16 batch 559 batch_loss: 0.13531145453453064\n",
      "training: 16 batch 560 batch_loss: 0.13094782829284668\n",
      "training: 16 batch 561 batch_loss: 0.13212800025939941\n",
      "training: 16 batch 562 batch_loss: 0.13513582944869995\n",
      "training: 16 batch 563 batch_loss: 0.13573065400123596\n",
      "training: 16 batch 564 batch_loss: 0.13120049238204956\n",
      "training: 16 batch 565 batch_loss: 0.1345708668231964\n",
      "training: 16 batch 566 batch_loss: 0.13546735048294067\n",
      "training: 16 batch 567 batch_loss: 0.13226085901260376\n",
      "training: 16 batch 568 batch_loss: 0.13375037908554077\n",
      "training: 16 batch 569 batch_loss: 0.1338801383972168\n",
      "training: 16 batch 570 batch_loss: 0.13297656178474426\n",
      "training: 16 batch 571 batch_loss: 0.13110321760177612\n",
      "training: 16 batch 572 batch_loss: 0.13786667585372925\n",
      "training: 16 batch 573 batch_loss: 0.13200649619102478\n",
      "training: 16 batch 574 batch_loss: 0.13686388731002808\n",
      "training: 16 batch 575 batch_loss: 0.1293526291847229\n",
      "training: 16 batch 576 batch_loss: 0.1250346302986145\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 16, Hit Ratio:0.025803399812787106 | Precision:0.03807136537894426 | Recall:0.0531565994655252 | NDCG:0.04830365020434441\n",
      "*Best Performance* \n",
      "Epoch: 1, Hit Ratio:0.025853367666801026 | Precision:0.03814508994396933 | Recall:0.04411659681161833 | MDCG:0.04755010954365849\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 17 batch 0 batch_loss: 0.13085627555847168\n",
      "training: 17 batch 1 batch_loss: 0.1316526234149933\n",
      "training: 17 batch 2 batch_loss: 0.13017132878303528\n",
      "training: 17 batch 3 batch_loss: 0.13066759705543518\n",
      "training: 17 batch 4 batch_loss: 0.13105937838554382\n",
      "training: 17 batch 5 batch_loss: 0.12989234924316406\n",
      "training: 17 batch 6 batch_loss: 0.13289159536361694\n",
      "training: 17 batch 7 batch_loss: 0.12956124544143677\n",
      "training: 17 batch 8 batch_loss: 0.12998920679092407\n",
      "training: 17 batch 9 batch_loss: 0.13108965754508972\n",
      "training: 17 batch 10 batch_loss: 0.13083156943321228\n",
      "training: 17 batch 11 batch_loss: 0.13355323672294617\n",
      "training: 17 batch 12 batch_loss: 0.1278184950351715\n",
      "training: 17 batch 13 batch_loss: 0.13261714577674866\n",
      "training: 17 batch 14 batch_loss: 0.1334550380706787\n",
      "training: 17 batch 15 batch_loss: 0.1321335732936859\n",
      "training: 17 batch 16 batch_loss: 0.133446604013443\n",
      "training: 17 batch 17 batch_loss: 0.13241922855377197\n",
      "training: 17 batch 18 batch_loss: 0.13489562273025513\n",
      "training: 17 batch 19 batch_loss: 0.13214582204818726\n",
      "training: 17 batch 20 batch_loss: 0.12898200750350952\n",
      "training: 17 batch 21 batch_loss: 0.13800185918807983\n",
      "training: 17 batch 22 batch_loss: 0.13115549087524414\n",
      "training: 17 batch 23 batch_loss: 0.1296471655368805\n",
      "training: 17 batch 24 batch_loss: 0.13205888867378235\n",
      "training: 17 batch 25 batch_loss: 0.13584434986114502\n",
      "training: 17 batch 26 batch_loss: 0.1331312358379364\n",
      "training: 17 batch 27 batch_loss: 0.13146474957466125\n",
      "training: 17 batch 28 batch_loss: 0.1309293508529663\n",
      "training: 17 batch 29 batch_loss: 0.12948763370513916\n",
      "training: 17 batch 30 batch_loss: 0.13474175333976746\n",
      "training: 17 batch 31 batch_loss: 0.13018077611923218\n",
      "training: 17 batch 32 batch_loss: 0.12768685817718506\n",
      "training: 17 batch 33 batch_loss: 0.13125279545783997\n",
      "training: 17 batch 34 batch_loss: 0.1306605041027069\n",
      "training: 17 batch 35 batch_loss: 0.13331753015518188\n",
      "training: 17 batch 36 batch_loss: 0.13008984923362732\n",
      "training: 17 batch 37 batch_loss: 0.12966957688331604\n",
      "training: 17 batch 38 batch_loss: 0.13314372301101685\n",
      "training: 17 batch 39 batch_loss: 0.12920814752578735\n",
      "training: 17 batch 40 batch_loss: 0.13379555940628052\n",
      "training: 17 batch 41 batch_loss: 0.13401135802268982\n",
      "training: 17 batch 42 batch_loss: 0.13248184323310852\n",
      "training: 17 batch 43 batch_loss: 0.1318453848361969\n",
      "training: 17 batch 44 batch_loss: 0.13063287734985352\n",
      "training: 17 batch 45 batch_loss: 0.13374876976013184\n",
      "training: 17 batch 46 batch_loss: 0.13141751289367676\n",
      "training: 17 batch 47 batch_loss: 0.1295311450958252\n",
      "training: 17 batch 48 batch_loss: 0.13352608680725098\n",
      "training: 17 batch 49 batch_loss: 0.13172411918640137\n",
      "training: 17 batch 50 batch_loss: 0.13056808710098267\n",
      "training: 17 batch 51 batch_loss: 0.1322476863861084\n",
      "training: 17 batch 52 batch_loss: 0.13453540205955505\n",
      "training: 17 batch 53 batch_loss: 0.13175636529922485\n",
      "training: 17 batch 54 batch_loss: 0.13012555241584778\n",
      "training: 17 batch 55 batch_loss: 0.13028347492218018\n",
      "training: 17 batch 56 batch_loss: 0.13452526926994324\n",
      "training: 17 batch 57 batch_loss: 0.13300389051437378\n",
      "training: 17 batch 58 batch_loss: 0.12989667057991028\n",
      "training: 17 batch 59 batch_loss: 0.13264882564544678\n",
      "training: 17 batch 60 batch_loss: 0.13271820545196533\n",
      "training: 17 batch 61 batch_loss: 0.13129854202270508\n",
      "training: 17 batch 62 batch_loss: 0.13397538661956787\n",
      "training: 17 batch 63 batch_loss: 0.13068613409996033\n",
      "training: 17 batch 64 batch_loss: 0.12886366248130798\n",
      "training: 17 batch 65 batch_loss: 0.13612812757492065\n",
      "training: 17 batch 66 batch_loss: 0.13079136610031128\n",
      "training: 17 batch 67 batch_loss: 0.13221392035484314\n",
      "training: 17 batch 68 batch_loss: 0.13282325863838196\n",
      "training: 17 batch 69 batch_loss: 0.13125458359718323\n",
      "training: 17 batch 70 batch_loss: 0.132725328207016\n",
      "training: 17 batch 71 batch_loss: 0.13234370946884155\n",
      "training: 17 batch 72 batch_loss: 0.1299959123134613\n",
      "training: 17 batch 73 batch_loss: 0.13167956471443176\n",
      "training: 17 batch 74 batch_loss: 0.1327214241027832\n",
      "training: 17 batch 75 batch_loss: 0.13183242082595825\n",
      "training: 17 batch 76 batch_loss: 0.13209155201911926\n",
      "training: 17 batch 77 batch_loss: 0.13576379418373108\n",
      "training: 17 batch 78 batch_loss: 0.13270694017410278\n",
      "training: 17 batch 79 batch_loss: 0.13295206427574158\n",
      "training: 17 batch 80 batch_loss: 0.1325470507144928\n",
      "training: 17 batch 81 batch_loss: 0.13280147314071655\n",
      "training: 17 batch 82 batch_loss: 0.13112157583236694\n",
      "training: 17 batch 83 batch_loss: 0.13326606154441833\n",
      "training: 17 batch 84 batch_loss: 0.1340121626853943\n",
      "training: 17 batch 85 batch_loss: 0.13304153084754944\n",
      "training: 17 batch 86 batch_loss: 0.13380494713783264\n",
      "training: 17 batch 87 batch_loss: 0.13187628984451294\n",
      "training: 17 batch 88 batch_loss: 0.1319350302219391\n",
      "training: 17 batch 89 batch_loss: 0.13311785459518433\n",
      "training: 17 batch 90 batch_loss: 0.1298573613166809\n",
      "training: 17 batch 91 batch_loss: 0.13225579261779785\n",
      "training: 17 batch 92 batch_loss: 0.13332408666610718\n",
      "training: 17 batch 93 batch_loss: 0.1302647590637207\n",
      "training: 17 batch 94 batch_loss: 0.13300395011901855\n",
      "training: 17 batch 95 batch_loss: 0.13264036178588867\n",
      "training: 17 batch 96 batch_loss: 0.1332733929157257\n",
      "training: 17 batch 97 batch_loss: 0.12995120882987976\n",
      "training: 17 batch 98 batch_loss: 0.12980374693870544\n",
      "training: 17 batch 99 batch_loss: 0.13520807027816772\n",
      "training: 17 batch 100 batch_loss: 0.130156010389328\n",
      "training: 17 batch 101 batch_loss: 0.1305856704711914\n",
      "training: 17 batch 102 batch_loss: 0.1284928023815155\n",
      "training: 17 batch 103 batch_loss: 0.13027551770210266\n",
      "training: 17 batch 104 batch_loss: 0.13018542528152466\n",
      "training: 17 batch 105 batch_loss: 0.133675217628479\n",
      "training: 17 batch 106 batch_loss: 0.13416478037834167\n",
      "training: 17 batch 107 batch_loss: 0.13309088349342346\n",
      "training: 17 batch 108 batch_loss: 0.13207244873046875\n",
      "training: 17 batch 109 batch_loss: 0.13148531317710876\n",
      "training: 17 batch 110 batch_loss: 0.134012371301651\n",
      "training: 17 batch 111 batch_loss: 0.13521912693977356\n",
      "training: 17 batch 112 batch_loss: 0.13104262948036194\n",
      "training: 17 batch 113 batch_loss: 0.13376113772392273\n",
      "training: 17 batch 114 batch_loss: 0.12968462705612183\n",
      "training: 17 batch 115 batch_loss: 0.1324969232082367\n",
      "training: 17 batch 116 batch_loss: 0.13418975472450256\n",
      "training: 17 batch 117 batch_loss: 0.1315983533859253\n",
      "training: 17 batch 118 batch_loss: 0.13234534859657288\n",
      "training: 17 batch 119 batch_loss: 0.13282406330108643\n",
      "training: 17 batch 120 batch_loss: 0.12951761484146118\n",
      "training: 17 batch 121 batch_loss: 0.12843048572540283\n",
      "training: 17 batch 122 batch_loss: 0.13602861762046814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 17 batch 123 batch_loss: 0.133000910282135\n",
      "training: 17 batch 124 batch_loss: 0.13120916485786438\n",
      "training: 17 batch 125 batch_loss: 0.13075828552246094\n",
      "training: 17 batch 126 batch_loss: 0.13375595211982727\n",
      "training: 17 batch 127 batch_loss: 0.12950566411018372\n",
      "training: 17 batch 128 batch_loss: 0.13143545389175415\n",
      "training: 17 batch 129 batch_loss: 0.13529586791992188\n",
      "training: 17 batch 130 batch_loss: 0.1340997815132141\n",
      "training: 17 batch 131 batch_loss: 0.12955233454704285\n",
      "training: 17 batch 132 batch_loss: 0.13462615013122559\n",
      "training: 17 batch 133 batch_loss: 0.13302907347679138\n",
      "training: 17 batch 134 batch_loss: 0.13230794668197632\n",
      "training: 17 batch 135 batch_loss: 0.13233667612075806\n",
      "training: 17 batch 136 batch_loss: 0.13028404116630554\n",
      "training: 17 batch 137 batch_loss: 0.13170424103736877\n",
      "training: 17 batch 138 batch_loss: 0.13431602716445923\n",
      "training: 17 batch 139 batch_loss: 0.13314002752304077\n",
      "training: 17 batch 140 batch_loss: 0.13354137539863586\n",
      "training: 17 batch 141 batch_loss: 0.13277816772460938\n",
      "training: 17 batch 142 batch_loss: 0.1334029734134674\n",
      "training: 17 batch 143 batch_loss: 0.13293004035949707\n",
      "training: 17 batch 144 batch_loss: 0.13270655274391174\n",
      "training: 17 batch 145 batch_loss: 0.13421642780303955\n",
      "training: 17 batch 146 batch_loss: 0.13203144073486328\n",
      "training: 17 batch 147 batch_loss: 0.13266631960868835\n",
      "training: 17 batch 148 batch_loss: 0.13187146186828613\n",
      "training: 17 batch 149 batch_loss: 0.13311845064163208\n",
      "training: 17 batch 150 batch_loss: 0.13361021876335144\n",
      "training: 17 batch 151 batch_loss: 0.13115254044532776\n",
      "training: 17 batch 152 batch_loss: 0.132787823677063\n",
      "training: 17 batch 153 batch_loss: 0.13294345140457153\n",
      "training: 17 batch 154 batch_loss: 0.13488087058067322\n",
      "training: 17 batch 155 batch_loss: 0.1331416666507721\n",
      "training: 17 batch 156 batch_loss: 0.13280385732650757\n",
      "training: 17 batch 157 batch_loss: 0.13204437494277954\n",
      "training: 17 batch 158 batch_loss: 0.13394266366958618\n",
      "training: 17 batch 159 batch_loss: 0.13098400831222534\n",
      "training: 17 batch 160 batch_loss: 0.13377419114112854\n",
      "training: 17 batch 161 batch_loss: 0.13545578718185425\n",
      "training: 17 batch 162 batch_loss: 0.13223570585250854\n",
      "training: 17 batch 163 batch_loss: 0.1334666907787323\n",
      "training: 17 batch 164 batch_loss: 0.13511040806770325\n",
      "training: 17 batch 165 batch_loss: 0.1319294571876526\n",
      "training: 17 batch 166 batch_loss: 0.13324889540672302\n",
      "training: 17 batch 167 batch_loss: 0.13317269086837769\n",
      "training: 17 batch 168 batch_loss: 0.13302037119865417\n",
      "training: 17 batch 169 batch_loss: 0.12890517711639404\n",
      "training: 17 batch 170 batch_loss: 0.13457873463630676\n",
      "training: 17 batch 171 batch_loss: 0.13345512747764587\n",
      "training: 17 batch 172 batch_loss: 0.13707125186920166\n",
      "training: 17 batch 173 batch_loss: 0.1319904923439026\n",
      "training: 17 batch 174 batch_loss: 0.13084399700164795\n",
      "training: 17 batch 175 batch_loss: 0.13269919157028198\n",
      "training: 17 batch 176 batch_loss: 0.12801843881607056\n",
      "training: 17 batch 177 batch_loss: 0.1329134702682495\n",
      "training: 17 batch 178 batch_loss: 0.13226869702339172\n",
      "training: 17 batch 179 batch_loss: 0.13149431347846985\n",
      "training: 17 batch 180 batch_loss: 0.13006895780563354\n",
      "training: 17 batch 181 batch_loss: 0.13160890340805054\n",
      "training: 17 batch 182 batch_loss: 0.1336040496826172\n",
      "training: 17 batch 183 batch_loss: 0.13124316930770874\n",
      "training: 17 batch 184 batch_loss: 0.13315188884735107\n",
      "training: 17 batch 185 batch_loss: 0.13401532173156738\n",
      "training: 17 batch 186 batch_loss: 0.133701890707016\n",
      "training: 17 batch 187 batch_loss: 0.13523101806640625\n",
      "training: 17 batch 188 batch_loss: 0.1318700611591339\n",
      "training: 17 batch 189 batch_loss: 0.13118863105773926\n",
      "training: 17 batch 190 batch_loss: 0.13225814700126648\n",
      "training: 17 batch 191 batch_loss: 0.1350449025630951\n",
      "training: 17 batch 192 batch_loss: 0.13353827595710754\n",
      "training: 17 batch 193 batch_loss: 0.13351446390151978\n",
      "training: 17 batch 194 batch_loss: 0.1306467354297638\n",
      "training: 17 batch 195 batch_loss: 0.13504528999328613\n",
      "training: 17 batch 196 batch_loss: 0.13283896446228027\n",
      "training: 17 batch 197 batch_loss: 0.1331695318222046\n",
      "training: 17 batch 198 batch_loss: 0.13062328100204468\n",
      "training: 17 batch 199 batch_loss: 0.13174143433570862\n",
      "training: 17 batch 200 batch_loss: 0.13301533460617065\n",
      "training: 17 batch 201 batch_loss: 0.1326230764389038\n",
      "training: 17 batch 202 batch_loss: 0.13367989659309387\n",
      "training: 17 batch 203 batch_loss: 0.13229519128799438\n",
      "training: 17 batch 204 batch_loss: 0.13202327489852905\n",
      "training: 17 batch 205 batch_loss: 0.13334527611732483\n",
      "training: 17 batch 206 batch_loss: 0.13632512092590332\n",
      "training: 17 batch 207 batch_loss: 0.1358923614025116\n",
      "training: 17 batch 208 batch_loss: 0.13388949632644653\n",
      "training: 17 batch 209 batch_loss: 0.13186052441596985\n",
      "training: 17 batch 210 batch_loss: 0.13450604677200317\n",
      "training: 17 batch 211 batch_loss: 0.13240379095077515\n",
      "training: 17 batch 212 batch_loss: 0.1323975920677185\n",
      "training: 17 batch 213 batch_loss: 0.13591450452804565\n",
      "training: 17 batch 214 batch_loss: 0.13191872835159302\n",
      "training: 17 batch 215 batch_loss: 0.1325090229511261\n",
      "training: 17 batch 216 batch_loss: 0.13472595810890198\n",
      "training: 17 batch 217 batch_loss: 0.13393563032150269\n",
      "training: 17 batch 218 batch_loss: 0.12986981868743896\n",
      "training: 17 batch 219 batch_loss: 0.13166064023971558\n",
      "training: 17 batch 220 batch_loss: 0.13246169686317444\n",
      "training: 17 batch 221 batch_loss: 0.13453948497772217\n",
      "training: 17 batch 222 batch_loss: 0.13545411825180054\n",
      "training: 17 batch 223 batch_loss: 0.13320204615592957\n",
      "training: 17 batch 224 batch_loss: 0.13146507740020752\n",
      "training: 17 batch 225 batch_loss: 0.13432711362838745\n",
      "training: 17 batch 226 batch_loss: 0.13514164090156555\n",
      "training: 17 batch 227 batch_loss: 0.13199558854103088\n",
      "training: 17 batch 228 batch_loss: 0.13233616948127747\n",
      "training: 17 batch 229 batch_loss: 0.1327185034751892\n",
      "training: 17 batch 230 batch_loss: 0.130354106426239\n",
      "training: 17 batch 231 batch_loss: 0.13212603330612183\n",
      "training: 17 batch 232 batch_loss: 0.13101962208747864\n",
      "training: 17 batch 233 batch_loss: 0.13108789920806885\n",
      "training: 17 batch 234 batch_loss: 0.13471630215644836\n",
      "training: 17 batch 235 batch_loss: 0.13162872195243835\n",
      "training: 17 batch 236 batch_loss: 0.13008573651313782\n",
      "training: 17 batch 237 batch_loss: 0.13168704509735107\n",
      "training: 17 batch 238 batch_loss: 0.13062784075737\n",
      "training: 17 batch 239 batch_loss: 0.13404729962348938\n",
      "training: 17 batch 240 batch_loss: 0.1349848508834839\n",
      "training: 17 batch 241 batch_loss: 0.13281318545341492\n",
      "training: 17 batch 242 batch_loss: 0.13146013021469116\n",
      "training: 17 batch 243 batch_loss: 0.13316991925239563\n",
      "training: 17 batch 244 batch_loss: 0.1336662471294403\n",
      "training: 17 batch 245 batch_loss: 0.13216832280158997\n",
      "training: 17 batch 246 batch_loss: 0.1340322494506836\n",
      "training: 17 batch 247 batch_loss: 0.1363779604434967\n",
      "training: 17 batch 248 batch_loss: 0.1340508759021759\n",
      "training: 17 batch 249 batch_loss: 0.1317659318447113\n",
      "training: 17 batch 250 batch_loss: 0.13517224788665771\n",
      "training: 17 batch 251 batch_loss: 0.13546812534332275\n",
      "training: 17 batch 252 batch_loss: 0.13572612404823303\n",
      "training: 17 batch 253 batch_loss: 0.1311522126197815\n",
      "training: 17 batch 254 batch_loss: 0.1334037482738495\n",
      "training: 17 batch 255 batch_loss: 0.13348692655563354\n",
      "training: 17 batch 256 batch_loss: 0.13498300313949585\n",
      "training: 17 batch 257 batch_loss: 0.13879939913749695\n",
      "training: 17 batch 258 batch_loss: 0.1331755816936493\n",
      "training: 17 batch 259 batch_loss: 0.1348135769367218\n",
      "training: 17 batch 260 batch_loss: 0.13051679730415344\n",
      "training: 17 batch 261 batch_loss: 0.13556578755378723\n",
      "training: 17 batch 262 batch_loss: 0.134740948677063\n",
      "training: 17 batch 263 batch_loss: 0.133690744638443\n",
      "training: 17 batch 264 batch_loss: 0.13140636682510376\n",
      "training: 17 batch 265 batch_loss: 0.13125330209732056\n",
      "training: 17 batch 266 batch_loss: 0.13455188274383545\n",
      "training: 17 batch 267 batch_loss: 0.13237369060516357\n",
      "training: 17 batch 268 batch_loss: 0.13397231698036194\n",
      "training: 17 batch 269 batch_loss: 0.13364335894584656\n",
      "training: 17 batch 270 batch_loss: 0.13410481810569763\n",
      "training: 17 batch 271 batch_loss: 0.13490015268325806\n",
      "training: 17 batch 272 batch_loss: 0.13500240445137024\n",
      "training: 17 batch 273 batch_loss: 0.13241314888000488\n",
      "training: 17 batch 274 batch_loss: 0.13428810238838196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 17 batch 275 batch_loss: 0.13431543111801147\n",
      "training: 17 batch 276 batch_loss: 0.13366490602493286\n",
      "training: 17 batch 277 batch_loss: 0.13274335861206055\n",
      "training: 17 batch 278 batch_loss: 0.13565555214881897\n",
      "training: 17 batch 279 batch_loss: 0.1357414424419403\n",
      "training: 17 batch 280 batch_loss: 0.13365978002548218\n",
      "training: 17 batch 281 batch_loss: 0.13064917922019958\n",
      "training: 17 batch 282 batch_loss: 0.13059982657432556\n",
      "training: 17 batch 283 batch_loss: 0.13154631853103638\n",
      "training: 17 batch 284 batch_loss: 0.13540658354759216\n",
      "training: 17 batch 285 batch_loss: 0.13324615359306335\n",
      "training: 17 batch 286 batch_loss: 0.12925386428833008\n",
      "training: 17 batch 287 batch_loss: 0.13176828622817993\n",
      "training: 17 batch 288 batch_loss: 0.13660573959350586\n",
      "training: 17 batch 289 batch_loss: 0.13530677556991577\n",
      "training: 17 batch 290 batch_loss: 0.12905585765838623\n",
      "training: 17 batch 291 batch_loss: 0.13527321815490723\n",
      "training: 17 batch 292 batch_loss: 0.13115784525871277\n",
      "training: 17 batch 293 batch_loss: 0.13219201564788818\n",
      "training: 17 batch 294 batch_loss: 0.13087016344070435\n",
      "training: 17 batch 295 batch_loss: 0.13242700695991516\n",
      "training: 17 batch 296 batch_loss: 0.13484352827072144\n",
      "training: 17 batch 297 batch_loss: 0.13656294345855713\n",
      "training: 17 batch 298 batch_loss: 0.13226577639579773\n",
      "training: 17 batch 299 batch_loss: 0.1365717053413391\n",
      "training: 17 batch 300 batch_loss: 0.13274770975112915\n",
      "training: 17 batch 301 batch_loss: 0.1344430148601532\n",
      "training: 17 batch 302 batch_loss: 0.13249501585960388\n",
      "training: 17 batch 303 batch_loss: 0.13172733783721924\n",
      "training: 17 batch 304 batch_loss: 0.13083311915397644\n",
      "training: 17 batch 305 batch_loss: 0.13445809483528137\n",
      "training: 17 batch 306 batch_loss: 0.13269758224487305\n",
      "training: 17 batch 307 batch_loss: 0.13539400696754456\n",
      "training: 17 batch 308 batch_loss: 0.1331140697002411\n",
      "training: 17 batch 309 batch_loss: 0.13279908895492554\n",
      "training: 17 batch 310 batch_loss: 0.13351264595985413\n",
      "training: 17 batch 311 batch_loss: 0.1336856484413147\n",
      "training: 17 batch 312 batch_loss: 0.1354946494102478\n",
      "training: 17 batch 313 batch_loss: 0.1335252821445465\n",
      "training: 17 batch 314 batch_loss: 0.13614970445632935\n",
      "training: 17 batch 315 batch_loss: 0.13216039538383484\n",
      "training: 17 batch 316 batch_loss: 0.13141843676567078\n",
      "training: 17 batch 317 batch_loss: 0.13439533114433289\n",
      "training: 17 batch 318 batch_loss: 0.13273242115974426\n",
      "training: 17 batch 319 batch_loss: 0.13589689135551453\n",
      "training: 17 batch 320 batch_loss: 0.1356293261051178\n",
      "training: 17 batch 321 batch_loss: 0.13518363237380981\n",
      "training: 17 batch 322 batch_loss: 0.13533127307891846\n",
      "training: 17 batch 323 batch_loss: 0.13459578156471252\n",
      "training: 17 batch 324 batch_loss: 0.13354560732841492\n",
      "training: 17 batch 325 batch_loss: 0.1302393674850464\n",
      "training: 17 batch 326 batch_loss: 0.133399099111557\n",
      "training: 17 batch 327 batch_loss: 0.13430291414260864\n",
      "training: 17 batch 328 batch_loss: 0.13447526097297668\n",
      "training: 17 batch 329 batch_loss: 0.1321394443511963\n",
      "training: 17 batch 330 batch_loss: 0.13316431641578674\n",
      "training: 17 batch 331 batch_loss: 0.1332625150680542\n",
      "training: 17 batch 332 batch_loss: 0.13489317893981934\n",
      "training: 17 batch 333 batch_loss: 0.13400739431381226\n",
      "training: 17 batch 334 batch_loss: 0.13557079434394836\n",
      "training: 17 batch 335 batch_loss: 0.13303685188293457\n",
      "training: 17 batch 336 batch_loss: 0.13637739419937134\n",
      "training: 17 batch 337 batch_loss: 0.13798508048057556\n",
      "training: 17 batch 338 batch_loss: 0.13229313492774963\n",
      "training: 17 batch 339 batch_loss: 0.13390907645225525\n",
      "training: 17 batch 340 batch_loss: 0.13404056429862976\n",
      "training: 17 batch 341 batch_loss: 0.13258647918701172\n",
      "training: 17 batch 342 batch_loss: 0.13183709979057312\n",
      "training: 17 batch 343 batch_loss: 0.13407573103904724\n",
      "training: 17 batch 344 batch_loss: 0.133611798286438\n",
      "training: 17 batch 345 batch_loss: 0.12938711047172546\n",
      "training: 17 batch 346 batch_loss: 0.12788590788841248\n",
      "training: 17 batch 347 batch_loss: 0.13410231471061707\n",
      "training: 17 batch 348 batch_loss: 0.13016092777252197\n",
      "training: 17 batch 349 batch_loss: 0.13490813970565796\n",
      "training: 17 batch 350 batch_loss: 0.13414087891578674\n",
      "training: 17 batch 351 batch_loss: 0.13519948720932007\n",
      "training: 17 batch 352 batch_loss: 0.13442784547805786\n",
      "training: 17 batch 353 batch_loss: 0.13410452008247375\n",
      "training: 17 batch 354 batch_loss: 0.13466420769691467\n",
      "training: 17 batch 355 batch_loss: 0.13567614555358887\n",
      "training: 17 batch 356 batch_loss: 0.13352257013320923\n",
      "training: 17 batch 357 batch_loss: 0.1337166428565979\n",
      "training: 17 batch 358 batch_loss: 0.13517802953720093\n",
      "training: 17 batch 359 batch_loss: 0.1321345865726471\n",
      "training: 17 batch 360 batch_loss: 0.13659101724624634\n",
      "training: 17 batch 361 batch_loss: 0.1331847906112671\n",
      "training: 17 batch 362 batch_loss: 0.13570335507392883\n",
      "training: 17 batch 363 batch_loss: 0.1337776482105255\n",
      "training: 17 batch 364 batch_loss: 0.13306602835655212\n",
      "training: 17 batch 365 batch_loss: 0.13376367092132568\n",
      "training: 17 batch 366 batch_loss: 0.13506853580474854\n",
      "training: 17 batch 367 batch_loss: 0.13185691833496094\n",
      "training: 17 batch 368 batch_loss: 0.131170392036438\n",
      "training: 17 batch 369 batch_loss: 0.14102035760879517\n",
      "training: 17 batch 370 batch_loss: 0.13648998737335205\n",
      "training: 17 batch 371 batch_loss: 0.13644513487815857\n",
      "training: 17 batch 372 batch_loss: 0.13513413071632385\n",
      "training: 17 batch 373 batch_loss: 0.13469380140304565\n",
      "training: 17 batch 374 batch_loss: 0.13245418667793274\n",
      "training: 17 batch 375 batch_loss: 0.13519778847694397\n",
      "training: 17 batch 376 batch_loss: 0.12858542799949646\n",
      "training: 17 batch 377 batch_loss: 0.13612240552902222\n",
      "training: 17 batch 378 batch_loss: 0.13650855422019958\n",
      "training: 17 batch 379 batch_loss: 0.1364843249320984\n",
      "training: 17 batch 380 batch_loss: 0.13400274515151978\n",
      "training: 17 batch 381 batch_loss: 0.13503900170326233\n",
      "training: 17 batch 382 batch_loss: 0.13433092832565308\n",
      "training: 17 batch 383 batch_loss: 0.13650107383728027\n",
      "training: 17 batch 384 batch_loss: 0.12878721952438354\n",
      "training: 17 batch 385 batch_loss: 0.13288429379463196\n",
      "training: 17 batch 386 batch_loss: 0.13070666790008545\n",
      "training: 17 batch 387 batch_loss: 0.13359996676445007\n",
      "training: 17 batch 388 batch_loss: 0.13283240795135498\n",
      "training: 17 batch 389 batch_loss: 0.13467824459075928\n",
      "training: 17 batch 390 batch_loss: 0.13293546438217163\n",
      "training: 17 batch 391 batch_loss: 0.13247796893119812\n",
      "training: 17 batch 392 batch_loss: 0.1340641975402832\n",
      "training: 17 batch 393 batch_loss: 0.13433822989463806\n",
      "training: 17 batch 394 batch_loss: 0.13503050804138184\n",
      "training: 17 batch 395 batch_loss: 0.13423648476600647\n",
      "training: 17 batch 396 batch_loss: 0.13148587942123413\n",
      "training: 17 batch 397 batch_loss: 0.1342330276966095\n",
      "training: 17 batch 398 batch_loss: 0.13558083772659302\n",
      "training: 17 batch 399 batch_loss: 0.1373799443244934\n",
      "training: 17 batch 400 batch_loss: 0.13366329669952393\n",
      "training: 17 batch 401 batch_loss: 0.13389119505882263\n",
      "training: 17 batch 402 batch_loss: 0.13351958990097046\n",
      "training: 17 batch 403 batch_loss: 0.13616284728050232\n",
      "training: 17 batch 404 batch_loss: 0.13205629587173462\n",
      "training: 17 batch 405 batch_loss: 0.13193532824516296\n",
      "training: 17 batch 406 batch_loss: 0.13633954524993896\n",
      "training: 17 batch 407 batch_loss: 0.1336686909198761\n",
      "training: 17 batch 408 batch_loss: 0.1347648799419403\n",
      "training: 17 batch 409 batch_loss: 0.1356140375137329\n",
      "training: 17 batch 410 batch_loss: 0.1352563500404358\n",
      "training: 17 batch 411 batch_loss: 0.13206720352172852\n",
      "training: 17 batch 412 batch_loss: 0.13393577933311462\n",
      "training: 17 batch 413 batch_loss: 0.13521748781204224\n",
      "training: 17 batch 414 batch_loss: 0.13687840104103088\n",
      "training: 17 batch 415 batch_loss: 0.1342024803161621\n",
      "training: 17 batch 416 batch_loss: 0.13384240865707397\n",
      "training: 17 batch 417 batch_loss: 0.13156679272651672\n",
      "training: 17 batch 418 batch_loss: 0.13452807068824768\n",
      "training: 17 batch 419 batch_loss: 0.13175728917121887\n",
      "training: 17 batch 420 batch_loss: 0.1336553394794464\n",
      "training: 17 batch 421 batch_loss: 0.13696810603141785\n",
      "training: 17 batch 422 batch_loss: 0.13098719716072083\n",
      "training: 17 batch 423 batch_loss: 0.13442018628120422\n",
      "training: 17 batch 424 batch_loss: 0.13342997431755066\n",
      "training: 17 batch 425 batch_loss: 0.13500410318374634\n",
      "training: 17 batch 426 batch_loss: 0.13276630640029907\n",
      "training: 17 batch 427 batch_loss: 0.13331907987594604\n",
      "training: 17 batch 428 batch_loss: 0.1328912079334259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 17 batch 429 batch_loss: 0.13231161236763\n",
      "training: 17 batch 430 batch_loss: 0.1324554681777954\n",
      "training: 17 batch 431 batch_loss: 0.13625213503837585\n",
      "training: 17 batch 432 batch_loss: 0.13533610105514526\n",
      "training: 17 batch 433 batch_loss: 0.13215020298957825\n",
      "training: 17 batch 434 batch_loss: 0.13428619503974915\n",
      "training: 17 batch 435 batch_loss: 0.1347833275794983\n",
      "training: 17 batch 436 batch_loss: 0.13143795728683472\n",
      "training: 17 batch 437 batch_loss: 0.1372397541999817\n",
      "training: 17 batch 438 batch_loss: 0.13919252157211304\n",
      "training: 17 batch 439 batch_loss: 0.1397455334663391\n",
      "training: 17 batch 440 batch_loss: 0.1316830813884735\n",
      "training: 17 batch 441 batch_loss: 0.13674446940422058\n",
      "training: 17 batch 442 batch_loss: 0.13588359951972961\n",
      "training: 17 batch 443 batch_loss: 0.132892906665802\n",
      "training: 17 batch 444 batch_loss: 0.1319960355758667\n",
      "training: 17 batch 445 batch_loss: 0.13361358642578125\n",
      "training: 17 batch 446 batch_loss: 0.13338539004325867\n",
      "training: 17 batch 447 batch_loss: 0.13412058353424072\n",
      "training: 17 batch 448 batch_loss: 0.13351663947105408\n",
      "training: 17 batch 449 batch_loss: 0.13744017481803894\n",
      "training: 17 batch 450 batch_loss: 0.13614201545715332\n",
      "training: 17 batch 451 batch_loss: 0.1322740912437439\n",
      "training: 17 batch 452 batch_loss: 0.13408982753753662\n",
      "training: 17 batch 453 batch_loss: 0.13220331072807312\n",
      "training: 17 batch 454 batch_loss: 0.1396443247795105\n",
      "training: 17 batch 455 batch_loss: 0.13507866859436035\n",
      "training: 17 batch 456 batch_loss: 0.1332806944847107\n",
      "training: 17 batch 457 batch_loss: 0.13497838377952576\n",
      "training: 17 batch 458 batch_loss: 0.13643458485603333\n",
      "training: 17 batch 459 batch_loss: 0.13504773378372192\n",
      "training: 17 batch 460 batch_loss: 0.13392961025238037\n",
      "training: 17 batch 461 batch_loss: 0.134789377450943\n",
      "training: 17 batch 462 batch_loss: 0.1364988386631012\n",
      "training: 17 batch 463 batch_loss: 0.1328846514225006\n",
      "training: 17 batch 464 batch_loss: 0.13304904103279114\n",
      "training: 17 batch 465 batch_loss: 0.13416138291358948\n",
      "training: 17 batch 466 batch_loss: 0.13413521647453308\n",
      "training: 17 batch 467 batch_loss: 0.13302955031394958\n",
      "training: 17 batch 468 batch_loss: 0.1362241804599762\n",
      "training: 17 batch 469 batch_loss: 0.13472908735275269\n",
      "training: 17 batch 470 batch_loss: 0.1332327425479889\n",
      "training: 17 batch 471 batch_loss: 0.13506057858467102\n",
      "training: 17 batch 472 batch_loss: 0.13531464338302612\n",
      "training: 17 batch 473 batch_loss: 0.1357528269290924\n",
      "training: 17 batch 474 batch_loss: 0.1330869197845459\n",
      "training: 17 batch 475 batch_loss: 0.13234883546829224\n",
      "training: 17 batch 476 batch_loss: 0.13661900162696838\n",
      "training: 17 batch 477 batch_loss: 0.13365209102630615\n",
      "training: 17 batch 478 batch_loss: 0.13610342144966125\n",
      "training: 17 batch 479 batch_loss: 0.13501468300819397\n",
      "training: 17 batch 480 batch_loss: 0.13305479288101196\n",
      "training: 17 batch 481 batch_loss: 0.1341336965560913\n",
      "training: 17 batch 482 batch_loss: 0.13476544618606567\n",
      "training: 17 batch 483 batch_loss: 0.13889098167419434\n",
      "training: 17 batch 484 batch_loss: 0.13364377617835999\n",
      "training: 17 batch 485 batch_loss: 0.13659831881523132\n",
      "training: 17 batch 486 batch_loss: 0.1344383955001831\n",
      "training: 17 batch 487 batch_loss: 0.13467338681221008\n",
      "training: 17 batch 488 batch_loss: 0.13361528515815735\n",
      "training: 17 batch 489 batch_loss: 0.1347348392009735\n",
      "training: 17 batch 490 batch_loss: 0.134295254945755\n",
      "training: 17 batch 491 batch_loss: 0.1363324224948883\n",
      "training: 17 batch 492 batch_loss: 0.1319519579410553\n",
      "training: 17 batch 493 batch_loss: 0.13491004705429077\n",
      "training: 17 batch 494 batch_loss: 0.1362459361553192\n",
      "training: 17 batch 495 batch_loss: 0.13284626603126526\n",
      "training: 17 batch 496 batch_loss: 0.13015520572662354\n",
      "training: 17 batch 497 batch_loss: 0.13488981127738953\n",
      "training: 17 batch 498 batch_loss: 0.1359773576259613\n",
      "training: 17 batch 499 batch_loss: 0.13722887635231018\n",
      "training: 17 batch 500 batch_loss: 0.13653850555419922\n",
      "training: 17 batch 501 batch_loss: 0.13528397679328918\n",
      "training: 17 batch 502 batch_loss: 0.13578256964683533\n",
      "training: 17 batch 503 batch_loss: 0.1398918628692627\n",
      "training: 17 batch 504 batch_loss: 0.13563746213912964\n",
      "training: 17 batch 505 batch_loss: 0.1365930140018463\n",
      "training: 17 batch 506 batch_loss: 0.13198518753051758\n",
      "training: 17 batch 507 batch_loss: 0.13810431957244873\n",
      "training: 17 batch 508 batch_loss: 0.1337762475013733\n",
      "training: 17 batch 509 batch_loss: 0.13737505674362183\n",
      "training: 17 batch 510 batch_loss: 0.13690876960754395\n",
      "training: 17 batch 511 batch_loss: 0.133578360080719\n",
      "training: 17 batch 512 batch_loss: 0.13435202836990356\n",
      "training: 17 batch 513 batch_loss: 0.13175305724143982\n",
      "training: 17 batch 514 batch_loss: 0.13438603281974792\n",
      "training: 17 batch 515 batch_loss: 0.13496840000152588\n",
      "training: 17 batch 516 batch_loss: 0.13276126980781555\n",
      "training: 17 batch 517 batch_loss: 0.13714978098869324\n",
      "training: 17 batch 518 batch_loss: 0.1323087215423584\n",
      "training: 17 batch 519 batch_loss: 0.13200226426124573\n",
      "training: 17 batch 520 batch_loss: 0.13253793120384216\n",
      "training: 17 batch 521 batch_loss: 0.1358625292778015\n",
      "training: 17 batch 522 batch_loss: 0.13582372665405273\n",
      "training: 17 batch 523 batch_loss: 0.13762563467025757\n",
      "training: 17 batch 524 batch_loss: 0.1362171471118927\n",
      "training: 17 batch 525 batch_loss: 0.13672396540641785\n",
      "training: 17 batch 526 batch_loss: 0.13595446944236755\n",
      "training: 17 batch 527 batch_loss: 0.1347905397415161\n",
      "training: 17 batch 528 batch_loss: 0.13616561889648438\n",
      "training: 17 batch 529 batch_loss: 0.13811591267585754\n",
      "training: 17 batch 530 batch_loss: 0.1335771381855011\n",
      "training: 17 batch 531 batch_loss: 0.13657855987548828\n",
      "training: 17 batch 532 batch_loss: 0.1350489854812622\n",
      "training: 17 batch 533 batch_loss: 0.13596045970916748\n",
      "training: 17 batch 534 batch_loss: 0.13465672731399536\n",
      "training: 17 batch 535 batch_loss: 0.13700321316719055\n",
      "training: 17 batch 536 batch_loss: 0.13348525762557983\n",
      "training: 17 batch 537 batch_loss: 0.13527214527130127\n",
      "training: 17 batch 538 batch_loss: 0.1353532373905182\n",
      "training: 17 batch 539 batch_loss: 0.13668280839920044\n",
      "training: 17 batch 540 batch_loss: 0.1371687948703766\n",
      "training: 17 batch 541 batch_loss: 0.1340399980545044\n",
      "training: 17 batch 542 batch_loss: 0.13670668005943298\n",
      "training: 17 batch 543 batch_loss: 0.13376224040985107\n",
      "training: 17 batch 544 batch_loss: 0.13498669862747192\n",
      "training: 17 batch 545 batch_loss: 0.1357024610042572\n",
      "training: 17 batch 546 batch_loss: 0.13453739881515503\n",
      "training: 17 batch 547 batch_loss: 0.13487794995307922\n",
      "training: 17 batch 548 batch_loss: 0.13635575771331787\n",
      "training: 17 batch 549 batch_loss: 0.13673773407936096\n",
      "training: 17 batch 550 batch_loss: 0.1351766288280487\n",
      "training: 17 batch 551 batch_loss: 0.13495111465454102\n",
      "training: 17 batch 552 batch_loss: 0.1341446042060852\n",
      "training: 17 batch 553 batch_loss: 0.1338648498058319\n",
      "training: 17 batch 554 batch_loss: 0.13602828979492188\n",
      "training: 17 batch 555 batch_loss: 0.13464117050170898\n",
      "training: 17 batch 556 batch_loss: 0.13464480638504028\n",
      "training: 17 batch 557 batch_loss: 0.13429945707321167\n",
      "training: 17 batch 558 batch_loss: 0.13506942987442017\n",
      "training: 17 batch 559 batch_loss: 0.13546070456504822\n",
      "training: 17 batch 560 batch_loss: 0.13416239619255066\n",
      "training: 17 batch 561 batch_loss: 0.13672024011611938\n",
      "training: 17 batch 562 batch_loss: 0.1350407600402832\n",
      "training: 17 batch 563 batch_loss: 0.13902083039283752\n",
      "training: 17 batch 564 batch_loss: 0.13298383355140686\n",
      "training: 17 batch 565 batch_loss: 0.13351309299468994\n",
      "training: 17 batch 566 batch_loss: 0.13458967208862305\n",
      "training: 17 batch 567 batch_loss: 0.13534986972808838\n",
      "training: 17 batch 568 batch_loss: 0.13526985049247742\n",
      "training: 17 batch 569 batch_loss: 0.13566875457763672\n",
      "training: 17 batch 570 batch_loss: 0.13599920272827148\n",
      "training: 17 batch 571 batch_loss: 0.1342562437057495\n",
      "training: 17 batch 572 batch_loss: 0.1332544982433319\n",
      "training: 17 batch 573 batch_loss: 0.1327146589756012\n",
      "training: 17 batch 574 batch_loss: 0.13505041599273682\n",
      "training: 17 batch 575 batch_loss: 0.1318071484565735\n",
      "training: 17 batch 576 batch_loss: 0.1358695924282074\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 17, Hit Ratio:0.026043245512053913 | Precision:0.03842524329106458 | Recall:0.05362035382891245 | NDCG:0.04867840126701081\n",
      "*Best Performance* \n",
      "Epoch: 17, Hit Ratio:0.026043245512053913 | Precision:0.03842524329106458 | Recall:0.05362035382891245 | MDCG:0.04867840126701081\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 18 batch 0 batch_loss: 0.13424623012542725\n",
      "training: 18 batch 1 batch_loss: 0.1350916624069214\n",
      "training: 18 batch 2 batch_loss: 0.13329234719276428\n",
      "training: 18 batch 3 batch_loss: 0.13364508748054504\n",
      "training: 18 batch 4 batch_loss: 0.13328301906585693\n",
      "training: 18 batch 5 batch_loss: 0.13265880942344666\n",
      "training: 18 batch 6 batch_loss: 0.1380128264427185\n",
      "training: 18 batch 7 batch_loss: 0.13179326057434082\n",
      "training: 18 batch 8 batch_loss: 0.13542291522026062\n",
      "training: 18 batch 9 batch_loss: 0.13358071446418762\n",
      "training: 18 batch 10 batch_loss: 0.13360285758972168\n",
      "training: 18 batch 11 batch_loss: 0.1368366777896881\n",
      "training: 18 batch 12 batch_loss: 0.13339784741401672\n",
      "training: 18 batch 13 batch_loss: 0.13478171825408936\n",
      "training: 18 batch 14 batch_loss: 0.13177216053009033\n",
      "training: 18 batch 15 batch_loss: 0.13539621233940125\n",
      "training: 18 batch 16 batch_loss: 0.1312500238418579\n",
      "training: 18 batch 17 batch_loss: 0.13439717888832092\n",
      "training: 18 batch 18 batch_loss: 0.13263994455337524\n",
      "training: 18 batch 19 batch_loss: 0.13467058539390564\n",
      "training: 18 batch 20 batch_loss: 0.13536256551742554\n",
      "training: 18 batch 21 batch_loss: 0.1348857879638672\n",
      "training: 18 batch 22 batch_loss: 0.1324097216129303\n",
      "training: 18 batch 23 batch_loss: 0.13438329100608826\n",
      "training: 18 batch 24 batch_loss: 0.13031676411628723\n",
      "training: 18 batch 25 batch_loss: 0.13002505898475647\n",
      "training: 18 batch 26 batch_loss: 0.13209208846092224\n",
      "training: 18 batch 27 batch_loss: 0.13182523846626282\n",
      "training: 18 batch 28 batch_loss: 0.13248741626739502\n",
      "training: 18 batch 29 batch_loss: 0.13132032752037048\n",
      "training: 18 batch 30 batch_loss: 0.13288426399230957\n",
      "training: 18 batch 31 batch_loss: 0.13214093446731567\n",
      "training: 18 batch 32 batch_loss: 0.13423269987106323\n",
      "training: 18 batch 33 batch_loss: 0.13706210255622864\n",
      "training: 18 batch 34 batch_loss: 0.13348472118377686\n",
      "training: 18 batch 35 batch_loss: 0.13542437553405762\n",
      "training: 18 batch 36 batch_loss: 0.13378769159317017\n",
      "training: 18 batch 37 batch_loss: 0.13426196575164795\n",
      "training: 18 batch 38 batch_loss: 0.13376489281654358\n",
      "training: 18 batch 39 batch_loss: 0.13573530316352844\n",
      "training: 18 batch 40 batch_loss: 0.13200396299362183\n",
      "training: 18 batch 41 batch_loss: 0.13423001766204834\n",
      "training: 18 batch 42 batch_loss: 0.13469725847244263\n",
      "training: 18 batch 43 batch_loss: 0.1336727738380432\n",
      "training: 18 batch 44 batch_loss: 0.13574647903442383\n",
      "training: 18 batch 45 batch_loss: 0.13142842054367065\n",
      "training: 18 batch 46 batch_loss: 0.13588643074035645\n",
      "training: 18 batch 47 batch_loss: 0.13328230381011963\n",
      "training: 18 batch 48 batch_loss: 0.13267916440963745\n",
      "training: 18 batch 49 batch_loss: 0.13370466232299805\n",
      "training: 18 batch 50 batch_loss: 0.1329582929611206\n",
      "training: 18 batch 51 batch_loss: 0.13594645261764526\n",
      "training: 18 batch 52 batch_loss: 0.1357887089252472\n",
      "training: 18 batch 53 batch_loss: 0.13392239809036255\n",
      "training: 18 batch 54 batch_loss: 0.13105186820030212\n",
      "training: 18 batch 55 batch_loss: 0.13436269760131836\n",
      "training: 18 batch 56 batch_loss: 0.13150078058242798\n",
      "training: 18 batch 57 batch_loss: 0.13363012671470642\n",
      "training: 18 batch 58 batch_loss: 0.13253647089004517\n",
      "training: 18 batch 59 batch_loss: 0.1336228847503662\n",
      "training: 18 batch 60 batch_loss: 0.13287976384162903\n",
      "training: 18 batch 61 batch_loss: 0.1330026090145111\n",
      "training: 18 batch 62 batch_loss: 0.1355525553226471\n",
      "training: 18 batch 63 batch_loss: 0.13390648365020752\n",
      "training: 18 batch 64 batch_loss: 0.1320955753326416\n",
      "training: 18 batch 65 batch_loss: 0.1351359784603119\n",
      "training: 18 batch 66 batch_loss: 0.1355595588684082\n",
      "training: 18 batch 67 batch_loss: 0.13482826948165894\n",
      "training: 18 batch 68 batch_loss: 0.1323237121105194\n",
      "training: 18 batch 69 batch_loss: 0.13360118865966797\n",
      "training: 18 batch 70 batch_loss: 0.13196370005607605\n",
      "training: 18 batch 71 batch_loss: 0.13472309708595276\n",
      "training: 18 batch 72 batch_loss: 0.1365983486175537\n",
      "training: 18 batch 73 batch_loss: 0.13442537188529968\n",
      "training: 18 batch 74 batch_loss: 0.13543325662612915\n",
      "training: 18 batch 75 batch_loss: 0.13864955306053162\n",
      "training: 18 batch 76 batch_loss: 0.13313844799995422\n",
      "training: 18 batch 77 batch_loss: 0.1324867308139801\n",
      "training: 18 batch 78 batch_loss: 0.13368907570838928\n",
      "training: 18 batch 79 batch_loss: 0.13672855496406555\n",
      "training: 18 batch 80 batch_loss: 0.1347673535346985\n",
      "training: 18 batch 81 batch_loss: 0.13734254240989685\n",
      "training: 18 batch 82 batch_loss: 0.13185831904411316\n",
      "training: 18 batch 83 batch_loss: 0.13508448004722595\n",
      "training: 18 batch 84 batch_loss: 0.13263559341430664\n",
      "training: 18 batch 85 batch_loss: 0.1362578272819519\n",
      "training: 18 batch 86 batch_loss: 0.1325395107269287\n",
      "training: 18 batch 87 batch_loss: 0.1334603726863861\n",
      "training: 18 batch 88 batch_loss: 0.13543978333473206\n",
      "training: 18 batch 89 batch_loss: 0.13684329390525818\n",
      "training: 18 batch 90 batch_loss: 0.1338675320148468\n",
      "training: 18 batch 91 batch_loss: 0.13391873240470886\n",
      "training: 18 batch 92 batch_loss: 0.13283267617225647\n",
      "training: 18 batch 93 batch_loss: 0.1324973702430725\n",
      "training: 18 batch 94 batch_loss: 0.1329711377620697\n",
      "training: 18 batch 95 batch_loss: 0.1306195855140686\n",
      "training: 18 batch 96 batch_loss: 0.132584810256958\n",
      "training: 18 batch 97 batch_loss: 0.13757622241973877\n",
      "training: 18 batch 98 batch_loss: 0.13571378588676453\n",
      "training: 18 batch 99 batch_loss: 0.13278689980506897\n",
      "training: 18 batch 100 batch_loss: 0.1340561807155609\n",
      "training: 18 batch 101 batch_loss: 0.1343872845172882\n",
      "training: 18 batch 102 batch_loss: 0.13561436533927917\n",
      "training: 18 batch 103 batch_loss: 0.13496598601341248\n",
      "training: 18 batch 104 batch_loss: 0.13561734557151794\n",
      "training: 18 batch 105 batch_loss: 0.1343315839767456\n",
      "training: 18 batch 106 batch_loss: 0.13546675443649292\n",
      "training: 18 batch 107 batch_loss: 0.13516205549240112\n",
      "training: 18 batch 108 batch_loss: 0.1368444263935089\n",
      "training: 18 batch 109 batch_loss: 0.13497376441955566\n",
      "training: 18 batch 110 batch_loss: 0.13298818469047546\n",
      "training: 18 batch 111 batch_loss: 0.1349942684173584\n",
      "training: 18 batch 112 batch_loss: 0.1339326798915863\n",
      "training: 18 batch 113 batch_loss: 0.1384274959564209\n",
      "training: 18 batch 114 batch_loss: 0.13398611545562744\n",
      "training: 18 batch 115 batch_loss: 0.13321805000305176\n",
      "training: 18 batch 116 batch_loss: 0.1331651210784912\n",
      "training: 18 batch 117 batch_loss: 0.13532677292823792\n",
      "training: 18 batch 118 batch_loss: 0.13552796840667725\n",
      "training: 18 batch 119 batch_loss: 0.13639190793037415\n",
      "training: 18 batch 120 batch_loss: 0.13391029834747314\n",
      "training: 18 batch 121 batch_loss: 0.1338062286376953\n",
      "training: 18 batch 122 batch_loss: 0.13630706071853638\n",
      "training: 18 batch 123 batch_loss: 0.13682681322097778\n",
      "training: 18 batch 124 batch_loss: 0.1338876187801361\n",
      "training: 18 batch 125 batch_loss: 0.13216477632522583\n",
      "training: 18 batch 126 batch_loss: 0.13626420497894287\n",
      "training: 18 batch 127 batch_loss: 0.13466626405715942\n",
      "training: 18 batch 128 batch_loss: 0.13602280616760254\n",
      "training: 18 batch 129 batch_loss: 0.13252738118171692\n",
      "training: 18 batch 130 batch_loss: 0.13412323594093323\n",
      "training: 18 batch 131 batch_loss: 0.13424092531204224\n",
      "training: 18 batch 132 batch_loss: 0.13559120893478394\n",
      "training: 18 batch 133 batch_loss: 0.13499704003334045\n",
      "training: 18 batch 134 batch_loss: 0.1331217885017395\n",
      "training: 18 batch 135 batch_loss: 0.1360430121421814\n",
      "training: 18 batch 136 batch_loss: 0.13369739055633545\n",
      "training: 18 batch 137 batch_loss: 0.1345841884613037\n",
      "training: 18 batch 138 batch_loss: 0.13466113805770874\n",
      "training: 18 batch 139 batch_loss: 0.13688772916793823\n",
      "training: 18 batch 140 batch_loss: 0.13627588748931885\n",
      "training: 18 batch 141 batch_loss: 0.13233458995819092\n",
      "training: 18 batch 142 batch_loss: 0.13336867094039917\n",
      "training: 18 batch 143 batch_loss: 0.133564293384552\n",
      "training: 18 batch 144 batch_loss: 0.13519972562789917\n",
      "training: 18 batch 145 batch_loss: 0.13638600707054138\n",
      "training: 18 batch 146 batch_loss: 0.13752275705337524\n",
      "training: 18 batch 147 batch_loss: 0.13958781957626343\n",
      "training: 18 batch 148 batch_loss: 0.13354116678237915\n",
      "training: 18 batch 149 batch_loss: 0.1363145411014557\n",
      "training: 18 batch 150 batch_loss: 0.13527023792266846\n",
      "training: 18 batch 151 batch_loss: 0.1327800452709198\n",
      "training: 18 batch 152 batch_loss: 0.1364850103855133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 18 batch 153 batch_loss: 0.1347055733203888\n",
      "training: 18 batch 154 batch_loss: 0.1324564814567566\n",
      "training: 18 batch 155 batch_loss: 0.13602852821350098\n",
      "training: 18 batch 156 batch_loss: 0.13593196868896484\n",
      "training: 18 batch 157 batch_loss: 0.13508069515228271\n",
      "training: 18 batch 158 batch_loss: 0.13826632499694824\n",
      "training: 18 batch 159 batch_loss: 0.13437047600746155\n",
      "training: 18 batch 160 batch_loss: 0.13691037893295288\n",
      "training: 18 batch 161 batch_loss: 0.13574069738388062\n",
      "training: 18 batch 162 batch_loss: 0.13652676343917847\n",
      "training: 18 batch 163 batch_loss: 0.1359724998474121\n",
      "training: 18 batch 164 batch_loss: 0.13379600644111633\n",
      "training: 18 batch 165 batch_loss: 0.13396036624908447\n",
      "training: 18 batch 166 batch_loss: 0.13582101464271545\n",
      "training: 18 batch 167 batch_loss: 0.13657215237617493\n",
      "training: 18 batch 168 batch_loss: 0.13417130708694458\n",
      "training: 18 batch 169 batch_loss: 0.13742485642433167\n",
      "training: 18 batch 170 batch_loss: 0.13380542397499084\n",
      "training: 18 batch 171 batch_loss: 0.13250795006752014\n",
      "training: 18 batch 172 batch_loss: 0.13344159722328186\n",
      "training: 18 batch 173 batch_loss: 0.13898441195487976\n",
      "training: 18 batch 174 batch_loss: 0.13822442293167114\n",
      "training: 18 batch 175 batch_loss: 0.13560152053833008\n",
      "training: 18 batch 176 batch_loss: 0.13712313771247864\n",
      "training: 18 batch 177 batch_loss: 0.13534915447235107\n",
      "training: 18 batch 178 batch_loss: 0.13498276472091675\n",
      "training: 18 batch 179 batch_loss: 0.13723796606063843\n",
      "training: 18 batch 180 batch_loss: 0.13321790099143982\n",
      "training: 18 batch 181 batch_loss: 0.13616666197776794\n",
      "training: 18 batch 182 batch_loss: 0.13472411036491394\n",
      "training: 18 batch 183 batch_loss: 0.13248905539512634\n",
      "training: 18 batch 184 batch_loss: 0.13478302955627441\n",
      "training: 18 batch 185 batch_loss: 0.1353464424610138\n",
      "training: 18 batch 186 batch_loss: 0.13437393307685852\n",
      "training: 18 batch 187 batch_loss: 0.13438087701797485\n",
      "training: 18 batch 188 batch_loss: 0.13499519228935242\n",
      "training: 18 batch 189 batch_loss: 0.13430434465408325\n",
      "training: 18 batch 190 batch_loss: 0.13398683071136475\n",
      "training: 18 batch 191 batch_loss: 0.13374832272529602\n",
      "training: 18 batch 192 batch_loss: 0.13579630851745605\n",
      "training: 18 batch 193 batch_loss: 0.13287103176116943\n",
      "training: 18 batch 194 batch_loss: 0.13428696990013123\n",
      "training: 18 batch 195 batch_loss: 0.13544508814811707\n",
      "training: 18 batch 196 batch_loss: 0.13330823183059692\n",
      "training: 18 batch 197 batch_loss: 0.1386500597000122\n",
      "training: 18 batch 198 batch_loss: 0.13669535517692566\n",
      "training: 18 batch 199 batch_loss: 0.13567563891410828\n",
      "training: 18 batch 200 batch_loss: 0.13666224479675293\n",
      "training: 18 batch 201 batch_loss: 0.13234379887580872\n",
      "training: 18 batch 202 batch_loss: 0.13682878017425537\n",
      "training: 18 batch 203 batch_loss: 0.13256996870040894\n",
      "training: 18 batch 204 batch_loss: 0.13538429141044617\n",
      "training: 18 batch 205 batch_loss: 0.13650932908058167\n",
      "training: 18 batch 206 batch_loss: 0.135623037815094\n",
      "training: 18 batch 207 batch_loss: 0.136430561542511\n",
      "training: 18 batch 208 batch_loss: 0.13514766097068787\n",
      "training: 18 batch 209 batch_loss: 0.13356032967567444\n",
      "training: 18 batch 210 batch_loss: 0.13259288668632507\n",
      "training: 18 batch 211 batch_loss: 0.1356072723865509\n",
      "training: 18 batch 212 batch_loss: 0.13561642169952393\n",
      "training: 18 batch 213 batch_loss: 0.13765868544578552\n",
      "training: 18 batch 214 batch_loss: 0.13772404193878174\n",
      "training: 18 batch 215 batch_loss: 0.1311662793159485\n",
      "training: 18 batch 216 batch_loss: 0.13737961649894714\n",
      "training: 18 batch 217 batch_loss: 0.13264593482017517\n",
      "training: 18 batch 218 batch_loss: 0.13493186235427856\n",
      "training: 18 batch 219 batch_loss: 0.13711559772491455\n",
      "training: 18 batch 220 batch_loss: 0.1367875635623932\n",
      "training: 18 batch 221 batch_loss: 0.1365332305431366\n",
      "training: 18 batch 222 batch_loss: 0.1354638934135437\n",
      "training: 18 batch 223 batch_loss: 0.13534307479858398\n",
      "training: 18 batch 224 batch_loss: 0.13113951683044434\n",
      "training: 18 batch 225 batch_loss: 0.1347876787185669\n",
      "training: 18 batch 226 batch_loss: 0.13676416873931885\n",
      "training: 18 batch 227 batch_loss: 0.13570243120193481\n",
      "training: 18 batch 228 batch_loss: 0.13486117124557495\n",
      "training: 18 batch 229 batch_loss: 0.1349170207977295\n",
      "training: 18 batch 230 batch_loss: 0.1369330883026123\n",
      "training: 18 batch 231 batch_loss: 0.13659656047821045\n",
      "training: 18 batch 232 batch_loss: 0.13558095693588257\n",
      "training: 18 batch 233 batch_loss: 0.13770684599876404\n",
      "training: 18 batch 234 batch_loss: 0.1375349760055542\n",
      "training: 18 batch 235 batch_loss: 0.1335357129573822\n",
      "training: 18 batch 236 batch_loss: 0.13496270775794983\n",
      "training: 18 batch 237 batch_loss: 0.13530460000038147\n",
      "training: 18 batch 238 batch_loss: 0.13438907265663147\n",
      "training: 18 batch 239 batch_loss: 0.13748756051063538\n",
      "training: 18 batch 240 batch_loss: 0.13855019211769104\n",
      "training: 18 batch 241 batch_loss: 0.1318332552909851\n",
      "training: 18 batch 242 batch_loss: 0.1370290219783783\n",
      "training: 18 batch 243 batch_loss: 0.13622471690177917\n",
      "training: 18 batch 244 batch_loss: 0.13362014293670654\n",
      "training: 18 batch 245 batch_loss: 0.1359775960445404\n",
      "training: 18 batch 246 batch_loss: 0.14020022749900818\n",
      "training: 18 batch 247 batch_loss: 0.13732507824897766\n",
      "training: 18 batch 248 batch_loss: 0.1348302960395813\n",
      "training: 18 batch 249 batch_loss: 0.13756176829338074\n",
      "training: 18 batch 250 batch_loss: 0.13538575172424316\n",
      "training: 18 batch 251 batch_loss: 0.13431310653686523\n",
      "training: 18 batch 252 batch_loss: 0.13714930415153503\n",
      "training: 18 batch 253 batch_loss: 0.13204410672187805\n",
      "training: 18 batch 254 batch_loss: 0.134138286113739\n",
      "training: 18 batch 255 batch_loss: 0.13603800535202026\n",
      "training: 18 batch 256 batch_loss: 0.13353681564331055\n",
      "training: 18 batch 257 batch_loss: 0.13591569662094116\n",
      "training: 18 batch 258 batch_loss: 0.1347341537475586\n",
      "training: 18 batch 259 batch_loss: 0.13483187556266785\n",
      "training: 18 batch 260 batch_loss: 0.13743531703948975\n",
      "training: 18 batch 261 batch_loss: 0.13499969244003296\n",
      "training: 18 batch 262 batch_loss: 0.13406535983085632\n",
      "training: 18 batch 263 batch_loss: 0.1353849172592163\n",
      "training: 18 batch 264 batch_loss: 0.13716870546340942\n",
      "training: 18 batch 265 batch_loss: 0.1332213282585144\n",
      "training: 18 batch 266 batch_loss: 0.1352570652961731\n",
      "training: 18 batch 267 batch_loss: 0.13817241787910461\n",
      "training: 18 batch 268 batch_loss: 0.1360846757888794\n",
      "training: 18 batch 269 batch_loss: 0.13549035787582397\n",
      "training: 18 batch 270 batch_loss: 0.13636666536331177\n",
      "training: 18 batch 271 batch_loss: 0.13489878177642822\n",
      "training: 18 batch 272 batch_loss: 0.13747772574424744\n",
      "training: 18 batch 273 batch_loss: 0.13904234766960144\n",
      "training: 18 batch 274 batch_loss: 0.13719457387924194\n",
      "training: 18 batch 275 batch_loss: 0.13627082109451294\n",
      "training: 18 batch 276 batch_loss: 0.13474413752555847\n",
      "training: 18 batch 277 batch_loss: 0.13457214832305908\n",
      "training: 18 batch 278 batch_loss: 0.13492518663406372\n",
      "training: 18 batch 279 batch_loss: 0.13822823762893677\n",
      "training: 18 batch 280 batch_loss: 0.13885241746902466\n",
      "training: 18 batch 281 batch_loss: 0.13776910305023193\n",
      "training: 18 batch 282 batch_loss: 0.13641351461410522\n",
      "training: 18 batch 283 batch_loss: 0.1364777386188507\n",
      "training: 18 batch 284 batch_loss: 0.13663679361343384\n",
      "training: 18 batch 285 batch_loss: 0.1373196542263031\n",
      "training: 18 batch 286 batch_loss: 0.13606563210487366\n",
      "training: 18 batch 287 batch_loss: 0.13393977284431458\n",
      "training: 18 batch 288 batch_loss: 0.13382118940353394\n",
      "training: 18 batch 289 batch_loss: 0.13480064272880554\n",
      "training: 18 batch 290 batch_loss: 0.1347929835319519\n",
      "training: 18 batch 291 batch_loss: 0.13525518774986267\n",
      "training: 18 batch 292 batch_loss: 0.13717800378799438\n",
      "training: 18 batch 293 batch_loss: 0.13566246628761292\n",
      "training: 18 batch 294 batch_loss: 0.13921770453453064\n",
      "training: 18 batch 295 batch_loss: 0.1381259560585022\n",
      "training: 18 batch 296 batch_loss: 0.1369321346282959\n",
      "training: 18 batch 297 batch_loss: 0.13530492782592773\n",
      "training: 18 batch 298 batch_loss: 0.13556474447250366\n",
      "training: 18 batch 299 batch_loss: 0.1384863257408142\n",
      "training: 18 batch 300 batch_loss: 0.1387186348438263\n",
      "training: 18 batch 301 batch_loss: 0.1365351676940918\n",
      "training: 18 batch 302 batch_loss: 0.13572901487350464\n",
      "training: 18 batch 303 batch_loss: 0.13725361227989197\n",
      "training: 18 batch 304 batch_loss: 0.13485541939735413\n",
      "training: 18 batch 305 batch_loss: 0.13597950339317322\n",
      "training: 18 batch 306 batch_loss: 0.13188466429710388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 18 batch 307 batch_loss: 0.13881570100784302\n",
      "training: 18 batch 308 batch_loss: 0.1380857527256012\n",
      "training: 18 batch 309 batch_loss: 0.1357225775718689\n",
      "training: 18 batch 310 batch_loss: 0.13701382279396057\n",
      "training: 18 batch 311 batch_loss: 0.135046124458313\n",
      "training: 18 batch 312 batch_loss: 0.1349065899848938\n",
      "training: 18 batch 313 batch_loss: 0.1390286386013031\n",
      "training: 18 batch 314 batch_loss: 0.1351432502269745\n",
      "training: 18 batch 315 batch_loss: 0.13578778505325317\n",
      "training: 18 batch 316 batch_loss: 0.13479527831077576\n",
      "training: 18 batch 317 batch_loss: 0.13829383254051208\n",
      "training: 18 batch 318 batch_loss: 0.13517987728118896\n",
      "training: 18 batch 319 batch_loss: 0.13525435328483582\n",
      "training: 18 batch 320 batch_loss: 0.13370981812477112\n",
      "training: 18 batch 321 batch_loss: 0.1357196569442749\n",
      "training: 18 batch 322 batch_loss: 0.13334444165229797\n",
      "training: 18 batch 323 batch_loss: 0.13890302181243896\n",
      "training: 18 batch 324 batch_loss: 0.13655376434326172\n",
      "training: 18 batch 325 batch_loss: 0.13780933618545532\n",
      "training: 18 batch 326 batch_loss: 0.13936924934387207\n",
      "training: 18 batch 327 batch_loss: 0.13762104511260986\n",
      "training: 18 batch 328 batch_loss: 0.1360868215560913\n",
      "training: 18 batch 329 batch_loss: 0.13776257634162903\n",
      "training: 18 batch 330 batch_loss: 0.13802272081375122\n",
      "training: 18 batch 331 batch_loss: 0.13882341980934143\n",
      "training: 18 batch 332 batch_loss: 0.13661682605743408\n",
      "training: 18 batch 333 batch_loss: 0.1336292028427124\n",
      "training: 18 batch 334 batch_loss: 0.13744184374809265\n",
      "training: 18 batch 335 batch_loss: 0.13545706868171692\n",
      "training: 18 batch 336 batch_loss: 0.13724854588508606\n",
      "training: 18 batch 337 batch_loss: 0.13804709911346436\n",
      "training: 18 batch 338 batch_loss: 0.13386869430541992\n",
      "training: 18 batch 339 batch_loss: 0.1383272111415863\n",
      "training: 18 batch 340 batch_loss: 0.13508093357086182\n",
      "training: 18 batch 341 batch_loss: 0.13529804348945618\n",
      "training: 18 batch 342 batch_loss: 0.13976573944091797\n",
      "training: 18 batch 343 batch_loss: 0.13687622547149658\n",
      "training: 18 batch 344 batch_loss: 0.13880318403244019\n",
      "training: 18 batch 345 batch_loss: 0.13534623384475708\n",
      "training: 18 batch 346 batch_loss: 0.1365925371646881\n",
      "training: 18 batch 347 batch_loss: 0.13545498251914978\n",
      "training: 18 batch 348 batch_loss: 0.136600524187088\n",
      "training: 18 batch 349 batch_loss: 0.13751429319381714\n",
      "training: 18 batch 350 batch_loss: 0.13734301924705505\n",
      "training: 18 batch 351 batch_loss: 0.13371717929840088\n",
      "training: 18 batch 352 batch_loss: 0.13552945852279663\n",
      "training: 18 batch 353 batch_loss: 0.13885319232940674\n",
      "training: 18 batch 354 batch_loss: 0.13756173849105835\n",
      "training: 18 batch 355 batch_loss: 0.13423246145248413\n",
      "training: 18 batch 356 batch_loss: 0.13726100325584412\n",
      "training: 18 batch 357 batch_loss: 0.1366037130355835\n",
      "training: 18 batch 358 batch_loss: 0.13733512163162231\n",
      "training: 18 batch 359 batch_loss: 0.13621297478675842\n",
      "training: 18 batch 360 batch_loss: 0.13979658484458923\n",
      "training: 18 batch 361 batch_loss: 0.13649660348892212\n",
      "training: 18 batch 362 batch_loss: 0.139834463596344\n",
      "training: 18 batch 363 batch_loss: 0.13736850023269653\n",
      "training: 18 batch 364 batch_loss: 0.13642075657844543\n",
      "training: 18 batch 365 batch_loss: 0.13203993439674377\n",
      "training: 18 batch 366 batch_loss: 0.13363537192344666\n",
      "training: 18 batch 367 batch_loss: 0.13700300455093384\n",
      "training: 18 batch 368 batch_loss: 0.13859298825263977\n",
      "training: 18 batch 369 batch_loss: 0.13764414191246033\n",
      "training: 18 batch 370 batch_loss: 0.13813701272010803\n",
      "training: 18 batch 371 batch_loss: 0.1367931365966797\n",
      "training: 18 batch 372 batch_loss: 0.13576865196228027\n",
      "training: 18 batch 373 batch_loss: 0.13716265559196472\n",
      "training: 18 batch 374 batch_loss: 0.1361384391784668\n",
      "training: 18 batch 375 batch_loss: 0.1356300413608551\n",
      "training: 18 batch 376 batch_loss: 0.13785994052886963\n",
      "training: 18 batch 377 batch_loss: 0.14106333255767822\n",
      "training: 18 batch 378 batch_loss: 0.13472601771354675\n",
      "training: 18 batch 379 batch_loss: 0.1351424753665924\n",
      "training: 18 batch 380 batch_loss: 0.13995599746704102\n",
      "training: 18 batch 381 batch_loss: 0.13609325885772705\n",
      "training: 18 batch 382 batch_loss: 0.1351919174194336\n",
      "training: 18 batch 383 batch_loss: 0.13602715730667114\n",
      "training: 18 batch 384 batch_loss: 0.13663843274116516\n",
      "training: 18 batch 385 batch_loss: 0.13610026240348816\n",
      "training: 18 batch 386 batch_loss: 0.13735005259513855\n",
      "training: 18 batch 387 batch_loss: 0.13384893536567688\n",
      "training: 18 batch 388 batch_loss: 0.1365252435207367\n",
      "training: 18 batch 389 batch_loss: 0.1323247253894806\n",
      "training: 18 batch 390 batch_loss: 0.13752853870391846\n",
      "training: 18 batch 391 batch_loss: 0.13909482955932617\n",
      "training: 18 batch 392 batch_loss: 0.1365422010421753\n",
      "training: 18 batch 393 batch_loss: 0.13425594568252563\n",
      "training: 18 batch 394 batch_loss: 0.13544690608978271\n",
      "training: 18 batch 395 batch_loss: 0.13790485262870789\n",
      "training: 18 batch 396 batch_loss: 0.13869819045066833\n",
      "training: 18 batch 397 batch_loss: 0.13722354173660278\n",
      "training: 18 batch 398 batch_loss: 0.1344270408153534\n",
      "training: 18 batch 399 batch_loss: 0.13531482219696045\n",
      "training: 18 batch 400 batch_loss: 0.13645833730697632\n",
      "training: 18 batch 401 batch_loss: 0.13776955008506775\n",
      "training: 18 batch 402 batch_loss: 0.13585329055786133\n",
      "training: 18 batch 403 batch_loss: 0.13429927825927734\n",
      "training: 18 batch 404 batch_loss: 0.13620632886886597\n",
      "training: 18 batch 405 batch_loss: 0.13858002424240112\n",
      "training: 18 batch 406 batch_loss: 0.13712432980537415\n",
      "training: 18 batch 407 batch_loss: 0.13473081588745117\n",
      "training: 18 batch 408 batch_loss: 0.13654330372810364\n",
      "training: 18 batch 409 batch_loss: 0.13487881422042847\n",
      "training: 18 batch 410 batch_loss: 0.13936543464660645\n",
      "training: 18 batch 411 batch_loss: 0.1390569806098938\n",
      "training: 18 batch 412 batch_loss: 0.1377929151058197\n",
      "training: 18 batch 413 batch_loss: 0.13578984141349792\n",
      "training: 18 batch 414 batch_loss: 0.13552787899971008\n",
      "training: 18 batch 415 batch_loss: 0.1383303701877594\n",
      "training: 18 batch 416 batch_loss: 0.1384270191192627\n",
      "training: 18 batch 417 batch_loss: 0.1380188763141632\n",
      "training: 18 batch 418 batch_loss: 0.13508355617523193\n",
      "training: 18 batch 419 batch_loss: 0.13598358631134033\n",
      "training: 18 batch 420 batch_loss: 0.13749858736991882\n",
      "training: 18 batch 421 batch_loss: 0.13517525792121887\n",
      "training: 18 batch 422 batch_loss: 0.13507616519927979\n",
      "training: 18 batch 423 batch_loss: 0.13433748483657837\n",
      "training: 18 batch 424 batch_loss: 0.13910338282585144\n",
      "training: 18 batch 425 batch_loss: 0.13590842485427856\n",
      "training: 18 batch 426 batch_loss: 0.13719448447227478\n",
      "training: 18 batch 427 batch_loss: 0.139445960521698\n",
      "training: 18 batch 428 batch_loss: 0.13592764735221863\n",
      "training: 18 batch 429 batch_loss: 0.1344068944454193\n",
      "training: 18 batch 430 batch_loss: 0.14031124114990234\n",
      "training: 18 batch 431 batch_loss: 0.13383585214614868\n",
      "training: 18 batch 432 batch_loss: 0.13720455765724182\n",
      "training: 18 batch 433 batch_loss: 0.13387995958328247\n",
      "training: 18 batch 434 batch_loss: 0.13410544395446777\n",
      "training: 18 batch 435 batch_loss: 0.13504540920257568\n",
      "training: 18 batch 436 batch_loss: 0.1384488344192505\n",
      "training: 18 batch 437 batch_loss: 0.14063194394111633\n",
      "training: 18 batch 438 batch_loss: 0.13869708776474\n",
      "training: 18 batch 439 batch_loss: 0.1393565833568573\n",
      "training: 18 batch 440 batch_loss: 0.1389918029308319\n",
      "training: 18 batch 441 batch_loss: 0.13457456231117249\n",
      "training: 18 batch 442 batch_loss: 0.1389995813369751\n",
      "training: 18 batch 443 batch_loss: 0.13479241728782654\n",
      "training: 18 batch 444 batch_loss: 0.1371571123600006\n",
      "training: 18 batch 445 batch_loss: 0.1398388147354126\n",
      "training: 18 batch 446 batch_loss: 0.1366199254989624\n",
      "training: 18 batch 447 batch_loss: 0.13498789072036743\n",
      "training: 18 batch 448 batch_loss: 0.1375272274017334\n",
      "training: 18 batch 449 batch_loss: 0.13584092259407043\n",
      "training: 18 batch 450 batch_loss: 0.13511326909065247\n",
      "training: 18 batch 451 batch_loss: 0.1357470452785492\n",
      "training: 18 batch 452 batch_loss: 0.13612771034240723\n",
      "training: 18 batch 453 batch_loss: 0.13824820518493652\n",
      "training: 18 batch 454 batch_loss: 0.1351158320903778\n",
      "training: 18 batch 455 batch_loss: 0.13576823472976685\n",
      "training: 18 batch 456 batch_loss: 0.13862082362174988\n",
      "training: 18 batch 457 batch_loss: 0.13551771640777588\n",
      "training: 18 batch 458 batch_loss: 0.1363157331943512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 18 batch 459 batch_loss: 0.14042234420776367\n",
      "training: 18 batch 460 batch_loss: 0.13807392120361328\n",
      "training: 18 batch 461 batch_loss: 0.1368710994720459\n",
      "training: 18 batch 462 batch_loss: 0.13647347688674927\n",
      "training: 18 batch 463 batch_loss: 0.1372852921485901\n",
      "training: 18 batch 464 batch_loss: 0.13948774337768555\n",
      "training: 18 batch 465 batch_loss: 0.13481512665748596\n",
      "training: 18 batch 466 batch_loss: 0.13538646697998047\n",
      "training: 18 batch 467 batch_loss: 0.13759753108024597\n",
      "training: 18 batch 468 batch_loss: 0.13798880577087402\n",
      "training: 18 batch 469 batch_loss: 0.13467276096343994\n",
      "training: 18 batch 470 batch_loss: 0.13902822136878967\n",
      "training: 18 batch 471 batch_loss: 0.13566544651985168\n",
      "training: 18 batch 472 batch_loss: 0.13665026426315308\n",
      "training: 18 batch 473 batch_loss: 0.13574209809303284\n",
      "training: 18 batch 474 batch_loss: 0.14106661081314087\n",
      "training: 18 batch 475 batch_loss: 0.1404435634613037\n",
      "training: 18 batch 476 batch_loss: 0.13774976134300232\n",
      "training: 18 batch 477 batch_loss: 0.13539668917655945\n",
      "training: 18 batch 478 batch_loss: 0.14179041981697083\n",
      "training: 18 batch 479 batch_loss: 0.13751041889190674\n",
      "training: 18 batch 480 batch_loss: 0.1373404860496521\n",
      "training: 18 batch 481 batch_loss: 0.1382608413696289\n",
      "training: 18 batch 482 batch_loss: 0.13827139139175415\n",
      "training: 18 batch 483 batch_loss: 0.13760876655578613\n",
      "training: 18 batch 484 batch_loss: 0.13503298163414001\n",
      "training: 18 batch 485 batch_loss: 0.13368594646453857\n",
      "training: 18 batch 486 batch_loss: 0.13841694593429565\n",
      "training: 18 batch 487 batch_loss: 0.141351580619812\n",
      "training: 18 batch 488 batch_loss: 0.13494902849197388\n",
      "training: 18 batch 489 batch_loss: 0.14009076356887817\n",
      "training: 18 batch 490 batch_loss: 0.13581249117851257\n",
      "training: 18 batch 491 batch_loss: 0.13360381126403809\n",
      "training: 18 batch 492 batch_loss: 0.13824039697647095\n",
      "training: 18 batch 493 batch_loss: 0.1342611312866211\n",
      "training: 18 batch 494 batch_loss: 0.13931244611740112\n",
      "training: 18 batch 495 batch_loss: 0.1367650330066681\n",
      "training: 18 batch 496 batch_loss: 0.1377905011177063\n",
      "training: 18 batch 497 batch_loss: 0.1383601725101471\n",
      "training: 18 batch 498 batch_loss: 0.13745129108428955\n",
      "training: 18 batch 499 batch_loss: 0.13934049010276794\n",
      "training: 18 batch 500 batch_loss: 0.13821065425872803\n",
      "training: 18 batch 501 batch_loss: 0.137509286403656\n",
      "training: 18 batch 502 batch_loss: 0.1371135413646698\n",
      "training: 18 batch 503 batch_loss: 0.1386646330356598\n",
      "training: 18 batch 504 batch_loss: 0.13789406418800354\n",
      "training: 18 batch 505 batch_loss: 0.13812974095344543\n",
      "training: 18 batch 506 batch_loss: 0.13891059160232544\n",
      "training: 18 batch 507 batch_loss: 0.1376413106918335\n",
      "training: 18 batch 508 batch_loss: 0.13614389300346375\n",
      "training: 18 batch 509 batch_loss: 0.1407921016216278\n",
      "training: 18 batch 510 batch_loss: 0.13636863231658936\n",
      "training: 18 batch 511 batch_loss: 0.14051386713981628\n",
      "training: 18 batch 512 batch_loss: 0.1382136344909668\n",
      "training: 18 batch 513 batch_loss: 0.1396772563457489\n",
      "training: 18 batch 514 batch_loss: 0.1351076066493988\n",
      "training: 18 batch 515 batch_loss: 0.13383927941322327\n",
      "training: 18 batch 516 batch_loss: 0.13843971490859985\n",
      "training: 18 batch 517 batch_loss: 0.13669630885124207\n",
      "training: 18 batch 518 batch_loss: 0.1366751790046692\n",
      "training: 18 batch 519 batch_loss: 0.13654229044914246\n",
      "training: 18 batch 520 batch_loss: 0.13604500889778137\n",
      "training: 18 batch 521 batch_loss: 0.13423746824264526\n",
      "training: 18 batch 522 batch_loss: 0.13256904482841492\n",
      "training: 18 batch 523 batch_loss: 0.1383630633354187\n",
      "training: 18 batch 524 batch_loss: 0.13894802331924438\n",
      "training: 18 batch 525 batch_loss: 0.14031150937080383\n",
      "training: 18 batch 526 batch_loss: 0.13574105501174927\n",
      "training: 18 batch 527 batch_loss: 0.13789039850234985\n",
      "training: 18 batch 528 batch_loss: 0.13965201377868652\n",
      "training: 18 batch 529 batch_loss: 0.1371992528438568\n",
      "training: 18 batch 530 batch_loss: 0.14118275046348572\n",
      "training: 18 batch 531 batch_loss: 0.13688617944717407\n",
      "training: 18 batch 532 batch_loss: 0.13761374354362488\n",
      "training: 18 batch 533 batch_loss: 0.13612571358680725\n",
      "training: 18 batch 534 batch_loss: 0.13681024312973022\n",
      "training: 18 batch 535 batch_loss: 0.13645264506340027\n",
      "training: 18 batch 536 batch_loss: 0.13922354578971863\n",
      "training: 18 batch 537 batch_loss: 0.13818621635437012\n",
      "training: 18 batch 538 batch_loss: 0.13668346405029297\n",
      "training: 18 batch 539 batch_loss: 0.1348547339439392\n",
      "training: 18 batch 540 batch_loss: 0.1398320496082306\n",
      "training: 18 batch 541 batch_loss: 0.1358519196510315\n",
      "training: 18 batch 542 batch_loss: 0.13704556226730347\n",
      "training: 18 batch 543 batch_loss: 0.1392928659915924\n",
      "training: 18 batch 544 batch_loss: 0.13896781206130981\n",
      "training: 18 batch 545 batch_loss: 0.1359190046787262\n",
      "training: 18 batch 546 batch_loss: 0.1437341272830963\n",
      "training: 18 batch 547 batch_loss: 0.1418432891368866\n",
      "training: 18 batch 548 batch_loss: 0.13839977979660034\n",
      "training: 18 batch 549 batch_loss: 0.13554078340530396\n",
      "training: 18 batch 550 batch_loss: 0.1386927366256714\n",
      "training: 18 batch 551 batch_loss: 0.13614919781684875\n",
      "training: 18 batch 552 batch_loss: 0.13535818457603455\n",
      "training: 18 batch 553 batch_loss: 0.13867327570915222\n",
      "training: 18 batch 554 batch_loss: 0.13574326038360596\n",
      "training: 18 batch 555 batch_loss: 0.13989490270614624\n",
      "training: 18 batch 556 batch_loss: 0.13444381952285767\n",
      "training: 18 batch 557 batch_loss: 0.13767600059509277\n",
      "training: 18 batch 558 batch_loss: 0.13614574074745178\n",
      "training: 18 batch 559 batch_loss: 0.13707470893859863\n",
      "training: 18 batch 560 batch_loss: 0.13815683126449585\n",
      "training: 18 batch 561 batch_loss: 0.13583767414093018\n",
      "training: 18 batch 562 batch_loss: 0.13713979721069336\n",
      "training: 18 batch 563 batch_loss: 0.1372322142124176\n",
      "training: 18 batch 564 batch_loss: 0.13706731796264648\n",
      "training: 18 batch 565 batch_loss: 0.13641858100891113\n",
      "training: 18 batch 566 batch_loss: 0.13843286037445068\n",
      "training: 18 batch 567 batch_loss: 0.13976112008094788\n",
      "training: 18 batch 568 batch_loss: 0.13931557536125183\n",
      "training: 18 batch 569 batch_loss: 0.13478851318359375\n",
      "training: 18 batch 570 batch_loss: 0.13756006956100464\n",
      "training: 18 batch 571 batch_loss: 0.1373271644115448\n",
      "training: 18 batch 572 batch_loss: 0.13790446519851685\n",
      "training: 18 batch 573 batch_loss: 0.14010822772979736\n",
      "training: 18 batch 574 batch_loss: 0.13974112272262573\n",
      "training: 18 batch 575 batch_loss: 0.13620269298553467\n",
      "training: 18 batch 576 batch_loss: 0.13814908266067505\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 18, Hit Ratio:0.02626310406971515 | Precision:0.038749631377174874 | Recall:0.054131106469194855 | NDCG:0.0493221275721554\n",
      "*Best Performance* \n",
      "Epoch: 18, Hit Ratio:0.02626310406971515 | Precision:0.038749631377174874 | Recall:0.054131106469194855 | MDCG:0.0493221275721554\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 19 batch 0 batch_loss: 0.1380903422832489\n",
      "training: 19 batch 1 batch_loss: 0.1330115795135498\n",
      "training: 19 batch 2 batch_loss: 0.13495603203773499\n",
      "training: 19 batch 3 batch_loss: 0.1362423598766327\n",
      "training: 19 batch 4 batch_loss: 0.1379784345626831\n",
      "training: 19 batch 5 batch_loss: 0.13564634323120117\n",
      "training: 19 batch 6 batch_loss: 0.13782620429992676\n",
      "training: 19 batch 7 batch_loss: 0.13552743196487427\n",
      "training: 19 batch 8 batch_loss: 0.13539382815361023\n",
      "training: 19 batch 9 batch_loss: 0.1361425817012787\n",
      "training: 19 batch 10 batch_loss: 0.13681524991989136\n",
      "training: 19 batch 11 batch_loss: 0.1395137906074524\n",
      "training: 19 batch 12 batch_loss: 0.13753917813301086\n",
      "training: 19 batch 13 batch_loss: 0.14051666855812073\n",
      "training: 19 batch 14 batch_loss: 0.13486206531524658\n",
      "training: 19 batch 15 batch_loss: 0.13653114438056946\n",
      "training: 19 batch 16 batch_loss: 0.13806933164596558\n",
      "training: 19 batch 17 batch_loss: 0.13334757089614868\n",
      "training: 19 batch 18 batch_loss: 0.1362541913986206\n",
      "training: 19 batch 19 batch_loss: 0.1338757872581482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 19 batch 20 batch_loss: 0.13607916235923767\n",
      "training: 19 batch 21 batch_loss: 0.13887688517570496\n",
      "training: 19 batch 22 batch_loss: 0.13644146919250488\n",
      "training: 19 batch 23 batch_loss: 0.13617336750030518\n",
      "training: 19 batch 24 batch_loss: 0.1372660994529724\n",
      "training: 19 batch 25 batch_loss: 0.13474112749099731\n",
      "training: 19 batch 26 batch_loss: 0.13318446278572083\n",
      "training: 19 batch 27 batch_loss: 0.1354842185974121\n",
      "training: 19 batch 28 batch_loss: 0.14060285687446594\n",
      "training: 19 batch 29 batch_loss: 0.13702598214149475\n",
      "training: 19 batch 30 batch_loss: 0.1346251368522644\n",
      "training: 19 batch 31 batch_loss: 0.1349448263645172\n",
      "training: 19 batch 32 batch_loss: 0.13563328981399536\n",
      "training: 19 batch 33 batch_loss: 0.13637113571166992\n",
      "training: 19 batch 34 batch_loss: 0.1343042254447937\n",
      "training: 19 batch 35 batch_loss: 0.13678643107414246\n",
      "training: 19 batch 36 batch_loss: 0.13837721943855286\n",
      "training: 19 batch 37 batch_loss: 0.13677728176116943\n",
      "training: 19 batch 38 batch_loss: 0.139908105134964\n",
      "training: 19 batch 39 batch_loss: 0.1356257200241089\n",
      "training: 19 batch 40 batch_loss: 0.138208270072937\n",
      "training: 19 batch 41 batch_loss: 0.1369129717350006\n",
      "training: 19 batch 42 batch_loss: 0.14131537079811096\n",
      "training: 19 batch 43 batch_loss: 0.13577643036842346\n",
      "training: 19 batch 44 batch_loss: 0.13879671692848206\n",
      "training: 19 batch 45 batch_loss: 0.1386590600013733\n",
      "training: 19 batch 46 batch_loss: 0.13397270441055298\n",
      "training: 19 batch 47 batch_loss: 0.13744938373565674\n",
      "training: 19 batch 48 batch_loss: 0.1373211145401001\n",
      "training: 19 batch 49 batch_loss: 0.13915139436721802\n",
      "training: 19 batch 50 batch_loss: 0.13601604104042053\n",
      "training: 19 batch 51 batch_loss: 0.13415876030921936\n",
      "training: 19 batch 52 batch_loss: 0.13810110092163086\n",
      "training: 19 batch 53 batch_loss: 0.13772115111351013\n",
      "training: 19 batch 54 batch_loss: 0.13923707604408264\n",
      "training: 19 batch 55 batch_loss: 0.13320234417915344\n",
      "training: 19 batch 56 batch_loss: 0.13646379113197327\n",
      "training: 19 batch 57 batch_loss: 0.13863033056259155\n",
      "training: 19 batch 58 batch_loss: 0.13687458634376526\n",
      "training: 19 batch 59 batch_loss: 0.1340707540512085\n",
      "training: 19 batch 60 batch_loss: 0.14134716987609863\n",
      "training: 19 batch 61 batch_loss: 0.1389451026916504\n",
      "training: 19 batch 62 batch_loss: 0.13936281204223633\n",
      "training: 19 batch 63 batch_loss: 0.13756072521209717\n",
      "training: 19 batch 64 batch_loss: 0.13679152727127075\n",
      "training: 19 batch 65 batch_loss: 0.1363421082496643\n",
      "training: 19 batch 66 batch_loss: 0.13693922758102417\n",
      "training: 19 batch 67 batch_loss: 0.13776245713233948\n",
      "training: 19 batch 68 batch_loss: 0.13501179218292236\n",
      "training: 19 batch 69 batch_loss: 0.13589677214622498\n",
      "training: 19 batch 70 batch_loss: 0.13610145449638367\n",
      "training: 19 batch 71 batch_loss: 0.13756972551345825\n",
      "training: 19 batch 72 batch_loss: 0.13679593801498413\n",
      "training: 19 batch 73 batch_loss: 0.13591253757476807\n",
      "training: 19 batch 74 batch_loss: 0.1383463442325592\n",
      "training: 19 batch 75 batch_loss: 0.13778969645500183\n",
      "training: 19 batch 76 batch_loss: 0.13667911291122437\n",
      "training: 19 batch 77 batch_loss: 0.13947772979736328\n",
      "training: 19 batch 78 batch_loss: 0.137295663356781\n",
      "training: 19 batch 79 batch_loss: 0.13960528373718262\n",
      "training: 19 batch 80 batch_loss: 0.13690781593322754\n",
      "training: 19 batch 81 batch_loss: 0.13890042901039124\n",
      "training: 19 batch 82 batch_loss: 0.13825243711471558\n",
      "training: 19 batch 83 batch_loss: 0.1374989151954651\n",
      "training: 19 batch 84 batch_loss: 0.13539355993270874\n",
      "training: 19 batch 85 batch_loss: 0.13847410678863525\n",
      "training: 19 batch 86 batch_loss: 0.13156211376190186\n",
      "training: 19 batch 87 batch_loss: 0.14163488149642944\n",
      "training: 19 batch 88 batch_loss: 0.13843417167663574\n",
      "training: 19 batch 89 batch_loss: 0.13620322942733765\n",
      "training: 19 batch 90 batch_loss: 0.1388927400112152\n",
      "training: 19 batch 91 batch_loss: 0.13654786348342896\n",
      "training: 19 batch 92 batch_loss: 0.13811463117599487\n",
      "training: 19 batch 93 batch_loss: 0.13480329513549805\n",
      "training: 19 batch 94 batch_loss: 0.13763338327407837\n",
      "training: 19 batch 95 batch_loss: 0.1385917067527771\n",
      "training: 19 batch 96 batch_loss: 0.13552528619766235\n",
      "training: 19 batch 97 batch_loss: 0.1374155879020691\n",
      "training: 19 batch 98 batch_loss: 0.13779732584953308\n",
      "training: 19 batch 99 batch_loss: 0.13790661096572876\n",
      "training: 19 batch 100 batch_loss: 0.1402280330657959\n",
      "training: 19 batch 101 batch_loss: 0.14086180925369263\n",
      "training: 19 batch 102 batch_loss: 0.13681328296661377\n",
      "training: 19 batch 103 batch_loss: 0.13485798239707947\n",
      "training: 19 batch 104 batch_loss: 0.13731050491333008\n",
      "training: 19 batch 105 batch_loss: 0.13543027639389038\n",
      "training: 19 batch 106 batch_loss: 0.13579779863357544\n",
      "training: 19 batch 107 batch_loss: 0.13656577467918396\n",
      "training: 19 batch 108 batch_loss: 0.13864350318908691\n",
      "training: 19 batch 109 batch_loss: 0.13833516836166382\n",
      "training: 19 batch 110 batch_loss: 0.13600149750709534\n",
      "training: 19 batch 111 batch_loss: 0.1393892765045166\n",
      "training: 19 batch 112 batch_loss: 0.13935807347297668\n",
      "training: 19 batch 113 batch_loss: 0.13907551765441895\n",
      "training: 19 batch 114 batch_loss: 0.13553619384765625\n",
      "training: 19 batch 115 batch_loss: 0.13570469617843628\n",
      "training: 19 batch 116 batch_loss: 0.13980308175086975\n",
      "training: 19 batch 117 batch_loss: 0.13613393902778625\n",
      "training: 19 batch 118 batch_loss: 0.13713470101356506\n",
      "training: 19 batch 119 batch_loss: 0.13906580209732056\n",
      "training: 19 batch 120 batch_loss: 0.13619330525398254\n",
      "training: 19 batch 121 batch_loss: 0.13900786638259888\n",
      "training: 19 batch 122 batch_loss: 0.13753652572631836\n",
      "training: 19 batch 123 batch_loss: 0.13683989644050598\n",
      "training: 19 batch 124 batch_loss: 0.1384405493736267\n",
      "training: 19 batch 125 batch_loss: 0.13962802290916443\n",
      "training: 19 batch 126 batch_loss: 0.13617944717407227\n",
      "training: 19 batch 127 batch_loss: 0.13773411512374878\n",
      "training: 19 batch 128 batch_loss: 0.1360621452331543\n",
      "training: 19 batch 129 batch_loss: 0.13629543781280518\n",
      "training: 19 batch 130 batch_loss: 0.13620388507843018\n",
      "training: 19 batch 131 batch_loss: 0.13617107272148132\n",
      "training: 19 batch 132 batch_loss: 0.1350022256374359\n",
      "training: 19 batch 133 batch_loss: 0.13491716980934143\n",
      "training: 19 batch 134 batch_loss: 0.13551121950149536\n",
      "training: 19 batch 135 batch_loss: 0.1365881860256195\n",
      "training: 19 batch 136 batch_loss: 0.13740140199661255\n",
      "training: 19 batch 137 batch_loss: 0.13727864623069763\n",
      "training: 19 batch 138 batch_loss: 0.13846608996391296\n",
      "training: 19 batch 139 batch_loss: 0.1396465003490448\n",
      "training: 19 batch 140 batch_loss: 0.1364755630493164\n",
      "training: 19 batch 141 batch_loss: 0.13788780570030212\n",
      "training: 19 batch 142 batch_loss: 0.13709107041358948\n",
      "training: 19 batch 143 batch_loss: 0.1399683654308319\n",
      "training: 19 batch 144 batch_loss: 0.139367938041687\n",
      "training: 19 batch 145 batch_loss: 0.13649967312812805\n",
      "training: 19 batch 146 batch_loss: 0.1374295949935913\n",
      "training: 19 batch 147 batch_loss: 0.13318833708763123\n",
      "training: 19 batch 148 batch_loss: 0.13475242257118225\n",
      "training: 19 batch 149 batch_loss: 0.13637393712997437\n",
      "training: 19 batch 150 batch_loss: 0.13600540161132812\n",
      "training: 19 batch 151 batch_loss: 0.1377890408039093\n",
      "training: 19 batch 152 batch_loss: 0.1378892958164215\n",
      "training: 19 batch 153 batch_loss: 0.13682520389556885\n",
      "training: 19 batch 154 batch_loss: 0.14033904671669006\n",
      "training: 19 batch 155 batch_loss: 0.1369989514350891\n",
      "training: 19 batch 156 batch_loss: 0.13842836022377014\n",
      "training: 19 batch 157 batch_loss: 0.13752266764640808\n",
      "training: 19 batch 158 batch_loss: 0.13790380954742432\n",
      "training: 19 batch 159 batch_loss: 0.1368519365787506\n",
      "training: 19 batch 160 batch_loss: 0.13948529958724976\n",
      "training: 19 batch 161 batch_loss: 0.1376326084136963\n",
      "training: 19 batch 162 batch_loss: 0.1345941722393036\n",
      "training: 19 batch 163 batch_loss: 0.136386901140213\n",
      "training: 19 batch 164 batch_loss: 0.1361493170261383\n",
      "training: 19 batch 165 batch_loss: 0.13557660579681396\n",
      "training: 19 batch 166 batch_loss: 0.1378563642501831\n",
      "training: 19 batch 167 batch_loss: 0.1379813849925995\n",
      "training: 19 batch 168 batch_loss: 0.13764670491218567\n",
      "training: 19 batch 169 batch_loss: 0.14181753993034363\n",
      "training: 19 batch 170 batch_loss: 0.1358182728290558\n",
      "training: 19 batch 171 batch_loss: 0.139361172914505\n",
      "training: 19 batch 172 batch_loss: 0.13740822672843933\n",
      "training: 19 batch 173 batch_loss: 0.1392998993396759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 19 batch 174 batch_loss: 0.1376051902770996\n",
      "training: 19 batch 175 batch_loss: 0.13860434293746948\n",
      "training: 19 batch 176 batch_loss: 0.13746580481529236\n",
      "training: 19 batch 177 batch_loss: 0.13740676641464233\n",
      "training: 19 batch 178 batch_loss: 0.13744193315505981\n",
      "training: 19 batch 179 batch_loss: 0.13809460401535034\n",
      "training: 19 batch 180 batch_loss: 0.13528764247894287\n",
      "training: 19 batch 181 batch_loss: 0.140315979719162\n",
      "training: 19 batch 182 batch_loss: 0.13755059242248535\n",
      "training: 19 batch 183 batch_loss: 0.13665896654129028\n",
      "training: 19 batch 184 batch_loss: 0.13407781720161438\n",
      "training: 19 batch 185 batch_loss: 0.13726326823234558\n",
      "training: 19 batch 186 batch_loss: 0.13749143481254578\n",
      "training: 19 batch 187 batch_loss: 0.1351025402545929\n",
      "training: 19 batch 188 batch_loss: 0.13915100693702698\n",
      "training: 19 batch 189 batch_loss: 0.13947191834449768\n",
      "training: 19 batch 190 batch_loss: 0.13876375555992126\n",
      "training: 19 batch 191 batch_loss: 0.13871577382087708\n",
      "training: 19 batch 192 batch_loss: 0.134526789188385\n",
      "training: 19 batch 193 batch_loss: 0.1367880403995514\n",
      "training: 19 batch 194 batch_loss: 0.13642579317092896\n",
      "training: 19 batch 195 batch_loss: 0.13650661706924438\n",
      "training: 19 batch 196 batch_loss: 0.13570308685302734\n",
      "training: 19 batch 197 batch_loss: 0.1381460428237915\n",
      "training: 19 batch 198 batch_loss: 0.14024928212165833\n",
      "training: 19 batch 199 batch_loss: 0.13685521483421326\n",
      "training: 19 batch 200 batch_loss: 0.13776826858520508\n",
      "training: 19 batch 201 batch_loss: 0.13744205236434937\n",
      "training: 19 batch 202 batch_loss: 0.13806653022766113\n",
      "training: 19 batch 203 batch_loss: 0.13736414909362793\n",
      "training: 19 batch 204 batch_loss: 0.13978394865989685\n",
      "training: 19 batch 205 batch_loss: 0.13978055119514465\n",
      "training: 19 batch 206 batch_loss: 0.13871270418167114\n",
      "training: 19 batch 207 batch_loss: 0.13959625363349915\n",
      "training: 19 batch 208 batch_loss: 0.14014846086502075\n",
      "training: 19 batch 209 batch_loss: 0.13721787929534912\n",
      "training: 19 batch 210 batch_loss: 0.1384708285331726\n",
      "training: 19 batch 211 batch_loss: 0.13767287135124207\n",
      "training: 19 batch 212 batch_loss: 0.14108777046203613\n",
      "training: 19 batch 213 batch_loss: 0.1351626217365265\n",
      "training: 19 batch 214 batch_loss: 0.1377905309200287\n",
      "training: 19 batch 215 batch_loss: 0.13811257481575012\n",
      "training: 19 batch 216 batch_loss: 0.13732725381851196\n",
      "training: 19 batch 217 batch_loss: 0.13686254620552063\n",
      "training: 19 batch 218 batch_loss: 0.13987547159194946\n",
      "training: 19 batch 219 batch_loss: 0.1373865008354187\n",
      "training: 19 batch 220 batch_loss: 0.1383204460144043\n",
      "training: 19 batch 221 batch_loss: 0.14018505811691284\n",
      "training: 19 batch 222 batch_loss: 0.14060980081558228\n",
      "training: 19 batch 223 batch_loss: 0.13472139835357666\n",
      "training: 19 batch 224 batch_loss: 0.13939368724822998\n",
      "training: 19 batch 225 batch_loss: 0.1394495964050293\n",
      "training: 19 batch 226 batch_loss: 0.14085564017295837\n",
      "training: 19 batch 227 batch_loss: 0.1397564709186554\n",
      "training: 19 batch 228 batch_loss: 0.13667035102844238\n",
      "training: 19 batch 229 batch_loss: 0.13462349772453308\n",
      "training: 19 batch 230 batch_loss: 0.13878798484802246\n",
      "training: 19 batch 231 batch_loss: 0.13445886969566345\n",
      "training: 19 batch 232 batch_loss: 0.13822594285011292\n",
      "training: 19 batch 233 batch_loss: 0.13913404941558838\n",
      "training: 19 batch 234 batch_loss: 0.1385313868522644\n",
      "training: 19 batch 235 batch_loss: 0.13632750511169434\n",
      "training: 19 batch 236 batch_loss: 0.1387936770915985\n",
      "training: 19 batch 237 batch_loss: 0.14002299308776855\n",
      "training: 19 batch 238 batch_loss: 0.13704314827919006\n",
      "training: 19 batch 239 batch_loss: 0.13812965154647827\n",
      "training: 19 batch 240 batch_loss: 0.13988885283470154\n",
      "training: 19 batch 241 batch_loss: 0.13755068182945251\n",
      "training: 19 batch 242 batch_loss: 0.13860389590263367\n",
      "training: 19 batch 243 batch_loss: 0.1399095356464386\n",
      "training: 19 batch 244 batch_loss: 0.13995271921157837\n",
      "training: 19 batch 245 batch_loss: 0.13878792524337769\n",
      "training: 19 batch 246 batch_loss: 0.13909012079238892\n",
      "training: 19 batch 247 batch_loss: 0.13712656497955322\n",
      "training: 19 batch 248 batch_loss: 0.13939329981803894\n",
      "training: 19 batch 249 batch_loss: 0.13739222288131714\n",
      "training: 19 batch 250 batch_loss: 0.13688123226165771\n",
      "training: 19 batch 251 batch_loss: 0.14100810885429382\n",
      "training: 19 batch 252 batch_loss: 0.13941329717636108\n",
      "training: 19 batch 253 batch_loss: 0.13939452171325684\n",
      "training: 19 batch 254 batch_loss: 0.1418294906616211\n",
      "training: 19 batch 255 batch_loss: 0.14135050773620605\n",
      "training: 19 batch 256 batch_loss: 0.1378549337387085\n",
      "training: 19 batch 257 batch_loss: 0.1400798261165619\n",
      "training: 19 batch 258 batch_loss: 0.1400279700756073\n",
      "training: 19 batch 259 batch_loss: 0.13877147436141968\n",
      "training: 19 batch 260 batch_loss: 0.13902249932289124\n",
      "training: 19 batch 261 batch_loss: 0.13954249024391174\n",
      "training: 19 batch 262 batch_loss: 0.1350102722644806\n",
      "training: 19 batch 263 batch_loss: 0.1379048228263855\n",
      "training: 19 batch 264 batch_loss: 0.13853806257247925\n",
      "training: 19 batch 265 batch_loss: 0.13813242316246033\n",
      "training: 19 batch 266 batch_loss: 0.136053204536438\n",
      "training: 19 batch 267 batch_loss: 0.13673818111419678\n",
      "training: 19 batch 268 batch_loss: 0.13763287663459778\n",
      "training: 19 batch 269 batch_loss: 0.13810661435127258\n",
      "training: 19 batch 270 batch_loss: 0.13889893889427185\n",
      "training: 19 batch 271 batch_loss: 0.1390974521636963\n",
      "training: 19 batch 272 batch_loss: 0.1406671404838562\n",
      "training: 19 batch 273 batch_loss: 0.14067476987838745\n",
      "training: 19 batch 274 batch_loss: 0.13711416721343994\n",
      "training: 19 batch 275 batch_loss: 0.13815361261367798\n",
      "training: 19 batch 276 batch_loss: 0.13965639472007751\n",
      "training: 19 batch 277 batch_loss: 0.14143970608711243\n",
      "training: 19 batch 278 batch_loss: 0.13662311434745789\n",
      "training: 19 batch 279 batch_loss: 0.13860195875167847\n",
      "training: 19 batch 280 batch_loss: 0.1385120153427124\n",
      "training: 19 batch 281 batch_loss: 0.14020204544067383\n",
      "training: 19 batch 282 batch_loss: 0.139913409948349\n",
      "training: 19 batch 283 batch_loss: 0.14091253280639648\n",
      "training: 19 batch 284 batch_loss: 0.13860589265823364\n",
      "training: 19 batch 285 batch_loss: 0.13575708866119385\n",
      "training: 19 batch 286 batch_loss: 0.13967928290367126\n",
      "training: 19 batch 287 batch_loss: 0.1385425627231598\n",
      "training: 19 batch 288 batch_loss: 0.13740688562393188\n",
      "training: 19 batch 289 batch_loss: 0.13717037439346313\n",
      "training: 19 batch 290 batch_loss: 0.1393459141254425\n",
      "training: 19 batch 291 batch_loss: 0.1395619511604309\n",
      "training: 19 batch 292 batch_loss: 0.13487499952316284\n",
      "training: 19 batch 293 batch_loss: 0.14050695300102234\n",
      "training: 19 batch 294 batch_loss: 0.13822534680366516\n",
      "training: 19 batch 295 batch_loss: 0.13642284274101257\n",
      "training: 19 batch 296 batch_loss: 0.1390695571899414\n",
      "training: 19 batch 297 batch_loss: 0.13840001821517944\n",
      "training: 19 batch 298 batch_loss: 0.13859471678733826\n",
      "training: 19 batch 299 batch_loss: 0.1406896412372589\n",
      "training: 19 batch 300 batch_loss: 0.14092350006103516\n",
      "training: 19 batch 301 batch_loss: 0.1399308741092682\n",
      "training: 19 batch 302 batch_loss: 0.13540732860565186\n",
      "training: 19 batch 303 batch_loss: 0.13949674367904663\n",
      "training: 19 batch 304 batch_loss: 0.1388069987297058\n",
      "training: 19 batch 305 batch_loss: 0.14014160633087158\n",
      "training: 19 batch 306 batch_loss: 0.1374955177307129\n",
      "training: 19 batch 307 batch_loss: 0.1365523636341095\n",
      "training: 19 batch 308 batch_loss: 0.13932907581329346\n",
      "training: 19 batch 309 batch_loss: 0.14038971066474915\n",
      "training: 19 batch 310 batch_loss: 0.13796085119247437\n",
      "training: 19 batch 311 batch_loss: 0.13800477981567383\n",
      "training: 19 batch 312 batch_loss: 0.1390814185142517\n",
      "training: 19 batch 313 batch_loss: 0.13860085606575012\n",
      "training: 19 batch 314 batch_loss: 0.1364802122116089\n",
      "training: 19 batch 315 batch_loss: 0.14037132263183594\n",
      "training: 19 batch 316 batch_loss: 0.13943827152252197\n",
      "training: 19 batch 317 batch_loss: 0.14053025841712952\n",
      "training: 19 batch 318 batch_loss: 0.13818997144699097\n",
      "training: 19 batch 319 batch_loss: 0.13699224591255188\n",
      "training: 19 batch 320 batch_loss: 0.13816329836845398\n",
      "training: 19 batch 321 batch_loss: 0.13896045088768005\n",
      "training: 19 batch 322 batch_loss: 0.13679081201553345\n",
      "training: 19 batch 323 batch_loss: 0.1422726809978485\n",
      "training: 19 batch 324 batch_loss: 0.13914859294891357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 19 batch 325 batch_loss: 0.13917005062103271\n",
      "training: 19 batch 326 batch_loss: 0.13748860359191895\n",
      "training: 19 batch 327 batch_loss: 0.1393021047115326\n",
      "training: 19 batch 328 batch_loss: 0.14107495546340942\n",
      "training: 19 batch 329 batch_loss: 0.13996928930282593\n",
      "training: 19 batch 330 batch_loss: 0.13877525925636292\n",
      "training: 19 batch 331 batch_loss: 0.13725754618644714\n",
      "training: 19 batch 332 batch_loss: 0.1367158591747284\n",
      "training: 19 batch 333 batch_loss: 0.13970312476158142\n",
      "training: 19 batch 334 batch_loss: 0.1333480179309845\n",
      "training: 19 batch 335 batch_loss: 0.13987144827842712\n",
      "training: 19 batch 336 batch_loss: 0.14131352305412292\n",
      "training: 19 batch 337 batch_loss: 0.1378592848777771\n",
      "training: 19 batch 338 batch_loss: 0.1354956328868866\n",
      "training: 19 batch 339 batch_loss: 0.14174386858940125\n",
      "training: 19 batch 340 batch_loss: 0.14070677757263184\n",
      "training: 19 batch 341 batch_loss: 0.1403830647468567\n",
      "training: 19 batch 342 batch_loss: 0.13905906677246094\n",
      "training: 19 batch 343 batch_loss: 0.13638821244239807\n",
      "training: 19 batch 344 batch_loss: 0.13957479596138\n",
      "training: 19 batch 345 batch_loss: 0.13852500915527344\n",
      "training: 19 batch 346 batch_loss: 0.1354604959487915\n",
      "training: 19 batch 347 batch_loss: 0.1405201554298401\n",
      "training: 19 batch 348 batch_loss: 0.13641813397407532\n",
      "training: 19 batch 349 batch_loss: 0.14266341924667358\n",
      "training: 19 batch 350 batch_loss: 0.13850384950637817\n",
      "training: 19 batch 351 batch_loss: 0.14003032445907593\n",
      "training: 19 batch 352 batch_loss: 0.13721221685409546\n",
      "training: 19 batch 353 batch_loss: 0.13722780346870422\n",
      "training: 19 batch 354 batch_loss: 0.1395379900932312\n",
      "training: 19 batch 355 batch_loss: 0.13660544157028198\n",
      "training: 19 batch 356 batch_loss: 0.13821613788604736\n",
      "training: 19 batch 357 batch_loss: 0.13910171389579773\n",
      "training: 19 batch 358 batch_loss: 0.13916024565696716\n",
      "training: 19 batch 359 batch_loss: 0.13884204626083374\n",
      "training: 19 batch 360 batch_loss: 0.13946813344955444\n",
      "training: 19 batch 361 batch_loss: 0.1402122974395752\n",
      "training: 19 batch 362 batch_loss: 0.1395466923713684\n",
      "training: 19 batch 363 batch_loss: 0.1381811499595642\n",
      "training: 19 batch 364 batch_loss: 0.13998943567276\n",
      "training: 19 batch 365 batch_loss: 0.1388639211654663\n",
      "training: 19 batch 366 batch_loss: 0.14140743017196655\n",
      "training: 19 batch 367 batch_loss: 0.1384311020374298\n",
      "training: 19 batch 368 batch_loss: 0.14037993550300598\n",
      "training: 19 batch 369 batch_loss: 0.14004787802696228\n",
      "training: 19 batch 370 batch_loss: 0.13807755708694458\n",
      "training: 19 batch 371 batch_loss: 0.13718900084495544\n",
      "training: 19 batch 372 batch_loss: 0.13818031549453735\n",
      "training: 19 batch 373 batch_loss: 0.13880661129951477\n",
      "training: 19 batch 374 batch_loss: 0.139784574508667\n",
      "training: 19 batch 375 batch_loss: 0.14268025755882263\n",
      "training: 19 batch 376 batch_loss: 0.13601434230804443\n",
      "training: 19 batch 377 batch_loss: 0.13839510083198547\n",
      "training: 19 batch 378 batch_loss: 0.13826891779899597\n",
      "training: 19 batch 379 batch_loss: 0.13987496495246887\n",
      "training: 19 batch 380 batch_loss: 0.13681066036224365\n",
      "training: 19 batch 381 batch_loss: 0.1379949450492859\n",
      "training: 19 batch 382 batch_loss: 0.13823866844177246\n",
      "training: 19 batch 383 batch_loss: 0.13779622316360474\n",
      "training: 19 batch 384 batch_loss: 0.14026230573654175\n",
      "training: 19 batch 385 batch_loss: 0.13808682560920715\n",
      "training: 19 batch 386 batch_loss: 0.1400890350341797\n",
      "training: 19 batch 387 batch_loss: 0.1395261287689209\n",
      "training: 19 batch 388 batch_loss: 0.14080703258514404\n",
      "training: 19 batch 389 batch_loss: 0.13869315385818481\n",
      "training: 19 batch 390 batch_loss: 0.13949286937713623\n",
      "training: 19 batch 391 batch_loss: 0.1356492042541504\n",
      "training: 19 batch 392 batch_loss: 0.1387154459953308\n",
      "training: 19 batch 393 batch_loss: 0.14002415537834167\n",
      "training: 19 batch 394 batch_loss: 0.14027899503707886\n",
      "training: 19 batch 395 batch_loss: 0.13675642013549805\n",
      "training: 19 batch 396 batch_loss: 0.13872361183166504\n",
      "training: 19 batch 397 batch_loss: 0.13967102766036987\n",
      "training: 19 batch 398 batch_loss: 0.13878202438354492\n",
      "training: 19 batch 399 batch_loss: 0.1403624415397644\n",
      "training: 19 batch 400 batch_loss: 0.13876694440841675\n",
      "training: 19 batch 401 batch_loss: 0.14107251167297363\n",
      "training: 19 batch 402 batch_loss: 0.13745617866516113\n",
      "training: 19 batch 403 batch_loss: 0.1396523118019104\n",
      "training: 19 batch 404 batch_loss: 0.14033761620521545\n",
      "training: 19 batch 405 batch_loss: 0.14166539907455444\n",
      "training: 19 batch 406 batch_loss: 0.13843515515327454\n",
      "training: 19 batch 407 batch_loss: 0.1402193307876587\n",
      "training: 19 batch 408 batch_loss: 0.14014989137649536\n",
      "training: 19 batch 409 batch_loss: 0.14202293753623962\n",
      "training: 19 batch 410 batch_loss: 0.13740992546081543\n",
      "training: 19 batch 411 batch_loss: 0.1359153687953949\n",
      "training: 19 batch 412 batch_loss: 0.1398332417011261\n",
      "training: 19 batch 413 batch_loss: 0.13720834255218506\n",
      "training: 19 batch 414 batch_loss: 0.1384282112121582\n",
      "training: 19 batch 415 batch_loss: 0.1410943865776062\n",
      "training: 19 batch 416 batch_loss: 0.14166608452796936\n",
      "training: 19 batch 417 batch_loss: 0.13868343830108643\n",
      "training: 19 batch 418 batch_loss: 0.13937893509864807\n",
      "training: 19 batch 419 batch_loss: 0.14199838042259216\n",
      "training: 19 batch 420 batch_loss: 0.141706645488739\n",
      "training: 19 batch 421 batch_loss: 0.13872680068016052\n",
      "training: 19 batch 422 batch_loss: 0.14209339022636414\n",
      "training: 19 batch 423 batch_loss: 0.13817083835601807\n",
      "training: 19 batch 424 batch_loss: 0.14237380027770996\n",
      "training: 19 batch 425 batch_loss: 0.13922333717346191\n",
      "training: 19 batch 426 batch_loss: 0.1395319104194641\n",
      "training: 19 batch 427 batch_loss: 0.1414181888103485\n",
      "training: 19 batch 428 batch_loss: 0.13887807726860046\n",
      "training: 19 batch 429 batch_loss: 0.14189669489860535\n",
      "training: 19 batch 430 batch_loss: 0.13946080207824707\n",
      "training: 19 batch 431 batch_loss: 0.14003941416740417\n",
      "training: 19 batch 432 batch_loss: 0.13929811120033264\n",
      "training: 19 batch 433 batch_loss: 0.137905091047287\n",
      "training: 19 batch 434 batch_loss: 0.14033633470535278\n",
      "training: 19 batch 435 batch_loss: 0.1395283341407776\n",
      "training: 19 batch 436 batch_loss: 0.14098960161209106\n",
      "training: 19 batch 437 batch_loss: 0.13824310898780823\n",
      "training: 19 batch 438 batch_loss: 0.14327624440193176\n",
      "training: 19 batch 439 batch_loss: 0.13698095083236694\n",
      "training: 19 batch 440 batch_loss: 0.13948461413383484\n",
      "training: 19 batch 441 batch_loss: 0.14094281196594238\n",
      "training: 19 batch 442 batch_loss: 0.1424543857574463\n",
      "training: 19 batch 443 batch_loss: 0.1393825113773346\n",
      "training: 19 batch 444 batch_loss: 0.13586094975471497\n",
      "training: 19 batch 445 batch_loss: 0.1397205889225006\n",
      "training: 19 batch 446 batch_loss: 0.1390966773033142\n",
      "training: 19 batch 447 batch_loss: 0.14343616366386414\n",
      "training: 19 batch 448 batch_loss: 0.1384669542312622\n",
      "training: 19 batch 449 batch_loss: 0.1407110095024109\n",
      "training: 19 batch 450 batch_loss: 0.14184138178825378\n",
      "training: 19 batch 451 batch_loss: 0.13971969485282898\n",
      "training: 19 batch 452 batch_loss: 0.1406269073486328\n",
      "training: 19 batch 453 batch_loss: 0.1387133002281189\n",
      "training: 19 batch 454 batch_loss: 0.1370970606803894\n",
      "training: 19 batch 455 batch_loss: 0.1400345265865326\n",
      "training: 19 batch 456 batch_loss: 0.13861173391342163\n",
      "training: 19 batch 457 batch_loss: 0.14240789413452148\n",
      "training: 19 batch 458 batch_loss: 0.13992077112197876\n",
      "training: 19 batch 459 batch_loss: 0.1403786540031433\n",
      "training: 19 batch 460 batch_loss: 0.14171496033668518\n",
      "training: 19 batch 461 batch_loss: 0.14042076468467712\n",
      "training: 19 batch 462 batch_loss: 0.13890305161476135\n",
      "training: 19 batch 463 batch_loss: 0.13698554039001465\n",
      "training: 19 batch 464 batch_loss: 0.13918930292129517\n",
      "training: 19 batch 465 batch_loss: 0.13549694418907166\n",
      "training: 19 batch 466 batch_loss: 0.14054259657859802\n",
      "training: 19 batch 467 batch_loss: 0.13663959503173828\n",
      "training: 19 batch 468 batch_loss: 0.1375802457332611\n",
      "training: 19 batch 469 batch_loss: 0.13688555359840393\n",
      "training: 19 batch 470 batch_loss: 0.13783323764801025\n",
      "training: 19 batch 471 batch_loss: 0.14031371474266052\n",
      "training: 19 batch 472 batch_loss: 0.140253484249115\n",
      "training: 19 batch 473 batch_loss: 0.13711154460906982\n",
      "training: 19 batch 474 batch_loss: 0.1398608684539795\n",
      "training: 19 batch 475 batch_loss: 0.1410788595676422\n",
      "training: 19 batch 476 batch_loss: 0.1399112343788147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 19 batch 477 batch_loss: 0.13744616508483887\n",
      "training: 19 batch 478 batch_loss: 0.13796162605285645\n",
      "training: 19 batch 479 batch_loss: 0.14127251505851746\n",
      "training: 19 batch 480 batch_loss: 0.1377388834953308\n",
      "training: 19 batch 481 batch_loss: 0.14046046137809753\n",
      "training: 19 batch 482 batch_loss: 0.13794147968292236\n",
      "training: 19 batch 483 batch_loss: 0.14215174317359924\n",
      "training: 19 batch 484 batch_loss: 0.14295679330825806\n",
      "training: 19 batch 485 batch_loss: 0.13700509071350098\n",
      "training: 19 batch 486 batch_loss: 0.13816317915916443\n",
      "training: 19 batch 487 batch_loss: 0.13445010781288147\n",
      "training: 19 batch 488 batch_loss: 0.13873842358589172\n",
      "training: 19 batch 489 batch_loss: 0.140586256980896\n",
      "training: 19 batch 490 batch_loss: 0.13812187314033508\n",
      "training: 19 batch 491 batch_loss: 0.14115101099014282\n",
      "training: 19 batch 492 batch_loss: 0.14018982648849487\n",
      "training: 19 batch 493 batch_loss: 0.13902384042739868\n",
      "training: 19 batch 494 batch_loss: 0.1352827250957489\n",
      "training: 19 batch 495 batch_loss: 0.13903379440307617\n",
      "training: 19 batch 496 batch_loss: 0.14184394478797913\n",
      "training: 19 batch 497 batch_loss: 0.1413652002811432\n",
      "training: 19 batch 498 batch_loss: 0.13729381561279297\n",
      "training: 19 batch 499 batch_loss: 0.1377248764038086\n",
      "training: 19 batch 500 batch_loss: 0.13691914081573486\n",
      "training: 19 batch 501 batch_loss: 0.13774952292442322\n",
      "training: 19 batch 502 batch_loss: 0.1404854655265808\n",
      "training: 19 batch 503 batch_loss: 0.14016222953796387\n",
      "training: 19 batch 504 batch_loss: 0.13810110092163086\n",
      "training: 19 batch 505 batch_loss: 0.13881024718284607\n",
      "training: 19 batch 506 batch_loss: 0.1383121907711029\n",
      "training: 19 batch 507 batch_loss: 0.13999786972999573\n",
      "training: 19 batch 508 batch_loss: 0.1385067105293274\n",
      "training: 19 batch 509 batch_loss: 0.1438884139060974\n",
      "training: 19 batch 510 batch_loss: 0.14026647806167603\n",
      "training: 19 batch 511 batch_loss: 0.13711366057395935\n",
      "training: 19 batch 512 batch_loss: 0.1408042013645172\n",
      "training: 19 batch 513 batch_loss: 0.14047813415527344\n",
      "training: 19 batch 514 batch_loss: 0.13649901747703552\n",
      "training: 19 batch 515 batch_loss: 0.14115718007087708\n",
      "training: 19 batch 516 batch_loss: 0.13990524411201477\n",
      "training: 19 batch 517 batch_loss: 0.14086458086967468\n",
      "training: 19 batch 518 batch_loss: 0.13894888758659363\n",
      "training: 19 batch 519 batch_loss: 0.14244064688682556\n",
      "training: 19 batch 520 batch_loss: 0.13867637515068054\n",
      "training: 19 batch 521 batch_loss: 0.1433410942554474\n",
      "training: 19 batch 522 batch_loss: 0.1413584053516388\n",
      "training: 19 batch 523 batch_loss: 0.13936683535575867\n",
      "training: 19 batch 524 batch_loss: 0.14074808359146118\n",
      "training: 19 batch 525 batch_loss: 0.13833099603652954\n",
      "training: 19 batch 526 batch_loss: 0.14080995321273804\n",
      "training: 19 batch 527 batch_loss: 0.14012014865875244\n",
      "training: 19 batch 528 batch_loss: 0.14087489247322083\n",
      "training: 19 batch 529 batch_loss: 0.14336377382278442\n",
      "training: 19 batch 530 batch_loss: 0.14143070578575134\n",
      "training: 19 batch 531 batch_loss: 0.1396162509918213\n",
      "training: 19 batch 532 batch_loss: 0.1366228461265564\n",
      "training: 19 batch 533 batch_loss: 0.13924354314804077\n",
      "training: 19 batch 534 batch_loss: 0.13923519849777222\n",
      "training: 19 batch 535 batch_loss: 0.14350485801696777\n",
      "training: 19 batch 536 batch_loss: 0.1373191475868225\n",
      "training: 19 batch 537 batch_loss: 0.14270544052124023\n",
      "training: 19 batch 538 batch_loss: 0.13895562291145325\n",
      "training: 19 batch 539 batch_loss: 0.13895311951637268\n",
      "training: 19 batch 540 batch_loss: 0.14066928625106812\n",
      "training: 19 batch 541 batch_loss: 0.13972580432891846\n",
      "training: 19 batch 542 batch_loss: 0.1370953619480133\n",
      "training: 19 batch 543 batch_loss: 0.13970941305160522\n",
      "training: 19 batch 544 batch_loss: 0.13768696784973145\n",
      "training: 19 batch 545 batch_loss: 0.1381756067276001\n",
      "training: 19 batch 546 batch_loss: 0.13962632417678833\n",
      "training: 19 batch 547 batch_loss: 0.14301100373268127\n",
      "training: 19 batch 548 batch_loss: 0.1373080611228943\n",
      "training: 19 batch 549 batch_loss: 0.14077258110046387\n",
      "training: 19 batch 550 batch_loss: 0.13973194360733032\n",
      "training: 19 batch 551 batch_loss: 0.14377975463867188\n",
      "training: 19 batch 552 batch_loss: 0.138635516166687\n",
      "training: 19 batch 553 batch_loss: 0.14047136902809143\n",
      "training: 19 batch 554 batch_loss: 0.14130976796150208\n",
      "training: 19 batch 555 batch_loss: 0.13955330848693848\n",
      "training: 19 batch 556 batch_loss: 0.13772276043891907\n",
      "training: 19 batch 557 batch_loss: 0.14197582006454468\n",
      "training: 19 batch 558 batch_loss: 0.13984575867652893\n",
      "training: 19 batch 559 batch_loss: 0.13997647166252136\n",
      "training: 19 batch 560 batch_loss: 0.1365179419517517\n",
      "training: 19 batch 561 batch_loss: 0.142103374004364\n",
      "training: 19 batch 562 batch_loss: 0.13807249069213867\n",
      "training: 19 batch 563 batch_loss: 0.13998034596443176\n",
      "training: 19 batch 564 batch_loss: 0.13651633262634277\n",
      "training: 19 batch 565 batch_loss: 0.14222124218940735\n",
      "training: 19 batch 566 batch_loss: 0.13808363676071167\n",
      "training: 19 batch 567 batch_loss: 0.14112675189971924\n",
      "training: 19 batch 568 batch_loss: 0.13707491755485535\n",
      "training: 19 batch 569 batch_loss: 0.1400662362575531\n",
      "training: 19 batch 570 batch_loss: 0.14128848910331726\n",
      "training: 19 batch 571 batch_loss: 0.13949793577194214\n",
      "training: 19 batch 572 batch_loss: 0.14167308807373047\n",
      "training: 19 batch 573 batch_loss: 0.13848629593849182\n",
      "training: 19 batch 574 batch_loss: 0.13985300064086914\n",
      "training: 19 batch 575 batch_loss: 0.1400676667690277\n",
      "training: 19 batch 576 batch_loss: 0.13743799924850464\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 19, Hit Ratio:0.02669948999477003 | Precision:0.039393492578393786 | Recall:0.05473250466105078 | NDCG:0.05010472088395425\n",
      "*Best Performance* \n",
      "Epoch: 19, Hit Ratio:0.02669948999477003 | Precision:0.039393492578393786 | Recall:0.05473250466105078 | MDCG:0.05010472088395425\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 20 batch 0 batch_loss: 0.13910847902297974\n",
      "training: 20 batch 1 batch_loss: 0.1371878981590271\n",
      "training: 20 batch 2 batch_loss: 0.13874760270118713\n",
      "training: 20 batch 3 batch_loss: 0.1353548765182495\n",
      "training: 20 batch 4 batch_loss: 0.13905438780784607\n",
      "training: 20 batch 5 batch_loss: 0.13777166604995728\n",
      "training: 20 batch 6 batch_loss: 0.13980966806411743\n",
      "training: 20 batch 7 batch_loss: 0.14009234309196472\n",
      "training: 20 batch 8 batch_loss: 0.13748455047607422\n",
      "training: 20 batch 9 batch_loss: 0.1378462016582489\n",
      "training: 20 batch 10 batch_loss: 0.13797011971473694\n",
      "training: 20 batch 11 batch_loss: 0.1415521800518036\n",
      "training: 20 batch 12 batch_loss: 0.13753873109817505\n",
      "training: 20 batch 13 batch_loss: 0.13890424370765686\n",
      "training: 20 batch 14 batch_loss: 0.13644260168075562\n",
      "training: 20 batch 15 batch_loss: 0.13835957646369934\n",
      "training: 20 batch 16 batch_loss: 0.14013946056365967\n",
      "training: 20 batch 17 batch_loss: 0.13707324862480164\n",
      "training: 20 batch 18 batch_loss: 0.13795572519302368\n",
      "training: 20 batch 19 batch_loss: 0.13872534036636353\n",
      "training: 20 batch 20 batch_loss: 0.1403447985649109\n",
      "training: 20 batch 21 batch_loss: 0.14081493020057678\n",
      "training: 20 batch 22 batch_loss: 0.1397920548915863\n",
      "training: 20 batch 23 batch_loss: 0.13969436287879944\n",
      "training: 20 batch 24 batch_loss: 0.13978517055511475\n",
      "training: 20 batch 25 batch_loss: 0.13936471939086914\n",
      "training: 20 batch 26 batch_loss: 0.14028173685073853\n",
      "training: 20 batch 27 batch_loss: 0.13573798537254333\n",
      "training: 20 batch 28 batch_loss: 0.13788890838623047\n",
      "training: 20 batch 29 batch_loss: 0.1374073028564453\n",
      "training: 20 batch 30 batch_loss: 0.13998162746429443\n",
      "training: 20 batch 31 batch_loss: 0.13643190264701843\n",
      "training: 20 batch 32 batch_loss: 0.14015069603919983\n",
      "training: 20 batch 33 batch_loss: 0.13893166184425354\n",
      "training: 20 batch 34 batch_loss: 0.139297217130661\n",
      "training: 20 batch 35 batch_loss: 0.13772571086883545\n",
      "training: 20 batch 36 batch_loss: 0.1346931755542755\n",
      "training: 20 batch 37 batch_loss: 0.14111003279685974\n",
      "training: 20 batch 38 batch_loss: 0.14032810926437378\n",
      "training: 20 batch 39 batch_loss: 0.13756781816482544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 20 batch 40 batch_loss: 0.1383209228515625\n",
      "training: 20 batch 41 batch_loss: 0.13766276836395264\n",
      "training: 20 batch 42 batch_loss: 0.13948208093643188\n",
      "training: 20 batch 43 batch_loss: 0.13981610536575317\n",
      "training: 20 batch 44 batch_loss: 0.1370687484741211\n",
      "training: 20 batch 45 batch_loss: 0.14062103629112244\n",
      "training: 20 batch 46 batch_loss: 0.13702630996704102\n",
      "training: 20 batch 47 batch_loss: 0.13941293954849243\n",
      "training: 20 batch 48 batch_loss: 0.13808706402778625\n",
      "training: 20 batch 49 batch_loss: 0.13959220051765442\n",
      "training: 20 batch 50 batch_loss: 0.14034032821655273\n",
      "training: 20 batch 51 batch_loss: 0.13848504424095154\n",
      "training: 20 batch 52 batch_loss: 0.1403895914554596\n",
      "training: 20 batch 53 batch_loss: 0.13809052109718323\n",
      "training: 20 batch 54 batch_loss: 0.140642911195755\n",
      "training: 20 batch 55 batch_loss: 0.13920220732688904\n",
      "training: 20 batch 56 batch_loss: 0.13787004351615906\n",
      "training: 20 batch 57 batch_loss: 0.14003166556358337\n",
      "training: 20 batch 58 batch_loss: 0.14170783758163452\n",
      "training: 20 batch 59 batch_loss: 0.13957995176315308\n",
      "training: 20 batch 60 batch_loss: 0.13995972275733948\n",
      "training: 20 batch 61 batch_loss: 0.13887527585029602\n",
      "training: 20 batch 62 batch_loss: 0.1403055191040039\n",
      "training: 20 batch 63 batch_loss: 0.13975748419761658\n",
      "training: 20 batch 64 batch_loss: 0.14042547345161438\n",
      "training: 20 batch 65 batch_loss: 0.13831856846809387\n",
      "training: 20 batch 66 batch_loss: 0.1372683346271515\n",
      "training: 20 batch 67 batch_loss: 0.1395263671875\n",
      "training: 20 batch 68 batch_loss: 0.13770446181297302\n",
      "training: 20 batch 69 batch_loss: 0.1387263536453247\n",
      "training: 20 batch 70 batch_loss: 0.1394469141960144\n",
      "training: 20 batch 71 batch_loss: 0.13954642415046692\n",
      "training: 20 batch 72 batch_loss: 0.14050507545471191\n",
      "training: 20 batch 73 batch_loss: 0.13888412714004517\n",
      "training: 20 batch 74 batch_loss: 0.13854560256004333\n",
      "training: 20 batch 75 batch_loss: 0.13904288411140442\n",
      "training: 20 batch 76 batch_loss: 0.14116325974464417\n",
      "training: 20 batch 77 batch_loss: 0.14015668630599976\n",
      "training: 20 batch 78 batch_loss: 0.13851487636566162\n",
      "training: 20 batch 79 batch_loss: 0.13788074254989624\n",
      "training: 20 batch 80 batch_loss: 0.1392332911491394\n",
      "training: 20 batch 81 batch_loss: 0.139466792345047\n",
      "training: 20 batch 82 batch_loss: 0.1404837667942047\n",
      "training: 20 batch 83 batch_loss: 0.14065539836883545\n",
      "training: 20 batch 84 batch_loss: 0.13968777656555176\n",
      "training: 20 batch 85 batch_loss: 0.1387864053249359\n",
      "training: 20 batch 86 batch_loss: 0.1402750015258789\n",
      "training: 20 batch 87 batch_loss: 0.14205071330070496\n",
      "training: 20 batch 88 batch_loss: 0.13945990800857544\n",
      "training: 20 batch 89 batch_loss: 0.13731634616851807\n",
      "training: 20 batch 90 batch_loss: 0.13986584544181824\n",
      "training: 20 batch 91 batch_loss: 0.14336106181144714\n",
      "training: 20 batch 92 batch_loss: 0.13761401176452637\n",
      "training: 20 batch 93 batch_loss: 0.141378253698349\n",
      "training: 20 batch 94 batch_loss: 0.13580185174942017\n",
      "training: 20 batch 95 batch_loss: 0.14075109362602234\n",
      "training: 20 batch 96 batch_loss: 0.1401958465576172\n",
      "training: 20 batch 97 batch_loss: 0.14068305492401123\n",
      "training: 20 batch 98 batch_loss: 0.1394972801208496\n",
      "training: 20 batch 99 batch_loss: 0.13977476954460144\n",
      "training: 20 batch 100 batch_loss: 0.14053359627723694\n",
      "training: 20 batch 101 batch_loss: 0.13770943880081177\n",
      "training: 20 batch 102 batch_loss: 0.13682708144187927\n",
      "training: 20 batch 103 batch_loss: 0.13782349228858948\n",
      "training: 20 batch 104 batch_loss: 0.13733378052711487\n",
      "training: 20 batch 105 batch_loss: 0.14087265729904175\n",
      "training: 20 batch 106 batch_loss: 0.14085090160369873\n",
      "training: 20 batch 107 batch_loss: 0.13755929470062256\n",
      "training: 20 batch 108 batch_loss: 0.14044636487960815\n",
      "training: 20 batch 109 batch_loss: 0.13860559463500977\n",
      "training: 20 batch 110 batch_loss: 0.14019986987113953\n",
      "training: 20 batch 111 batch_loss: 0.1394464075565338\n",
      "training: 20 batch 112 batch_loss: 0.14237025380134583\n",
      "training: 20 batch 113 batch_loss: 0.13960209488868713\n",
      "training: 20 batch 114 batch_loss: 0.1388154923915863\n",
      "training: 20 batch 115 batch_loss: 0.13613131642341614\n",
      "training: 20 batch 116 batch_loss: 0.14144805073738098\n",
      "training: 20 batch 117 batch_loss: 0.14150935411453247\n",
      "training: 20 batch 118 batch_loss: 0.13907259702682495\n",
      "training: 20 batch 119 batch_loss: 0.13953393697738647\n",
      "training: 20 batch 120 batch_loss: 0.13920053839683533\n",
      "training: 20 batch 121 batch_loss: 0.1400754451751709\n",
      "training: 20 batch 122 batch_loss: 0.14209327101707458\n",
      "training: 20 batch 123 batch_loss: 0.14076760411262512\n",
      "training: 20 batch 124 batch_loss: 0.13895946741104126\n",
      "training: 20 batch 125 batch_loss: 0.13882654905319214\n",
      "training: 20 batch 126 batch_loss: 0.13820365071296692\n",
      "training: 20 batch 127 batch_loss: 0.13946962356567383\n",
      "training: 20 batch 128 batch_loss: 0.14083543419837952\n",
      "training: 20 batch 129 batch_loss: 0.14116108417510986\n",
      "training: 20 batch 130 batch_loss: 0.14096355438232422\n",
      "training: 20 batch 131 batch_loss: 0.13494831323623657\n",
      "training: 20 batch 132 batch_loss: 0.1393762230873108\n",
      "training: 20 batch 133 batch_loss: 0.13649749755859375\n",
      "training: 20 batch 134 batch_loss: 0.1384144425392151\n",
      "training: 20 batch 135 batch_loss: 0.1378912329673767\n",
      "training: 20 batch 136 batch_loss: 0.139509379863739\n",
      "training: 20 batch 137 batch_loss: 0.1344846785068512\n",
      "training: 20 batch 138 batch_loss: 0.13626587390899658\n",
      "training: 20 batch 139 batch_loss: 0.14189240336418152\n",
      "training: 20 batch 140 batch_loss: 0.13931730389595032\n",
      "training: 20 batch 141 batch_loss: 0.1379188895225525\n",
      "training: 20 batch 142 batch_loss: 0.1370806097984314\n",
      "training: 20 batch 143 batch_loss: 0.13722878694534302\n",
      "training: 20 batch 144 batch_loss: 0.14123982191085815\n",
      "training: 20 batch 145 batch_loss: 0.13742214441299438\n",
      "training: 20 batch 146 batch_loss: 0.1400175392627716\n",
      "training: 20 batch 147 batch_loss: 0.13971763849258423\n",
      "training: 20 batch 148 batch_loss: 0.14047765731811523\n",
      "training: 20 batch 149 batch_loss: 0.14094239473342896\n",
      "training: 20 batch 150 batch_loss: 0.14147478342056274\n",
      "training: 20 batch 151 batch_loss: 0.13796228170394897\n",
      "training: 20 batch 152 batch_loss: 0.13898953795433044\n",
      "training: 20 batch 153 batch_loss: 0.13957393169403076\n",
      "training: 20 batch 154 batch_loss: 0.13770964741706848\n",
      "training: 20 batch 155 batch_loss: 0.14253225922584534\n",
      "training: 20 batch 156 batch_loss: 0.14007669687271118\n",
      "training: 20 batch 157 batch_loss: 0.14377769827842712\n",
      "training: 20 batch 158 batch_loss: 0.14009755849838257\n",
      "training: 20 batch 159 batch_loss: 0.14165949821472168\n",
      "training: 20 batch 160 batch_loss: 0.1386919617652893\n",
      "training: 20 batch 161 batch_loss: 0.13931232690811157\n",
      "training: 20 batch 162 batch_loss: 0.13750648498535156\n",
      "training: 20 batch 163 batch_loss: 0.14109522104263306\n",
      "training: 20 batch 164 batch_loss: 0.13885372877120972\n",
      "training: 20 batch 165 batch_loss: 0.14147672057151794\n",
      "training: 20 batch 166 batch_loss: 0.13717246055603027\n",
      "training: 20 batch 167 batch_loss: 0.13804569840431213\n",
      "training: 20 batch 168 batch_loss: 0.14128321409225464\n",
      "training: 20 batch 169 batch_loss: 0.1391848623752594\n",
      "training: 20 batch 170 batch_loss: 0.13877025246620178\n",
      "training: 20 batch 171 batch_loss: 0.1386643946170807\n",
      "training: 20 batch 172 batch_loss: 0.1418088972568512\n",
      "training: 20 batch 173 batch_loss: 0.14203062653541565\n",
      "training: 20 batch 174 batch_loss: 0.14201152324676514\n",
      "training: 20 batch 175 batch_loss: 0.14040857553482056\n",
      "training: 20 batch 176 batch_loss: 0.1425946056842804\n",
      "training: 20 batch 177 batch_loss: 0.13655799627304077\n",
      "training: 20 batch 178 batch_loss: 0.1396925151348114\n",
      "training: 20 batch 179 batch_loss: 0.13924366235733032\n",
      "training: 20 batch 180 batch_loss: 0.14183029532432556\n",
      "training: 20 batch 181 batch_loss: 0.13753730058670044\n",
      "training: 20 batch 182 batch_loss: 0.13771101832389832\n",
      "training: 20 batch 183 batch_loss: 0.1383175551891327\n",
      "training: 20 batch 184 batch_loss: 0.13830974698066711\n",
      "training: 20 batch 185 batch_loss: 0.14239174127578735\n",
      "training: 20 batch 186 batch_loss: 0.13796603679656982\n",
      "training: 20 batch 187 batch_loss: 0.14040020108222961\n",
      "training: 20 batch 188 batch_loss: 0.1429690718650818\n",
      "training: 20 batch 189 batch_loss: 0.1382550299167633\n",
      "training: 20 batch 190 batch_loss: 0.13982728123664856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 20 batch 191 batch_loss: 0.14191848039627075\n",
      "training: 20 batch 192 batch_loss: 0.1387769877910614\n",
      "training: 20 batch 193 batch_loss: 0.13859352469444275\n",
      "training: 20 batch 194 batch_loss: 0.14053595066070557\n",
      "training: 20 batch 195 batch_loss: 0.13967540860176086\n",
      "training: 20 batch 196 batch_loss: 0.13844087719917297\n",
      "training: 20 batch 197 batch_loss: 0.1405407190322876\n",
      "training: 20 batch 198 batch_loss: 0.1394735872745514\n",
      "training: 20 batch 199 batch_loss: 0.14343738555908203\n",
      "training: 20 batch 200 batch_loss: 0.14166823029518127\n",
      "training: 20 batch 201 batch_loss: 0.1412244737148285\n",
      "training: 20 batch 202 batch_loss: 0.14100682735443115\n",
      "training: 20 batch 203 batch_loss: 0.1368241310119629\n",
      "training: 20 batch 204 batch_loss: 0.14159363508224487\n",
      "training: 20 batch 205 batch_loss: 0.14269602298736572\n",
      "training: 20 batch 206 batch_loss: 0.14309218525886536\n",
      "training: 20 batch 207 batch_loss: 0.1411101520061493\n",
      "training: 20 batch 208 batch_loss: 0.14050135016441345\n",
      "training: 20 batch 209 batch_loss: 0.14238637685775757\n",
      "training: 20 batch 210 batch_loss: 0.13805469870567322\n",
      "training: 20 batch 211 batch_loss: 0.13975024223327637\n",
      "training: 20 batch 212 batch_loss: 0.13964015245437622\n",
      "training: 20 batch 213 batch_loss: 0.14216938614845276\n",
      "training: 20 batch 214 batch_loss: 0.13825002312660217\n",
      "training: 20 batch 215 batch_loss: 0.13941988348960876\n",
      "training: 20 batch 216 batch_loss: 0.1418798863887787\n",
      "training: 20 batch 217 batch_loss: 0.1401207447052002\n",
      "training: 20 batch 218 batch_loss: 0.14160054922103882\n",
      "training: 20 batch 219 batch_loss: 0.1386702060699463\n",
      "training: 20 batch 220 batch_loss: 0.13963425159454346\n",
      "training: 20 batch 221 batch_loss: 0.1420367956161499\n",
      "training: 20 batch 222 batch_loss: 0.14016950130462646\n",
      "training: 20 batch 223 batch_loss: 0.14123579859733582\n",
      "training: 20 batch 224 batch_loss: 0.1422947347164154\n",
      "training: 20 batch 225 batch_loss: 0.1409931182861328\n",
      "training: 20 batch 226 batch_loss: 0.1379416584968567\n",
      "training: 20 batch 227 batch_loss: 0.14226248860359192\n",
      "training: 20 batch 228 batch_loss: 0.14127948880195618\n",
      "training: 20 batch 229 batch_loss: 0.14021140336990356\n",
      "training: 20 batch 230 batch_loss: 0.1396443247795105\n",
      "training: 20 batch 231 batch_loss: 0.14239224791526794\n",
      "training: 20 batch 232 batch_loss: 0.1386801302433014\n",
      "training: 20 batch 233 batch_loss: 0.1384257972240448\n",
      "training: 20 batch 234 batch_loss: 0.14150947332382202\n",
      "training: 20 batch 235 batch_loss: 0.14124438166618347\n",
      "training: 20 batch 236 batch_loss: 0.14157602190971375\n",
      "training: 20 batch 237 batch_loss: 0.14153534173965454\n",
      "training: 20 batch 238 batch_loss: 0.14449283480644226\n",
      "training: 20 batch 239 batch_loss: 0.1381649374961853\n",
      "training: 20 batch 240 batch_loss: 0.14159590005874634\n",
      "training: 20 batch 241 batch_loss: 0.14196860790252686\n",
      "training: 20 batch 242 batch_loss: 0.14152437448501587\n",
      "training: 20 batch 243 batch_loss: 0.1394578516483307\n",
      "training: 20 batch 244 batch_loss: 0.1380753517150879\n",
      "training: 20 batch 245 batch_loss: 0.13934078812599182\n",
      "training: 20 batch 246 batch_loss: 0.14412188529968262\n",
      "training: 20 batch 247 batch_loss: 0.14425009489059448\n",
      "training: 20 batch 248 batch_loss: 0.14216738939285278\n",
      "training: 20 batch 249 batch_loss: 0.13816872239112854\n",
      "training: 20 batch 250 batch_loss: 0.13902387022972107\n",
      "training: 20 batch 251 batch_loss: 0.14414569735527039\n",
      "training: 20 batch 252 batch_loss: 0.14101773500442505\n",
      "training: 20 batch 253 batch_loss: 0.14170771837234497\n",
      "training: 20 batch 254 batch_loss: 0.14164739847183228\n",
      "training: 20 batch 255 batch_loss: 0.140514075756073\n",
      "training: 20 batch 256 batch_loss: 0.14111590385437012\n",
      "training: 20 batch 257 batch_loss: 0.1445416808128357\n",
      "training: 20 batch 258 batch_loss: 0.14085400104522705\n",
      "training: 20 batch 259 batch_loss: 0.1423950493335724\n",
      "training: 20 batch 260 batch_loss: 0.14373168349266052\n",
      "training: 20 batch 261 batch_loss: 0.14025914669036865\n",
      "training: 20 batch 262 batch_loss: 0.14033463597297668\n",
      "training: 20 batch 263 batch_loss: 0.13940545916557312\n",
      "training: 20 batch 264 batch_loss: 0.14162027835845947\n",
      "training: 20 batch 265 batch_loss: 0.13952460885047913\n",
      "training: 20 batch 266 batch_loss: 0.1429370641708374\n",
      "training: 20 batch 267 batch_loss: 0.14452636241912842\n",
      "training: 20 batch 268 batch_loss: 0.14133217930793762\n",
      "training: 20 batch 269 batch_loss: 0.1374187469482422\n",
      "training: 20 batch 270 batch_loss: 0.1428491771221161\n",
      "training: 20 batch 271 batch_loss: 0.14208930730819702\n",
      "training: 20 batch 272 batch_loss: 0.14393684267997742\n",
      "training: 20 batch 273 batch_loss: 0.1422947645187378\n",
      "training: 20 batch 274 batch_loss: 0.13919755816459656\n",
      "training: 20 batch 275 batch_loss: 0.14511221647262573\n",
      "training: 20 batch 276 batch_loss: 0.1415138840675354\n",
      "training: 20 batch 277 batch_loss: 0.14137080311775208\n",
      "training: 20 batch 278 batch_loss: 0.13834506273269653\n",
      "training: 20 batch 279 batch_loss: 0.1391613483428955\n",
      "training: 20 batch 280 batch_loss: 0.14063355326652527\n",
      "training: 20 batch 281 batch_loss: 0.1407257616519928\n",
      "training: 20 batch 282 batch_loss: 0.1430448591709137\n",
      "training: 20 batch 283 batch_loss: 0.14014789462089539\n",
      "training: 20 batch 284 batch_loss: 0.13824599981307983\n",
      "training: 20 batch 285 batch_loss: 0.1420884132385254\n",
      "training: 20 batch 286 batch_loss: 0.1437123715877533\n",
      "training: 20 batch 287 batch_loss: 0.14020675420761108\n",
      "training: 20 batch 288 batch_loss: 0.1412200927734375\n",
      "training: 20 batch 289 batch_loss: 0.1438339352607727\n",
      "training: 20 batch 290 batch_loss: 0.14071720838546753\n",
      "training: 20 batch 291 batch_loss: 0.14019235968589783\n",
      "training: 20 batch 292 batch_loss: 0.14039930701255798\n",
      "training: 20 batch 293 batch_loss: 0.14065876603126526\n",
      "training: 20 batch 294 batch_loss: 0.1400684118270874\n",
      "training: 20 batch 295 batch_loss: 0.1392621099948883\n",
      "training: 20 batch 296 batch_loss: 0.1406984031200409\n",
      "training: 20 batch 297 batch_loss: 0.13846126198768616\n",
      "training: 20 batch 298 batch_loss: 0.14090511202812195\n",
      "training: 20 batch 299 batch_loss: 0.14217934012413025\n",
      "training: 20 batch 300 batch_loss: 0.13977155089378357\n",
      "training: 20 batch 301 batch_loss: 0.1397448182106018\n",
      "training: 20 batch 302 batch_loss: 0.1426454484462738\n",
      "training: 20 batch 303 batch_loss: 0.1366584300994873\n",
      "training: 20 batch 304 batch_loss: 0.13920173048973083\n",
      "training: 20 batch 305 batch_loss: 0.14199942350387573\n",
      "training: 20 batch 306 batch_loss: 0.14144667983055115\n",
      "training: 20 batch 307 batch_loss: 0.14167791604995728\n",
      "training: 20 batch 308 batch_loss: 0.14352819323539734\n",
      "training: 20 batch 309 batch_loss: 0.14201843738555908\n",
      "training: 20 batch 310 batch_loss: 0.13997629284858704\n",
      "training: 20 batch 311 batch_loss: 0.14052927494049072\n",
      "training: 20 batch 312 batch_loss: 0.14177778363227844\n",
      "training: 20 batch 313 batch_loss: 0.14195671677589417\n",
      "training: 20 batch 314 batch_loss: 0.14009878039360046\n",
      "training: 20 batch 315 batch_loss: 0.1398204267024994\n",
      "training: 20 batch 316 batch_loss: 0.1432906687259674\n",
      "training: 20 batch 317 batch_loss: 0.14008769392967224\n",
      "training: 20 batch 318 batch_loss: 0.14609888195991516\n",
      "training: 20 batch 319 batch_loss: 0.14149409532546997\n",
      "training: 20 batch 320 batch_loss: 0.13683968782424927\n",
      "training: 20 batch 321 batch_loss: 0.14172610640525818\n",
      "training: 20 batch 322 batch_loss: 0.14068084955215454\n",
      "training: 20 batch 323 batch_loss: 0.14252927899360657\n",
      "training: 20 batch 324 batch_loss: 0.14202207326889038\n",
      "training: 20 batch 325 batch_loss: 0.13901862502098083\n",
      "training: 20 batch 326 batch_loss: 0.14012500643730164\n",
      "training: 20 batch 327 batch_loss: 0.13556572794914246\n",
      "training: 20 batch 328 batch_loss: 0.14251947402954102\n",
      "training: 20 batch 329 batch_loss: 0.14227938652038574\n",
      "training: 20 batch 330 batch_loss: 0.14013659954071045\n",
      "training: 20 batch 331 batch_loss: 0.14213114976882935\n",
      "training: 20 batch 332 batch_loss: 0.14064541459083557\n",
      "training: 20 batch 333 batch_loss: 0.14005032181739807\n",
      "training: 20 batch 334 batch_loss: 0.13906359672546387\n",
      "training: 20 batch 335 batch_loss: 0.142414391040802\n",
      "training: 20 batch 336 batch_loss: 0.13991540670394897\n",
      "training: 20 batch 337 batch_loss: 0.13747954368591309\n",
      "training: 20 batch 338 batch_loss: 0.14217951893806458\n",
      "training: 20 batch 339 batch_loss: 0.1376360058784485\n",
      "training: 20 batch 340 batch_loss: 0.14219316840171814\n",
      "training: 20 batch 341 batch_loss: 0.13921895623207092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 20 batch 342 batch_loss: 0.14181974530220032\n",
      "training: 20 batch 343 batch_loss: 0.1410943865776062\n",
      "training: 20 batch 344 batch_loss: 0.14254924654960632\n",
      "training: 20 batch 345 batch_loss: 0.1379934549331665\n",
      "training: 20 batch 346 batch_loss: 0.1420351266860962\n",
      "training: 20 batch 347 batch_loss: 0.1378655731678009\n",
      "training: 20 batch 348 batch_loss: 0.14212417602539062\n",
      "training: 20 batch 349 batch_loss: 0.13982465863227844\n",
      "training: 20 batch 350 batch_loss: 0.14240044355392456\n",
      "training: 20 batch 351 batch_loss: 0.14078077673912048\n",
      "training: 20 batch 352 batch_loss: 0.14396175742149353\n",
      "training: 20 batch 353 batch_loss: 0.14496192336082458\n",
      "training: 20 batch 354 batch_loss: 0.13906797766685486\n",
      "training: 20 batch 355 batch_loss: 0.14033612608909607\n",
      "training: 20 batch 356 batch_loss: 0.1418268084526062\n",
      "training: 20 batch 357 batch_loss: 0.14153620600700378\n",
      "training: 20 batch 358 batch_loss: 0.1399281620979309\n",
      "training: 20 batch 359 batch_loss: 0.13889944553375244\n",
      "training: 20 batch 360 batch_loss: 0.14301013946533203\n",
      "training: 20 batch 361 batch_loss: 0.13982966542243958\n",
      "training: 20 batch 362 batch_loss: 0.14217182993888855\n",
      "training: 20 batch 363 batch_loss: 0.14169487357139587\n",
      "training: 20 batch 364 batch_loss: 0.13938993215560913\n",
      "training: 20 batch 365 batch_loss: 0.14389771223068237\n",
      "training: 20 batch 366 batch_loss: 0.14380860328674316\n",
      "training: 20 batch 367 batch_loss: 0.14004376530647278\n",
      "training: 20 batch 368 batch_loss: 0.138315349817276\n",
      "training: 20 batch 369 batch_loss: 0.14345279335975647\n",
      "training: 20 batch 370 batch_loss: 0.14476296305656433\n",
      "training: 20 batch 371 batch_loss: 0.14179086685180664\n",
      "training: 20 batch 372 batch_loss: 0.14134815335273743\n",
      "training: 20 batch 373 batch_loss: 0.1413668394088745\n",
      "training: 20 batch 374 batch_loss: 0.13917657732963562\n",
      "training: 20 batch 375 batch_loss: 0.13912075757980347\n",
      "training: 20 batch 376 batch_loss: 0.13706541061401367\n",
      "training: 20 batch 377 batch_loss: 0.1413504183292389\n",
      "training: 20 batch 378 batch_loss: 0.1429724395275116\n",
      "training: 20 batch 379 batch_loss: 0.1436532735824585\n",
      "training: 20 batch 380 batch_loss: 0.1388159990310669\n",
      "training: 20 batch 381 batch_loss: 0.14021113514900208\n",
      "training: 20 batch 382 batch_loss: 0.1431005895137787\n",
      "training: 20 batch 383 batch_loss: 0.14333665370941162\n",
      "training: 20 batch 384 batch_loss: 0.14049333333969116\n",
      "training: 20 batch 385 batch_loss: 0.13964694738388062\n",
      "training: 20 batch 386 batch_loss: 0.13940906524658203\n",
      "training: 20 batch 387 batch_loss: 0.14426708221435547\n",
      "training: 20 batch 388 batch_loss: 0.1439710557460785\n",
      "training: 20 batch 389 batch_loss: 0.14187893271446228\n",
      "training: 20 batch 390 batch_loss: 0.14509353041648865\n",
      "training: 20 batch 391 batch_loss: 0.1442243456840515\n",
      "training: 20 batch 392 batch_loss: 0.14020392298698425\n",
      "training: 20 batch 393 batch_loss: 0.14175468683242798\n",
      "training: 20 batch 394 batch_loss: 0.13935723900794983\n",
      "training: 20 batch 395 batch_loss: 0.14220315217971802\n",
      "training: 20 batch 396 batch_loss: 0.143297016620636\n",
      "training: 20 batch 397 batch_loss: 0.1394197642803192\n",
      "training: 20 batch 398 batch_loss: 0.14136749505996704\n",
      "training: 20 batch 399 batch_loss: 0.1399765908718109\n",
      "training: 20 batch 400 batch_loss: 0.13900387287139893\n",
      "training: 20 batch 401 batch_loss: 0.14152288436889648\n",
      "training: 20 batch 402 batch_loss: 0.14191901683807373\n",
      "training: 20 batch 403 batch_loss: 0.14131373167037964\n",
      "training: 20 batch 404 batch_loss: 0.14107143878936768\n",
      "training: 20 batch 405 batch_loss: 0.1424662470817566\n",
      "training: 20 batch 406 batch_loss: 0.14329782128334045\n",
      "training: 20 batch 407 batch_loss: 0.14119061827659607\n",
      "training: 20 batch 408 batch_loss: 0.13914158940315247\n",
      "training: 20 batch 409 batch_loss: 0.13979631662368774\n",
      "training: 20 batch 410 batch_loss: 0.14198237657546997\n",
      "training: 20 batch 411 batch_loss: 0.13953939080238342\n",
      "training: 20 batch 412 batch_loss: 0.1396438479423523\n",
      "training: 20 batch 413 batch_loss: 0.14528319239616394\n",
      "training: 20 batch 414 batch_loss: 0.1413523554801941\n",
      "training: 20 batch 415 batch_loss: 0.14006495475769043\n",
      "training: 20 batch 416 batch_loss: 0.1421794593334198\n",
      "training: 20 batch 417 batch_loss: 0.14234915375709534\n",
      "training: 20 batch 418 batch_loss: 0.14092203974723816\n",
      "training: 20 batch 419 batch_loss: 0.14458882808685303\n",
      "training: 20 batch 420 batch_loss: 0.14117702841758728\n",
      "training: 20 batch 421 batch_loss: 0.14231956005096436\n",
      "training: 20 batch 422 batch_loss: 0.14552301168441772\n",
      "training: 20 batch 423 batch_loss: 0.14146897196769714\n",
      "training: 20 batch 424 batch_loss: 0.14165452122688293\n",
      "training: 20 batch 425 batch_loss: 0.14005115628242493\n",
      "training: 20 batch 426 batch_loss: 0.14249974489212036\n",
      "training: 20 batch 427 batch_loss: 0.13883793354034424\n",
      "training: 20 batch 428 batch_loss: 0.1399296522140503\n",
      "training: 20 batch 429 batch_loss: 0.14327508211135864\n",
      "training: 20 batch 430 batch_loss: 0.1430884301662445\n",
      "training: 20 batch 431 batch_loss: 0.14179113507270813\n",
      "training: 20 batch 432 batch_loss: 0.14101099967956543\n",
      "training: 20 batch 433 batch_loss: 0.13801077008247375\n",
      "training: 20 batch 434 batch_loss: 0.13915151357650757\n",
      "training: 20 batch 435 batch_loss: 0.1386663317680359\n",
      "training: 20 batch 436 batch_loss: 0.14065104722976685\n",
      "training: 20 batch 437 batch_loss: 0.14167940616607666\n",
      "training: 20 batch 438 batch_loss: 0.1448301076889038\n",
      "training: 20 batch 439 batch_loss: 0.14296606183052063\n",
      "training: 20 batch 440 batch_loss: 0.14221426844596863\n",
      "training: 20 batch 441 batch_loss: 0.14186254143714905\n",
      "training: 20 batch 442 batch_loss: 0.14453107118606567\n",
      "training: 20 batch 443 batch_loss: 0.1433657705783844\n",
      "training: 20 batch 444 batch_loss: 0.14315831661224365\n",
      "training: 20 batch 445 batch_loss: 0.13793247938156128\n",
      "training: 20 batch 446 batch_loss: 0.13916534185409546\n",
      "training: 20 batch 447 batch_loss: 0.1410495936870575\n",
      "training: 20 batch 448 batch_loss: 0.14407017827033997\n",
      "training: 20 batch 449 batch_loss: 0.14243793487548828\n",
      "training: 20 batch 450 batch_loss: 0.14261338114738464\n",
      "training: 20 batch 451 batch_loss: 0.14136099815368652\n",
      "training: 20 batch 452 batch_loss: 0.14452919363975525\n",
      "training: 20 batch 453 batch_loss: 0.14106282591819763\n",
      "training: 20 batch 454 batch_loss: 0.13854771852493286\n",
      "training: 20 batch 455 batch_loss: 0.1438750922679901\n",
      "training: 20 batch 456 batch_loss: 0.1409870684146881\n",
      "training: 20 batch 457 batch_loss: 0.14508739113807678\n",
      "training: 20 batch 458 batch_loss: 0.13677069544792175\n",
      "training: 20 batch 459 batch_loss: 0.14214253425598145\n",
      "training: 20 batch 460 batch_loss: 0.14286750555038452\n",
      "training: 20 batch 461 batch_loss: 0.1440628170967102\n",
      "training: 20 batch 462 batch_loss: 0.1379428207874298\n",
      "training: 20 batch 463 batch_loss: 0.14316964149475098\n",
      "training: 20 batch 464 batch_loss: 0.14375576376914978\n",
      "training: 20 batch 465 batch_loss: 0.14170461893081665\n",
      "training: 20 batch 466 batch_loss: 0.14415529370307922\n",
      "training: 20 batch 467 batch_loss: 0.1446487307548523\n",
      "training: 20 batch 468 batch_loss: 0.14082643389701843\n",
      "training: 20 batch 469 batch_loss: 0.14596182107925415\n",
      "training: 20 batch 470 batch_loss: 0.14482873678207397\n",
      "training: 20 batch 471 batch_loss: 0.14288169145584106\n",
      "training: 20 batch 472 batch_loss: 0.14375662803649902\n",
      "training: 20 batch 473 batch_loss: 0.14082598686218262\n",
      "training: 20 batch 474 batch_loss: 0.1410975158214569\n",
      "training: 20 batch 475 batch_loss: 0.1398155689239502\n",
      "training: 20 batch 476 batch_loss: 0.141015887260437\n",
      "training: 20 batch 477 batch_loss: 0.14141753315925598\n",
      "training: 20 batch 478 batch_loss: 0.14207151532173157\n",
      "training: 20 batch 479 batch_loss: 0.1441054344177246\n",
      "training: 20 batch 480 batch_loss: 0.14259737730026245\n",
      "training: 20 batch 481 batch_loss: 0.14330896735191345\n",
      "training: 20 batch 482 batch_loss: 0.13945221900939941\n",
      "training: 20 batch 483 batch_loss: 0.14144748449325562\n",
      "training: 20 batch 484 batch_loss: 0.14062541723251343\n",
      "training: 20 batch 485 batch_loss: 0.14255410432815552\n",
      "training: 20 batch 486 batch_loss: 0.14088037610054016\n",
      "training: 20 batch 487 batch_loss: 0.13999643921852112\n",
      "training: 20 batch 488 batch_loss: 0.1449812650680542\n",
      "training: 20 batch 489 batch_loss: 0.14223015308380127\n",
      "training: 20 batch 490 batch_loss: 0.14182692766189575\n",
      "training: 20 batch 491 batch_loss: 0.14072257280349731\n",
      "training: 20 batch 492 batch_loss: 0.14008685946464539\n",
      "training: 20 batch 493 batch_loss: 0.14042985439300537\n",
      "training: 20 batch 494 batch_loss: 0.14340373873710632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 20 batch 495 batch_loss: 0.14389526844024658\n",
      "training: 20 batch 496 batch_loss: 0.14010119438171387\n",
      "training: 20 batch 497 batch_loss: 0.14452660083770752\n",
      "training: 20 batch 498 batch_loss: 0.14200085401535034\n",
      "training: 20 batch 499 batch_loss: 0.14484372735023499\n",
      "training: 20 batch 500 batch_loss: 0.14175400137901306\n",
      "training: 20 batch 501 batch_loss: 0.14201083779335022\n",
      "training: 20 batch 502 batch_loss: 0.14629894495010376\n",
      "training: 20 batch 503 batch_loss: 0.14269974827766418\n",
      "training: 20 batch 504 batch_loss: 0.1444394290447235\n",
      "training: 20 batch 505 batch_loss: 0.14430034160614014\n",
      "training: 20 batch 506 batch_loss: 0.1410072147846222\n",
      "training: 20 batch 507 batch_loss: 0.14382147789001465\n",
      "training: 20 batch 508 batch_loss: 0.14492926001548767\n",
      "training: 20 batch 509 batch_loss: 0.14290320873260498\n",
      "training: 20 batch 510 batch_loss: 0.14022773504257202\n",
      "training: 20 batch 511 batch_loss: 0.14318042993545532\n",
      "training: 20 batch 512 batch_loss: 0.1399519443511963\n",
      "training: 20 batch 513 batch_loss: 0.14093321561813354\n",
      "training: 20 batch 514 batch_loss: 0.14587491750717163\n",
      "training: 20 batch 515 batch_loss: 0.14283931255340576\n",
      "training: 20 batch 516 batch_loss: 0.14298051595687866\n",
      "training: 20 batch 517 batch_loss: 0.14309117197990417\n",
      "training: 20 batch 518 batch_loss: 0.14431709051132202\n",
      "training: 20 batch 519 batch_loss: 0.13945487141609192\n",
      "training: 20 batch 520 batch_loss: 0.14196348190307617\n",
      "training: 20 batch 521 batch_loss: 0.14099696278572083\n",
      "training: 20 batch 522 batch_loss: 0.14416879415512085\n",
      "training: 20 batch 523 batch_loss: 0.14425420761108398\n",
      "training: 20 batch 524 batch_loss: 0.14293569326400757\n",
      "training: 20 batch 525 batch_loss: 0.14476197957992554\n",
      "training: 20 batch 526 batch_loss: 0.14029830694198608\n",
      "training: 20 batch 527 batch_loss: 0.1395188868045807\n",
      "training: 20 batch 528 batch_loss: 0.1429525911808014\n",
      "training: 20 batch 529 batch_loss: 0.1404494047164917\n",
      "training: 20 batch 530 batch_loss: 0.14041432738304138\n",
      "training: 20 batch 531 batch_loss: 0.1397455930709839\n",
      "training: 20 batch 532 batch_loss: 0.14109399914741516\n",
      "training: 20 batch 533 batch_loss: 0.1425182819366455\n",
      "training: 20 batch 534 batch_loss: 0.1435948610305786\n",
      "training: 20 batch 535 batch_loss: 0.14144766330718994\n",
      "training: 20 batch 536 batch_loss: 0.14514148235321045\n",
      "training: 20 batch 537 batch_loss: 0.14090502262115479\n",
      "training: 20 batch 538 batch_loss: 0.1460680365562439\n",
      "training: 20 batch 539 batch_loss: 0.14189693331718445\n",
      "training: 20 batch 540 batch_loss: 0.14048755168914795\n",
      "training: 20 batch 541 batch_loss: 0.14447295665740967\n",
      "training: 20 batch 542 batch_loss: 0.1429629623889923\n",
      "training: 20 batch 543 batch_loss: 0.14310401678085327\n",
      "training: 20 batch 544 batch_loss: 0.14127084612846375\n",
      "training: 20 batch 545 batch_loss: 0.14533749222755432\n",
      "training: 20 batch 546 batch_loss: 0.14266592264175415\n",
      "training: 20 batch 547 batch_loss: 0.1420269012451172\n",
      "training: 20 batch 548 batch_loss: 0.14156848192214966\n",
      "training: 20 batch 549 batch_loss: 0.14141306281089783\n",
      "training: 20 batch 550 batch_loss: 0.1405356526374817\n",
      "training: 20 batch 551 batch_loss: 0.14367780089378357\n",
      "training: 20 batch 552 batch_loss: 0.14466869831085205\n",
      "training: 20 batch 553 batch_loss: 0.14495739340782166\n",
      "training: 20 batch 554 batch_loss: 0.1398148536682129\n",
      "training: 20 batch 555 batch_loss: 0.14166715741157532\n",
      "training: 20 batch 556 batch_loss: 0.1416797637939453\n",
      "training: 20 batch 557 batch_loss: 0.1451387107372284\n",
      "training: 20 batch 558 batch_loss: 0.14318829774856567\n",
      "training: 20 batch 559 batch_loss: 0.14161086082458496\n",
      "training: 20 batch 560 batch_loss: 0.1435646414756775\n",
      "training: 20 batch 561 batch_loss: 0.1417226791381836\n",
      "training: 20 batch 562 batch_loss: 0.13937994837760925\n",
      "training: 20 batch 563 batch_loss: 0.14311903715133667\n",
      "training: 20 batch 564 batch_loss: 0.14380532503128052\n",
      "training: 20 batch 565 batch_loss: 0.1437833309173584\n",
      "training: 20 batch 566 batch_loss: 0.14362266659736633\n",
      "training: 20 batch 567 batch_loss: 0.14128243923187256\n",
      "training: 20 batch 568 batch_loss: 0.14393815398216248\n",
      "training: 20 batch 569 batch_loss: 0.14225715398788452\n",
      "training: 20 batch 570 batch_loss: 0.14164206385612488\n",
      "training: 20 batch 571 batch_loss: 0.14156025648117065\n",
      "training: 20 batch 572 batch_loss: 0.1403934359550476\n",
      "training: 20 batch 573 batch_loss: 0.14211910963058472\n",
      "training: 20 batch 574 batch_loss: 0.14065101742744446\n",
      "training: 20 batch 575 batch_loss: 0.14286813139915466\n",
      "training: 20 batch 576 batch_loss: 0.1392466425895691\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 20, Hit Ratio:0.02752562518113347 | Precision:0.040612405386808215 | Recall:0.05565703171805906 | NDCG:0.05147962604807236\n",
      "*Best Performance* \n",
      "Epoch: 20, Hit Ratio:0.02752562518113347 | Precision:0.040612405386808215 | Recall:0.05565703171805906 | MDCG:0.05147962604807236\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 21 batch 0 batch_loss: 0.13887643814086914\n",
      "training: 21 batch 1 batch_loss: 0.13950973749160767\n",
      "training: 21 batch 2 batch_loss: 0.14428317546844482\n",
      "training: 21 batch 3 batch_loss: 0.14190012216567993\n",
      "training: 21 batch 4 batch_loss: 0.14224833250045776\n",
      "training: 21 batch 5 batch_loss: 0.14193052053451538\n",
      "training: 21 batch 6 batch_loss: 0.13908037543296814\n",
      "training: 21 batch 7 batch_loss: 0.14110276103019714\n",
      "training: 21 batch 8 batch_loss: 0.1408047378063202\n",
      "training: 21 batch 9 batch_loss: 0.14140775799751282\n",
      "training: 21 batch 10 batch_loss: 0.1416577398777008\n",
      "training: 21 batch 11 batch_loss: 0.14164572954177856\n",
      "training: 21 batch 12 batch_loss: 0.1397101879119873\n",
      "training: 21 batch 13 batch_loss: 0.14222148060798645\n",
      "training: 21 batch 14 batch_loss: 0.1417214572429657\n",
      "training: 21 batch 15 batch_loss: 0.14250844717025757\n",
      "training: 21 batch 16 batch_loss: 0.13891646265983582\n",
      "training: 21 batch 17 batch_loss: 0.1390787959098816\n",
      "training: 21 batch 18 batch_loss: 0.14116179943084717\n",
      "training: 21 batch 19 batch_loss: 0.14396235346794128\n",
      "training: 21 batch 20 batch_loss: 0.1395787000656128\n",
      "training: 21 batch 21 batch_loss: 0.14017820358276367\n",
      "training: 21 batch 22 batch_loss: 0.14324727654457092\n",
      "training: 21 batch 23 batch_loss: 0.14319032430648804\n",
      "training: 21 batch 24 batch_loss: 0.1392500102519989\n",
      "training: 21 batch 25 batch_loss: 0.1397087574005127\n",
      "training: 21 batch 26 batch_loss: 0.14011582732200623\n",
      "training: 21 batch 27 batch_loss: 0.1391679346561432\n",
      "training: 21 batch 28 batch_loss: 0.1419585943222046\n",
      "training: 21 batch 29 batch_loss: 0.1413823366165161\n",
      "training: 21 batch 30 batch_loss: 0.14180678129196167\n",
      "training: 21 batch 31 batch_loss: 0.1410723626613617\n",
      "training: 21 batch 32 batch_loss: 0.13966619968414307\n",
      "training: 21 batch 33 batch_loss: 0.14043134450912476\n",
      "training: 21 batch 34 batch_loss: 0.1413324475288391\n",
      "training: 21 batch 35 batch_loss: 0.13949495553970337\n",
      "training: 21 batch 36 batch_loss: 0.14202025532722473\n",
      "training: 21 batch 37 batch_loss: 0.14394938945770264\n",
      "training: 21 batch 38 batch_loss: 0.1428285539150238\n",
      "training: 21 batch 39 batch_loss: 0.13969320058822632\n",
      "training: 21 batch 40 batch_loss: 0.13704192638397217\n",
      "training: 21 batch 41 batch_loss: 0.14144611358642578\n",
      "training: 21 batch 42 batch_loss: 0.14157146215438843\n",
      "training: 21 batch 43 batch_loss: 0.14025980234146118\n",
      "training: 21 batch 44 batch_loss: 0.1439252495765686\n",
      "training: 21 batch 45 batch_loss: 0.13928037881851196\n",
      "training: 21 batch 46 batch_loss: 0.14373818039894104\n",
      "training: 21 batch 47 batch_loss: 0.14386224746704102\n",
      "training: 21 batch 48 batch_loss: 0.1396365761756897\n",
      "training: 21 batch 49 batch_loss: 0.14340639114379883\n",
      "training: 21 batch 50 batch_loss: 0.13928282260894775\n",
      "training: 21 batch 51 batch_loss: 0.13976281881332397\n",
      "training: 21 batch 52 batch_loss: 0.1435375213623047\n",
      "training: 21 batch 53 batch_loss: 0.13930168747901917\n",
      "training: 21 batch 54 batch_loss: 0.13817626237869263\n",
      "training: 21 batch 55 batch_loss: 0.13990461826324463\n",
      "training: 21 batch 56 batch_loss: 0.1402735710144043\n",
      "training: 21 batch 57 batch_loss: 0.14457616209983826\n",
      "training: 21 batch 58 batch_loss: 0.14390718936920166\n",
      "training: 21 batch 59 batch_loss: 0.14341643452644348\n",
      "training: 21 batch 60 batch_loss: 0.14317211508750916\n",
      "training: 21 batch 61 batch_loss: 0.14134261012077332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 21 batch 62 batch_loss: 0.1440873146057129\n",
      "training: 21 batch 63 batch_loss: 0.1427271068096161\n",
      "training: 21 batch 64 batch_loss: 0.14218354225158691\n",
      "training: 21 batch 65 batch_loss: 0.13984113931655884\n",
      "training: 21 batch 66 batch_loss: 0.14291054010391235\n",
      "training: 21 batch 67 batch_loss: 0.14051160216331482\n",
      "training: 21 batch 68 batch_loss: 0.14068827033042908\n",
      "training: 21 batch 69 batch_loss: 0.14480477571487427\n",
      "training: 21 batch 70 batch_loss: 0.14235657453536987\n",
      "training: 21 batch 71 batch_loss: 0.14164969325065613\n",
      "training: 21 batch 72 batch_loss: 0.13880407810211182\n",
      "training: 21 batch 73 batch_loss: 0.14375600218772888\n",
      "training: 21 batch 74 batch_loss: 0.14086970686912537\n",
      "training: 21 batch 75 batch_loss: 0.14290106296539307\n",
      "training: 21 batch 76 batch_loss: 0.14192527532577515\n",
      "training: 21 batch 77 batch_loss: 0.14410632848739624\n",
      "training: 21 batch 78 batch_loss: 0.14128533005714417\n",
      "training: 21 batch 79 batch_loss: 0.14367079734802246\n",
      "training: 21 batch 80 batch_loss: 0.14190474152565002\n",
      "training: 21 batch 81 batch_loss: 0.14041709899902344\n",
      "training: 21 batch 82 batch_loss: 0.14114177227020264\n",
      "training: 21 batch 83 batch_loss: 0.14090266823768616\n",
      "training: 21 batch 84 batch_loss: 0.14462926983833313\n",
      "training: 21 batch 85 batch_loss: 0.14041194319725037\n",
      "training: 21 batch 86 batch_loss: 0.13980954885482788\n",
      "training: 21 batch 87 batch_loss: 0.14308017492294312\n",
      "training: 21 batch 88 batch_loss: 0.1409195065498352\n",
      "training: 21 batch 89 batch_loss: 0.14437904953956604\n",
      "training: 21 batch 90 batch_loss: 0.14208710193634033\n",
      "training: 21 batch 91 batch_loss: 0.14319190382957458\n",
      "training: 21 batch 92 batch_loss: 0.14262881875038147\n",
      "training: 21 batch 93 batch_loss: 0.14228537678718567\n",
      "training: 21 batch 94 batch_loss: 0.1406227946281433\n",
      "training: 21 batch 95 batch_loss: 0.14305421710014343\n",
      "training: 21 batch 96 batch_loss: 0.13948053121566772\n",
      "training: 21 batch 97 batch_loss: 0.14319312572479248\n",
      "training: 21 batch 98 batch_loss: 0.14076802134513855\n",
      "training: 21 batch 99 batch_loss: 0.1443234086036682\n",
      "training: 21 batch 100 batch_loss: 0.13798949122428894\n",
      "training: 21 batch 101 batch_loss: 0.1435975730419159\n",
      "training: 21 batch 102 batch_loss: 0.14187461137771606\n",
      "training: 21 batch 103 batch_loss: 0.1415240466594696\n",
      "training: 21 batch 104 batch_loss: 0.1412624716758728\n",
      "training: 21 batch 105 batch_loss: 0.14211812615394592\n",
      "training: 21 batch 106 batch_loss: 0.14428222179412842\n",
      "training: 21 batch 107 batch_loss: 0.13844192028045654\n",
      "training: 21 batch 108 batch_loss: 0.14222180843353271\n",
      "training: 21 batch 109 batch_loss: 0.1449250876903534\n",
      "training: 21 batch 110 batch_loss: 0.14067673683166504\n",
      "training: 21 batch 111 batch_loss: 0.14274153113365173\n",
      "training: 21 batch 112 batch_loss: 0.1420941948890686\n",
      "training: 21 batch 113 batch_loss: 0.13795432448387146\n",
      "training: 21 batch 114 batch_loss: 0.1444297730922699\n",
      "training: 21 batch 115 batch_loss: 0.14437124133110046\n",
      "training: 21 batch 116 batch_loss: 0.14140498638153076\n",
      "training: 21 batch 117 batch_loss: 0.140267014503479\n",
      "training: 21 batch 118 batch_loss: 0.14301171898841858\n",
      "training: 21 batch 119 batch_loss: 0.13963088393211365\n",
      "training: 21 batch 120 batch_loss: 0.14148539304733276\n",
      "training: 21 batch 121 batch_loss: 0.14303845167160034\n",
      "training: 21 batch 122 batch_loss: 0.14386484026908875\n",
      "training: 21 batch 123 batch_loss: 0.14110255241394043\n",
      "training: 21 batch 124 batch_loss: 0.14417880773544312\n",
      "training: 21 batch 125 batch_loss: 0.14274927973747253\n",
      "training: 21 batch 126 batch_loss: 0.1417621374130249\n",
      "training: 21 batch 127 batch_loss: 0.14417460560798645\n",
      "training: 21 batch 128 batch_loss: 0.14141947031021118\n",
      "training: 21 batch 129 batch_loss: 0.14196044206619263\n",
      "training: 21 batch 130 batch_loss: 0.1426190733909607\n",
      "training: 21 batch 131 batch_loss: 0.14001092314720154\n",
      "training: 21 batch 132 batch_loss: 0.14172226190567017\n",
      "training: 21 batch 133 batch_loss: 0.14230236411094666\n",
      "training: 21 batch 134 batch_loss: 0.14082658290863037\n",
      "training: 21 batch 135 batch_loss: 0.14228028059005737\n",
      "training: 21 batch 136 batch_loss: 0.14223802089691162\n",
      "training: 21 batch 137 batch_loss: 0.13901615142822266\n",
      "training: 21 batch 138 batch_loss: 0.13907688856124878\n",
      "training: 21 batch 139 batch_loss: 0.1403009593486786\n",
      "training: 21 batch 140 batch_loss: 0.14358627796173096\n",
      "training: 21 batch 141 batch_loss: 0.14062201976776123\n",
      "training: 21 batch 142 batch_loss: 0.1424664855003357\n",
      "training: 21 batch 143 batch_loss: 0.14330312609672546\n",
      "training: 21 batch 144 batch_loss: 0.13991209864616394\n",
      "training: 21 batch 145 batch_loss: 0.1433205008506775\n",
      "training: 21 batch 146 batch_loss: 0.13915616273880005\n",
      "training: 21 batch 147 batch_loss: 0.140579491853714\n",
      "training: 21 batch 148 batch_loss: 0.14104923605918884\n",
      "training: 21 batch 149 batch_loss: 0.14075511693954468\n",
      "training: 21 batch 150 batch_loss: 0.1408013105392456\n",
      "training: 21 batch 151 batch_loss: 0.14416855573654175\n",
      "training: 21 batch 152 batch_loss: 0.14771592617034912\n",
      "training: 21 batch 153 batch_loss: 0.14341473579406738\n",
      "training: 21 batch 154 batch_loss: 0.14222750067710876\n",
      "training: 21 batch 155 batch_loss: 0.14336013793945312\n",
      "training: 21 batch 156 batch_loss: 0.14373543858528137\n",
      "training: 21 batch 157 batch_loss: 0.14390313625335693\n",
      "training: 21 batch 158 batch_loss: 0.1417371928691864\n",
      "training: 21 batch 159 batch_loss: 0.14360421895980835\n",
      "training: 21 batch 160 batch_loss: 0.1432051658630371\n",
      "training: 21 batch 161 batch_loss: 0.1463756263256073\n",
      "training: 21 batch 162 batch_loss: 0.14347997307777405\n",
      "training: 21 batch 163 batch_loss: 0.14502397179603577\n",
      "training: 21 batch 164 batch_loss: 0.14558184146881104\n",
      "training: 21 batch 165 batch_loss: 0.14400014281272888\n",
      "training: 21 batch 166 batch_loss: 0.1470581293106079\n",
      "training: 21 batch 167 batch_loss: 0.1421111524105072\n",
      "training: 21 batch 168 batch_loss: 0.13933810591697693\n",
      "training: 21 batch 169 batch_loss: 0.1408672332763672\n",
      "training: 21 batch 170 batch_loss: 0.14275327324867249\n",
      "training: 21 batch 171 batch_loss: 0.13905414938926697\n",
      "training: 21 batch 172 batch_loss: 0.14048361778259277\n",
      "training: 21 batch 173 batch_loss: 0.13939768075942993\n",
      "training: 21 batch 174 batch_loss: 0.14328375458717346\n",
      "training: 21 batch 175 batch_loss: 0.14133739471435547\n",
      "training: 21 batch 176 batch_loss: 0.13873150944709778\n",
      "training: 21 batch 177 batch_loss: 0.14304065704345703\n",
      "training: 21 batch 178 batch_loss: 0.14160817861557007\n",
      "training: 21 batch 179 batch_loss: 0.14256662130355835\n",
      "training: 21 batch 180 batch_loss: 0.14071989059448242\n",
      "training: 21 batch 181 batch_loss: 0.14364877343177795\n",
      "training: 21 batch 182 batch_loss: 0.1421341598033905\n",
      "training: 21 batch 183 batch_loss: 0.1455959975719452\n",
      "training: 21 batch 184 batch_loss: 0.1427825689315796\n",
      "training: 21 batch 185 batch_loss: 0.1414221227169037\n",
      "training: 21 batch 186 batch_loss: 0.14102545380592346\n",
      "training: 21 batch 187 batch_loss: 0.1421622931957245\n",
      "training: 21 batch 188 batch_loss: 0.1420910358428955\n",
      "training: 21 batch 189 batch_loss: 0.13801467418670654\n",
      "training: 21 batch 190 batch_loss: 0.14755648374557495\n",
      "training: 21 batch 191 batch_loss: 0.14381492137908936\n",
      "training: 21 batch 192 batch_loss: 0.14755740761756897\n",
      "training: 21 batch 193 batch_loss: 0.14403516054153442\n",
      "training: 21 batch 194 batch_loss: 0.14273440837860107\n",
      "training: 21 batch 195 batch_loss: 0.1429540514945984\n",
      "training: 21 batch 196 batch_loss: 0.14594629406929016\n",
      "training: 21 batch 197 batch_loss: 0.14173990488052368\n",
      "training: 21 batch 198 batch_loss: 0.14040136337280273\n",
      "training: 21 batch 199 batch_loss: 0.14039883017539978\n",
      "training: 21 batch 200 batch_loss: 0.1425812840461731\n",
      "training: 21 batch 201 batch_loss: 0.14049172401428223\n",
      "training: 21 batch 202 batch_loss: 0.14517027139663696\n",
      "training: 21 batch 203 batch_loss: 0.13938599824905396\n",
      "training: 21 batch 204 batch_loss: 0.14435619115829468\n",
      "training: 21 batch 205 batch_loss: 0.14635446667671204\n",
      "training: 21 batch 206 batch_loss: 0.1448265016078949\n",
      "training: 21 batch 207 batch_loss: 0.14340904355049133\n",
      "training: 21 batch 208 batch_loss: 0.1468890905380249\n",
      "training: 21 batch 209 batch_loss: 0.14256906509399414\n",
      "training: 21 batch 210 batch_loss: 0.14333981275558472\n",
      "training: 21 batch 211 batch_loss: 0.144168883562088\n",
      "training: 21 batch 212 batch_loss: 0.14332085847854614\n",
      "training: 21 batch 213 batch_loss: 0.14605799317359924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 21 batch 214 batch_loss: 0.14334028959274292\n",
      "training: 21 batch 215 batch_loss: 0.14075657725334167\n",
      "training: 21 batch 216 batch_loss: 0.14271867275238037\n",
      "training: 21 batch 217 batch_loss: 0.14552998542785645\n",
      "training: 21 batch 218 batch_loss: 0.14365941286087036\n",
      "training: 21 batch 219 batch_loss: 0.14356034994125366\n",
      "training: 21 batch 220 batch_loss: 0.14157432317733765\n",
      "training: 21 batch 221 batch_loss: 0.1441890001296997\n",
      "training: 21 batch 222 batch_loss: 0.14211779832839966\n",
      "training: 21 batch 223 batch_loss: 0.14472275972366333\n",
      "training: 21 batch 224 batch_loss: 0.14567962288856506\n",
      "training: 21 batch 225 batch_loss: 0.14470067620277405\n",
      "training: 21 batch 226 batch_loss: 0.14270779490470886\n",
      "training: 21 batch 227 batch_loss: 0.14360898733139038\n",
      "training: 21 batch 228 batch_loss: 0.1402132213115692\n",
      "training: 21 batch 229 batch_loss: 0.1412167251110077\n",
      "training: 21 batch 230 batch_loss: 0.14346998929977417\n",
      "training: 21 batch 231 batch_loss: 0.14340516924858093\n",
      "training: 21 batch 232 batch_loss: 0.14126384258270264\n",
      "training: 21 batch 233 batch_loss: 0.14319860935211182\n",
      "training: 21 batch 234 batch_loss: 0.13948503136634827\n",
      "training: 21 batch 235 batch_loss: 0.1424056589603424\n",
      "training: 21 batch 236 batch_loss: 0.14065924286842346\n",
      "training: 21 batch 237 batch_loss: 0.14268481731414795\n",
      "training: 21 batch 238 batch_loss: 0.14295253157615662\n",
      "training: 21 batch 239 batch_loss: 0.14467120170593262\n",
      "training: 21 batch 240 batch_loss: 0.14645719528198242\n",
      "training: 21 batch 241 batch_loss: 0.14383602142333984\n",
      "training: 21 batch 242 batch_loss: 0.14383581280708313\n",
      "training: 21 batch 243 batch_loss: 0.14952737092971802\n",
      "training: 21 batch 244 batch_loss: 0.14465492963790894\n",
      "training: 21 batch 245 batch_loss: 0.14039745926856995\n",
      "training: 21 batch 246 batch_loss: 0.14198929071426392\n",
      "training: 21 batch 247 batch_loss: 0.1440172791481018\n",
      "training: 21 batch 248 batch_loss: 0.14026504755020142\n",
      "training: 21 batch 249 batch_loss: 0.14408665895462036\n",
      "training: 21 batch 250 batch_loss: 0.14368784427642822\n",
      "training: 21 batch 251 batch_loss: 0.14428049325942993\n",
      "training: 21 batch 252 batch_loss: 0.14104929566383362\n",
      "training: 21 batch 253 batch_loss: 0.1454855501651764\n",
      "training: 21 batch 254 batch_loss: 0.1404629647731781\n",
      "training: 21 batch 255 batch_loss: 0.14734995365142822\n",
      "training: 21 batch 256 batch_loss: 0.14309659600257874\n",
      "training: 21 batch 257 batch_loss: 0.14598962664604187\n",
      "training: 21 batch 258 batch_loss: 0.14547431468963623\n",
      "training: 21 batch 259 batch_loss: 0.14730221033096313\n",
      "training: 21 batch 260 batch_loss: 0.1421075463294983\n",
      "training: 21 batch 261 batch_loss: 0.14097172021865845\n",
      "training: 21 batch 262 batch_loss: 0.1448020040988922\n",
      "training: 21 batch 263 batch_loss: 0.1429087221622467\n",
      "training: 21 batch 264 batch_loss: 0.1461142599582672\n",
      "training: 21 batch 265 batch_loss: 0.1413325071334839\n",
      "training: 21 batch 266 batch_loss: 0.14097347855567932\n",
      "training: 21 batch 267 batch_loss: 0.140102356672287\n",
      "training: 21 batch 268 batch_loss: 0.14022758603096008\n",
      "training: 21 batch 269 batch_loss: 0.14311864972114563\n",
      "training: 21 batch 270 batch_loss: 0.14246195554733276\n",
      "training: 21 batch 271 batch_loss: 0.14534246921539307\n",
      "training: 21 batch 272 batch_loss: 0.14243844151496887\n",
      "training: 21 batch 273 batch_loss: 0.14372268319129944\n",
      "training: 21 batch 274 batch_loss: 0.1439753770828247\n",
      "training: 21 batch 275 batch_loss: 0.1414065659046173\n",
      "training: 21 batch 276 batch_loss: 0.14288601279258728\n",
      "training: 21 batch 277 batch_loss: 0.14380159974098206\n",
      "training: 21 batch 278 batch_loss: 0.14154615998268127\n",
      "training: 21 batch 279 batch_loss: 0.1433998942375183\n",
      "training: 21 batch 280 batch_loss: 0.14316332340240479\n",
      "training: 21 batch 281 batch_loss: 0.14351415634155273\n",
      "training: 21 batch 282 batch_loss: 0.1435990035533905\n",
      "training: 21 batch 283 batch_loss: 0.1451924443244934\n",
      "training: 21 batch 284 batch_loss: 0.14086079597473145\n",
      "training: 21 batch 285 batch_loss: 0.13927438855171204\n",
      "training: 21 batch 286 batch_loss: 0.14455190300941467\n",
      "training: 21 batch 287 batch_loss: 0.14268136024475098\n",
      "training: 21 batch 288 batch_loss: 0.14141541719436646\n",
      "training: 21 batch 289 batch_loss: 0.14304393529891968\n",
      "training: 21 batch 290 batch_loss: 0.14238721132278442\n",
      "training: 21 batch 291 batch_loss: 0.1450803279876709\n",
      "training: 21 batch 292 batch_loss: 0.1444167196750641\n",
      "training: 21 batch 293 batch_loss: 0.1454387605190277\n",
      "training: 21 batch 294 batch_loss: 0.14296066761016846\n",
      "training: 21 batch 295 batch_loss: 0.14392676949501038\n",
      "training: 21 batch 296 batch_loss: 0.14354851841926575\n",
      "training: 21 batch 297 batch_loss: 0.1408068835735321\n",
      "training: 21 batch 298 batch_loss: 0.14674419164657593\n",
      "training: 21 batch 299 batch_loss: 0.14547929167747498\n",
      "training: 21 batch 300 batch_loss: 0.1443818211555481\n",
      "training: 21 batch 301 batch_loss: 0.14651116728782654\n",
      "training: 21 batch 302 batch_loss: 0.14173966646194458\n",
      "training: 21 batch 303 batch_loss: 0.142861008644104\n",
      "training: 21 batch 304 batch_loss: 0.14477384090423584\n",
      "training: 21 batch 305 batch_loss: 0.14380759000778198\n",
      "training: 21 batch 306 batch_loss: 0.1410588026046753\n",
      "training: 21 batch 307 batch_loss: 0.14251583814620972\n",
      "training: 21 batch 308 batch_loss: 0.14404678344726562\n",
      "training: 21 batch 309 batch_loss: 0.14265871047973633\n",
      "training: 21 batch 310 batch_loss: 0.14531278610229492\n",
      "training: 21 batch 311 batch_loss: 0.14567846059799194\n",
      "training: 21 batch 312 batch_loss: 0.14398294687271118\n",
      "training: 21 batch 313 batch_loss: 0.1430308222770691\n",
      "training: 21 batch 314 batch_loss: 0.14360028505325317\n",
      "training: 21 batch 315 batch_loss: 0.14386126399040222\n",
      "training: 21 batch 316 batch_loss: 0.14591345191001892\n",
      "training: 21 batch 317 batch_loss: 0.1398998498916626\n",
      "training: 21 batch 318 batch_loss: 0.14741027355194092\n",
      "training: 21 batch 319 batch_loss: 0.14341670274734497\n",
      "training: 21 batch 320 batch_loss: 0.14091020822525024\n",
      "training: 21 batch 321 batch_loss: 0.14082369208335876\n",
      "training: 21 batch 322 batch_loss: 0.14160996675491333\n",
      "training: 21 batch 323 batch_loss: 0.1406766176223755\n",
      "training: 21 batch 324 batch_loss: 0.1422569751739502\n",
      "training: 21 batch 325 batch_loss: 0.14059242606163025\n",
      "training: 21 batch 326 batch_loss: 0.14326506853103638\n",
      "training: 21 batch 327 batch_loss: 0.14288759231567383\n",
      "training: 21 batch 328 batch_loss: 0.14182347059249878\n",
      "training: 21 batch 329 batch_loss: 0.14046630263328552\n",
      "training: 21 batch 330 batch_loss: 0.1419358253479004\n",
      "training: 21 batch 331 batch_loss: 0.14208447933197021\n",
      "training: 21 batch 332 batch_loss: 0.14607951045036316\n",
      "training: 21 batch 333 batch_loss: 0.1432223618030548\n",
      "training: 21 batch 334 batch_loss: 0.13973379135131836\n",
      "training: 21 batch 335 batch_loss: 0.14617034792900085\n",
      "training: 21 batch 336 batch_loss: 0.1414356231689453\n",
      "training: 21 batch 337 batch_loss: 0.14575672149658203\n",
      "training: 21 batch 338 batch_loss: 0.14631634950637817\n",
      "training: 21 batch 339 batch_loss: 0.1428813934326172\n",
      "training: 21 batch 340 batch_loss: 0.1451488435268402\n",
      "training: 21 batch 341 batch_loss: 0.1427331566810608\n",
      "training: 21 batch 342 batch_loss: 0.1452634036540985\n",
      "training: 21 batch 343 batch_loss: 0.14294502139091492\n",
      "training: 21 batch 344 batch_loss: 0.14304545521736145\n",
      "training: 21 batch 345 batch_loss: 0.14287590980529785\n",
      "training: 21 batch 346 batch_loss: 0.14574909210205078\n",
      "training: 21 batch 347 batch_loss: 0.14037489891052246\n",
      "training: 21 batch 348 batch_loss: 0.14207059144973755\n",
      "training: 21 batch 349 batch_loss: 0.1431969404220581\n",
      "training: 21 batch 350 batch_loss: 0.14664781093597412\n",
      "training: 21 batch 351 batch_loss: 0.1445361077785492\n",
      "training: 21 batch 352 batch_loss: 0.14321205019950867\n",
      "training: 21 batch 353 batch_loss: 0.1433137059211731\n",
      "training: 21 batch 354 batch_loss: 0.14553046226501465\n",
      "training: 21 batch 355 batch_loss: 0.1444770097732544\n",
      "training: 21 batch 356 batch_loss: 0.14547544717788696\n",
      "training: 21 batch 357 batch_loss: 0.1450335681438446\n",
      "training: 21 batch 358 batch_loss: 0.1443878412246704\n",
      "training: 21 batch 359 batch_loss: 0.1453883945941925\n",
      "training: 21 batch 360 batch_loss: 0.14232555031776428\n",
      "training: 21 batch 361 batch_loss: 0.14339974522590637\n",
      "training: 21 batch 362 batch_loss: 0.14193665981292725\n",
      "training: 21 batch 363 batch_loss: 0.14078915119171143\n",
      "training: 21 batch 364 batch_loss: 0.14175757765769958\n",
      "training: 21 batch 365 batch_loss: 0.14188742637634277\n",
      "training: 21 batch 366 batch_loss: 0.14459756016731262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 21 batch 367 batch_loss: 0.1432526707649231\n",
      "training: 21 batch 368 batch_loss: 0.14536675810813904\n",
      "training: 21 batch 369 batch_loss: 0.1449350118637085\n",
      "training: 21 batch 370 batch_loss: 0.14013713598251343\n",
      "training: 21 batch 371 batch_loss: 0.14254993200302124\n",
      "training: 21 batch 372 batch_loss: 0.1453661322593689\n",
      "training: 21 batch 373 batch_loss: 0.14052793383598328\n",
      "training: 21 batch 374 batch_loss: 0.14483210444450378\n",
      "training: 21 batch 375 batch_loss: 0.14110708236694336\n",
      "training: 21 batch 376 batch_loss: 0.14567220211029053\n",
      "training: 21 batch 377 batch_loss: 0.1441137194633484\n",
      "training: 21 batch 378 batch_loss: 0.1424168050289154\n",
      "training: 21 batch 379 batch_loss: 0.14375731348991394\n",
      "training: 21 batch 380 batch_loss: 0.14696606993675232\n",
      "training: 21 batch 381 batch_loss: 0.1463351547718048\n",
      "training: 21 batch 382 batch_loss: 0.14691218733787537\n",
      "training: 21 batch 383 batch_loss: 0.14297303557395935\n",
      "training: 21 batch 384 batch_loss: 0.14366525411605835\n",
      "training: 21 batch 385 batch_loss: 0.14468234777450562\n",
      "training: 21 batch 386 batch_loss: 0.14456120133399963\n",
      "training: 21 batch 387 batch_loss: 0.14371919631958008\n",
      "training: 21 batch 388 batch_loss: 0.14561188220977783\n",
      "training: 21 batch 389 batch_loss: 0.14498257637023926\n",
      "training: 21 batch 390 batch_loss: 0.14384326338768005\n",
      "training: 21 batch 391 batch_loss: 0.14385542273521423\n",
      "training: 21 batch 392 batch_loss: 0.14090663194656372\n",
      "training: 21 batch 393 batch_loss: 0.1464507281780243\n",
      "training: 21 batch 394 batch_loss: 0.1438341736793518\n",
      "training: 21 batch 395 batch_loss: 0.14243179559707642\n",
      "training: 21 batch 396 batch_loss: 0.14071619510650635\n",
      "training: 21 batch 397 batch_loss: 0.14657068252563477\n",
      "training: 21 batch 398 batch_loss: 0.14173823595046997\n",
      "training: 21 batch 399 batch_loss: 0.14170986413955688\n",
      "training: 21 batch 400 batch_loss: 0.14590108394622803\n",
      "training: 21 batch 401 batch_loss: 0.14188331365585327\n",
      "training: 21 batch 402 batch_loss: 0.14549750089645386\n",
      "training: 21 batch 403 batch_loss: 0.14337605237960815\n",
      "training: 21 batch 404 batch_loss: 0.14441034197807312\n",
      "training: 21 batch 405 batch_loss: 0.14566361904144287\n",
      "training: 21 batch 406 batch_loss: 0.14113754034042358\n",
      "training: 21 batch 407 batch_loss: 0.14512944221496582\n",
      "training: 21 batch 408 batch_loss: 0.1452658474445343\n",
      "training: 21 batch 409 batch_loss: 0.14087900519371033\n",
      "training: 21 batch 410 batch_loss: 0.14526116847991943\n",
      "training: 21 batch 411 batch_loss: 0.14413094520568848\n",
      "training: 21 batch 412 batch_loss: 0.14491057395935059\n",
      "training: 21 batch 413 batch_loss: 0.14296823740005493\n",
      "training: 21 batch 414 batch_loss: 0.14562895894050598\n",
      "training: 21 batch 415 batch_loss: 0.14226320385932922\n",
      "training: 21 batch 416 batch_loss: 0.14814242720603943\n",
      "training: 21 batch 417 batch_loss: 0.14351043105125427\n",
      "training: 21 batch 418 batch_loss: 0.1446920931339264\n",
      "training: 21 batch 419 batch_loss: 0.14222654700279236\n",
      "training: 21 batch 420 batch_loss: 0.14573487639427185\n",
      "training: 21 batch 421 batch_loss: 0.13940837979316711\n",
      "training: 21 batch 422 batch_loss: 0.14599817991256714\n",
      "training: 21 batch 423 batch_loss: 0.14320003986358643\n",
      "training: 21 batch 424 batch_loss: 0.14555031061172485\n",
      "training: 21 batch 425 batch_loss: 0.147447407245636\n",
      "training: 21 batch 426 batch_loss: 0.14788788557052612\n",
      "training: 21 batch 427 batch_loss: 0.1432737410068512\n",
      "training: 21 batch 428 batch_loss: 0.1459648609161377\n",
      "training: 21 batch 429 batch_loss: 0.1409914791584015\n",
      "training: 21 batch 430 batch_loss: 0.1469266414642334\n",
      "training: 21 batch 431 batch_loss: 0.14309030771255493\n",
      "training: 21 batch 432 batch_loss: 0.14291995763778687\n",
      "training: 21 batch 433 batch_loss: 0.14516565203666687\n",
      "training: 21 batch 434 batch_loss: 0.14307251572608948\n",
      "training: 21 batch 435 batch_loss: 0.1422666609287262\n",
      "training: 21 batch 436 batch_loss: 0.14496749639511108\n",
      "training: 21 batch 437 batch_loss: 0.1493908166885376\n",
      "training: 21 batch 438 batch_loss: 0.14434996247291565\n",
      "training: 21 batch 439 batch_loss: 0.14444664120674133\n",
      "training: 21 batch 440 batch_loss: 0.14337870478630066\n",
      "training: 21 batch 441 batch_loss: 0.14541539549827576\n",
      "training: 21 batch 442 batch_loss: 0.14671635627746582\n",
      "training: 21 batch 443 batch_loss: 0.14352375268936157\n",
      "training: 21 batch 444 batch_loss: 0.14615532755851746\n",
      "training: 21 batch 445 batch_loss: 0.1456426978111267\n",
      "training: 21 batch 446 batch_loss: 0.1474086046218872\n",
      "training: 21 batch 447 batch_loss: 0.14619389176368713\n",
      "training: 21 batch 448 batch_loss: 0.14595988392829895\n",
      "training: 21 batch 449 batch_loss: 0.14159071445465088\n",
      "training: 21 batch 450 batch_loss: 0.14421147108078003\n",
      "training: 21 batch 451 batch_loss: 0.14340323209762573\n",
      "training: 21 batch 452 batch_loss: 0.1439605951309204\n",
      "training: 21 batch 453 batch_loss: 0.14437353610992432\n",
      "training: 21 batch 454 batch_loss: 0.14614975452423096\n",
      "training: 21 batch 455 batch_loss: 0.1433720588684082\n",
      "training: 21 batch 456 batch_loss: 0.1461179554462433\n",
      "training: 21 batch 457 batch_loss: 0.14401966333389282\n",
      "training: 21 batch 458 batch_loss: 0.14394357800483704\n",
      "training: 21 batch 459 batch_loss: 0.14239582419395447\n",
      "training: 21 batch 460 batch_loss: 0.1402798593044281\n",
      "training: 21 batch 461 batch_loss: 0.13980183005332947\n",
      "training: 21 batch 462 batch_loss: 0.14488577842712402\n",
      "training: 21 batch 463 batch_loss: 0.14535635709762573\n",
      "training: 21 batch 464 batch_loss: 0.14354529976844788\n",
      "training: 21 batch 465 batch_loss: 0.14480465650558472\n",
      "training: 21 batch 466 batch_loss: 0.1446751356124878\n",
      "training: 21 batch 467 batch_loss: 0.14575621485710144\n",
      "training: 21 batch 468 batch_loss: 0.1443040370941162\n",
      "training: 21 batch 469 batch_loss: 0.14342349767684937\n",
      "training: 21 batch 470 batch_loss: 0.14346688985824585\n",
      "training: 21 batch 471 batch_loss: 0.1471414864063263\n",
      "training: 21 batch 472 batch_loss: 0.14205771684646606\n",
      "training: 21 batch 473 batch_loss: 0.14527946710586548\n",
      "training: 21 batch 474 batch_loss: 0.1412889063358307\n",
      "training: 21 batch 475 batch_loss: 0.14403855800628662\n",
      "training: 21 batch 476 batch_loss: 0.1459161937236786\n",
      "training: 21 batch 477 batch_loss: 0.1444830298423767\n",
      "training: 21 batch 478 batch_loss: 0.14033693075180054\n",
      "training: 21 batch 479 batch_loss: 0.14578977227210999\n",
      "training: 21 batch 480 batch_loss: 0.14001479744911194\n",
      "training: 21 batch 481 batch_loss: 0.1450507640838623\n",
      "training: 21 batch 482 batch_loss: 0.14014571905136108\n",
      "training: 21 batch 483 batch_loss: 0.14135339856147766\n",
      "training: 21 batch 484 batch_loss: 0.13931968808174133\n",
      "training: 21 batch 485 batch_loss: 0.14126023650169373\n",
      "training: 21 batch 486 batch_loss: 0.14289870858192444\n",
      "training: 21 batch 487 batch_loss: 0.14305102825164795\n",
      "training: 21 batch 488 batch_loss: 0.14483964443206787\n",
      "training: 21 batch 489 batch_loss: 0.14471369981765747\n",
      "training: 21 batch 490 batch_loss: 0.1469464898109436\n",
      "training: 21 batch 491 batch_loss: 0.14214396476745605\n",
      "training: 21 batch 492 batch_loss: 0.14315161108970642\n",
      "training: 21 batch 493 batch_loss: 0.1439121663570404\n",
      "training: 21 batch 494 batch_loss: 0.1447327733039856\n",
      "training: 21 batch 495 batch_loss: 0.14563721418380737\n",
      "training: 21 batch 496 batch_loss: 0.14734920859336853\n",
      "training: 21 batch 497 batch_loss: 0.14396795630455017\n",
      "training: 21 batch 498 batch_loss: 0.14590388536453247\n",
      "training: 21 batch 499 batch_loss: 0.14339584112167358\n",
      "training: 21 batch 500 batch_loss: 0.1443571150302887\n",
      "training: 21 batch 501 batch_loss: 0.14365452527999878\n",
      "training: 21 batch 502 batch_loss: 0.14384165406227112\n",
      "training: 21 batch 503 batch_loss: 0.1436750590801239\n",
      "training: 21 batch 504 batch_loss: 0.1447267234325409\n",
      "training: 21 batch 505 batch_loss: 0.14351701736450195\n",
      "training: 21 batch 506 batch_loss: 0.14666149020195007\n",
      "training: 21 batch 507 batch_loss: 0.1443142592906952\n",
      "training: 21 batch 508 batch_loss: 0.1471429467201233\n",
      "training: 21 batch 509 batch_loss: 0.14079511165618896\n",
      "training: 21 batch 510 batch_loss: 0.14201831817626953\n",
      "training: 21 batch 511 batch_loss: 0.14588940143585205\n",
      "training: 21 batch 512 batch_loss: 0.14057907462120056\n",
      "training: 21 batch 513 batch_loss: 0.14661017060279846\n",
      "training: 21 batch 514 batch_loss: 0.14099031686782837\n",
      "training: 21 batch 515 batch_loss: 0.1427462100982666\n",
      "training: 21 batch 516 batch_loss: 0.14660316705703735\n",
      "training: 21 batch 517 batch_loss: 0.145257830619812\n",
      "training: 21 batch 518 batch_loss: 0.14515039324760437\n",
      "training: 21 batch 519 batch_loss: 0.14502012729644775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 21 batch 520 batch_loss: 0.14709815382957458\n",
      "training: 21 batch 521 batch_loss: 0.14360201358795166\n",
      "training: 21 batch 522 batch_loss: 0.14297360181808472\n",
      "training: 21 batch 523 batch_loss: 0.1438344419002533\n",
      "training: 21 batch 524 batch_loss: 0.14504292607307434\n",
      "training: 21 batch 525 batch_loss: 0.1443711221218109\n",
      "training: 21 batch 526 batch_loss: 0.1440737247467041\n",
      "training: 21 batch 527 batch_loss: 0.14413133263587952\n",
      "training: 21 batch 528 batch_loss: 0.14444488286972046\n",
      "training: 21 batch 529 batch_loss: 0.14450490474700928\n",
      "training: 21 batch 530 batch_loss: 0.1413823962211609\n",
      "training: 21 batch 531 batch_loss: 0.14438596367835999\n",
      "training: 21 batch 532 batch_loss: 0.14267262816429138\n",
      "training: 21 batch 533 batch_loss: 0.14600300788879395\n",
      "training: 21 batch 534 batch_loss: 0.14653295278549194\n",
      "training: 21 batch 535 batch_loss: 0.14722409844398499\n",
      "training: 21 batch 536 batch_loss: 0.142156720161438\n",
      "training: 21 batch 537 batch_loss: 0.14445364475250244\n",
      "training: 21 batch 538 batch_loss: 0.14379853010177612\n",
      "training: 21 batch 539 batch_loss: 0.14611929655075073\n",
      "training: 21 batch 540 batch_loss: 0.14510288834571838\n",
      "training: 21 batch 541 batch_loss: 0.1432427167892456\n",
      "training: 21 batch 542 batch_loss: 0.1453213095664978\n",
      "training: 21 batch 543 batch_loss: 0.14390137791633606\n",
      "training: 21 batch 544 batch_loss: 0.14379745721817017\n",
      "training: 21 batch 545 batch_loss: 0.14507871866226196\n",
      "training: 21 batch 546 batch_loss: 0.14420494437217712\n",
      "training: 21 batch 547 batch_loss: 0.14447587728500366\n",
      "training: 21 batch 548 batch_loss: 0.14445555210113525\n",
      "training: 21 batch 549 batch_loss: 0.145990788936615\n",
      "training: 21 batch 550 batch_loss: 0.1410483419895172\n",
      "training: 21 batch 551 batch_loss: 0.14630615711212158\n",
      "training: 21 batch 552 batch_loss: 0.14435115456581116\n",
      "training: 21 batch 553 batch_loss: 0.14296525716781616\n",
      "training: 21 batch 554 batch_loss: 0.14465093612670898\n",
      "training: 21 batch 555 batch_loss: 0.14492768049240112\n",
      "training: 21 batch 556 batch_loss: 0.14724427461624146\n",
      "training: 21 batch 557 batch_loss: 0.143020898103714\n",
      "training: 21 batch 558 batch_loss: 0.14208346605300903\n",
      "training: 21 batch 559 batch_loss: 0.14691388607025146\n",
      "training: 21 batch 560 batch_loss: 0.1510857343673706\n",
      "training: 21 batch 561 batch_loss: 0.14621347188949585\n",
      "training: 21 batch 562 batch_loss: 0.14556562900543213\n",
      "training: 21 batch 563 batch_loss: 0.1454961597919464\n",
      "training: 21 batch 564 batch_loss: 0.1441650390625\n",
      "training: 21 batch 565 batch_loss: 0.14646074175834656\n",
      "training: 21 batch 566 batch_loss: 0.14607998728752136\n",
      "training: 21 batch 567 batch_loss: 0.14570710062980652\n",
      "training: 21 batch 568 batch_loss: 0.14377173781394958\n",
      "training: 21 batch 569 batch_loss: 0.14602121710777283\n",
      "training: 21 batch 570 batch_loss: 0.14557942748069763\n",
      "training: 21 batch 571 batch_loss: 0.14582273364067078\n",
      "training: 21 batch 572 batch_loss: 0.14350095391273499\n",
      "training: 21 batch 573 batch_loss: 0.14693352580070496\n",
      "training: 21 batch 574 batch_loss: 0.14655974507331848\n",
      "training: 21 batch 575 batch_loss: 0.1457575261592865\n",
      "training: 21 batch 576 batch_loss: 0.14545682072639465\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 21, Hit Ratio:0.027375721619091718 | Precision:0.04039123169173302 | Recall:0.055587712850131726 | NDCG:0.05125629313012039\n",
      "*Best Performance* \n",
      "Epoch: 20, Hit Ratio:0.02752562518113347 | Precision:0.040612405386808215 | Recall:0.05565703171805906 | MDCG:0.05147962604807236\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 22 batch 0 batch_loss: 0.14244318008422852\n",
      "training: 22 batch 1 batch_loss: 0.1419571340084076\n",
      "training: 22 batch 2 batch_loss: 0.14373698830604553\n",
      "training: 22 batch 3 batch_loss: 0.13953736424446106\n",
      "training: 22 batch 4 batch_loss: 0.14282387495040894\n",
      "training: 22 batch 5 batch_loss: 0.14220944046974182\n",
      "training: 22 batch 6 batch_loss: 0.14198938012123108\n",
      "training: 22 batch 7 batch_loss: 0.14346590638160706\n",
      "training: 22 batch 8 batch_loss: 0.1424335241317749\n",
      "training: 22 batch 9 batch_loss: 0.14298111200332642\n",
      "training: 22 batch 10 batch_loss: 0.14348956942558289\n",
      "training: 22 batch 11 batch_loss: 0.14196783304214478\n",
      "training: 22 batch 12 batch_loss: 0.14389237761497498\n",
      "training: 22 batch 13 batch_loss: 0.14540565013885498\n",
      "training: 22 batch 14 batch_loss: 0.1449572741985321\n",
      "training: 22 batch 15 batch_loss: 0.14344531297683716\n",
      "training: 22 batch 16 batch_loss: 0.14156708121299744\n",
      "training: 22 batch 17 batch_loss: 0.14578911662101746\n",
      "training: 22 batch 18 batch_loss: 0.14510661363601685\n",
      "training: 22 batch 19 batch_loss: 0.14151301980018616\n",
      "training: 22 batch 20 batch_loss: 0.14297175407409668\n",
      "training: 22 batch 21 batch_loss: 0.14410841464996338\n",
      "training: 22 batch 22 batch_loss: 0.14350727200508118\n",
      "training: 22 batch 23 batch_loss: 0.1416671872138977\n",
      "training: 22 batch 24 batch_loss: 0.14191165566444397\n",
      "training: 22 batch 25 batch_loss: 0.14228478074073792\n",
      "training: 22 batch 26 batch_loss: 0.1449093222618103\n",
      "training: 22 batch 27 batch_loss: 0.1407863199710846\n",
      "training: 22 batch 28 batch_loss: 0.1484188437461853\n",
      "training: 22 batch 29 batch_loss: 0.1398938000202179\n",
      "training: 22 batch 30 batch_loss: 0.1411534547805786\n",
      "training: 22 batch 31 batch_loss: 0.14273405075073242\n",
      "training: 22 batch 32 batch_loss: 0.14486947655677795\n",
      "training: 22 batch 33 batch_loss: 0.14348971843719482\n",
      "training: 22 batch 34 batch_loss: 0.14640012383460999\n",
      "training: 22 batch 35 batch_loss: 0.14335253834724426\n",
      "training: 22 batch 36 batch_loss: 0.1436627209186554\n",
      "training: 22 batch 37 batch_loss: 0.14355728030204773\n",
      "training: 22 batch 38 batch_loss: 0.1438709795475006\n",
      "training: 22 batch 39 batch_loss: 0.14593636989593506\n",
      "training: 22 batch 40 batch_loss: 0.14287865161895752\n",
      "training: 22 batch 41 batch_loss: 0.14430347084999084\n",
      "training: 22 batch 42 batch_loss: 0.1396547555923462\n",
      "training: 22 batch 43 batch_loss: 0.14264065027236938\n",
      "training: 22 batch 44 batch_loss: 0.14233529567718506\n",
      "training: 22 batch 45 batch_loss: 0.14664769172668457\n",
      "training: 22 batch 46 batch_loss: 0.14616137742996216\n",
      "training: 22 batch 47 batch_loss: 0.1477777361869812\n",
      "training: 22 batch 48 batch_loss: 0.14512646198272705\n",
      "training: 22 batch 49 batch_loss: 0.14626243710517883\n",
      "training: 22 batch 50 batch_loss: 0.14457598328590393\n",
      "training: 22 batch 51 batch_loss: 0.14284667372703552\n",
      "training: 22 batch 52 batch_loss: 0.14540857076644897\n",
      "training: 22 batch 53 batch_loss: 0.1453898549079895\n",
      "training: 22 batch 54 batch_loss: 0.1441577970981598\n",
      "training: 22 batch 55 batch_loss: 0.1454475224018097\n",
      "training: 22 batch 56 batch_loss: 0.14388173818588257\n",
      "training: 22 batch 57 batch_loss: 0.14181795716285706\n",
      "training: 22 batch 58 batch_loss: 0.14593136310577393\n",
      "training: 22 batch 59 batch_loss: 0.1415659785270691\n",
      "training: 22 batch 60 batch_loss: 0.14353278279304504\n",
      "training: 22 batch 61 batch_loss: 0.14237725734710693\n",
      "training: 22 batch 62 batch_loss: 0.14414939284324646\n",
      "training: 22 batch 63 batch_loss: 0.14067381620407104\n",
      "training: 22 batch 64 batch_loss: 0.1444683074951172\n",
      "training: 22 batch 65 batch_loss: 0.143254816532135\n",
      "training: 22 batch 66 batch_loss: 0.1441972255706787\n",
      "training: 22 batch 67 batch_loss: 0.14398059248924255\n",
      "training: 22 batch 68 batch_loss: 0.14466172456741333\n",
      "training: 22 batch 69 batch_loss: 0.14454185962677002\n",
      "training: 22 batch 70 batch_loss: 0.14408338069915771\n",
      "training: 22 batch 71 batch_loss: 0.14109236001968384\n",
      "training: 22 batch 72 batch_loss: 0.14597946405410767\n",
      "training: 22 batch 73 batch_loss: 0.14028024673461914\n",
      "training: 22 batch 74 batch_loss: 0.13977795839309692\n",
      "training: 22 batch 75 batch_loss: 0.1438310146331787\n",
      "training: 22 batch 76 batch_loss: 0.14403823018074036\n",
      "training: 22 batch 77 batch_loss: 0.1456376612186432\n",
      "training: 22 batch 78 batch_loss: 0.14341631531715393\n",
      "training: 22 batch 79 batch_loss: 0.14422854781150818\n",
      "training: 22 batch 80 batch_loss: 0.1448647379875183\n",
      "training: 22 batch 81 batch_loss: 0.14378318190574646\n",
      "training: 22 batch 82 batch_loss: 0.14217564463615417\n",
      "training: 22 batch 83 batch_loss: 0.1430101990699768\n",
      "training: 22 batch 84 batch_loss: 0.14435866475105286\n",
      "training: 22 batch 85 batch_loss: 0.14432001113891602\n",
      "training: 22 batch 86 batch_loss: 0.14218273758888245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 22 batch 87 batch_loss: 0.14599990844726562\n",
      "training: 22 batch 88 batch_loss: 0.14390182495117188\n",
      "training: 22 batch 89 batch_loss: 0.1441633105278015\n",
      "training: 22 batch 90 batch_loss: 0.14232081174850464\n",
      "training: 22 batch 91 batch_loss: 0.14378628134727478\n",
      "training: 22 batch 92 batch_loss: 0.1448649764060974\n",
      "training: 22 batch 93 batch_loss: 0.1417645514011383\n",
      "training: 22 batch 94 batch_loss: 0.1458321511745453\n",
      "training: 22 batch 95 batch_loss: 0.14389005303382874\n",
      "training: 22 batch 96 batch_loss: 0.14106029272079468\n",
      "training: 22 batch 97 batch_loss: 0.1414615511894226\n",
      "training: 22 batch 98 batch_loss: 0.1453557312488556\n",
      "training: 22 batch 99 batch_loss: 0.14385372400283813\n",
      "training: 22 batch 100 batch_loss: 0.13997533917427063\n",
      "training: 22 batch 101 batch_loss: 0.14721408486366272\n",
      "training: 22 batch 102 batch_loss: 0.14587271213531494\n",
      "training: 22 batch 103 batch_loss: 0.14551743865013123\n",
      "training: 22 batch 104 batch_loss: 0.1448509693145752\n",
      "training: 22 batch 105 batch_loss: 0.1425533890724182\n",
      "training: 22 batch 106 batch_loss: 0.14416226744651794\n",
      "training: 22 batch 107 batch_loss: 0.14676395058631897\n",
      "training: 22 batch 108 batch_loss: 0.1434660255908966\n",
      "training: 22 batch 109 batch_loss: 0.14442569017410278\n",
      "training: 22 batch 110 batch_loss: 0.14730599522590637\n",
      "training: 22 batch 111 batch_loss: 0.14723700284957886\n",
      "training: 22 batch 112 batch_loss: 0.1428462266921997\n",
      "training: 22 batch 113 batch_loss: 0.14643561840057373\n",
      "training: 22 batch 114 batch_loss: 0.14562857151031494\n",
      "training: 22 batch 115 batch_loss: 0.1453789472579956\n",
      "training: 22 batch 116 batch_loss: 0.1436803936958313\n",
      "training: 22 batch 117 batch_loss: 0.14627695083618164\n",
      "training: 22 batch 118 batch_loss: 0.14220422506332397\n",
      "training: 22 batch 119 batch_loss: 0.14281463623046875\n",
      "training: 22 batch 120 batch_loss: 0.14650613069534302\n",
      "training: 22 batch 121 batch_loss: 0.1465173363685608\n",
      "training: 22 batch 122 batch_loss: 0.14387929439544678\n",
      "training: 22 batch 123 batch_loss: 0.14431703090667725\n",
      "training: 22 batch 124 batch_loss: 0.1472998559474945\n",
      "training: 22 batch 125 batch_loss: 0.14448419213294983\n",
      "training: 22 batch 126 batch_loss: 0.14525002241134644\n",
      "training: 22 batch 127 batch_loss: 0.1435498595237732\n",
      "training: 22 batch 128 batch_loss: 0.14149221777915955\n",
      "training: 22 batch 129 batch_loss: 0.1438528597354889\n",
      "training: 22 batch 130 batch_loss: 0.1437699794769287\n",
      "training: 22 batch 131 batch_loss: 0.14583927392959595\n",
      "training: 22 batch 132 batch_loss: 0.14649364352226257\n",
      "training: 22 batch 133 batch_loss: 0.14504238963127136\n",
      "training: 22 batch 134 batch_loss: 0.14555588364601135\n",
      "training: 22 batch 135 batch_loss: 0.14075124263763428\n",
      "training: 22 batch 136 batch_loss: 0.14398685097694397\n",
      "training: 22 batch 137 batch_loss: 0.14592918753623962\n",
      "training: 22 batch 138 batch_loss: 0.14527004957199097\n",
      "training: 22 batch 139 batch_loss: 0.14600834250450134\n",
      "training: 22 batch 140 batch_loss: 0.14538145065307617\n",
      "training: 22 batch 141 batch_loss: 0.1457926630973816\n",
      "training: 22 batch 142 batch_loss: 0.14682498574256897\n",
      "training: 22 batch 143 batch_loss: 0.14288678765296936\n",
      "training: 22 batch 144 batch_loss: 0.14338800311088562\n",
      "training: 22 batch 145 batch_loss: 0.14721152186393738\n",
      "training: 22 batch 146 batch_loss: 0.1462906002998352\n",
      "training: 22 batch 147 batch_loss: 0.14610272645950317\n",
      "training: 22 batch 148 batch_loss: 0.14508363604545593\n",
      "training: 22 batch 149 batch_loss: 0.14357846975326538\n",
      "training: 22 batch 150 batch_loss: 0.14751824736595154\n",
      "training: 22 batch 151 batch_loss: 0.14171302318572998\n",
      "training: 22 batch 152 batch_loss: 0.14303770661354065\n",
      "training: 22 batch 153 batch_loss: 0.14269402623176575\n",
      "training: 22 batch 154 batch_loss: 0.14639312028884888\n",
      "training: 22 batch 155 batch_loss: 0.14445793628692627\n",
      "training: 22 batch 156 batch_loss: 0.14461681246757507\n",
      "training: 22 batch 157 batch_loss: 0.1458793580532074\n",
      "training: 22 batch 158 batch_loss: 0.14814764261245728\n",
      "training: 22 batch 159 batch_loss: 0.14639508724212646\n",
      "training: 22 batch 160 batch_loss: 0.14284715056419373\n",
      "training: 22 batch 161 batch_loss: 0.14330363273620605\n",
      "training: 22 batch 162 batch_loss: 0.14569345116615295\n",
      "training: 22 batch 163 batch_loss: 0.14323294162750244\n",
      "training: 22 batch 164 batch_loss: 0.14417436718940735\n",
      "training: 22 batch 165 batch_loss: 0.14576280117034912\n",
      "training: 22 batch 166 batch_loss: 0.1461687684059143\n",
      "training: 22 batch 167 batch_loss: 0.14331096410751343\n",
      "training: 22 batch 168 batch_loss: 0.14602899551391602\n",
      "training: 22 batch 169 batch_loss: 0.14572572708129883\n",
      "training: 22 batch 170 batch_loss: 0.1420747935771942\n",
      "training: 22 batch 171 batch_loss: 0.14400571584701538\n",
      "training: 22 batch 172 batch_loss: 0.14382007718086243\n",
      "training: 22 batch 173 batch_loss: 0.14471983909606934\n",
      "training: 22 batch 174 batch_loss: 0.14588886499404907\n",
      "training: 22 batch 175 batch_loss: 0.14099794626235962\n",
      "training: 22 batch 176 batch_loss: 0.14661166071891785\n",
      "training: 22 batch 177 batch_loss: 0.14687496423721313\n",
      "training: 22 batch 178 batch_loss: 0.14335897564888\n",
      "training: 22 batch 179 batch_loss: 0.1412847638130188\n",
      "training: 22 batch 180 batch_loss: 0.14176291227340698\n",
      "training: 22 batch 181 batch_loss: 0.14247575402259827\n",
      "training: 22 batch 182 batch_loss: 0.1460283398628235\n",
      "training: 22 batch 183 batch_loss: 0.1464434266090393\n",
      "training: 22 batch 184 batch_loss: 0.14329016208648682\n",
      "training: 22 batch 185 batch_loss: 0.1444377899169922\n",
      "training: 22 batch 186 batch_loss: 0.14391347765922546\n",
      "training: 22 batch 187 batch_loss: 0.14662182331085205\n",
      "training: 22 batch 188 batch_loss: 0.14470148086547852\n",
      "training: 22 batch 189 batch_loss: 0.14708733558654785\n",
      "training: 22 batch 190 batch_loss: 0.14372748136520386\n",
      "training: 22 batch 191 batch_loss: 0.1426105797290802\n",
      "training: 22 batch 192 batch_loss: 0.14716431498527527\n",
      "training: 22 batch 193 batch_loss: 0.1445150375366211\n",
      "training: 22 batch 194 batch_loss: 0.1468523144721985\n",
      "training: 22 batch 195 batch_loss: 0.1469516158103943\n",
      "training: 22 batch 196 batch_loss: 0.14520153403282166\n",
      "training: 22 batch 197 batch_loss: 0.14637473225593567\n",
      "training: 22 batch 198 batch_loss: 0.14326918125152588\n",
      "training: 22 batch 199 batch_loss: 0.14798122644424438\n",
      "training: 22 batch 200 batch_loss: 0.14478978514671326\n",
      "training: 22 batch 201 batch_loss: 0.14485928416252136\n",
      "training: 22 batch 202 batch_loss: 0.145907461643219\n",
      "training: 22 batch 203 batch_loss: 0.14656206965446472\n",
      "training: 22 batch 204 batch_loss: 0.1464490294456482\n",
      "training: 22 batch 205 batch_loss: 0.1439191699028015\n",
      "training: 22 batch 206 batch_loss: 0.14279329776763916\n",
      "training: 22 batch 207 batch_loss: 0.14825117588043213\n",
      "training: 22 batch 208 batch_loss: 0.14391908049583435\n",
      "training: 22 batch 209 batch_loss: 0.14265602827072144\n",
      "training: 22 batch 210 batch_loss: 0.14684909582138062\n",
      "training: 22 batch 211 batch_loss: 0.14355123043060303\n",
      "training: 22 batch 212 batch_loss: 0.149034321308136\n",
      "training: 22 batch 213 batch_loss: 0.14629438519477844\n",
      "training: 22 batch 214 batch_loss: 0.14542371034622192\n",
      "training: 22 batch 215 batch_loss: 0.14134597778320312\n",
      "training: 22 batch 216 batch_loss: 0.1431334912776947\n",
      "training: 22 batch 217 batch_loss: 0.14592373371124268\n",
      "training: 22 batch 218 batch_loss: 0.14366167783737183\n",
      "training: 22 batch 219 batch_loss: 0.14373135566711426\n",
      "training: 22 batch 220 batch_loss: 0.14171850681304932\n",
      "training: 22 batch 221 batch_loss: 0.14451199769973755\n",
      "training: 22 batch 222 batch_loss: 0.1441475749015808\n",
      "training: 22 batch 223 batch_loss: 0.14477092027664185\n",
      "training: 22 batch 224 batch_loss: 0.14320790767669678\n",
      "training: 22 batch 225 batch_loss: 0.14739108085632324\n",
      "training: 22 batch 226 batch_loss: 0.14675813913345337\n",
      "training: 22 batch 227 batch_loss: 0.1451461911201477\n",
      "training: 22 batch 228 batch_loss: 0.14421474933624268\n",
      "training: 22 batch 229 batch_loss: 0.14619481563568115\n",
      "training: 22 batch 230 batch_loss: 0.14316147565841675\n",
      "training: 22 batch 231 batch_loss: 0.1459311544895172\n",
      "training: 22 batch 232 batch_loss: 0.14302268624305725\n",
      "training: 22 batch 233 batch_loss: 0.14621183276176453\n",
      "training: 22 batch 234 batch_loss: 0.14777085185050964\n",
      "training: 22 batch 235 batch_loss: 0.1431519091129303\n",
      "training: 22 batch 236 batch_loss: 0.1434338092803955\n",
      "training: 22 batch 237 batch_loss: 0.1438351273536682\n",
      "training: 22 batch 238 batch_loss: 0.14299851655960083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 22 batch 239 batch_loss: 0.1466502547264099\n",
      "training: 22 batch 240 batch_loss: 0.14993980526924133\n",
      "training: 22 batch 241 batch_loss: 0.1465986669063568\n",
      "training: 22 batch 242 batch_loss: 0.1449805498123169\n",
      "training: 22 batch 243 batch_loss: 0.14616885781288147\n",
      "training: 22 batch 244 batch_loss: 0.1421295404434204\n",
      "training: 22 batch 245 batch_loss: 0.14675426483154297\n",
      "training: 22 batch 246 batch_loss: 0.1415356993675232\n",
      "training: 22 batch 247 batch_loss: 0.1426677107810974\n",
      "training: 22 batch 248 batch_loss: 0.14585590362548828\n",
      "training: 22 batch 249 batch_loss: 0.14531755447387695\n",
      "training: 22 batch 250 batch_loss: 0.14643248915672302\n",
      "training: 22 batch 251 batch_loss: 0.14280632138252258\n",
      "training: 22 batch 252 batch_loss: 0.14466670155525208\n",
      "training: 22 batch 253 batch_loss: 0.14576014876365662\n",
      "training: 22 batch 254 batch_loss: 0.14380773901939392\n",
      "training: 22 batch 255 batch_loss: 0.14913544058799744\n",
      "training: 22 batch 256 batch_loss: 0.1451035737991333\n",
      "training: 22 batch 257 batch_loss: 0.14530834555625916\n",
      "training: 22 batch 258 batch_loss: 0.14720994234085083\n",
      "training: 22 batch 259 batch_loss: 0.14350169897079468\n",
      "training: 22 batch 260 batch_loss: 0.1432308554649353\n",
      "training: 22 batch 261 batch_loss: 0.14783543348312378\n",
      "training: 22 batch 262 batch_loss: 0.14737150073051453\n",
      "training: 22 batch 263 batch_loss: 0.14347577095031738\n",
      "training: 22 batch 264 batch_loss: 0.1448456048965454\n",
      "training: 22 batch 265 batch_loss: 0.144072026014328\n",
      "training: 22 batch 266 batch_loss: 0.14121681451797485\n",
      "training: 22 batch 267 batch_loss: 0.14620721340179443\n",
      "training: 22 batch 268 batch_loss: 0.14771774411201477\n",
      "training: 22 batch 269 batch_loss: 0.14535966515541077\n",
      "training: 22 batch 270 batch_loss: 0.14434197545051575\n",
      "training: 22 batch 271 batch_loss: 0.14491695165634155\n",
      "training: 22 batch 272 batch_loss: 0.14544042944908142\n",
      "training: 22 batch 273 batch_loss: 0.14760029315948486\n",
      "training: 22 batch 274 batch_loss: 0.14546895027160645\n",
      "training: 22 batch 275 batch_loss: 0.14633771777153015\n",
      "training: 22 batch 276 batch_loss: 0.1452646255493164\n",
      "training: 22 batch 277 batch_loss: 0.1466553807258606\n",
      "training: 22 batch 278 batch_loss: 0.1424502432346344\n",
      "training: 22 batch 279 batch_loss: 0.14597809314727783\n",
      "training: 22 batch 280 batch_loss: 0.14398705959320068\n",
      "training: 22 batch 281 batch_loss: 0.14462429285049438\n",
      "training: 22 batch 282 batch_loss: 0.14506027102470398\n",
      "training: 22 batch 283 batch_loss: 0.14435100555419922\n",
      "training: 22 batch 284 batch_loss: 0.14325234293937683\n",
      "training: 22 batch 285 batch_loss: 0.1449125111103058\n",
      "training: 22 batch 286 batch_loss: 0.145379900932312\n",
      "training: 22 batch 287 batch_loss: 0.1458292007446289\n",
      "training: 22 batch 288 batch_loss: 0.1424570083618164\n",
      "training: 22 batch 289 batch_loss: 0.1444251835346222\n",
      "training: 22 batch 290 batch_loss: 0.14723458886146545\n",
      "training: 22 batch 291 batch_loss: 0.14404916763305664\n",
      "training: 22 batch 292 batch_loss: 0.14576247334480286\n",
      "training: 22 batch 293 batch_loss: 0.14456096291542053\n",
      "training: 22 batch 294 batch_loss: 0.14747938513755798\n",
      "training: 22 batch 295 batch_loss: 0.14900773763656616\n",
      "training: 22 batch 296 batch_loss: 0.14601650834083557\n",
      "training: 22 batch 297 batch_loss: 0.14701497554779053\n",
      "training: 22 batch 298 batch_loss: 0.147334486246109\n",
      "training: 22 batch 299 batch_loss: 0.1439165472984314\n",
      "training: 22 batch 300 batch_loss: 0.1452747881412506\n",
      "training: 22 batch 301 batch_loss: 0.14580291509628296\n",
      "training: 22 batch 302 batch_loss: 0.14467507600784302\n",
      "training: 22 batch 303 batch_loss: 0.1453932523727417\n",
      "training: 22 batch 304 batch_loss: 0.14708396792411804\n",
      "training: 22 batch 305 batch_loss: 0.14980566501617432\n",
      "training: 22 batch 306 batch_loss: 0.1464766263961792\n",
      "training: 22 batch 307 batch_loss: 0.1476411521434784\n",
      "training: 22 batch 308 batch_loss: 0.14660727977752686\n",
      "training: 22 batch 309 batch_loss: 0.14517870545387268\n",
      "training: 22 batch 310 batch_loss: 0.1461952030658722\n",
      "training: 22 batch 311 batch_loss: 0.14595645666122437\n",
      "training: 22 batch 312 batch_loss: 0.14539897441864014\n",
      "training: 22 batch 313 batch_loss: 0.14438191056251526\n",
      "training: 22 batch 314 batch_loss: 0.144649475812912\n",
      "training: 22 batch 315 batch_loss: 0.14617317914962769\n",
      "training: 22 batch 316 batch_loss: 0.14657500386238098\n",
      "training: 22 batch 317 batch_loss: 0.14610901474952698\n",
      "training: 22 batch 318 batch_loss: 0.14413046836853027\n",
      "training: 22 batch 319 batch_loss: 0.14599251747131348\n",
      "training: 22 batch 320 batch_loss: 0.1427566111087799\n",
      "training: 22 batch 321 batch_loss: 0.14594823122024536\n",
      "training: 22 batch 322 batch_loss: 0.14410942792892456\n",
      "training: 22 batch 323 batch_loss: 0.14179039001464844\n",
      "training: 22 batch 324 batch_loss: 0.1433282196521759\n",
      "training: 22 batch 325 batch_loss: 0.1472669243812561\n",
      "training: 22 batch 326 batch_loss: 0.1474476456642151\n",
      "training: 22 batch 327 batch_loss: 0.1444469690322876\n",
      "training: 22 batch 328 batch_loss: 0.1476942002773285\n",
      "training: 22 batch 329 batch_loss: 0.14920908212661743\n",
      "training: 22 batch 330 batch_loss: 0.14399081468582153\n",
      "training: 22 batch 331 batch_loss: 0.14506807923316956\n",
      "training: 22 batch 332 batch_loss: 0.14397743344306946\n",
      "training: 22 batch 333 batch_loss: 0.14738070964813232\n",
      "training: 22 batch 334 batch_loss: 0.14472320675849915\n",
      "training: 22 batch 335 batch_loss: 0.14559686183929443\n",
      "training: 22 batch 336 batch_loss: 0.1435455083847046\n",
      "training: 22 batch 337 batch_loss: 0.14619332551956177\n",
      "training: 22 batch 338 batch_loss: 0.1465156078338623\n",
      "training: 22 batch 339 batch_loss: 0.14531797170639038\n",
      "training: 22 batch 340 batch_loss: 0.14422574639320374\n",
      "training: 22 batch 341 batch_loss: 0.1448661983013153\n",
      "training: 22 batch 342 batch_loss: 0.14307978749275208\n",
      "training: 22 batch 343 batch_loss: 0.1468304991722107\n",
      "training: 22 batch 344 batch_loss: 0.14838728308677673\n",
      "training: 22 batch 345 batch_loss: 0.149164617061615\n",
      "training: 22 batch 346 batch_loss: 0.1449367105960846\n",
      "training: 22 batch 347 batch_loss: 0.14547160267829895\n",
      "training: 22 batch 348 batch_loss: 0.1467314064502716\n",
      "training: 22 batch 349 batch_loss: 0.14230138063430786\n",
      "training: 22 batch 350 batch_loss: 0.14969348907470703\n",
      "training: 22 batch 351 batch_loss: 0.14449888467788696\n",
      "training: 22 batch 352 batch_loss: 0.15296882390975952\n",
      "training: 22 batch 353 batch_loss: 0.1445799171924591\n",
      "training: 22 batch 354 batch_loss: 0.14695262908935547\n",
      "training: 22 batch 355 batch_loss: 0.1479896903038025\n",
      "training: 22 batch 356 batch_loss: 0.1465960443019867\n",
      "training: 22 batch 357 batch_loss: 0.14483585953712463\n",
      "training: 22 batch 358 batch_loss: 0.14580410718917847\n",
      "training: 22 batch 359 batch_loss: 0.14548224210739136\n",
      "training: 22 batch 360 batch_loss: 0.14530399441719055\n",
      "training: 22 batch 361 batch_loss: 0.14299547672271729\n",
      "training: 22 batch 362 batch_loss: 0.14656630158424377\n",
      "training: 22 batch 363 batch_loss: 0.14856210350990295\n",
      "training: 22 batch 364 batch_loss: 0.14612451195716858\n",
      "training: 22 batch 365 batch_loss: 0.14941716194152832\n",
      "training: 22 batch 366 batch_loss: 0.147020161151886\n",
      "training: 22 batch 367 batch_loss: 0.14863672852516174\n",
      "training: 22 batch 368 batch_loss: 0.14574658870697021\n",
      "training: 22 batch 369 batch_loss: 0.14361929893493652\n",
      "training: 22 batch 370 batch_loss: 0.14668405055999756\n",
      "training: 22 batch 371 batch_loss: 0.14370161294937134\n",
      "training: 22 batch 372 batch_loss: 0.14690923690795898\n",
      "training: 22 batch 373 batch_loss: 0.14344316720962524\n",
      "training: 22 batch 374 batch_loss: 0.14629721641540527\n",
      "training: 22 batch 375 batch_loss: 0.1458185315132141\n",
      "training: 22 batch 376 batch_loss: 0.147264301776886\n",
      "training: 22 batch 377 batch_loss: 0.1439211070537567\n",
      "training: 22 batch 378 batch_loss: 0.14839684963226318\n",
      "training: 22 batch 379 batch_loss: 0.14603033661842346\n",
      "training: 22 batch 380 batch_loss: 0.1491418182849884\n",
      "training: 22 batch 381 batch_loss: 0.14451834559440613\n",
      "training: 22 batch 382 batch_loss: 0.14317259192466736\n",
      "training: 22 batch 383 batch_loss: 0.1495538353919983\n",
      "training: 22 batch 384 batch_loss: 0.14799851179122925\n",
      "training: 22 batch 385 batch_loss: 0.14664030075073242\n",
      "training: 22 batch 386 batch_loss: 0.14483055472373962\n",
      "training: 22 batch 387 batch_loss: 0.14747834205627441\n",
      "training: 22 batch 388 batch_loss: 0.1474822759628296\n",
      "training: 22 batch 389 batch_loss: 0.1441548466682434\n",
      "training: 22 batch 390 batch_loss: 0.14254963397979736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 22 batch 391 batch_loss: 0.14267587661743164\n",
      "training: 22 batch 392 batch_loss: 0.14958956837654114\n",
      "training: 22 batch 393 batch_loss: 0.14479920268058777\n",
      "training: 22 batch 394 batch_loss: 0.14818990230560303\n",
      "training: 22 batch 395 batch_loss: 0.1452924907207489\n",
      "training: 22 batch 396 batch_loss: 0.14615556597709656\n",
      "training: 22 batch 397 batch_loss: 0.1442016363143921\n",
      "training: 22 batch 398 batch_loss: 0.14935284852981567\n",
      "training: 22 batch 399 batch_loss: 0.14669302105903625\n",
      "training: 22 batch 400 batch_loss: 0.14523044228553772\n",
      "training: 22 batch 401 batch_loss: 0.14410147070884705\n",
      "training: 22 batch 402 batch_loss: 0.1484026312828064\n",
      "training: 22 batch 403 batch_loss: 0.14699727296829224\n",
      "training: 22 batch 404 batch_loss: 0.14621424674987793\n",
      "training: 22 batch 405 batch_loss: 0.14572596549987793\n",
      "training: 22 batch 406 batch_loss: 0.14579114317893982\n",
      "training: 22 batch 407 batch_loss: 0.14676055312156677\n",
      "training: 22 batch 408 batch_loss: 0.14491912722587585\n",
      "training: 22 batch 409 batch_loss: 0.14736297726631165\n",
      "training: 22 batch 410 batch_loss: 0.15280717611312866\n",
      "training: 22 batch 411 batch_loss: 0.15042075514793396\n",
      "training: 22 batch 412 batch_loss: 0.14774033427238464\n",
      "training: 22 batch 413 batch_loss: 0.14718839526176453\n",
      "training: 22 batch 414 batch_loss: 0.14797985553741455\n",
      "training: 22 batch 415 batch_loss: 0.1499408483505249\n",
      "training: 22 batch 416 batch_loss: 0.14733532071113586\n",
      "training: 22 batch 417 batch_loss: 0.14477676153182983\n",
      "training: 22 batch 418 batch_loss: 0.14846214652061462\n",
      "training: 22 batch 419 batch_loss: 0.1475105583667755\n",
      "training: 22 batch 420 batch_loss: 0.14581653475761414\n",
      "training: 22 batch 421 batch_loss: 0.14698505401611328\n",
      "training: 22 batch 422 batch_loss: 0.14644211530685425\n",
      "training: 22 batch 423 batch_loss: 0.1429000198841095\n",
      "training: 22 batch 424 batch_loss: 0.14557868242263794\n",
      "training: 22 batch 425 batch_loss: 0.14688697457313538\n",
      "training: 22 batch 426 batch_loss: 0.14375194907188416\n",
      "training: 22 batch 427 batch_loss: 0.14678767323493958\n",
      "training: 22 batch 428 batch_loss: 0.14973771572113037\n",
      "training: 22 batch 429 batch_loss: 0.14550086855888367\n",
      "training: 22 batch 430 batch_loss: 0.14776694774627686\n",
      "training: 22 batch 431 batch_loss: 0.14705386757850647\n",
      "training: 22 batch 432 batch_loss: 0.14547184109687805\n",
      "training: 22 batch 433 batch_loss: 0.1486843228340149\n",
      "training: 22 batch 434 batch_loss: 0.14777067303657532\n",
      "training: 22 batch 435 batch_loss: 0.14618390798568726\n",
      "training: 22 batch 436 batch_loss: 0.1466989815235138\n",
      "training: 22 batch 437 batch_loss: 0.1454281508922577\n",
      "training: 22 batch 438 batch_loss: 0.1485050916671753\n",
      "training: 22 batch 439 batch_loss: 0.14899706840515137\n",
      "training: 22 batch 440 batch_loss: 0.1460437774658203\n",
      "training: 22 batch 441 batch_loss: 0.1455574333667755\n",
      "training: 22 batch 442 batch_loss: 0.14742302894592285\n",
      "training: 22 batch 443 batch_loss: 0.146601140499115\n",
      "training: 22 batch 444 batch_loss: 0.14921262860298157\n",
      "training: 22 batch 445 batch_loss: 0.147818922996521\n",
      "training: 22 batch 446 batch_loss: 0.14884155988693237\n",
      "training: 22 batch 447 batch_loss: 0.14481550455093384\n",
      "training: 22 batch 448 batch_loss: 0.14534711837768555\n",
      "training: 22 batch 449 batch_loss: 0.14743056893348694\n",
      "training: 22 batch 450 batch_loss: 0.14609509706497192\n",
      "training: 22 batch 451 batch_loss: 0.1444786787033081\n",
      "training: 22 batch 452 batch_loss: 0.14794334769248962\n",
      "training: 22 batch 453 batch_loss: 0.14590492844581604\n",
      "training: 22 batch 454 batch_loss: 0.14701583981513977\n",
      "training: 22 batch 455 batch_loss: 0.14682751893997192\n",
      "training: 22 batch 456 batch_loss: 0.14630800485610962\n",
      "training: 22 batch 457 batch_loss: 0.14937609434127808\n",
      "training: 22 batch 458 batch_loss: 0.14816617965698242\n",
      "training: 22 batch 459 batch_loss: 0.1470397710800171\n",
      "training: 22 batch 460 batch_loss: 0.14716386795043945\n",
      "training: 22 batch 461 batch_loss: 0.14580193161964417\n",
      "training: 22 batch 462 batch_loss: 0.14616376161575317\n",
      "training: 22 batch 463 batch_loss: 0.1452404260635376\n",
      "training: 22 batch 464 batch_loss: 0.14584630727767944\n",
      "training: 22 batch 465 batch_loss: 0.14946258068084717\n",
      "training: 22 batch 466 batch_loss: 0.14604851603507996\n",
      "training: 22 batch 467 batch_loss: 0.14806151390075684\n",
      "training: 22 batch 468 batch_loss: 0.14584708213806152\n",
      "training: 22 batch 469 batch_loss: 0.14521640539169312\n",
      "training: 22 batch 470 batch_loss: 0.1456381380558014\n",
      "training: 22 batch 471 batch_loss: 0.14934080839157104\n",
      "training: 22 batch 472 batch_loss: 0.1471753716468811\n",
      "training: 22 batch 473 batch_loss: 0.14815425872802734\n",
      "training: 22 batch 474 batch_loss: 0.1481150984764099\n",
      "training: 22 batch 475 batch_loss: 0.15158528089523315\n",
      "training: 22 batch 476 batch_loss: 0.14960286021232605\n",
      "training: 22 batch 477 batch_loss: 0.14725464582443237\n",
      "training: 22 batch 478 batch_loss: 0.14777547121047974\n",
      "training: 22 batch 479 batch_loss: 0.1460038125514984\n",
      "training: 22 batch 480 batch_loss: 0.14800187945365906\n",
      "training: 22 batch 481 batch_loss: 0.14584693312644958\n",
      "training: 22 batch 482 batch_loss: 0.14936009049415588\n",
      "training: 22 batch 483 batch_loss: 0.14703452587127686\n",
      "training: 22 batch 484 batch_loss: 0.1486613154411316\n",
      "training: 22 batch 485 batch_loss: 0.14411872625350952\n",
      "training: 22 batch 486 batch_loss: 0.14519208669662476\n",
      "training: 22 batch 487 batch_loss: 0.1440275013446808\n",
      "training: 22 batch 488 batch_loss: 0.14494356513023376\n",
      "training: 22 batch 489 batch_loss: 0.1470949351787567\n",
      "training: 22 batch 490 batch_loss: 0.14765754342079163\n",
      "training: 22 batch 491 batch_loss: 0.14714932441711426\n",
      "training: 22 batch 492 batch_loss: 0.14915049076080322\n",
      "training: 22 batch 493 batch_loss: 0.14472758769989014\n",
      "training: 22 batch 494 batch_loss: 0.14804717898368835\n",
      "training: 22 batch 495 batch_loss: 0.1468839943408966\n",
      "training: 22 batch 496 batch_loss: 0.14705586433410645\n",
      "training: 22 batch 497 batch_loss: 0.1477397084236145\n",
      "training: 22 batch 498 batch_loss: 0.1441115140914917\n",
      "training: 22 batch 499 batch_loss: 0.1481834053993225\n",
      "training: 22 batch 500 batch_loss: 0.1493242383003235\n",
      "training: 22 batch 501 batch_loss: 0.15048998594284058\n",
      "training: 22 batch 502 batch_loss: 0.14619436860084534\n",
      "training: 22 batch 503 batch_loss: 0.14931344985961914\n",
      "training: 22 batch 504 batch_loss: 0.14767244458198547\n",
      "training: 22 batch 505 batch_loss: 0.14603915810585022\n",
      "training: 22 batch 506 batch_loss: 0.14556390047073364\n",
      "training: 22 batch 507 batch_loss: 0.1456073522567749\n",
      "training: 22 batch 508 batch_loss: 0.1459994614124298\n",
      "training: 22 batch 509 batch_loss: 0.144383043050766\n",
      "training: 22 batch 510 batch_loss: 0.1475021243095398\n",
      "training: 22 batch 511 batch_loss: 0.14458322525024414\n",
      "training: 22 batch 512 batch_loss: 0.1466529667377472\n",
      "training: 22 batch 513 batch_loss: 0.146642804145813\n",
      "training: 22 batch 514 batch_loss: 0.14609813690185547\n",
      "training: 22 batch 515 batch_loss: 0.14670801162719727\n",
      "training: 22 batch 516 batch_loss: 0.14720386266708374\n",
      "training: 22 batch 517 batch_loss: 0.14543941617012024\n",
      "training: 22 batch 518 batch_loss: 0.1472013294696808\n",
      "training: 22 batch 519 batch_loss: 0.1478167474269867\n",
      "training: 22 batch 520 batch_loss: 0.14631518721580505\n",
      "training: 22 batch 521 batch_loss: 0.1501924693584442\n",
      "training: 22 batch 522 batch_loss: 0.1493169367313385\n",
      "training: 22 batch 523 batch_loss: 0.1490185260772705\n",
      "training: 22 batch 524 batch_loss: 0.14440828561782837\n",
      "training: 22 batch 525 batch_loss: 0.14785540103912354\n",
      "training: 22 batch 526 batch_loss: 0.14980265498161316\n",
      "training: 22 batch 527 batch_loss: 0.14725935459136963\n",
      "training: 22 batch 528 batch_loss: 0.14849194884300232\n",
      "training: 22 batch 529 batch_loss: 0.14734002947807312\n",
      "training: 22 batch 530 batch_loss: 0.14769434928894043\n",
      "training: 22 batch 531 batch_loss: 0.14821922779083252\n",
      "training: 22 batch 532 batch_loss: 0.14779168367385864\n",
      "training: 22 batch 533 batch_loss: 0.14568772912025452\n",
      "training: 22 batch 534 batch_loss: 0.1506970226764679\n",
      "training: 22 batch 535 batch_loss: 0.14577892422676086\n",
      "training: 22 batch 536 batch_loss: 0.14683961868286133\n",
      "training: 22 batch 537 batch_loss: 0.14538386464118958\n",
      "training: 22 batch 538 batch_loss: 0.1434904932975769\n",
      "training: 22 batch 539 batch_loss: 0.14466089010238647\n",
      "training: 22 batch 540 batch_loss: 0.14597076177597046\n",
      "training: 22 batch 541 batch_loss: 0.1464080512523651\n",
      "training: 22 batch 542 batch_loss: 0.1471441090106964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 22 batch 543 batch_loss: 0.1482982039451599\n",
      "training: 22 batch 544 batch_loss: 0.14798250794410706\n",
      "training: 22 batch 545 batch_loss: 0.1501936912536621\n",
      "training: 22 batch 546 batch_loss: 0.14814332127571106\n",
      "training: 22 batch 547 batch_loss: 0.14855468273162842\n",
      "training: 22 batch 548 batch_loss: 0.1522367298603058\n",
      "training: 22 batch 549 batch_loss: 0.14825567603111267\n",
      "training: 22 batch 550 batch_loss: 0.1485896110534668\n",
      "training: 22 batch 551 batch_loss: 0.1487533450126648\n",
      "training: 22 batch 552 batch_loss: 0.14826077222824097\n",
      "training: 22 batch 553 batch_loss: 0.14924699068069458\n",
      "training: 22 batch 554 batch_loss: 0.1478099822998047\n",
      "training: 22 batch 555 batch_loss: 0.14821481704711914\n",
      "training: 22 batch 556 batch_loss: 0.14791792631149292\n",
      "training: 22 batch 557 batch_loss: 0.1438661515712738\n",
      "training: 22 batch 558 batch_loss: 0.14946162700653076\n",
      "training: 22 batch 559 batch_loss: 0.14438563585281372\n",
      "training: 22 batch 560 batch_loss: 0.1451270878314972\n",
      "training: 22 batch 561 batch_loss: 0.14754247665405273\n",
      "training: 22 batch 562 batch_loss: 0.1502823829650879\n",
      "training: 22 batch 563 batch_loss: 0.14829397201538086\n",
      "training: 22 batch 564 batch_loss: 0.14790576696395874\n",
      "training: 22 batch 565 batch_loss: 0.14552253484725952\n",
      "training: 22 batch 566 batch_loss: 0.1491032838821411\n",
      "training: 22 batch 567 batch_loss: 0.1496697962284088\n",
      "training: 22 batch 568 batch_loss: 0.15070292353630066\n",
      "training: 22 batch 569 batch_loss: 0.14786076545715332\n",
      "training: 22 batch 570 batch_loss: 0.14648514986038208\n",
      "training: 22 batch 571 batch_loss: 0.14619317650794983\n",
      "training: 22 batch 572 batch_loss: 0.1495191752910614\n",
      "training: 22 batch 573 batch_loss: 0.14710426330566406\n",
      "training: 22 batch 574 batch_loss: 0.14573144912719727\n",
      "training: 22 batch 575 batch_loss: 0.14629605412483215\n",
      "training: 22 batch 576 batch_loss: 0.14344435930252075\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 22, Hit Ratio:0.027875400159230894 | Precision:0.04112847734198368 | Recall:0.05641659782602108 | NDCG:0.05226697806585051\n",
      "*Best Performance* \n",
      "Epoch: 22, Hit Ratio:0.027875400159230894 | Precision:0.04112847734198368 | Recall:0.05641659782602108 | MDCG:0.05226697806585051\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 23 batch 0 batch_loss: 0.14600101113319397\n",
      "training: 23 batch 1 batch_loss: 0.14347130060195923\n",
      "training: 23 batch 2 batch_loss: 0.14417460560798645\n",
      "training: 23 batch 3 batch_loss: 0.143606036901474\n",
      "training: 23 batch 4 batch_loss: 0.1466350555419922\n",
      "training: 23 batch 5 batch_loss: 0.14557099342346191\n",
      "training: 23 batch 6 batch_loss: 0.14670109748840332\n",
      "training: 23 batch 7 batch_loss: 0.14229729771614075\n",
      "training: 23 batch 8 batch_loss: 0.1450776755809784\n",
      "training: 23 batch 9 batch_loss: 0.1410810351371765\n",
      "training: 23 batch 10 batch_loss: 0.14310336112976074\n",
      "training: 23 batch 11 batch_loss: 0.14669308066368103\n",
      "training: 23 batch 12 batch_loss: 0.1454392969608307\n",
      "training: 23 batch 13 batch_loss: 0.14376300573349\n",
      "training: 23 batch 14 batch_loss: 0.143016517162323\n",
      "training: 23 batch 15 batch_loss: 0.1468256413936615\n",
      "training: 23 batch 16 batch_loss: 0.141860693693161\n",
      "training: 23 batch 17 batch_loss: 0.14638873934745789\n",
      "training: 23 batch 18 batch_loss: 0.14794081449508667\n",
      "training: 23 batch 19 batch_loss: 0.1438121497631073\n",
      "training: 23 batch 20 batch_loss: 0.14659985899925232\n",
      "training: 23 batch 21 batch_loss: 0.14507371187210083\n",
      "training: 23 batch 22 batch_loss: 0.14361995458602905\n",
      "training: 23 batch 23 batch_loss: 0.14739349484443665\n",
      "training: 23 batch 24 batch_loss: 0.14462724328041077\n",
      "training: 23 batch 25 batch_loss: 0.14789807796478271\n",
      "training: 23 batch 26 batch_loss: 0.1471693217754364\n",
      "training: 23 batch 27 batch_loss: 0.1480686366558075\n",
      "training: 23 batch 28 batch_loss: 0.14521503448486328\n",
      "training: 23 batch 29 batch_loss: 0.1472136378288269\n",
      "training: 23 batch 30 batch_loss: 0.14495906233787537\n",
      "training: 23 batch 31 batch_loss: 0.14759927988052368\n",
      "training: 23 batch 32 batch_loss: 0.14568951725959778\n",
      "training: 23 batch 33 batch_loss: 0.14786386489868164\n",
      "training: 23 batch 34 batch_loss: 0.14464706182479858\n",
      "training: 23 batch 35 batch_loss: 0.14474555850028992\n",
      "training: 23 batch 36 batch_loss: 0.14699584245681763\n",
      "training: 23 batch 37 batch_loss: 0.14429697394371033\n",
      "training: 23 batch 38 batch_loss: 0.14333122968673706\n",
      "training: 23 batch 39 batch_loss: 0.14753204584121704\n",
      "training: 23 batch 40 batch_loss: 0.14750882983207703\n",
      "training: 23 batch 41 batch_loss: 0.14541354775428772\n",
      "training: 23 batch 42 batch_loss: 0.14348280429840088\n",
      "training: 23 batch 43 batch_loss: 0.1463891863822937\n",
      "training: 23 batch 44 batch_loss: 0.1451077163219452\n",
      "training: 23 batch 45 batch_loss: 0.14373734593391418\n",
      "training: 23 batch 46 batch_loss: 0.14492225646972656\n",
      "training: 23 batch 47 batch_loss: 0.14670920372009277\n",
      "training: 23 batch 48 batch_loss: 0.14508825540542603\n",
      "training: 23 batch 49 batch_loss: 0.14588654041290283\n",
      "training: 23 batch 50 batch_loss: 0.14638397097587585\n",
      "training: 23 batch 51 batch_loss: 0.14410412311553955\n",
      "training: 23 batch 52 batch_loss: 0.14721739292144775\n",
      "training: 23 batch 53 batch_loss: 0.14925837516784668\n",
      "training: 23 batch 54 batch_loss: 0.14510083198547363\n",
      "training: 23 batch 55 batch_loss: 0.1462603211402893\n",
      "training: 23 batch 56 batch_loss: 0.14921507239341736\n",
      "training: 23 batch 57 batch_loss: 0.14727568626403809\n",
      "training: 23 batch 58 batch_loss: 0.14849114418029785\n",
      "training: 23 batch 59 batch_loss: 0.14420989155769348\n",
      "training: 23 batch 60 batch_loss: 0.1461363434791565\n",
      "training: 23 batch 61 batch_loss: 0.14744558930397034\n",
      "training: 23 batch 62 batch_loss: 0.14472180604934692\n",
      "training: 23 batch 63 batch_loss: 0.14633208513259888\n",
      "training: 23 batch 64 batch_loss: 0.14127659797668457\n",
      "training: 23 batch 65 batch_loss: 0.1449110507965088\n",
      "training: 23 batch 66 batch_loss: 0.1473347544670105\n",
      "training: 23 batch 67 batch_loss: 0.14665934443473816\n",
      "training: 23 batch 68 batch_loss: 0.14633536338806152\n",
      "training: 23 batch 69 batch_loss: 0.14608454704284668\n",
      "training: 23 batch 70 batch_loss: 0.14858761429786682\n",
      "training: 23 batch 71 batch_loss: 0.14279168844223022\n",
      "training: 23 batch 72 batch_loss: 0.1459098756313324\n",
      "training: 23 batch 73 batch_loss: 0.14493009448051453\n",
      "training: 23 batch 74 batch_loss: 0.14548224210739136\n",
      "training: 23 batch 75 batch_loss: 0.1518341600894928\n",
      "training: 23 batch 76 batch_loss: 0.14736241102218628\n",
      "training: 23 batch 77 batch_loss: 0.1481614112854004\n",
      "training: 23 batch 78 batch_loss: 0.14702129364013672\n",
      "training: 23 batch 79 batch_loss: 0.14662843942642212\n",
      "training: 23 batch 80 batch_loss: 0.14722931385040283\n",
      "training: 23 batch 81 batch_loss: 0.14744749665260315\n",
      "training: 23 batch 82 batch_loss: 0.1460002064704895\n",
      "training: 23 batch 83 batch_loss: 0.14632612466812134\n",
      "training: 23 batch 84 batch_loss: 0.1437111794948578\n",
      "training: 23 batch 85 batch_loss: 0.1496177315711975\n",
      "training: 23 batch 86 batch_loss: 0.14653515815734863\n",
      "training: 23 batch 87 batch_loss: 0.14597761631011963\n",
      "training: 23 batch 88 batch_loss: 0.14833521842956543\n",
      "training: 23 batch 89 batch_loss: 0.14839330315589905\n",
      "training: 23 batch 90 batch_loss: 0.14596465229988098\n",
      "training: 23 batch 91 batch_loss: 0.14554116129875183\n",
      "training: 23 batch 92 batch_loss: 0.148638516664505\n",
      "training: 23 batch 93 batch_loss: 0.14547616243362427\n",
      "training: 23 batch 94 batch_loss: 0.14793583750724792\n",
      "training: 23 batch 95 batch_loss: 0.14686927199363708\n",
      "training: 23 batch 96 batch_loss: 0.1472070813179016\n",
      "training: 23 batch 97 batch_loss: 0.1491459608078003\n",
      "training: 23 batch 98 batch_loss: 0.14876902103424072\n",
      "training: 23 batch 99 batch_loss: 0.14506882429122925\n",
      "training: 23 batch 100 batch_loss: 0.14711719751358032\n",
      "training: 23 batch 101 batch_loss: 0.1472177505493164\n",
      "training: 23 batch 102 batch_loss: 0.1478738784790039\n",
      "training: 23 batch 103 batch_loss: 0.14697733521461487\n",
      "training: 23 batch 104 batch_loss: 0.14773845672607422\n",
      "training: 23 batch 105 batch_loss: 0.14796113967895508\n",
      "training: 23 batch 106 batch_loss: 0.1433313488960266\n",
      "training: 23 batch 107 batch_loss: 0.1503162980079651\n",
      "training: 23 batch 108 batch_loss: 0.1471262276172638\n",
      "training: 23 batch 109 batch_loss: 0.1506134569644928\n",
      "training: 23 batch 110 batch_loss: 0.14576244354248047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 23 batch 111 batch_loss: 0.14634579420089722\n",
      "training: 23 batch 112 batch_loss: 0.1469484567642212\n",
      "training: 23 batch 113 batch_loss: 0.14606726169586182\n",
      "training: 23 batch 114 batch_loss: 0.14285069704055786\n",
      "training: 23 batch 115 batch_loss: 0.14444401860237122\n",
      "training: 23 batch 116 batch_loss: 0.14582204818725586\n",
      "training: 23 batch 117 batch_loss: 0.14765548706054688\n",
      "training: 23 batch 118 batch_loss: 0.14481893181800842\n",
      "training: 23 batch 119 batch_loss: 0.14802420139312744\n",
      "training: 23 batch 120 batch_loss: 0.14505913853645325\n",
      "training: 23 batch 121 batch_loss: 0.1500852108001709\n",
      "training: 23 batch 122 batch_loss: 0.14637553691864014\n",
      "training: 23 batch 123 batch_loss: 0.1489475667476654\n",
      "training: 23 batch 124 batch_loss: 0.14524632692337036\n",
      "training: 23 batch 125 batch_loss: 0.14784374833106995\n",
      "training: 23 batch 126 batch_loss: 0.14616313576698303\n",
      "training: 23 batch 127 batch_loss: 0.15002265572547913\n",
      "training: 23 batch 128 batch_loss: 0.14671224355697632\n",
      "training: 23 batch 129 batch_loss: 0.14714926481246948\n",
      "training: 23 batch 130 batch_loss: 0.14997678995132446\n",
      "training: 23 batch 131 batch_loss: 0.1474783718585968\n",
      "training: 23 batch 132 batch_loss: 0.14418494701385498\n",
      "training: 23 batch 133 batch_loss: 0.14900389313697815\n",
      "training: 23 batch 134 batch_loss: 0.145992249250412\n",
      "training: 23 batch 135 batch_loss: 0.14765459299087524\n",
      "training: 23 batch 136 batch_loss: 0.14728543162345886\n",
      "training: 23 batch 137 batch_loss: 0.15085846185684204\n",
      "training: 23 batch 138 batch_loss: 0.14297199249267578\n",
      "training: 23 batch 139 batch_loss: 0.14720690250396729\n",
      "training: 23 batch 140 batch_loss: 0.1489834189414978\n",
      "training: 23 batch 141 batch_loss: 0.14481228590011597\n",
      "training: 23 batch 142 batch_loss: 0.1454830765724182\n",
      "training: 23 batch 143 batch_loss: 0.14963045716285706\n",
      "training: 23 batch 144 batch_loss: 0.1508183777332306\n",
      "training: 23 batch 145 batch_loss: 0.14781036972999573\n",
      "training: 23 batch 146 batch_loss: 0.1459338366985321\n",
      "training: 23 batch 147 batch_loss: 0.14677390456199646\n",
      "training: 23 batch 148 batch_loss: 0.1464710235595703\n",
      "training: 23 batch 149 batch_loss: 0.14673733711242676\n",
      "training: 23 batch 150 batch_loss: 0.14646440744400024\n",
      "training: 23 batch 151 batch_loss: 0.1463099718093872\n",
      "training: 23 batch 152 batch_loss: 0.14687973260879517\n",
      "training: 23 batch 153 batch_loss: 0.14614489674568176\n",
      "training: 23 batch 154 batch_loss: 0.14886781573295593\n",
      "training: 23 batch 155 batch_loss: 0.14884889125823975\n",
      "training: 23 batch 156 batch_loss: 0.1450287103652954\n",
      "training: 23 batch 157 batch_loss: 0.1480359435081482\n",
      "training: 23 batch 158 batch_loss: 0.1486455202102661\n",
      "training: 23 batch 159 batch_loss: 0.1439538598060608\n",
      "training: 23 batch 160 batch_loss: 0.1449832022190094\n",
      "training: 23 batch 161 batch_loss: 0.1513921022415161\n",
      "training: 23 batch 162 batch_loss: 0.1475021243095398\n",
      "training: 23 batch 163 batch_loss: 0.14562293887138367\n",
      "training: 23 batch 164 batch_loss: 0.148983895778656\n",
      "training: 23 batch 165 batch_loss: 0.1446027159690857\n",
      "training: 23 batch 166 batch_loss: 0.14761370420455933\n",
      "training: 23 batch 167 batch_loss: 0.14680486917495728\n",
      "training: 23 batch 168 batch_loss: 0.1447727084159851\n",
      "training: 23 batch 169 batch_loss: 0.1473904848098755\n",
      "training: 23 batch 170 batch_loss: 0.14848148822784424\n",
      "training: 23 batch 171 batch_loss: 0.14506971836090088\n",
      "training: 23 batch 172 batch_loss: 0.14551421999931335\n",
      "training: 23 batch 173 batch_loss: 0.14761653542518616\n",
      "training: 23 batch 174 batch_loss: 0.1451411247253418\n",
      "training: 23 batch 175 batch_loss: 0.14952868223190308\n",
      "training: 23 batch 176 batch_loss: 0.14991432428359985\n",
      "training: 23 batch 177 batch_loss: 0.14702379703521729\n",
      "training: 23 batch 178 batch_loss: 0.14573544263839722\n",
      "training: 23 batch 179 batch_loss: 0.14702123403549194\n",
      "training: 23 batch 180 batch_loss: 0.14865240454673767\n",
      "training: 23 batch 181 batch_loss: 0.14919987320899963\n",
      "training: 23 batch 182 batch_loss: 0.14915508031845093\n",
      "training: 23 batch 183 batch_loss: 0.147996187210083\n",
      "training: 23 batch 184 batch_loss: 0.14683324098587036\n",
      "training: 23 batch 185 batch_loss: 0.14687791466712952\n",
      "training: 23 batch 186 batch_loss: 0.14855358004570007\n",
      "training: 23 batch 187 batch_loss: 0.14976021647453308\n",
      "training: 23 batch 188 batch_loss: 0.14820805191993713\n",
      "training: 23 batch 189 batch_loss: 0.14884290099143982\n",
      "training: 23 batch 190 batch_loss: 0.1469084918498993\n",
      "training: 23 batch 191 batch_loss: 0.14539819955825806\n",
      "training: 23 batch 192 batch_loss: 0.1445031464099884\n",
      "training: 23 batch 193 batch_loss: 0.14870324730873108\n",
      "training: 23 batch 194 batch_loss: 0.14516842365264893\n",
      "training: 23 batch 195 batch_loss: 0.1492084264755249\n",
      "training: 23 batch 196 batch_loss: 0.14422199130058289\n",
      "training: 23 batch 197 batch_loss: 0.15019601583480835\n",
      "training: 23 batch 198 batch_loss: 0.15028533339500427\n",
      "training: 23 batch 199 batch_loss: 0.1474926471710205\n",
      "training: 23 batch 200 batch_loss: 0.14787155389785767\n",
      "training: 23 batch 201 batch_loss: 0.148664653301239\n",
      "training: 23 batch 202 batch_loss: 0.14672255516052246\n",
      "training: 23 batch 203 batch_loss: 0.14444035291671753\n",
      "training: 23 batch 204 batch_loss: 0.14740514755249023\n",
      "training: 23 batch 205 batch_loss: 0.14844974875450134\n",
      "training: 23 batch 206 batch_loss: 0.14721211791038513\n",
      "training: 23 batch 207 batch_loss: 0.14819437265396118\n",
      "training: 23 batch 208 batch_loss: 0.146327406167984\n",
      "training: 23 batch 209 batch_loss: 0.14807826280593872\n",
      "training: 23 batch 210 batch_loss: 0.1495155692100525\n",
      "training: 23 batch 211 batch_loss: 0.14771318435668945\n",
      "training: 23 batch 212 batch_loss: 0.14674213528633118\n",
      "training: 23 batch 213 batch_loss: 0.1500881016254425\n",
      "training: 23 batch 214 batch_loss: 0.14648020267486572\n",
      "training: 23 batch 215 batch_loss: 0.14749053120613098\n",
      "training: 23 batch 216 batch_loss: 0.14880239963531494\n",
      "training: 23 batch 217 batch_loss: 0.14916303753852844\n",
      "training: 23 batch 218 batch_loss: 0.14709728956222534\n",
      "training: 23 batch 219 batch_loss: 0.14901798963546753\n",
      "training: 23 batch 220 batch_loss: 0.14711323380470276\n",
      "training: 23 batch 221 batch_loss: 0.14697355031967163\n",
      "training: 23 batch 222 batch_loss: 0.1459721028804779\n",
      "training: 23 batch 223 batch_loss: 0.14829981327056885\n",
      "training: 23 batch 224 batch_loss: 0.14777886867523193\n",
      "training: 23 batch 225 batch_loss: 0.14722532033920288\n",
      "training: 23 batch 226 batch_loss: 0.14998507499694824\n",
      "training: 23 batch 227 batch_loss: 0.15210029482841492\n",
      "training: 23 batch 228 batch_loss: 0.14640825986862183\n",
      "training: 23 batch 229 batch_loss: 0.14534008502960205\n",
      "training: 23 batch 230 batch_loss: 0.1430496871471405\n",
      "training: 23 batch 231 batch_loss: 0.14680150151252747\n",
      "training: 23 batch 232 batch_loss: 0.14747929573059082\n",
      "training: 23 batch 233 batch_loss: 0.14726069569587708\n",
      "training: 23 batch 234 batch_loss: 0.14765900373458862\n",
      "training: 23 batch 235 batch_loss: 0.14322501420974731\n",
      "training: 23 batch 236 batch_loss: 0.14680573344230652\n",
      "training: 23 batch 237 batch_loss: 0.1462322473526001\n",
      "training: 23 batch 238 batch_loss: 0.14707601070404053\n",
      "training: 23 batch 239 batch_loss: 0.14824900031089783\n",
      "training: 23 batch 240 batch_loss: 0.1490064263343811\n",
      "training: 23 batch 241 batch_loss: 0.15096774697303772\n",
      "training: 23 batch 242 batch_loss: 0.1464812159538269\n",
      "training: 23 batch 243 batch_loss: 0.1480872631072998\n",
      "training: 23 batch 244 batch_loss: 0.14057660102844238\n",
      "training: 23 batch 245 batch_loss: 0.14651012420654297\n",
      "training: 23 batch 246 batch_loss: 0.14951184391975403\n",
      "training: 23 batch 247 batch_loss: 0.14432448148727417\n",
      "training: 23 batch 248 batch_loss: 0.14895525574684143\n",
      "training: 23 batch 249 batch_loss: 0.14942434430122375\n",
      "training: 23 batch 250 batch_loss: 0.1470956802368164\n",
      "training: 23 batch 251 batch_loss: 0.14560925960540771\n",
      "training: 23 batch 252 batch_loss: 0.14702987670898438\n",
      "training: 23 batch 253 batch_loss: 0.14889314770698547\n",
      "training: 23 batch 254 batch_loss: 0.14965897798538208\n",
      "training: 23 batch 255 batch_loss: 0.1459808051586151\n",
      "training: 23 batch 256 batch_loss: 0.14687055349349976\n",
      "training: 23 batch 257 batch_loss: 0.15247738361358643\n",
      "training: 23 batch 258 batch_loss: 0.14639312028884888\n",
      "training: 23 batch 259 batch_loss: 0.14915362000465393\n",
      "training: 23 batch 260 batch_loss: 0.15043237805366516\n",
      "training: 23 batch 261 batch_loss: 0.14870041608810425\n",
      "training: 23 batch 262 batch_loss: 0.14696910977363586\n",
      "training: 23 batch 263 batch_loss: 0.14718952775001526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 23 batch 264 batch_loss: 0.14788168668746948\n",
      "training: 23 batch 265 batch_loss: 0.14529350399971008\n",
      "training: 23 batch 266 batch_loss: 0.14838150143623352\n",
      "training: 23 batch 267 batch_loss: 0.14880412817001343\n",
      "training: 23 batch 268 batch_loss: 0.14540472626686096\n",
      "training: 23 batch 269 batch_loss: 0.14910435676574707\n",
      "training: 23 batch 270 batch_loss: 0.15051981806755066\n",
      "training: 23 batch 271 batch_loss: 0.14833682775497437\n",
      "training: 23 batch 272 batch_loss: 0.14816492795944214\n",
      "training: 23 batch 273 batch_loss: 0.14974075555801392\n",
      "training: 23 batch 274 batch_loss: 0.1435956358909607\n",
      "training: 23 batch 275 batch_loss: 0.148454487323761\n",
      "training: 23 batch 276 batch_loss: 0.14816492795944214\n",
      "training: 23 batch 277 batch_loss: 0.15129706263542175\n",
      "training: 23 batch 278 batch_loss: 0.14395198225975037\n",
      "training: 23 batch 279 batch_loss: 0.1508934497833252\n",
      "training: 23 batch 280 batch_loss: 0.14633020758628845\n",
      "training: 23 batch 281 batch_loss: 0.14292222261428833\n",
      "training: 23 batch 282 batch_loss: 0.14895707368850708\n",
      "training: 23 batch 283 batch_loss: 0.14948123693466187\n",
      "training: 23 batch 284 batch_loss: 0.14630144834518433\n",
      "training: 23 batch 285 batch_loss: 0.14924341440200806\n",
      "training: 23 batch 286 batch_loss: 0.15016889572143555\n",
      "training: 23 batch 287 batch_loss: 0.14677035808563232\n",
      "training: 23 batch 288 batch_loss: 0.15117183327674866\n",
      "training: 23 batch 289 batch_loss: 0.14474141597747803\n",
      "training: 23 batch 290 batch_loss: 0.1503593921661377\n",
      "training: 23 batch 291 batch_loss: 0.1507035493850708\n",
      "training: 23 batch 292 batch_loss: 0.14871200919151306\n",
      "training: 23 batch 293 batch_loss: 0.14608314633369446\n",
      "training: 23 batch 294 batch_loss: 0.14679312705993652\n",
      "training: 23 batch 295 batch_loss: 0.149978369474411\n",
      "training: 23 batch 296 batch_loss: 0.14624521136283875\n",
      "training: 23 batch 297 batch_loss: 0.15025430917739868\n",
      "training: 23 batch 298 batch_loss: 0.14992544054985046\n",
      "training: 23 batch 299 batch_loss: 0.14867553114891052\n",
      "training: 23 batch 300 batch_loss: 0.14792346954345703\n",
      "training: 23 batch 301 batch_loss: 0.1471908986568451\n",
      "training: 23 batch 302 batch_loss: 0.14832419157028198\n",
      "training: 23 batch 303 batch_loss: 0.14900219440460205\n",
      "training: 23 batch 304 batch_loss: 0.15014302730560303\n",
      "training: 23 batch 305 batch_loss: 0.1460910439491272\n",
      "training: 23 batch 306 batch_loss: 0.14944347739219666\n",
      "training: 23 batch 307 batch_loss: 0.14928853511810303\n",
      "training: 23 batch 308 batch_loss: 0.14851266145706177\n",
      "training: 23 batch 309 batch_loss: 0.14670327305793762\n",
      "training: 23 batch 310 batch_loss: 0.1502002477645874\n",
      "training: 23 batch 311 batch_loss: 0.14439791440963745\n",
      "training: 23 batch 312 batch_loss: 0.14994758367538452\n",
      "training: 23 batch 313 batch_loss: 0.1458040177822113\n",
      "training: 23 batch 314 batch_loss: 0.14614173769950867\n",
      "training: 23 batch 315 batch_loss: 0.1529999077320099\n",
      "training: 23 batch 316 batch_loss: 0.15000426769256592\n",
      "training: 23 batch 317 batch_loss: 0.146743506193161\n",
      "training: 23 batch 318 batch_loss: 0.14512214064598083\n",
      "training: 23 batch 319 batch_loss: 0.14625629782676697\n",
      "training: 23 batch 320 batch_loss: 0.14846378564834595\n",
      "training: 23 batch 321 batch_loss: 0.14853233098983765\n",
      "training: 23 batch 322 batch_loss: 0.14651522040367126\n",
      "training: 23 batch 323 batch_loss: 0.15173518657684326\n",
      "training: 23 batch 324 batch_loss: 0.14887598156929016\n",
      "training: 23 batch 325 batch_loss: 0.14746087789535522\n",
      "training: 23 batch 326 batch_loss: 0.14972859621047974\n",
      "training: 23 batch 327 batch_loss: 0.14763545989990234\n",
      "training: 23 batch 328 batch_loss: 0.14911913871765137\n",
      "training: 23 batch 329 batch_loss: 0.14712989330291748\n",
      "training: 23 batch 330 batch_loss: 0.14892390370368958\n",
      "training: 23 batch 331 batch_loss: 0.14623454213142395\n",
      "training: 23 batch 332 batch_loss: 0.15001678466796875\n",
      "training: 23 batch 333 batch_loss: 0.14384061098098755\n",
      "training: 23 batch 334 batch_loss: 0.15015974640846252\n",
      "training: 23 batch 335 batch_loss: 0.14736273884773254\n",
      "training: 23 batch 336 batch_loss: 0.14999568462371826\n",
      "training: 23 batch 337 batch_loss: 0.14839312434196472\n",
      "training: 23 batch 338 batch_loss: 0.1471748948097229\n",
      "training: 23 batch 339 batch_loss: 0.14776501059532166\n",
      "training: 23 batch 340 batch_loss: 0.14984890818595886\n",
      "training: 23 batch 341 batch_loss: 0.14626574516296387\n",
      "training: 23 batch 342 batch_loss: 0.1461639702320099\n",
      "training: 23 batch 343 batch_loss: 0.14916986227035522\n",
      "training: 23 batch 344 batch_loss: 0.14912346005439758\n",
      "training: 23 batch 345 batch_loss: 0.14900749921798706\n",
      "training: 23 batch 346 batch_loss: 0.1458468735218048\n",
      "training: 23 batch 347 batch_loss: 0.14717742800712585\n",
      "training: 23 batch 348 batch_loss: 0.14723718166351318\n",
      "training: 23 batch 349 batch_loss: 0.14876285195350647\n",
      "training: 23 batch 350 batch_loss: 0.14456990361213684\n",
      "training: 23 batch 351 batch_loss: 0.1458977460861206\n",
      "training: 23 batch 352 batch_loss: 0.14884552359580994\n",
      "training: 23 batch 353 batch_loss: 0.15101534128189087\n",
      "training: 23 batch 354 batch_loss: 0.15111234784126282\n",
      "training: 23 batch 355 batch_loss: 0.14575743675231934\n",
      "training: 23 batch 356 batch_loss: 0.1479530930519104\n",
      "training: 23 batch 357 batch_loss: 0.14948579668998718\n",
      "training: 23 batch 358 batch_loss: 0.14597490429878235\n",
      "training: 23 batch 359 batch_loss: 0.1451701521873474\n",
      "training: 23 batch 360 batch_loss: 0.14706841111183167\n",
      "training: 23 batch 361 batch_loss: 0.1505739986896515\n",
      "training: 23 batch 362 batch_loss: 0.148194819688797\n",
      "training: 23 batch 363 batch_loss: 0.14973855018615723\n",
      "training: 23 batch 364 batch_loss: 0.14750534296035767\n",
      "training: 23 batch 365 batch_loss: 0.14567357301712036\n",
      "training: 23 batch 366 batch_loss: 0.1505979597568512\n",
      "training: 23 batch 367 batch_loss: 0.15083801746368408\n",
      "training: 23 batch 368 batch_loss: 0.1469484269618988\n",
      "training: 23 batch 369 batch_loss: 0.1524420976638794\n",
      "training: 23 batch 370 batch_loss: 0.14618414640426636\n",
      "training: 23 batch 371 batch_loss: 0.14880770444869995\n",
      "training: 23 batch 372 batch_loss: 0.14644914865493774\n",
      "training: 23 batch 373 batch_loss: 0.15035530924797058\n",
      "training: 23 batch 374 batch_loss: 0.14597797393798828\n",
      "training: 23 batch 375 batch_loss: 0.14975836873054504\n",
      "training: 23 batch 376 batch_loss: 0.14624488353729248\n",
      "training: 23 batch 377 batch_loss: 0.14791959524154663\n",
      "training: 23 batch 378 batch_loss: 0.14846423268318176\n",
      "training: 23 batch 379 batch_loss: 0.14753282070159912\n",
      "training: 23 batch 380 batch_loss: 0.14683040976524353\n",
      "training: 23 batch 381 batch_loss: 0.14875030517578125\n",
      "training: 23 batch 382 batch_loss: 0.14803534746170044\n",
      "training: 23 batch 383 batch_loss: 0.14684516191482544\n",
      "training: 23 batch 384 batch_loss: 0.14432787895202637\n",
      "training: 23 batch 385 batch_loss: 0.14557021856307983\n",
      "training: 23 batch 386 batch_loss: 0.15200307965278625\n",
      "training: 23 batch 387 batch_loss: 0.14911916851997375\n",
      "training: 23 batch 388 batch_loss: 0.15020009875297546\n",
      "training: 23 batch 389 batch_loss: 0.14962992072105408\n",
      "training: 23 batch 390 batch_loss: 0.1500852406024933\n",
      "training: 23 batch 391 batch_loss: 0.14724105596542358\n",
      "training: 23 batch 392 batch_loss: 0.14828753471374512\n",
      "training: 23 batch 393 batch_loss: 0.15012189745903015\n",
      "training: 23 batch 394 batch_loss: 0.14823490381240845\n",
      "training: 23 batch 395 batch_loss: 0.1470317542552948\n",
      "training: 23 batch 396 batch_loss: 0.1494021713733673\n",
      "training: 23 batch 397 batch_loss: 0.15202316641807556\n",
      "training: 23 batch 398 batch_loss: 0.14428946375846863\n",
      "training: 23 batch 399 batch_loss: 0.1487404704093933\n",
      "training: 23 batch 400 batch_loss: 0.14901599287986755\n",
      "training: 23 batch 401 batch_loss: 0.14900851249694824\n",
      "training: 23 batch 402 batch_loss: 0.15018251538276672\n",
      "training: 23 batch 403 batch_loss: 0.15106117725372314\n",
      "training: 23 batch 404 batch_loss: 0.15105828642845154\n",
      "training: 23 batch 405 batch_loss: 0.1476086974143982\n",
      "training: 23 batch 406 batch_loss: 0.15002983808517456\n",
      "training: 23 batch 407 batch_loss: 0.15106475353240967\n",
      "training: 23 batch 408 batch_loss: 0.14727401733398438\n",
      "training: 23 batch 409 batch_loss: 0.14768293499946594\n",
      "training: 23 batch 410 batch_loss: 0.15028977394104004\n",
      "training: 23 batch 411 batch_loss: 0.1481226682662964\n",
      "training: 23 batch 412 batch_loss: 0.14821916818618774\n",
      "training: 23 batch 413 batch_loss: 0.15153756737709045\n",
      "training: 23 batch 414 batch_loss: 0.1450604498386383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 23 batch 415 batch_loss: 0.1455020308494568\n",
      "training: 23 batch 416 batch_loss: 0.14870232343673706\n",
      "training: 23 batch 417 batch_loss: 0.14803844690322876\n",
      "training: 23 batch 418 batch_loss: 0.147781640291214\n",
      "training: 23 batch 419 batch_loss: 0.15148887038230896\n",
      "training: 23 batch 420 batch_loss: 0.14862412214279175\n",
      "training: 23 batch 421 batch_loss: 0.15041318535804749\n",
      "training: 23 batch 422 batch_loss: 0.14741435647010803\n",
      "training: 23 batch 423 batch_loss: 0.14590615034103394\n",
      "training: 23 batch 424 batch_loss: 0.1500703990459442\n",
      "training: 23 batch 425 batch_loss: 0.14893829822540283\n",
      "training: 23 batch 426 batch_loss: 0.14813727140426636\n",
      "training: 23 batch 427 batch_loss: 0.14842209219932556\n",
      "training: 23 batch 428 batch_loss: 0.14843082427978516\n",
      "training: 23 batch 429 batch_loss: 0.15185493230819702\n",
      "training: 23 batch 430 batch_loss: 0.14709332585334778\n",
      "training: 23 batch 431 batch_loss: 0.14805743098258972\n",
      "training: 23 batch 432 batch_loss: 0.1523076295852661\n",
      "training: 23 batch 433 batch_loss: 0.14821112155914307\n",
      "training: 23 batch 434 batch_loss: 0.149907648563385\n",
      "training: 23 batch 435 batch_loss: 0.1484009027481079\n",
      "training: 23 batch 436 batch_loss: 0.15090683102607727\n",
      "training: 23 batch 437 batch_loss: 0.14816626906394958\n",
      "training: 23 batch 438 batch_loss: 0.14740511775016785\n",
      "training: 23 batch 439 batch_loss: 0.1516682207584381\n",
      "training: 23 batch 440 batch_loss: 0.1479421854019165\n",
      "training: 23 batch 441 batch_loss: 0.1510905921459198\n",
      "training: 23 batch 442 batch_loss: 0.15135467052459717\n",
      "training: 23 batch 443 batch_loss: 0.14877533912658691\n",
      "training: 23 batch 444 batch_loss: 0.1455630362033844\n",
      "training: 23 batch 445 batch_loss: 0.14766791462898254\n",
      "training: 23 batch 446 batch_loss: 0.14769574999809265\n",
      "training: 23 batch 447 batch_loss: 0.14661931991577148\n",
      "training: 23 batch 448 batch_loss: 0.14862218499183655\n",
      "training: 23 batch 449 batch_loss: 0.15023157000541687\n",
      "training: 23 batch 450 batch_loss: 0.14940479397773743\n",
      "training: 23 batch 451 batch_loss: 0.14880093932151794\n",
      "training: 23 batch 452 batch_loss: 0.14806538820266724\n",
      "training: 23 batch 453 batch_loss: 0.1484454870223999\n",
      "training: 23 batch 454 batch_loss: 0.14668872952461243\n",
      "training: 23 batch 455 batch_loss: 0.14715576171875\n",
      "training: 23 batch 456 batch_loss: 0.15133094787597656\n",
      "training: 23 batch 457 batch_loss: 0.15097245573997498\n",
      "training: 23 batch 458 batch_loss: 0.1482618749141693\n",
      "training: 23 batch 459 batch_loss: 0.1513085663318634\n",
      "training: 23 batch 460 batch_loss: 0.14879834651947021\n",
      "training: 23 batch 461 batch_loss: 0.1487899124622345\n",
      "training: 23 batch 462 batch_loss: 0.14846864342689514\n",
      "training: 23 batch 463 batch_loss: 0.15219196677207947\n",
      "training: 23 batch 464 batch_loss: 0.15088823437690735\n",
      "training: 23 batch 465 batch_loss: 0.1453881859779358\n",
      "training: 23 batch 466 batch_loss: 0.15087521076202393\n",
      "training: 23 batch 467 batch_loss: 0.15263846516609192\n",
      "training: 23 batch 468 batch_loss: 0.14812436699867249\n",
      "training: 23 batch 469 batch_loss: 0.14783412218093872\n",
      "training: 23 batch 470 batch_loss: 0.14959901571273804\n",
      "training: 23 batch 471 batch_loss: 0.15015137195587158\n",
      "training: 23 batch 472 batch_loss: 0.14814624190330505\n",
      "training: 23 batch 473 batch_loss: 0.14933642745018005\n",
      "training: 23 batch 474 batch_loss: 0.15098577737808228\n",
      "training: 23 batch 475 batch_loss: 0.1450413465499878\n",
      "training: 23 batch 476 batch_loss: 0.1476615071296692\n",
      "training: 23 batch 477 batch_loss: 0.14904356002807617\n",
      "training: 23 batch 478 batch_loss: 0.14859211444854736\n",
      "training: 23 batch 479 batch_loss: 0.1507423222064972\n",
      "training: 23 batch 480 batch_loss: 0.14971762895584106\n",
      "training: 23 batch 481 batch_loss: 0.1534486711025238\n",
      "training: 23 batch 482 batch_loss: 0.14913639426231384\n",
      "training: 23 batch 483 batch_loss: 0.15129786729812622\n",
      "training: 23 batch 484 batch_loss: 0.1501833200454712\n",
      "training: 23 batch 485 batch_loss: 0.1507410705089569\n",
      "training: 23 batch 486 batch_loss: 0.14948710799217224\n",
      "training: 23 batch 487 batch_loss: 0.14677837491035461\n",
      "training: 23 batch 488 batch_loss: 0.15298894047737122\n",
      "training: 23 batch 489 batch_loss: 0.15044671297073364\n",
      "training: 23 batch 490 batch_loss: 0.14782407879829407\n",
      "training: 23 batch 491 batch_loss: 0.1447218954563141\n",
      "training: 23 batch 492 batch_loss: 0.1463419795036316\n",
      "training: 23 batch 493 batch_loss: 0.14638710021972656\n",
      "training: 23 batch 494 batch_loss: 0.15032488107681274\n",
      "training: 23 batch 495 batch_loss: 0.14849025011062622\n",
      "training: 23 batch 496 batch_loss: 0.15253326296806335\n",
      "training: 23 batch 497 batch_loss: 0.15087193250656128\n",
      "training: 23 batch 498 batch_loss: 0.15027526021003723\n",
      "training: 23 batch 499 batch_loss: 0.15029343962669373\n",
      "training: 23 batch 500 batch_loss: 0.14948970079421997\n",
      "training: 23 batch 501 batch_loss: 0.14996251463890076\n",
      "training: 23 batch 502 batch_loss: 0.15474000573158264\n",
      "training: 23 batch 503 batch_loss: 0.14802411198616028\n",
      "training: 23 batch 504 batch_loss: 0.14966973662376404\n",
      "training: 23 batch 505 batch_loss: 0.1487153172492981\n",
      "training: 23 batch 506 batch_loss: 0.146763414144516\n",
      "training: 23 batch 507 batch_loss: 0.1478290855884552\n",
      "training: 23 batch 508 batch_loss: 0.15157437324523926\n",
      "training: 23 batch 509 batch_loss: 0.14934569597244263\n",
      "training: 23 batch 510 batch_loss: 0.15104922652244568\n",
      "training: 23 batch 511 batch_loss: 0.14949989318847656\n",
      "training: 23 batch 512 batch_loss: 0.14923018217086792\n",
      "training: 23 batch 513 batch_loss: 0.14994698762893677\n",
      "training: 23 batch 514 batch_loss: 0.15124458074569702\n",
      "training: 23 batch 515 batch_loss: 0.15103906393051147\n",
      "training: 23 batch 516 batch_loss: 0.15208134055137634\n",
      "training: 23 batch 517 batch_loss: 0.14956051111221313\n",
      "training: 23 batch 518 batch_loss: 0.14808619022369385\n",
      "training: 23 batch 519 batch_loss: 0.14678990840911865\n",
      "training: 23 batch 520 batch_loss: 0.14918863773345947\n",
      "training: 23 batch 521 batch_loss: 0.14804837107658386\n",
      "training: 23 batch 522 batch_loss: 0.15237721800804138\n",
      "training: 23 batch 523 batch_loss: 0.14855316281318665\n",
      "training: 23 batch 524 batch_loss: 0.14561855792999268\n",
      "training: 23 batch 525 batch_loss: 0.14968612790107727\n",
      "training: 23 batch 526 batch_loss: 0.148695707321167\n",
      "training: 23 batch 527 batch_loss: 0.1465621292591095\n",
      "training: 23 batch 528 batch_loss: 0.15006670355796814\n",
      "training: 23 batch 529 batch_loss: 0.15044859051704407\n",
      "training: 23 batch 530 batch_loss: 0.14728227257728577\n",
      "training: 23 batch 531 batch_loss: 0.15020543336868286\n",
      "training: 23 batch 532 batch_loss: 0.14866521954536438\n",
      "training: 23 batch 533 batch_loss: 0.14784547686576843\n",
      "training: 23 batch 534 batch_loss: 0.15014812350273132\n",
      "training: 23 batch 535 batch_loss: 0.14888140559196472\n",
      "training: 23 batch 536 batch_loss: 0.14615052938461304\n",
      "training: 23 batch 537 batch_loss: 0.15393474698066711\n",
      "training: 23 batch 538 batch_loss: 0.14994612336158752\n",
      "training: 23 batch 539 batch_loss: 0.14826682209968567\n",
      "training: 23 batch 540 batch_loss: 0.1461849808692932\n",
      "training: 23 batch 541 batch_loss: 0.1489163637161255\n",
      "training: 23 batch 542 batch_loss: 0.14843285083770752\n",
      "training: 23 batch 543 batch_loss: 0.14962521195411682\n",
      "training: 23 batch 544 batch_loss: 0.14645534753799438\n",
      "training: 23 batch 545 batch_loss: 0.15058350563049316\n",
      "training: 23 batch 546 batch_loss: 0.14932331442832947\n",
      "training: 23 batch 547 batch_loss: 0.15065041184425354\n",
      "training: 23 batch 548 batch_loss: 0.14718475937843323\n",
      "training: 23 batch 549 batch_loss: 0.14953219890594482\n",
      "training: 23 batch 550 batch_loss: 0.14973852038383484\n",
      "training: 23 batch 551 batch_loss: 0.14946123957633972\n",
      "training: 23 batch 552 batch_loss: 0.14734169840812683\n",
      "training: 23 batch 553 batch_loss: 0.1530173420906067\n",
      "training: 23 batch 554 batch_loss: 0.15297645330429077\n",
      "training: 23 batch 555 batch_loss: 0.14735808968544006\n",
      "training: 23 batch 556 batch_loss: 0.15189743041992188\n",
      "training: 23 batch 557 batch_loss: 0.14519181847572327\n",
      "training: 23 batch 558 batch_loss: 0.1504795253276825\n",
      "training: 23 batch 559 batch_loss: 0.1536703109741211\n",
      "training: 23 batch 560 batch_loss: 0.1520000398159027\n",
      "training: 23 batch 561 batch_loss: 0.1501365602016449\n",
      "training: 23 batch 562 batch_loss: 0.1477055549621582\n",
      "training: 23 batch 563 batch_loss: 0.14760512113571167\n",
      "training: 23 batch 564 batch_loss: 0.1466170847415924\n",
      "training: 23 batch 565 batch_loss: 0.15098029375076294\n",
      "training: 23 batch 566 batch_loss: 0.14637795090675354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 23 batch 567 batch_loss: 0.1478215754032135\n",
      "training: 23 batch 568 batch_loss: 0.14949682354927063\n",
      "training: 23 batch 569 batch_loss: 0.15047547221183777\n",
      "training: 23 batch 570 batch_loss: 0.14911797642707825\n",
      "training: 23 batch 571 batch_loss: 0.1496277153491974\n",
      "training: 23 batch 572 batch_loss: 0.15050393342971802\n",
      "training: 23 batch 573 batch_loss: 0.14900225400924683\n",
      "training: 23 batch 574 batch_loss: 0.14808234572410583\n",
      "training: 23 batch 575 batch_loss: 0.1504681408405304\n",
      "training: 23 batch 576 batch_loss: 0.14552947878837585\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 23, Hit Ratio:0.028475014407397907 | Precision:0.042013172122284476 | Recall:0.05746027730018805 | NDCG:0.05339332461710241\n",
      "*Best Performance* \n",
      "Epoch: 23, Hit Ratio:0.028475014407397907 | Precision:0.042013172122284476 | Recall:0.05746027730018805 | MDCG:0.05339332461710241\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 24 batch 0 batch_loss: 0.15075016021728516\n",
      "training: 24 batch 1 batch_loss: 0.14564228057861328\n",
      "training: 24 batch 2 batch_loss: 0.1464812159538269\n",
      "training: 24 batch 3 batch_loss: 0.14803916215896606\n",
      "training: 24 batch 4 batch_loss: 0.1482398509979248\n",
      "training: 24 batch 5 batch_loss: 0.14547601342201233\n",
      "training: 24 batch 6 batch_loss: 0.143499493598938\n",
      "training: 24 batch 7 batch_loss: 0.14699023962020874\n",
      "training: 24 batch 8 batch_loss: 0.14771446585655212\n",
      "training: 24 batch 9 batch_loss: 0.1491134762763977\n",
      "training: 24 batch 10 batch_loss: 0.1510021686553955\n",
      "training: 24 batch 11 batch_loss: 0.1490151286125183\n",
      "training: 24 batch 12 batch_loss: 0.14616045355796814\n",
      "training: 24 batch 13 batch_loss: 0.14702540636062622\n",
      "training: 24 batch 14 batch_loss: 0.1489105522632599\n",
      "training: 24 batch 15 batch_loss: 0.14604496955871582\n",
      "training: 24 batch 16 batch_loss: 0.14484050869941711\n",
      "training: 24 batch 17 batch_loss: 0.14869585633277893\n",
      "training: 24 batch 18 batch_loss: 0.1470797061920166\n",
      "training: 24 batch 19 batch_loss: 0.14749246835708618\n",
      "training: 24 batch 20 batch_loss: 0.1513836681842804\n",
      "training: 24 batch 21 batch_loss: 0.15018150210380554\n",
      "training: 24 batch 22 batch_loss: 0.1495855152606964\n",
      "training: 24 batch 23 batch_loss: 0.1456165611743927\n",
      "training: 24 batch 24 batch_loss: 0.14494135975837708\n",
      "training: 24 batch 25 batch_loss: 0.1489669680595398\n",
      "training: 24 batch 26 batch_loss: 0.14741021394729614\n",
      "training: 24 batch 27 batch_loss: 0.1472100019454956\n",
      "training: 24 batch 28 batch_loss: 0.15161222219467163\n",
      "training: 24 batch 29 batch_loss: 0.14790460467338562\n",
      "training: 24 batch 30 batch_loss: 0.14888453483581543\n",
      "training: 24 batch 31 batch_loss: 0.15297538042068481\n",
      "training: 24 batch 32 batch_loss: 0.14734235405921936\n",
      "training: 24 batch 33 batch_loss: 0.14774063229560852\n",
      "training: 24 batch 34 batch_loss: 0.1469523310661316\n",
      "training: 24 batch 35 batch_loss: 0.14674049615859985\n",
      "training: 24 batch 36 batch_loss: 0.14991715550422668\n",
      "training: 24 batch 37 batch_loss: 0.1488180160522461\n",
      "training: 24 batch 38 batch_loss: 0.14962539076805115\n",
      "training: 24 batch 39 batch_loss: 0.1458500623703003\n",
      "training: 24 batch 40 batch_loss: 0.14663425087928772\n",
      "training: 24 batch 41 batch_loss: 0.1481708288192749\n",
      "training: 24 batch 42 batch_loss: 0.14969992637634277\n",
      "training: 24 batch 43 batch_loss: 0.149992436170578\n",
      "training: 24 batch 44 batch_loss: 0.14770403504371643\n",
      "training: 24 batch 45 batch_loss: 0.14815810322761536\n",
      "training: 24 batch 46 batch_loss: 0.14703065156936646\n",
      "training: 24 batch 47 batch_loss: 0.14940279722213745\n",
      "training: 24 batch 48 batch_loss: 0.15041068196296692\n",
      "training: 24 batch 49 batch_loss: 0.14719262719154358\n",
      "training: 24 batch 50 batch_loss: 0.14886629581451416\n",
      "training: 24 batch 51 batch_loss: 0.14779743552207947\n",
      "training: 24 batch 52 batch_loss: 0.15044254064559937\n",
      "training: 24 batch 53 batch_loss: 0.14874297380447388\n",
      "training: 24 batch 54 batch_loss: 0.14704424142837524\n",
      "training: 24 batch 55 batch_loss: 0.14701133966445923\n",
      "training: 24 batch 56 batch_loss: 0.14973515272140503\n",
      "training: 24 batch 57 batch_loss: 0.14702996611595154\n",
      "training: 24 batch 58 batch_loss: 0.14637154340744019\n",
      "training: 24 batch 59 batch_loss: 0.1486455500125885\n",
      "training: 24 batch 60 batch_loss: 0.14846870303153992\n",
      "training: 24 batch 61 batch_loss: 0.14713627099990845\n",
      "training: 24 batch 62 batch_loss: 0.1476280689239502\n",
      "training: 24 batch 63 batch_loss: 0.14929944276809692\n",
      "training: 24 batch 64 batch_loss: 0.14712780714035034\n",
      "training: 24 batch 65 batch_loss: 0.14902663230895996\n",
      "training: 24 batch 66 batch_loss: 0.1482144594192505\n",
      "training: 24 batch 67 batch_loss: 0.149025559425354\n",
      "training: 24 batch 68 batch_loss: 0.14952456951141357\n",
      "training: 24 batch 69 batch_loss: 0.15227878093719482\n",
      "training: 24 batch 70 batch_loss: 0.14977693557739258\n",
      "training: 24 batch 71 batch_loss: 0.14908957481384277\n",
      "training: 24 batch 72 batch_loss: 0.15019991993904114\n",
      "training: 24 batch 73 batch_loss: 0.15191495418548584\n",
      "training: 24 batch 74 batch_loss: 0.15106898546218872\n",
      "training: 24 batch 75 batch_loss: 0.1496749222278595\n",
      "training: 24 batch 76 batch_loss: 0.14803466200828552\n",
      "training: 24 batch 77 batch_loss: 0.1466934084892273\n",
      "training: 24 batch 78 batch_loss: 0.15144836902618408\n",
      "training: 24 batch 79 batch_loss: 0.1488240361213684\n",
      "training: 24 batch 80 batch_loss: 0.1470259428024292\n",
      "training: 24 batch 81 batch_loss: 0.1508057713508606\n",
      "training: 24 batch 82 batch_loss: 0.14589005708694458\n",
      "training: 24 batch 83 batch_loss: 0.1495499312877655\n",
      "training: 24 batch 84 batch_loss: 0.14943158626556396\n",
      "training: 24 batch 85 batch_loss: 0.1540856957435608\n",
      "training: 24 batch 86 batch_loss: 0.15083158016204834\n",
      "training: 24 batch 87 batch_loss: 0.1466846466064453\n",
      "training: 24 batch 88 batch_loss: 0.14824652671813965\n",
      "training: 24 batch 89 batch_loss: 0.14870184659957886\n",
      "training: 24 batch 90 batch_loss: 0.14527499675750732\n",
      "training: 24 batch 91 batch_loss: 0.14594417810440063\n",
      "training: 24 batch 92 batch_loss: 0.14901840686798096\n",
      "training: 24 batch 93 batch_loss: 0.14709609746932983\n",
      "training: 24 batch 94 batch_loss: 0.1493546962738037\n",
      "training: 24 batch 95 batch_loss: 0.1515830159187317\n",
      "training: 24 batch 96 batch_loss: 0.15067410469055176\n",
      "training: 24 batch 97 batch_loss: 0.1483967900276184\n",
      "training: 24 batch 98 batch_loss: 0.15058749914169312\n",
      "training: 24 batch 99 batch_loss: 0.14818885922431946\n",
      "training: 24 batch 100 batch_loss: 0.1437050700187683\n",
      "training: 24 batch 101 batch_loss: 0.14748135209083557\n",
      "training: 24 batch 102 batch_loss: 0.1464097499847412\n",
      "training: 24 batch 103 batch_loss: 0.14617466926574707\n",
      "training: 24 batch 104 batch_loss: 0.14821532368659973\n",
      "training: 24 batch 105 batch_loss: 0.14975136518478394\n",
      "training: 24 batch 106 batch_loss: 0.14812135696411133\n",
      "training: 24 batch 107 batch_loss: 0.14920032024383545\n",
      "training: 24 batch 108 batch_loss: 0.14895674586296082\n",
      "training: 24 batch 109 batch_loss: 0.14671379327774048\n",
      "training: 24 batch 110 batch_loss: 0.15012502670288086\n",
      "training: 24 batch 111 batch_loss: 0.14952883124351501\n",
      "training: 24 batch 112 batch_loss: 0.15120893716812134\n",
      "training: 24 batch 113 batch_loss: 0.1491161286830902\n",
      "training: 24 batch 114 batch_loss: 0.15136665105819702\n",
      "training: 24 batch 115 batch_loss: 0.15101736783981323\n",
      "training: 24 batch 116 batch_loss: 0.14909371733665466\n",
      "training: 24 batch 117 batch_loss: 0.14926233887672424\n",
      "training: 24 batch 118 batch_loss: 0.14948245882987976\n",
      "training: 24 batch 119 batch_loss: 0.14788398146629333\n",
      "training: 24 batch 120 batch_loss: 0.1499633491039276\n",
      "training: 24 batch 121 batch_loss: 0.14775192737579346\n",
      "training: 24 batch 122 batch_loss: 0.14826536178588867\n",
      "training: 24 batch 123 batch_loss: 0.1456322968006134\n",
      "training: 24 batch 124 batch_loss: 0.14776557683944702\n",
      "training: 24 batch 125 batch_loss: 0.14542782306671143\n",
      "training: 24 batch 126 batch_loss: 0.14775434136390686\n",
      "training: 24 batch 127 batch_loss: 0.15064740180969238\n",
      "training: 24 batch 128 batch_loss: 0.15083777904510498\n",
      "training: 24 batch 129 batch_loss: 0.14804700016975403\n",
      "training: 24 batch 130 batch_loss: 0.15278348326683044\n",
      "training: 24 batch 131 batch_loss: 0.14807164669036865\n",
      "training: 24 batch 132 batch_loss: 0.14748874306678772\n",
      "training: 24 batch 133 batch_loss: 0.146734356880188\n",
      "training: 24 batch 134 batch_loss: 0.14927250146865845\n",
      "training: 24 batch 135 batch_loss: 0.14979279041290283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 24 batch 136 batch_loss: 0.14701694250106812\n",
      "training: 24 batch 137 batch_loss: 0.14851069450378418\n",
      "training: 24 batch 138 batch_loss: 0.14932844042778015\n",
      "training: 24 batch 139 batch_loss: 0.15093079209327698\n",
      "training: 24 batch 140 batch_loss: 0.14737245440483093\n",
      "training: 24 batch 141 batch_loss: 0.15256032347679138\n",
      "training: 24 batch 142 batch_loss: 0.15070724487304688\n",
      "training: 24 batch 143 batch_loss: 0.15056359767913818\n",
      "training: 24 batch 144 batch_loss: 0.1523771584033966\n",
      "training: 24 batch 145 batch_loss: 0.14886364340782166\n",
      "training: 24 batch 146 batch_loss: 0.15159374475479126\n",
      "training: 24 batch 147 batch_loss: 0.14949148893356323\n",
      "training: 24 batch 148 batch_loss: 0.14607200026512146\n",
      "training: 24 batch 149 batch_loss: 0.15023624897003174\n",
      "training: 24 batch 150 batch_loss: 0.14845594763755798\n",
      "training: 24 batch 151 batch_loss: 0.14572888612747192\n",
      "training: 24 batch 152 batch_loss: 0.1480259895324707\n",
      "training: 24 batch 153 batch_loss: 0.15031594038009644\n",
      "training: 24 batch 154 batch_loss: 0.15055236220359802\n",
      "training: 24 batch 155 batch_loss: 0.14709267020225525\n",
      "training: 24 batch 156 batch_loss: 0.14929869771003723\n",
      "training: 24 batch 157 batch_loss: 0.15007245540618896\n",
      "training: 24 batch 158 batch_loss: 0.14962682127952576\n",
      "training: 24 batch 159 batch_loss: 0.15036645531654358\n",
      "training: 24 batch 160 batch_loss: 0.1477920413017273\n",
      "training: 24 batch 161 batch_loss: 0.15170419216156006\n",
      "training: 24 batch 162 batch_loss: 0.14982590079307556\n",
      "training: 24 batch 163 batch_loss: 0.14720579981803894\n",
      "training: 24 batch 164 batch_loss: 0.15132585167884827\n",
      "training: 24 batch 165 batch_loss: 0.15024802088737488\n",
      "training: 24 batch 166 batch_loss: 0.15019872784614563\n",
      "training: 24 batch 167 batch_loss: 0.1493350863456726\n",
      "training: 24 batch 168 batch_loss: 0.14848566055297852\n",
      "training: 24 batch 169 batch_loss: 0.14734619855880737\n",
      "training: 24 batch 170 batch_loss: 0.14924558997154236\n",
      "training: 24 batch 171 batch_loss: 0.1490178108215332\n",
      "training: 24 batch 172 batch_loss: 0.15232473611831665\n",
      "training: 24 batch 173 batch_loss: 0.14797508716583252\n",
      "training: 24 batch 174 batch_loss: 0.1513732373714447\n",
      "training: 24 batch 175 batch_loss: 0.15377560257911682\n",
      "training: 24 batch 176 batch_loss: 0.14967206120491028\n",
      "training: 24 batch 177 batch_loss: 0.146805077791214\n",
      "training: 24 batch 178 batch_loss: 0.1496374011039734\n",
      "training: 24 batch 179 batch_loss: 0.15006881952285767\n",
      "training: 24 batch 180 batch_loss: 0.14811724424362183\n",
      "training: 24 batch 181 batch_loss: 0.14698806405067444\n",
      "training: 24 batch 182 batch_loss: 0.1487012803554535\n",
      "training: 24 batch 183 batch_loss: 0.15241125226020813\n",
      "training: 24 batch 184 batch_loss: 0.14943641424179077\n",
      "training: 24 batch 185 batch_loss: 0.15107154846191406\n",
      "training: 24 batch 186 batch_loss: 0.14639747142791748\n",
      "training: 24 batch 187 batch_loss: 0.14722508192062378\n",
      "training: 24 batch 188 batch_loss: 0.1485120952129364\n",
      "training: 24 batch 189 batch_loss: 0.14785033464431763\n",
      "training: 24 batch 190 batch_loss: 0.14680376648902893\n",
      "training: 24 batch 191 batch_loss: 0.15055197477340698\n",
      "training: 24 batch 192 batch_loss: 0.14912700653076172\n",
      "training: 24 batch 193 batch_loss: 0.14704322814941406\n",
      "training: 24 batch 194 batch_loss: 0.14881742000579834\n",
      "training: 24 batch 195 batch_loss: 0.1475047767162323\n",
      "training: 24 batch 196 batch_loss: 0.1502411961555481\n",
      "training: 24 batch 197 batch_loss: 0.14764922857284546\n",
      "training: 24 batch 198 batch_loss: 0.15248537063598633\n",
      "training: 24 batch 199 batch_loss: 0.1531607210636139\n",
      "training: 24 batch 200 batch_loss: 0.14992523193359375\n",
      "training: 24 batch 201 batch_loss: 0.14652729034423828\n",
      "training: 24 batch 202 batch_loss: 0.14946302771568298\n",
      "training: 24 batch 203 batch_loss: 0.14887279272079468\n",
      "training: 24 batch 204 batch_loss: 0.15141335129737854\n",
      "training: 24 batch 205 batch_loss: 0.1489214301109314\n",
      "training: 24 batch 206 batch_loss: 0.1499738097190857\n",
      "training: 24 batch 207 batch_loss: 0.14937996864318848\n",
      "training: 24 batch 208 batch_loss: 0.15093863010406494\n",
      "training: 24 batch 209 batch_loss: 0.14979904890060425\n",
      "training: 24 batch 210 batch_loss: 0.14828172326087952\n",
      "training: 24 batch 211 batch_loss: 0.1490907073020935\n",
      "training: 24 batch 212 batch_loss: 0.1512254774570465\n",
      "training: 24 batch 213 batch_loss: 0.1505098044872284\n",
      "training: 24 batch 214 batch_loss: 0.15040946006774902\n",
      "training: 24 batch 215 batch_loss: 0.14949965476989746\n",
      "training: 24 batch 216 batch_loss: 0.15190750360488892\n",
      "training: 24 batch 217 batch_loss: 0.14829036593437195\n",
      "training: 24 batch 218 batch_loss: 0.1508411467075348\n",
      "training: 24 batch 219 batch_loss: 0.1453399956226349\n",
      "training: 24 batch 220 batch_loss: 0.14996135234832764\n",
      "training: 24 batch 221 batch_loss: 0.14955991506576538\n",
      "training: 24 batch 222 batch_loss: 0.14733755588531494\n",
      "training: 24 batch 223 batch_loss: 0.15076899528503418\n",
      "training: 24 batch 224 batch_loss: 0.14588284492492676\n",
      "training: 24 batch 225 batch_loss: 0.1475180685520172\n",
      "training: 24 batch 226 batch_loss: 0.1490522027015686\n",
      "training: 24 batch 227 batch_loss: 0.1520761251449585\n",
      "training: 24 batch 228 batch_loss: 0.15026915073394775\n",
      "training: 24 batch 229 batch_loss: 0.1499323844909668\n",
      "training: 24 batch 230 batch_loss: 0.15084055066108704\n",
      "training: 24 batch 231 batch_loss: 0.15198156237602234\n",
      "training: 24 batch 232 batch_loss: 0.15146467089653015\n",
      "training: 24 batch 233 batch_loss: 0.1475473940372467\n",
      "training: 24 batch 234 batch_loss: 0.15306633710861206\n",
      "training: 24 batch 235 batch_loss: 0.14970654249191284\n",
      "training: 24 batch 236 batch_loss: 0.14750510454177856\n",
      "training: 24 batch 237 batch_loss: 0.15092051029205322\n",
      "training: 24 batch 238 batch_loss: 0.1475261151790619\n",
      "training: 24 batch 239 batch_loss: 0.14834946393966675\n",
      "training: 24 batch 240 batch_loss: 0.15105193853378296\n",
      "training: 24 batch 241 batch_loss: 0.1533755660057068\n",
      "training: 24 batch 242 batch_loss: 0.14726632833480835\n",
      "training: 24 batch 243 batch_loss: 0.14951881766319275\n",
      "training: 24 batch 244 batch_loss: 0.15021660923957825\n",
      "training: 24 batch 245 batch_loss: 0.1486280858516693\n",
      "training: 24 batch 246 batch_loss: 0.15059256553649902\n",
      "training: 24 batch 247 batch_loss: 0.1461816430091858\n",
      "training: 24 batch 248 batch_loss: 0.14999255537986755\n",
      "training: 24 batch 249 batch_loss: 0.1495356261730194\n",
      "training: 24 batch 250 batch_loss: 0.14834576845169067\n",
      "training: 24 batch 251 batch_loss: 0.14873316884040833\n",
      "training: 24 batch 252 batch_loss: 0.15254801511764526\n",
      "training: 24 batch 253 batch_loss: 0.15215909481048584\n",
      "training: 24 batch 254 batch_loss: 0.14864301681518555\n",
      "training: 24 batch 255 batch_loss: 0.14915382862091064\n",
      "training: 24 batch 256 batch_loss: 0.15101659297943115\n",
      "training: 24 batch 257 batch_loss: 0.15022128820419312\n",
      "training: 24 batch 258 batch_loss: 0.14866498112678528\n",
      "training: 24 batch 259 batch_loss: 0.1499224603176117\n",
      "training: 24 batch 260 batch_loss: 0.1504025161266327\n",
      "training: 24 batch 261 batch_loss: 0.14843466877937317\n",
      "training: 24 batch 262 batch_loss: 0.14736345410346985\n",
      "training: 24 batch 263 batch_loss: 0.14927130937576294\n",
      "training: 24 batch 264 batch_loss: 0.15060782432556152\n",
      "training: 24 batch 265 batch_loss: 0.15125775337219238\n",
      "training: 24 batch 266 batch_loss: 0.15399247407913208\n",
      "training: 24 batch 267 batch_loss: 0.1504078209400177\n",
      "training: 24 batch 268 batch_loss: 0.14801311492919922\n",
      "training: 24 batch 269 batch_loss: 0.14937442541122437\n",
      "training: 24 batch 270 batch_loss: 0.1494034230709076\n",
      "training: 24 batch 271 batch_loss: 0.15068545937538147\n",
      "training: 24 batch 272 batch_loss: 0.14978426694869995\n",
      "training: 24 batch 273 batch_loss: 0.1486091911792755\n",
      "training: 24 batch 274 batch_loss: 0.1497419774532318\n",
      "training: 24 batch 275 batch_loss: 0.14700403809547424\n",
      "training: 24 batch 276 batch_loss: 0.14610326290130615\n",
      "training: 24 batch 277 batch_loss: 0.14681866765022278\n",
      "training: 24 batch 278 batch_loss: 0.15253829956054688\n",
      "training: 24 batch 279 batch_loss: 0.1486572027206421\n",
      "training: 24 batch 280 batch_loss: 0.15297210216522217\n",
      "training: 24 batch 281 batch_loss: 0.149441659450531\n",
      "training: 24 batch 282 batch_loss: 0.15227115154266357\n",
      "training: 24 batch 283 batch_loss: 0.14855697751045227\n",
      "training: 24 batch 284 batch_loss: 0.15085762739181519\n",
      "training: 24 batch 285 batch_loss: 0.1472494900226593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 24 batch 286 batch_loss: 0.1509767770767212\n",
      "training: 24 batch 287 batch_loss: 0.1499445140361786\n",
      "training: 24 batch 288 batch_loss: 0.14805728197097778\n",
      "training: 24 batch 289 batch_loss: 0.1514396071434021\n",
      "training: 24 batch 290 batch_loss: 0.14940550923347473\n",
      "training: 24 batch 291 batch_loss: 0.15157482028007507\n",
      "training: 24 batch 292 batch_loss: 0.1489022970199585\n",
      "training: 24 batch 293 batch_loss: 0.1491711139678955\n",
      "training: 24 batch 294 batch_loss: 0.15086975693702698\n",
      "training: 24 batch 295 batch_loss: 0.14915567636489868\n",
      "training: 24 batch 296 batch_loss: 0.14941048622131348\n",
      "training: 24 batch 297 batch_loss: 0.15115952491760254\n",
      "training: 24 batch 298 batch_loss: 0.14725381135940552\n",
      "training: 24 batch 299 batch_loss: 0.14884787797927856\n",
      "training: 24 batch 300 batch_loss: 0.15553927421569824\n",
      "training: 24 batch 301 batch_loss: 0.1453540027141571\n",
      "training: 24 batch 302 batch_loss: 0.15052145719528198\n",
      "training: 24 batch 303 batch_loss: 0.15141159296035767\n",
      "training: 24 batch 304 batch_loss: 0.15327751636505127\n",
      "training: 24 batch 305 batch_loss: 0.1520807147026062\n",
      "training: 24 batch 306 batch_loss: 0.14553067088127136\n",
      "training: 24 batch 307 batch_loss: 0.15014272928237915\n",
      "training: 24 batch 308 batch_loss: 0.15083104372024536\n",
      "training: 24 batch 309 batch_loss: 0.15046393871307373\n",
      "training: 24 batch 310 batch_loss: 0.1478375792503357\n",
      "training: 24 batch 311 batch_loss: 0.1494356095790863\n",
      "training: 24 batch 312 batch_loss: 0.15214583277702332\n",
      "training: 24 batch 313 batch_loss: 0.14960816502571106\n",
      "training: 24 batch 314 batch_loss: 0.14907166361808777\n",
      "training: 24 batch 315 batch_loss: 0.15015774965286255\n",
      "training: 24 batch 316 batch_loss: 0.1461799442768097\n",
      "training: 24 batch 317 batch_loss: 0.1479279100894928\n",
      "training: 24 batch 318 batch_loss: 0.15424305200576782\n",
      "training: 24 batch 319 batch_loss: 0.15052402019500732\n",
      "training: 24 batch 320 batch_loss: 0.15015792846679688\n",
      "training: 24 batch 321 batch_loss: 0.1515120267868042\n",
      "training: 24 batch 322 batch_loss: 0.14974460005760193\n",
      "training: 24 batch 323 batch_loss: 0.14792656898498535\n",
      "training: 24 batch 324 batch_loss: 0.15087836980819702\n",
      "training: 24 batch 325 batch_loss: 0.15167289972305298\n",
      "training: 24 batch 326 batch_loss: 0.15252745151519775\n",
      "training: 24 batch 327 batch_loss: 0.15292084217071533\n",
      "training: 24 batch 328 batch_loss: 0.15007168054580688\n",
      "training: 24 batch 329 batch_loss: 0.14672020077705383\n",
      "training: 24 batch 330 batch_loss: 0.15474167466163635\n",
      "training: 24 batch 331 batch_loss: 0.14974674582481384\n",
      "training: 24 batch 332 batch_loss: 0.15474987030029297\n",
      "training: 24 batch 333 batch_loss: 0.15372306108474731\n",
      "training: 24 batch 334 batch_loss: 0.14917513728141785\n",
      "training: 24 batch 335 batch_loss: 0.1518145501613617\n",
      "training: 24 batch 336 batch_loss: 0.1531171202659607\n",
      "training: 24 batch 337 batch_loss: 0.15125462412834167\n",
      "training: 24 batch 338 batch_loss: 0.14808011054992676\n",
      "training: 24 batch 339 batch_loss: 0.1481134295463562\n",
      "training: 24 batch 340 batch_loss: 0.15212073922157288\n",
      "training: 24 batch 341 batch_loss: 0.1501278579235077\n",
      "training: 24 batch 342 batch_loss: 0.147305428981781\n",
      "training: 24 batch 343 batch_loss: 0.14984554052352905\n",
      "training: 24 batch 344 batch_loss: 0.14938965439796448\n",
      "training: 24 batch 345 batch_loss: 0.15151730179786682\n",
      "training: 24 batch 346 batch_loss: 0.1478564739227295\n",
      "training: 24 batch 347 batch_loss: 0.15220016241073608\n",
      "training: 24 batch 348 batch_loss: 0.15241515636444092\n",
      "training: 24 batch 349 batch_loss: 0.1498701572418213\n",
      "training: 24 batch 350 batch_loss: 0.14971208572387695\n",
      "training: 24 batch 351 batch_loss: 0.15266382694244385\n",
      "training: 24 batch 352 batch_loss: 0.1525610387325287\n",
      "training: 24 batch 353 batch_loss: 0.14912813901901245\n",
      "training: 24 batch 354 batch_loss: 0.15151864290237427\n",
      "training: 24 batch 355 batch_loss: 0.14986222982406616\n",
      "training: 24 batch 356 batch_loss: 0.1508186161518097\n",
      "training: 24 batch 357 batch_loss: 0.1485683023929596\n",
      "training: 24 batch 358 batch_loss: 0.14905241131782532\n",
      "training: 24 batch 359 batch_loss: 0.15213286876678467\n",
      "training: 24 batch 360 batch_loss: 0.14837905764579773\n",
      "training: 24 batch 361 batch_loss: 0.14858847856521606\n",
      "training: 24 batch 362 batch_loss: 0.1502610743045807\n",
      "training: 24 batch 363 batch_loss: 0.14962822198867798\n",
      "training: 24 batch 364 batch_loss: 0.15178653597831726\n",
      "training: 24 batch 365 batch_loss: 0.1497199833393097\n",
      "training: 24 batch 366 batch_loss: 0.15176251530647278\n",
      "training: 24 batch 367 batch_loss: 0.14826080203056335\n",
      "training: 24 batch 368 batch_loss: 0.15019714832305908\n",
      "training: 24 batch 369 batch_loss: 0.1493503749370575\n",
      "training: 24 batch 370 batch_loss: 0.15091785788536072\n",
      "training: 24 batch 371 batch_loss: 0.15020981431007385\n",
      "training: 24 batch 372 batch_loss: 0.14841696619987488\n",
      "training: 24 batch 373 batch_loss: 0.14949217438697815\n",
      "training: 24 batch 374 batch_loss: 0.1498895287513733\n",
      "training: 24 batch 375 batch_loss: 0.1529642641544342\n",
      "training: 24 batch 376 batch_loss: 0.15011563897132874\n",
      "training: 24 batch 377 batch_loss: 0.14988726377487183\n",
      "training: 24 batch 378 batch_loss: 0.14793887734413147\n",
      "training: 24 batch 379 batch_loss: 0.15060549974441528\n",
      "training: 24 batch 380 batch_loss: 0.1493479311466217\n",
      "training: 24 batch 381 batch_loss: 0.15113985538482666\n",
      "training: 24 batch 382 batch_loss: 0.15063893795013428\n",
      "training: 24 batch 383 batch_loss: 0.14869827032089233\n",
      "training: 24 batch 384 batch_loss: 0.14681372046470642\n",
      "training: 24 batch 385 batch_loss: 0.14874717593193054\n",
      "training: 24 batch 386 batch_loss: 0.15250468254089355\n",
      "training: 24 batch 387 batch_loss: 0.15298008918762207\n",
      "training: 24 batch 388 batch_loss: 0.15181705355644226\n",
      "training: 24 batch 389 batch_loss: 0.1474810242652893\n",
      "training: 24 batch 390 batch_loss: 0.1496770679950714\n",
      "training: 24 batch 391 batch_loss: 0.15175151824951172\n",
      "training: 24 batch 392 batch_loss: 0.15068486332893372\n",
      "training: 24 batch 393 batch_loss: 0.15453532338142395\n",
      "training: 24 batch 394 batch_loss: 0.1505190134048462\n",
      "training: 24 batch 395 batch_loss: 0.14978572726249695\n",
      "training: 24 batch 396 batch_loss: 0.15118524432182312\n",
      "training: 24 batch 397 batch_loss: 0.15204933285713196\n",
      "training: 24 batch 398 batch_loss: 0.15016993880271912\n",
      "training: 24 batch 399 batch_loss: 0.14907124638557434\n",
      "training: 24 batch 400 batch_loss: 0.14983123540878296\n",
      "training: 24 batch 401 batch_loss: 0.15228652954101562\n",
      "training: 24 batch 402 batch_loss: 0.15265649557113647\n",
      "training: 24 batch 403 batch_loss: 0.15040323138237\n",
      "training: 24 batch 404 batch_loss: 0.14916682243347168\n",
      "training: 24 batch 405 batch_loss: 0.1533057689666748\n",
      "training: 24 batch 406 batch_loss: 0.1535288691520691\n",
      "training: 24 batch 407 batch_loss: 0.15203902125358582\n",
      "training: 24 batch 408 batch_loss: 0.15278443694114685\n",
      "training: 24 batch 409 batch_loss: 0.15171605348587036\n",
      "training: 24 batch 410 batch_loss: 0.1497412919998169\n",
      "training: 24 batch 411 batch_loss: 0.15411314368247986\n",
      "training: 24 batch 412 batch_loss: 0.14994341135025024\n",
      "training: 24 batch 413 batch_loss: 0.15112006664276123\n",
      "training: 24 batch 414 batch_loss: 0.1500009000301361\n",
      "training: 24 batch 415 batch_loss: 0.14818426966667175\n",
      "training: 24 batch 416 batch_loss: 0.14777612686157227\n",
      "training: 24 batch 417 batch_loss: 0.1537875235080719\n",
      "training: 24 batch 418 batch_loss: 0.14874252676963806\n",
      "training: 24 batch 419 batch_loss: 0.1515336036682129\n",
      "training: 24 batch 420 batch_loss: 0.15472066402435303\n",
      "training: 24 batch 421 batch_loss: 0.15288472175598145\n",
      "training: 24 batch 422 batch_loss: 0.1536494791507721\n",
      "training: 24 batch 423 batch_loss: 0.153084397315979\n",
      "training: 24 batch 424 batch_loss: 0.1510021686553955\n",
      "training: 24 batch 425 batch_loss: 0.15252840518951416\n",
      "training: 24 batch 426 batch_loss: 0.1533489227294922\n",
      "training: 24 batch 427 batch_loss: 0.1491822600364685\n",
      "training: 24 batch 428 batch_loss: 0.15276873111724854\n",
      "training: 24 batch 429 batch_loss: 0.15479719638824463\n",
      "training: 24 batch 430 batch_loss: 0.1520633101463318\n",
      "training: 24 batch 431 batch_loss: 0.14860188961029053\n",
      "training: 24 batch 432 batch_loss: 0.15101879835128784\n",
      "training: 24 batch 433 batch_loss: 0.15015828609466553\n",
      "training: 24 batch 434 batch_loss: 0.15115144848823547\n",
      "training: 24 batch 435 batch_loss: 0.1506907343864441\n",
      "training: 24 batch 436 batch_loss: 0.1505478024482727\n",
      "training: 24 batch 437 batch_loss: 0.1517416536808014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 24 batch 438 batch_loss: 0.15045255422592163\n",
      "training: 24 batch 439 batch_loss: 0.1502077281475067\n",
      "training: 24 batch 440 batch_loss: 0.14669013023376465\n",
      "training: 24 batch 441 batch_loss: 0.15208184719085693\n",
      "training: 24 batch 442 batch_loss: 0.15102508664131165\n",
      "training: 24 batch 443 batch_loss: 0.14917594194412231\n",
      "training: 24 batch 444 batch_loss: 0.14806905388832092\n",
      "training: 24 batch 445 batch_loss: 0.15289732813835144\n",
      "training: 24 batch 446 batch_loss: 0.1502341628074646\n",
      "training: 24 batch 447 batch_loss: 0.15501630306243896\n",
      "training: 24 batch 448 batch_loss: 0.15218761563301086\n",
      "training: 24 batch 449 batch_loss: 0.15153583884239197\n",
      "training: 24 batch 450 batch_loss: 0.149574875831604\n",
      "training: 24 batch 451 batch_loss: 0.1523253619670868\n",
      "training: 24 batch 452 batch_loss: 0.15264654159545898\n",
      "training: 24 batch 453 batch_loss: 0.1508541703224182\n",
      "training: 24 batch 454 batch_loss: 0.1527893841266632\n",
      "training: 24 batch 455 batch_loss: 0.14912211894989014\n",
      "training: 24 batch 456 batch_loss: 0.15167683362960815\n",
      "training: 24 batch 457 batch_loss: 0.1510804295539856\n",
      "training: 24 batch 458 batch_loss: 0.1531577706336975\n",
      "training: 24 batch 459 batch_loss: 0.15340062975883484\n",
      "training: 24 batch 460 batch_loss: 0.15153703093528748\n",
      "training: 24 batch 461 batch_loss: 0.15064474940299988\n",
      "training: 24 batch 462 batch_loss: 0.14920353889465332\n",
      "training: 24 batch 463 batch_loss: 0.1500602662563324\n",
      "training: 24 batch 464 batch_loss: 0.14611372351646423\n",
      "training: 24 batch 465 batch_loss: 0.15247148275375366\n",
      "training: 24 batch 466 batch_loss: 0.1514889895915985\n",
      "training: 24 batch 467 batch_loss: 0.15055856108665466\n",
      "training: 24 batch 468 batch_loss: 0.15136104822158813\n",
      "training: 24 batch 469 batch_loss: 0.15087160468101501\n",
      "training: 24 batch 470 batch_loss: 0.1509423851966858\n",
      "training: 24 batch 471 batch_loss: 0.15212643146514893\n",
      "training: 24 batch 472 batch_loss: 0.15354642271995544\n",
      "training: 24 batch 473 batch_loss: 0.1526033580303192\n",
      "training: 24 batch 474 batch_loss: 0.14919185638427734\n",
      "training: 24 batch 475 batch_loss: 0.1492432951927185\n",
      "training: 24 batch 476 batch_loss: 0.15184158086776733\n",
      "training: 24 batch 477 batch_loss: 0.1528637707233429\n",
      "training: 24 batch 478 batch_loss: 0.1525251269340515\n",
      "training: 24 batch 479 batch_loss: 0.1525089144706726\n",
      "training: 24 batch 480 batch_loss: 0.15330550074577332\n",
      "training: 24 batch 481 batch_loss: 0.15233772993087769\n",
      "training: 24 batch 482 batch_loss: 0.1531432867050171\n",
      "training: 24 batch 483 batch_loss: 0.14846855401992798\n",
      "training: 24 batch 484 batch_loss: 0.1550503671169281\n",
      "training: 24 batch 485 batch_loss: 0.15469112992286682\n",
      "training: 24 batch 486 batch_loss: 0.15130358934402466\n",
      "training: 24 batch 487 batch_loss: 0.15254265069961548\n",
      "training: 24 batch 488 batch_loss: 0.14787417650222778\n",
      "training: 24 batch 489 batch_loss: 0.15440040826797485\n",
      "training: 24 batch 490 batch_loss: 0.14803218841552734\n",
      "training: 24 batch 491 batch_loss: 0.15181446075439453\n",
      "training: 24 batch 492 batch_loss: 0.1535995602607727\n",
      "training: 24 batch 493 batch_loss: 0.1508048176765442\n",
      "training: 24 batch 494 batch_loss: 0.1503702998161316\n",
      "training: 24 batch 495 batch_loss: 0.15581637620925903\n",
      "training: 24 batch 496 batch_loss: 0.15232649445533752\n",
      "training: 24 batch 497 batch_loss: 0.1447162926197052\n",
      "training: 24 batch 498 batch_loss: 0.14979135990142822\n",
      "training: 24 batch 499 batch_loss: 0.15203410387039185\n",
      "training: 24 batch 500 batch_loss: 0.15466558933258057\n",
      "training: 24 batch 501 batch_loss: 0.15343618392944336\n",
      "training: 24 batch 502 batch_loss: 0.15410423278808594\n",
      "training: 24 batch 503 batch_loss: 0.1501651406288147\n",
      "training: 24 batch 504 batch_loss: 0.153527170419693\n",
      "training: 24 batch 505 batch_loss: 0.15158319473266602\n",
      "training: 24 batch 506 batch_loss: 0.14687028527259827\n",
      "training: 24 batch 507 batch_loss: 0.15197190642356873\n",
      "training: 24 batch 508 batch_loss: 0.151582270860672\n",
      "training: 24 batch 509 batch_loss: 0.1531592607498169\n",
      "training: 24 batch 510 batch_loss: 0.15627577900886536\n",
      "training: 24 batch 511 batch_loss: 0.1514338254928589\n",
      "training: 24 batch 512 batch_loss: 0.15551415085792542\n",
      "training: 24 batch 513 batch_loss: 0.15200650691986084\n",
      "training: 24 batch 514 batch_loss: 0.1502743661403656\n",
      "training: 24 batch 515 batch_loss: 0.1525627076625824\n",
      "training: 24 batch 516 batch_loss: 0.15522024035453796\n",
      "training: 24 batch 517 batch_loss: 0.1531311273574829\n",
      "training: 24 batch 518 batch_loss: 0.15177792310714722\n",
      "training: 24 batch 519 batch_loss: 0.15198880434036255\n",
      "training: 24 batch 520 batch_loss: 0.15089455246925354\n",
      "training: 24 batch 521 batch_loss: 0.15096205472946167\n",
      "training: 24 batch 522 batch_loss: 0.15220537781715393\n",
      "training: 24 batch 523 batch_loss: 0.1527942717075348\n",
      "training: 24 batch 524 batch_loss: 0.15236732363700867\n",
      "training: 24 batch 525 batch_loss: 0.1526309847831726\n",
      "training: 24 batch 526 batch_loss: 0.1496601402759552\n",
      "training: 24 batch 527 batch_loss: 0.15250086784362793\n",
      "training: 24 batch 528 batch_loss: 0.14976608753204346\n",
      "training: 24 batch 529 batch_loss: 0.1523590087890625\n",
      "training: 24 batch 530 batch_loss: 0.15502187609672546\n",
      "training: 24 batch 531 batch_loss: 0.15343990921974182\n",
      "training: 24 batch 532 batch_loss: 0.15173280239105225\n",
      "training: 24 batch 533 batch_loss: 0.15131446719169617\n",
      "training: 24 batch 534 batch_loss: 0.15181097388267517\n",
      "training: 24 batch 535 batch_loss: 0.15300744771957397\n",
      "training: 24 batch 536 batch_loss: 0.15308433771133423\n",
      "training: 24 batch 537 batch_loss: 0.1532920002937317\n",
      "training: 24 batch 538 batch_loss: 0.15140926837921143\n",
      "training: 24 batch 539 batch_loss: 0.1509920060634613\n",
      "training: 24 batch 540 batch_loss: 0.15166398882865906\n",
      "training: 24 batch 541 batch_loss: 0.15160414576530457\n",
      "training: 24 batch 542 batch_loss: 0.15171846747398376\n",
      "training: 24 batch 543 batch_loss: 0.15084907412528992\n",
      "training: 24 batch 544 batch_loss: 0.15156006813049316\n",
      "training: 24 batch 545 batch_loss: 0.15492287278175354\n",
      "training: 24 batch 546 batch_loss: 0.1514972448348999\n",
      "training: 24 batch 547 batch_loss: 0.14648598432540894\n",
      "training: 24 batch 548 batch_loss: 0.1526232361793518\n",
      "training: 24 batch 549 batch_loss: 0.1520053744316101\n",
      "training: 24 batch 550 batch_loss: 0.15163126587867737\n",
      "training: 24 batch 551 batch_loss: 0.15148109197616577\n",
      "training: 24 batch 552 batch_loss: 0.15177002549171448\n",
      "training: 24 batch 553 batch_loss: 0.15313398838043213\n",
      "training: 24 batch 554 batch_loss: 0.14964213967323303\n",
      "training: 24 batch 555 batch_loss: 0.1507677137851715\n",
      "training: 24 batch 556 batch_loss: 0.14990830421447754\n",
      "training: 24 batch 557 batch_loss: 0.1494089961051941\n",
      "training: 24 batch 558 batch_loss: 0.15073296427726746\n",
      "training: 24 batch 559 batch_loss: 0.15130078792572021\n",
      "training: 24 batch 560 batch_loss: 0.15139779448509216\n",
      "training: 24 batch 561 batch_loss: 0.1533346176147461\n",
      "training: 24 batch 562 batch_loss: 0.15693581104278564\n",
      "training: 24 batch 563 batch_loss: 0.15533789992332458\n",
      "training: 24 batch 564 batch_loss: 0.1535441279411316\n",
      "training: 24 batch 565 batch_loss: 0.15034154057502747\n",
      "training: 24 batch 566 batch_loss: 0.15488314628601074\n",
      "training: 24 batch 567 batch_loss: 0.1516420543193817\n",
      "training: 24 batch 568 batch_loss: 0.15439912676811218\n",
      "training: 24 batch 569 batch_loss: 0.15038710832595825\n",
      "training: 24 batch 570 batch_loss: 0.15346896648406982\n",
      "training: 24 batch 571 batch_loss: 0.15243110060691833\n",
      "training: 24 batch 572 batch_loss: 0.15069270133972168\n",
      "training: 24 batch 573 batch_loss: 0.14867636561393738\n",
      "training: 24 batch 574 batch_loss: 0.15294814109802246\n",
      "training: 24 batch 575 batch_loss: 0.14598017930984497\n",
      "training: 24 batch 576 batch_loss: 0.15214887261390686\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 24, Hit Ratio:0.02903132318208619 | Precision:0.04283397227956355 | Recall:0.058055009572000306 | NDCG:0.054485315882160955\n",
      "*Best Performance* \n",
      "Epoch: 24, Hit Ratio:0.02903132318208619 | Precision:0.04283397227956355 | Recall:0.058055009572000306 | MDCG:0.054485315882160955\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 25 batch 0 batch_loss: 0.14854836463928223\n",
      "training: 25 batch 1 batch_loss: 0.14751580357551575\n",
      "training: 25 batch 2 batch_loss: 0.1506187617778778\n",
      "training: 25 batch 3 batch_loss: 0.1534891426563263\n",
      "training: 25 batch 4 batch_loss: 0.14949548244476318\n",
      "training: 25 batch 5 batch_loss: 0.1495894491672516\n",
      "training: 25 batch 6 batch_loss: 0.14779433608055115\n",
      "training: 25 batch 7 batch_loss: 0.15111947059631348\n",
      "training: 25 batch 8 batch_loss: 0.1514693796634674\n",
      "training: 25 batch 9 batch_loss: 0.15049317479133606\n",
      "training: 25 batch 10 batch_loss: 0.15283441543579102\n",
      "training: 25 batch 11 batch_loss: 0.151822030544281\n",
      "training: 25 batch 12 batch_loss: 0.14911934733390808\n",
      "training: 25 batch 13 batch_loss: 0.15119454264640808\n",
      "training: 25 batch 14 batch_loss: 0.15059810876846313\n",
      "training: 25 batch 15 batch_loss: 0.152923583984375\n",
      "training: 25 batch 16 batch_loss: 0.15294554829597473\n",
      "training: 25 batch 17 batch_loss: 0.1456635296344757\n",
      "training: 25 batch 18 batch_loss: 0.14783301949501038\n",
      "training: 25 batch 19 batch_loss: 0.14494642615318298\n",
      "training: 25 batch 20 batch_loss: 0.15075311064720154\n",
      "training: 25 batch 21 batch_loss: 0.15016990900039673\n",
      "training: 25 batch 22 batch_loss: 0.15035822987556458\n",
      "training: 25 batch 23 batch_loss: 0.15051555633544922\n",
      "training: 25 batch 24 batch_loss: 0.15092229843139648\n",
      "training: 25 batch 25 batch_loss: 0.14901664853096008\n",
      "training: 25 batch 26 batch_loss: 0.14959999918937683\n",
      "training: 25 batch 27 batch_loss: 0.14983436465263367\n",
      "training: 25 batch 28 batch_loss: 0.14981576800346375\n",
      "training: 25 batch 29 batch_loss: 0.1521289348602295\n",
      "training: 25 batch 30 batch_loss: 0.1511749029159546\n",
      "training: 25 batch 31 batch_loss: 0.14909660816192627\n",
      "training: 25 batch 32 batch_loss: 0.15515223145484924\n",
      "training: 25 batch 33 batch_loss: 0.14857572317123413\n",
      "training: 25 batch 34 batch_loss: 0.14962512254714966\n",
      "training: 25 batch 35 batch_loss: 0.14836639165878296\n",
      "training: 25 batch 36 batch_loss: 0.15213677287101746\n",
      "training: 25 batch 37 batch_loss: 0.15265309810638428\n",
      "training: 25 batch 38 batch_loss: 0.15198692679405212\n",
      "training: 25 batch 39 batch_loss: 0.15248370170593262\n",
      "training: 25 batch 40 batch_loss: 0.1527605950832367\n",
      "training: 25 batch 41 batch_loss: 0.1459977924823761\n",
      "training: 25 batch 42 batch_loss: 0.1474493145942688\n",
      "training: 25 batch 43 batch_loss: 0.14983981847763062\n",
      "training: 25 batch 44 batch_loss: 0.14738407731056213\n",
      "training: 25 batch 45 batch_loss: 0.14948755502700806\n",
      "training: 25 batch 46 batch_loss: 0.15297773480415344\n",
      "training: 25 batch 47 batch_loss: 0.14983078837394714\n",
      "training: 25 batch 48 batch_loss: 0.15064963698387146\n",
      "training: 25 batch 49 batch_loss: 0.15202099084854126\n",
      "training: 25 batch 50 batch_loss: 0.15028637647628784\n",
      "training: 25 batch 51 batch_loss: 0.1495858132839203\n",
      "training: 25 batch 52 batch_loss: 0.1503055989742279\n",
      "training: 25 batch 53 batch_loss: 0.15510612726211548\n",
      "training: 25 batch 54 batch_loss: 0.14765486121177673\n",
      "training: 25 batch 55 batch_loss: 0.15130165219306946\n",
      "training: 25 batch 56 batch_loss: 0.14712387323379517\n",
      "training: 25 batch 57 batch_loss: 0.15594196319580078\n",
      "training: 25 batch 58 batch_loss: 0.1485210657119751\n",
      "training: 25 batch 59 batch_loss: 0.15409868955612183\n",
      "training: 25 batch 60 batch_loss: 0.14664077758789062\n",
      "training: 25 batch 61 batch_loss: 0.14780542254447937\n",
      "training: 25 batch 62 batch_loss: 0.15061041712760925\n",
      "training: 25 batch 63 batch_loss: 0.14876186847686768\n",
      "training: 25 batch 64 batch_loss: 0.15186524391174316\n",
      "training: 25 batch 65 batch_loss: 0.1499423086643219\n",
      "training: 25 batch 66 batch_loss: 0.15061494708061218\n",
      "training: 25 batch 67 batch_loss: 0.14863094687461853\n",
      "training: 25 batch 68 batch_loss: 0.1514146327972412\n",
      "training: 25 batch 69 batch_loss: 0.15123283863067627\n",
      "training: 25 batch 70 batch_loss: 0.14886978268623352\n",
      "training: 25 batch 71 batch_loss: 0.14562606811523438\n",
      "training: 25 batch 72 batch_loss: 0.15434762835502625\n",
      "training: 25 batch 73 batch_loss: 0.15112397074699402\n",
      "training: 25 batch 74 batch_loss: 0.15157771110534668\n",
      "training: 25 batch 75 batch_loss: 0.15255925059318542\n",
      "training: 25 batch 76 batch_loss: 0.15304535627365112\n",
      "training: 25 batch 77 batch_loss: 0.14872097969055176\n",
      "training: 25 batch 78 batch_loss: 0.14910560846328735\n",
      "training: 25 batch 79 batch_loss: 0.14880016446113586\n",
      "training: 25 batch 80 batch_loss: 0.1497439444065094\n",
      "training: 25 batch 81 batch_loss: 0.1497533619403839\n",
      "training: 25 batch 82 batch_loss: 0.14732950925827026\n",
      "training: 25 batch 83 batch_loss: 0.15126150846481323\n",
      "training: 25 batch 84 batch_loss: 0.15041032433509827\n",
      "training: 25 batch 85 batch_loss: 0.15046629309654236\n",
      "training: 25 batch 86 batch_loss: 0.14987313747406006\n",
      "training: 25 batch 87 batch_loss: 0.14873653650283813\n",
      "training: 25 batch 88 batch_loss: 0.1519050896167755\n",
      "training: 25 batch 89 batch_loss: 0.15272486209869385\n",
      "training: 25 batch 90 batch_loss: 0.14779746532440186\n",
      "training: 25 batch 91 batch_loss: 0.14935070276260376\n",
      "training: 25 batch 92 batch_loss: 0.15304940938949585\n",
      "training: 25 batch 93 batch_loss: 0.15338760614395142\n",
      "training: 25 batch 94 batch_loss: 0.15085619688034058\n",
      "training: 25 batch 95 batch_loss: 0.15213945508003235\n",
      "training: 25 batch 96 batch_loss: 0.15262514352798462\n",
      "training: 25 batch 97 batch_loss: 0.14674827456474304\n",
      "training: 25 batch 98 batch_loss: 0.14717739820480347\n",
      "training: 25 batch 99 batch_loss: 0.15371406078338623\n",
      "training: 25 batch 100 batch_loss: 0.14937272667884827\n",
      "training: 25 batch 101 batch_loss: 0.14922374486923218\n",
      "training: 25 batch 102 batch_loss: 0.15146932005882263\n",
      "training: 25 batch 103 batch_loss: 0.15433689951896667\n",
      "training: 25 batch 104 batch_loss: 0.15259888768196106\n",
      "training: 25 batch 105 batch_loss: 0.1508471965789795\n",
      "training: 25 batch 106 batch_loss: 0.1508823037147522\n",
      "training: 25 batch 107 batch_loss: 0.1517702341079712\n",
      "training: 25 batch 108 batch_loss: 0.14935967326164246\n",
      "training: 25 batch 109 batch_loss: 0.15400588512420654\n",
      "training: 25 batch 110 batch_loss: 0.15037775039672852\n",
      "training: 25 batch 111 batch_loss: 0.1502901315689087\n",
      "training: 25 batch 112 batch_loss: 0.1493290662765503\n",
      "training: 25 batch 113 batch_loss: 0.15274649858474731\n",
      "training: 25 batch 114 batch_loss: 0.1522488296031952\n",
      "training: 25 batch 115 batch_loss: 0.15018397569656372\n",
      "training: 25 batch 116 batch_loss: 0.14739322662353516\n",
      "training: 25 batch 117 batch_loss: 0.15024858713150024\n",
      "training: 25 batch 118 batch_loss: 0.15094348788261414\n",
      "training: 25 batch 119 batch_loss: 0.15210479497909546\n",
      "training: 25 batch 120 batch_loss: 0.15041303634643555\n",
      "training: 25 batch 121 batch_loss: 0.15113282203674316\n",
      "training: 25 batch 122 batch_loss: 0.14935064315795898\n",
      "training: 25 batch 123 batch_loss: 0.15010342001914978\n",
      "training: 25 batch 124 batch_loss: 0.15090829133987427\n",
      "training: 25 batch 125 batch_loss: 0.15029826760292053\n",
      "training: 25 batch 126 batch_loss: 0.15240859985351562\n",
      "training: 25 batch 127 batch_loss: 0.1490820050239563\n",
      "training: 25 batch 128 batch_loss: 0.15066510438919067\n",
      "training: 25 batch 129 batch_loss: 0.15266239643096924\n",
      "training: 25 batch 130 batch_loss: 0.1499800980091095\n",
      "training: 25 batch 131 batch_loss: 0.14672592282295227\n",
      "training: 25 batch 132 batch_loss: 0.1509186327457428\n",
      "training: 25 batch 133 batch_loss: 0.1510695517063141\n",
      "training: 25 batch 134 batch_loss: 0.15328967571258545\n",
      "training: 25 batch 135 batch_loss: 0.15234637260437012\n",
      "training: 25 batch 136 batch_loss: 0.15305376052856445\n",
      "training: 25 batch 137 batch_loss: 0.1490318477153778\n",
      "training: 25 batch 138 batch_loss: 0.15078872442245483\n",
      "training: 25 batch 139 batch_loss: 0.1490229070186615\n",
      "training: 25 batch 140 batch_loss: 0.15124237537384033\n",
      "training: 25 batch 141 batch_loss: 0.1517035961151123\n",
      "training: 25 batch 142 batch_loss: 0.15060573816299438\n",
      "training: 25 batch 143 batch_loss: 0.15174931287765503\n",
      "training: 25 batch 144 batch_loss: 0.14969688653945923\n",
      "training: 25 batch 145 batch_loss: 0.152615487575531\n",
      "training: 25 batch 146 batch_loss: 0.15111702680587769\n",
      "training: 25 batch 147 batch_loss: 0.15363872051239014\n",
      "training: 25 batch 148 batch_loss: 0.15081286430358887\n",
      "training: 25 batch 149 batch_loss: 0.15297368168830872\n",
      "training: 25 batch 150 batch_loss: 0.15067583322525024\n",
      "training: 25 batch 151 batch_loss: 0.15254738926887512\n",
      "training: 25 batch 152 batch_loss: 0.14974838495254517\n",
      "training: 25 batch 153 batch_loss: 0.15067240595817566\n",
      "training: 25 batch 154 batch_loss: 0.1491195559501648\n",
      "training: 25 batch 155 batch_loss: 0.15228024125099182\n",
      "training: 25 batch 156 batch_loss: 0.15334263443946838\n",
      "training: 25 batch 157 batch_loss: 0.15054571628570557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 25 batch 158 batch_loss: 0.14790943264961243\n",
      "training: 25 batch 159 batch_loss: 0.15174302458763123\n",
      "training: 25 batch 160 batch_loss: 0.1515231728553772\n",
      "training: 25 batch 161 batch_loss: 0.15164253115653992\n",
      "training: 25 batch 162 batch_loss: 0.1533021628856659\n",
      "training: 25 batch 163 batch_loss: 0.1495177149772644\n",
      "training: 25 batch 164 batch_loss: 0.15432095527648926\n",
      "training: 25 batch 165 batch_loss: 0.15496695041656494\n",
      "training: 25 batch 166 batch_loss: 0.1508164405822754\n",
      "training: 25 batch 167 batch_loss: 0.15255862474441528\n",
      "training: 25 batch 168 batch_loss: 0.15239334106445312\n",
      "training: 25 batch 169 batch_loss: 0.15065091848373413\n",
      "training: 25 batch 170 batch_loss: 0.15293410420417786\n",
      "training: 25 batch 171 batch_loss: 0.151890367269516\n",
      "training: 25 batch 172 batch_loss: 0.15296682715415955\n",
      "training: 25 batch 173 batch_loss: 0.15233275294303894\n",
      "training: 25 batch 174 batch_loss: 0.15069109201431274\n",
      "training: 25 batch 175 batch_loss: 0.15091419219970703\n",
      "training: 25 batch 176 batch_loss: 0.15138322114944458\n",
      "training: 25 batch 177 batch_loss: 0.14957422018051147\n",
      "training: 25 batch 178 batch_loss: 0.15380579233169556\n",
      "training: 25 batch 179 batch_loss: 0.1512293517589569\n",
      "training: 25 batch 180 batch_loss: 0.15072301030158997\n",
      "training: 25 batch 181 batch_loss: 0.15521126985549927\n",
      "training: 25 batch 182 batch_loss: 0.14959001541137695\n",
      "training: 25 batch 183 batch_loss: 0.15217715501785278\n",
      "training: 25 batch 184 batch_loss: 0.1537240743637085\n",
      "training: 25 batch 185 batch_loss: 0.153682678937912\n",
      "training: 25 batch 186 batch_loss: 0.14800262451171875\n",
      "training: 25 batch 187 batch_loss: 0.15217429399490356\n",
      "training: 25 batch 188 batch_loss: 0.15069037675857544\n",
      "training: 25 batch 189 batch_loss: 0.1553586721420288\n",
      "training: 25 batch 190 batch_loss: 0.1512308120727539\n",
      "training: 25 batch 191 batch_loss: 0.1511377990245819\n",
      "training: 25 batch 192 batch_loss: 0.14963138103485107\n",
      "training: 25 batch 193 batch_loss: 0.15381982922554016\n",
      "training: 25 batch 194 batch_loss: 0.15229716897010803\n",
      "training: 25 batch 195 batch_loss: 0.1517406404018402\n",
      "training: 25 batch 196 batch_loss: 0.15587067604064941\n",
      "training: 25 batch 197 batch_loss: 0.15181201696395874\n",
      "training: 25 batch 198 batch_loss: 0.1504732370376587\n",
      "training: 25 batch 199 batch_loss: 0.15198883414268494\n",
      "training: 25 batch 200 batch_loss: 0.14995074272155762\n",
      "training: 25 batch 201 batch_loss: 0.15643629431724548\n",
      "training: 25 batch 202 batch_loss: 0.14844459295272827\n",
      "training: 25 batch 203 batch_loss: 0.1508600413799286\n",
      "training: 25 batch 204 batch_loss: 0.14982008934020996\n",
      "training: 25 batch 205 batch_loss: 0.1562144160270691\n",
      "training: 25 batch 206 batch_loss: 0.15320301055908203\n",
      "training: 25 batch 207 batch_loss: 0.14997583627700806\n",
      "training: 25 batch 208 batch_loss: 0.1512700915336609\n",
      "training: 25 batch 209 batch_loss: 0.14853480458259583\n",
      "training: 25 batch 210 batch_loss: 0.15000790357589722\n",
      "training: 25 batch 211 batch_loss: 0.1493643820285797\n",
      "training: 25 batch 212 batch_loss: 0.1511336863040924\n",
      "training: 25 batch 213 batch_loss: 0.15344125032424927\n",
      "training: 25 batch 214 batch_loss: 0.15308967232704163\n",
      "training: 25 batch 215 batch_loss: 0.15034037828445435\n",
      "training: 25 batch 216 batch_loss: 0.14836153388023376\n",
      "training: 25 batch 217 batch_loss: 0.15425840020179749\n",
      "training: 25 batch 218 batch_loss: 0.1525779366493225\n",
      "training: 25 batch 219 batch_loss: 0.15171846747398376\n",
      "training: 25 batch 220 batch_loss: 0.15465480089187622\n",
      "training: 25 batch 221 batch_loss: 0.15096256136894226\n",
      "training: 25 batch 222 batch_loss: 0.1513112187385559\n",
      "training: 25 batch 223 batch_loss: 0.15181514620780945\n",
      "training: 25 batch 224 batch_loss: 0.1546296775341034\n",
      "training: 25 batch 225 batch_loss: 0.15204790234565735\n",
      "training: 25 batch 226 batch_loss: 0.15430882573127747\n",
      "training: 25 batch 227 batch_loss: 0.15099072456359863\n",
      "training: 25 batch 228 batch_loss: 0.1508093774318695\n",
      "training: 25 batch 229 batch_loss: 0.1551796793937683\n",
      "training: 25 batch 230 batch_loss: 0.15323087573051453\n",
      "training: 25 batch 231 batch_loss: 0.15346038341522217\n",
      "training: 25 batch 232 batch_loss: 0.15329498052597046\n",
      "training: 25 batch 233 batch_loss: 0.15127646923065186\n",
      "training: 25 batch 234 batch_loss: 0.15357327461242676\n",
      "training: 25 batch 235 batch_loss: 0.15094438195228577\n",
      "training: 25 batch 236 batch_loss: 0.15313664078712463\n",
      "training: 25 batch 237 batch_loss: 0.15035700798034668\n",
      "training: 25 batch 238 batch_loss: 0.15170907974243164\n",
      "training: 25 batch 239 batch_loss: 0.1508566439151764\n",
      "training: 25 batch 240 batch_loss: 0.15439373254776\n",
      "training: 25 batch 241 batch_loss: 0.15280544757843018\n",
      "training: 25 batch 242 batch_loss: 0.15248414874076843\n",
      "training: 25 batch 243 batch_loss: 0.154041588306427\n",
      "training: 25 batch 244 batch_loss: 0.15270563960075378\n",
      "training: 25 batch 245 batch_loss: 0.15142154693603516\n",
      "training: 25 batch 246 batch_loss: 0.1478208601474762\n",
      "training: 25 batch 247 batch_loss: 0.15322592854499817\n",
      "training: 25 batch 248 batch_loss: 0.14947271347045898\n",
      "training: 25 batch 249 batch_loss: 0.15144401788711548\n",
      "training: 25 batch 250 batch_loss: 0.15089288353919983\n",
      "training: 25 batch 251 batch_loss: 0.15134382247924805\n",
      "training: 25 batch 252 batch_loss: 0.15169435739517212\n",
      "training: 25 batch 253 batch_loss: 0.15430331230163574\n",
      "training: 25 batch 254 batch_loss: 0.1516541838645935\n",
      "training: 25 batch 255 batch_loss: 0.15161633491516113\n",
      "training: 25 batch 256 batch_loss: 0.14794695377349854\n",
      "training: 25 batch 257 batch_loss: 0.14992836117744446\n",
      "training: 25 batch 258 batch_loss: 0.1523231863975525\n",
      "training: 25 batch 259 batch_loss: 0.15106508135795593\n",
      "training: 25 batch 260 batch_loss: 0.1536928117275238\n",
      "training: 25 batch 261 batch_loss: 0.15045487880706787\n",
      "training: 25 batch 262 batch_loss: 0.15032628178596497\n",
      "training: 25 batch 263 batch_loss: 0.1544177532196045\n",
      "training: 25 batch 264 batch_loss: 0.1508539319038391\n",
      "training: 25 batch 265 batch_loss: 0.15259143710136414\n",
      "training: 25 batch 266 batch_loss: 0.15113651752471924\n",
      "training: 25 batch 267 batch_loss: 0.14961636066436768\n",
      "training: 25 batch 268 batch_loss: 0.15080496668815613\n",
      "training: 25 batch 269 batch_loss: 0.15275666117668152\n",
      "training: 25 batch 270 batch_loss: 0.15070360898971558\n",
      "training: 25 batch 271 batch_loss: 0.15314653515815735\n",
      "training: 25 batch 272 batch_loss: 0.15109366178512573\n",
      "training: 25 batch 273 batch_loss: 0.15024656057357788\n",
      "training: 25 batch 274 batch_loss: 0.15446245670318604\n",
      "training: 25 batch 275 batch_loss: 0.15171462297439575\n",
      "training: 25 batch 276 batch_loss: 0.15373820066452026\n",
      "training: 25 batch 277 batch_loss: 0.15131470561027527\n",
      "training: 25 batch 278 batch_loss: 0.15422236919403076\n",
      "training: 25 batch 279 batch_loss: 0.15258699655532837\n",
      "training: 25 batch 280 batch_loss: 0.15083730220794678\n",
      "training: 25 batch 281 batch_loss: 0.1503836214542389\n",
      "training: 25 batch 282 batch_loss: 0.15361502766609192\n",
      "training: 25 batch 283 batch_loss: 0.15499266982078552\n",
      "training: 25 batch 284 batch_loss: 0.14883041381835938\n",
      "training: 25 batch 285 batch_loss: 0.15315347909927368\n",
      "training: 25 batch 286 batch_loss: 0.1553713083267212\n",
      "training: 25 batch 287 batch_loss: 0.1535455882549286\n",
      "training: 25 batch 288 batch_loss: 0.15052765607833862\n",
      "training: 25 batch 289 batch_loss: 0.15364402532577515\n",
      "training: 25 batch 290 batch_loss: 0.14735594391822815\n",
      "training: 25 batch 291 batch_loss: 0.15399742126464844\n",
      "training: 25 batch 292 batch_loss: 0.15437430143356323\n",
      "training: 25 batch 293 batch_loss: 0.15481162071228027\n",
      "training: 25 batch 294 batch_loss: 0.15196746587753296\n",
      "training: 25 batch 295 batch_loss: 0.15022534132003784\n",
      "training: 25 batch 296 batch_loss: 0.1526246964931488\n",
      "training: 25 batch 297 batch_loss: 0.1537863314151764\n",
      "training: 25 batch 298 batch_loss: 0.14821511507034302\n",
      "training: 25 batch 299 batch_loss: 0.15465334057807922\n",
      "training: 25 batch 300 batch_loss: 0.15154165029525757\n",
      "training: 25 batch 301 batch_loss: 0.15537157654762268\n",
      "training: 25 batch 302 batch_loss: 0.15397369861602783\n",
      "training: 25 batch 303 batch_loss: 0.15509188175201416\n",
      "training: 25 batch 304 batch_loss: 0.1524641215801239\n",
      "training: 25 batch 305 batch_loss: 0.1531139612197876\n",
      "training: 25 batch 306 batch_loss: 0.15448930859565735\n",
      "training: 25 batch 307 batch_loss: 0.1466485857963562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 25 batch 308 batch_loss: 0.1537591814994812\n",
      "training: 25 batch 309 batch_loss: 0.15477335453033447\n",
      "training: 25 batch 310 batch_loss: 0.15099459886550903\n",
      "training: 25 batch 311 batch_loss: 0.15247684717178345\n",
      "training: 25 batch 312 batch_loss: 0.15395134687423706\n",
      "training: 25 batch 313 batch_loss: 0.15618005394935608\n",
      "training: 25 batch 314 batch_loss: 0.15009096264839172\n",
      "training: 25 batch 315 batch_loss: 0.15003427863121033\n",
      "training: 25 batch 316 batch_loss: 0.15274694561958313\n",
      "training: 25 batch 317 batch_loss: 0.1550816297531128\n",
      "training: 25 batch 318 batch_loss: 0.1515730917453766\n",
      "training: 25 batch 319 batch_loss: 0.151960551738739\n",
      "training: 25 batch 320 batch_loss: 0.15526390075683594\n",
      "training: 25 batch 321 batch_loss: 0.15256226062774658\n",
      "training: 25 batch 322 batch_loss: 0.15020525455474854\n",
      "training: 25 batch 323 batch_loss: 0.14805379509925842\n",
      "training: 25 batch 324 batch_loss: 0.15156015753746033\n",
      "training: 25 batch 325 batch_loss: 0.15611320734024048\n",
      "training: 25 batch 326 batch_loss: 0.15242034196853638\n",
      "training: 25 batch 327 batch_loss: 0.15077045559883118\n",
      "training: 25 batch 328 batch_loss: 0.15291637182235718\n",
      "training: 25 batch 329 batch_loss: 0.15313148498535156\n",
      "training: 25 batch 330 batch_loss: 0.15440821647644043\n",
      "training: 25 batch 331 batch_loss: 0.15083777904510498\n",
      "training: 25 batch 332 batch_loss: 0.15292289853096008\n",
      "training: 25 batch 333 batch_loss: 0.1546413004398346\n",
      "training: 25 batch 334 batch_loss: 0.15399685502052307\n",
      "training: 25 batch 335 batch_loss: 0.1508127748966217\n",
      "training: 25 batch 336 batch_loss: 0.1531781554222107\n",
      "training: 25 batch 337 batch_loss: 0.1502314805984497\n",
      "training: 25 batch 338 batch_loss: 0.1527481973171234\n",
      "training: 25 batch 339 batch_loss: 0.1532338559627533\n",
      "training: 25 batch 340 batch_loss: 0.15271255373954773\n",
      "training: 25 batch 341 batch_loss: 0.1506001055240631\n",
      "training: 25 batch 342 batch_loss: 0.15106201171875\n",
      "training: 25 batch 343 batch_loss: 0.1494484841823578\n",
      "training: 25 batch 344 batch_loss: 0.1546913981437683\n",
      "training: 25 batch 345 batch_loss: 0.15606072545051575\n",
      "training: 25 batch 346 batch_loss: 0.15207886695861816\n",
      "training: 25 batch 347 batch_loss: 0.15406063199043274\n",
      "training: 25 batch 348 batch_loss: 0.15246880054473877\n",
      "training: 25 batch 349 batch_loss: 0.15472644567489624\n",
      "training: 25 batch 350 batch_loss: 0.15133005380630493\n",
      "training: 25 batch 351 batch_loss: 0.1531505584716797\n",
      "training: 25 batch 352 batch_loss: 0.15625843405723572\n",
      "training: 25 batch 353 batch_loss: 0.14827892184257507\n",
      "training: 25 batch 354 batch_loss: 0.15225663781166077\n",
      "training: 25 batch 355 batch_loss: 0.15304961800575256\n",
      "training: 25 batch 356 batch_loss: 0.15178853273391724\n",
      "training: 25 batch 357 batch_loss: 0.15319904685020447\n",
      "training: 25 batch 358 batch_loss: 0.15378707647323608\n",
      "training: 25 batch 359 batch_loss: 0.1545950174331665\n",
      "training: 25 batch 360 batch_loss: 0.15304481983184814\n",
      "training: 25 batch 361 batch_loss: 0.15132302045822144\n",
      "training: 25 batch 362 batch_loss: 0.15448087453842163\n",
      "training: 25 batch 363 batch_loss: 0.15379118919372559\n",
      "training: 25 batch 364 batch_loss: 0.15412670373916626\n",
      "training: 25 batch 365 batch_loss: 0.15556946396827698\n",
      "training: 25 batch 366 batch_loss: 0.15360134840011597\n",
      "training: 25 batch 367 batch_loss: 0.15448659658432007\n",
      "training: 25 batch 368 batch_loss: 0.15412622690200806\n",
      "training: 25 batch 369 batch_loss: 0.1508347988128662\n",
      "training: 25 batch 370 batch_loss: 0.1563086211681366\n",
      "training: 25 batch 371 batch_loss: 0.15186989307403564\n",
      "training: 25 batch 372 batch_loss: 0.15270647406578064\n",
      "training: 25 batch 373 batch_loss: 0.1495530605316162\n",
      "training: 25 batch 374 batch_loss: 0.14932739734649658\n",
      "training: 25 batch 375 batch_loss: 0.15257373452186584\n",
      "training: 25 batch 376 batch_loss: 0.1506640911102295\n",
      "training: 25 batch 377 batch_loss: 0.15498942136764526\n",
      "training: 25 batch 378 batch_loss: 0.1553977131843567\n",
      "training: 25 batch 379 batch_loss: 0.14994090795516968\n",
      "training: 25 batch 380 batch_loss: 0.15162882208824158\n",
      "training: 25 batch 381 batch_loss: 0.15205490589141846\n",
      "training: 25 batch 382 batch_loss: 0.15532532334327698\n",
      "training: 25 batch 383 batch_loss: 0.15542271733283997\n",
      "training: 25 batch 384 batch_loss: 0.15338143706321716\n",
      "training: 25 batch 385 batch_loss: 0.15441212058067322\n",
      "training: 25 batch 386 batch_loss: 0.1542469561100006\n",
      "training: 25 batch 387 batch_loss: 0.15147611498832703\n",
      "training: 25 batch 388 batch_loss: 0.15422624349594116\n",
      "training: 25 batch 389 batch_loss: 0.15083038806915283\n",
      "training: 25 batch 390 batch_loss: 0.15350231528282166\n",
      "training: 25 batch 391 batch_loss: 0.15295559167861938\n",
      "training: 25 batch 392 batch_loss: 0.1540859043598175\n",
      "training: 25 batch 393 batch_loss: 0.15224197506904602\n",
      "training: 25 batch 394 batch_loss: 0.1508752405643463\n",
      "training: 25 batch 395 batch_loss: 0.15579098463058472\n",
      "training: 25 batch 396 batch_loss: 0.15347343683242798\n",
      "training: 25 batch 397 batch_loss: 0.1554465889930725\n",
      "training: 25 batch 398 batch_loss: 0.1537814736366272\n",
      "training: 25 batch 399 batch_loss: 0.1517789661884308\n",
      "training: 25 batch 400 batch_loss: 0.15289807319641113\n",
      "training: 25 batch 401 batch_loss: 0.15211844444274902\n",
      "training: 25 batch 402 batch_loss: 0.1532415747642517\n",
      "training: 25 batch 403 batch_loss: 0.15363311767578125\n",
      "training: 25 batch 404 batch_loss: 0.15077859163284302\n",
      "training: 25 batch 405 batch_loss: 0.15336135029792786\n",
      "training: 25 batch 406 batch_loss: 0.15349453687667847\n",
      "training: 25 batch 407 batch_loss: 0.154690682888031\n",
      "training: 25 batch 408 batch_loss: 0.15263205766677856\n",
      "training: 25 batch 409 batch_loss: 0.15375229716300964\n",
      "training: 25 batch 410 batch_loss: 0.15066015720367432\n",
      "training: 25 batch 411 batch_loss: 0.15430408716201782\n",
      "training: 25 batch 412 batch_loss: 0.1561381220817566\n",
      "training: 25 batch 413 batch_loss: 0.15284982323646545\n",
      "training: 25 batch 414 batch_loss: 0.15511205792427063\n",
      "training: 25 batch 415 batch_loss: 0.15589767694473267\n",
      "training: 25 batch 416 batch_loss: 0.15416169166564941\n",
      "training: 25 batch 417 batch_loss: 0.15009230375289917\n",
      "training: 25 batch 418 batch_loss: 0.1504899263381958\n",
      "training: 25 batch 419 batch_loss: 0.15327763557434082\n",
      "training: 25 batch 420 batch_loss: 0.15107151865959167\n",
      "training: 25 batch 421 batch_loss: 0.15262466669082642\n",
      "training: 25 batch 422 batch_loss: 0.15304023027420044\n",
      "training: 25 batch 423 batch_loss: 0.15230074524879456\n",
      "training: 25 batch 424 batch_loss: 0.15231502056121826\n",
      "training: 25 batch 425 batch_loss: 0.1537657380104065\n",
      "training: 25 batch 426 batch_loss: 0.15493103861808777\n",
      "training: 25 batch 427 batch_loss: 0.1527864933013916\n",
      "training: 25 batch 428 batch_loss: 0.1560973823070526\n",
      "training: 25 batch 429 batch_loss: 0.14903786778450012\n",
      "training: 25 batch 430 batch_loss: 0.15474388003349304\n",
      "training: 25 batch 431 batch_loss: 0.1544044017791748\n",
      "training: 25 batch 432 batch_loss: 0.15179353952407837\n",
      "training: 25 batch 433 batch_loss: 0.15348821878433228\n",
      "training: 25 batch 434 batch_loss: 0.15399980545043945\n",
      "training: 25 batch 435 batch_loss: 0.15440213680267334\n",
      "training: 25 batch 436 batch_loss: 0.15114569664001465\n",
      "training: 25 batch 437 batch_loss: 0.1545170545578003\n",
      "training: 25 batch 438 batch_loss: 0.15169942378997803\n",
      "training: 25 batch 439 batch_loss: 0.15254932641983032\n",
      "training: 25 batch 440 batch_loss: 0.1544276475906372\n",
      "training: 25 batch 441 batch_loss: 0.15473732352256775\n",
      "training: 25 batch 442 batch_loss: 0.1523764729499817\n",
      "training: 25 batch 443 batch_loss: 0.1534755825996399\n",
      "training: 25 batch 444 batch_loss: 0.15480464696884155\n",
      "training: 25 batch 445 batch_loss: 0.15236982703208923\n",
      "training: 25 batch 446 batch_loss: 0.15373548865318298\n",
      "training: 25 batch 447 batch_loss: 0.15156316757202148\n",
      "training: 25 batch 448 batch_loss: 0.15133118629455566\n",
      "training: 25 batch 449 batch_loss: 0.15417125821113586\n",
      "training: 25 batch 450 batch_loss: 0.15550029277801514\n",
      "training: 25 batch 451 batch_loss: 0.15303939580917358\n",
      "training: 25 batch 452 batch_loss: 0.14941826462745667\n",
      "training: 25 batch 453 batch_loss: 0.15122681856155396\n",
      "training: 25 batch 454 batch_loss: 0.1536146104335785\n",
      "training: 25 batch 455 batch_loss: 0.15349316596984863\n",
      "training: 25 batch 456 batch_loss: 0.1543709635734558\n",
      "training: 25 batch 457 batch_loss: 0.15511834621429443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 25 batch 458 batch_loss: 0.1532570719718933\n",
      "training: 25 batch 459 batch_loss: 0.15391385555267334\n",
      "training: 25 batch 460 batch_loss: 0.15628385543823242\n",
      "training: 25 batch 461 batch_loss: 0.15084823966026306\n",
      "training: 25 batch 462 batch_loss: 0.1504838764667511\n",
      "training: 25 batch 463 batch_loss: 0.15066787600517273\n",
      "training: 25 batch 464 batch_loss: 0.15222826600074768\n",
      "training: 25 batch 465 batch_loss: 0.15570440888404846\n",
      "training: 25 batch 466 batch_loss: 0.15227383375167847\n",
      "training: 25 batch 467 batch_loss: 0.15172365307807922\n",
      "training: 25 batch 468 batch_loss: 0.15661093592643738\n",
      "training: 25 batch 469 batch_loss: 0.15332791209220886\n",
      "training: 25 batch 470 batch_loss: 0.1518014669418335\n",
      "training: 25 batch 471 batch_loss: 0.15415263175964355\n",
      "training: 25 batch 472 batch_loss: 0.15571552515029907\n",
      "training: 25 batch 473 batch_loss: 0.15187254548072815\n",
      "training: 25 batch 474 batch_loss: 0.15373042225837708\n",
      "training: 25 batch 475 batch_loss: 0.14959755539894104\n",
      "training: 25 batch 476 batch_loss: 0.15536269545555115\n",
      "training: 25 batch 477 batch_loss: 0.15457966923713684\n",
      "training: 25 batch 478 batch_loss: 0.14911547303199768\n",
      "training: 25 batch 479 batch_loss: 0.157627671957016\n",
      "training: 25 batch 480 batch_loss: 0.15450519323349\n",
      "training: 25 batch 481 batch_loss: 0.1526557207107544\n",
      "training: 25 batch 482 batch_loss: 0.15206611156463623\n",
      "training: 25 batch 483 batch_loss: 0.15502241253852844\n",
      "training: 25 batch 484 batch_loss: 0.1542452573776245\n",
      "training: 25 batch 485 batch_loss: 0.1554042100906372\n",
      "training: 25 batch 486 batch_loss: 0.15295052528381348\n",
      "training: 25 batch 487 batch_loss: 0.15375128388404846\n",
      "training: 25 batch 488 batch_loss: 0.15672168135643005\n",
      "training: 25 batch 489 batch_loss: 0.1552736759185791\n",
      "training: 25 batch 490 batch_loss: 0.1530022919178009\n",
      "training: 25 batch 491 batch_loss: 0.1546039879322052\n",
      "training: 25 batch 492 batch_loss: 0.15371063351631165\n",
      "training: 25 batch 493 batch_loss: 0.15662789344787598\n",
      "training: 25 batch 494 batch_loss: 0.1590983271598816\n",
      "training: 25 batch 495 batch_loss: 0.1528935432434082\n",
      "training: 25 batch 496 batch_loss: 0.15479227900505066\n",
      "training: 25 batch 497 batch_loss: 0.15425127744674683\n",
      "training: 25 batch 498 batch_loss: 0.1532391905784607\n",
      "training: 25 batch 499 batch_loss: 0.15183961391448975\n",
      "training: 25 batch 500 batch_loss: 0.15597406029701233\n",
      "training: 25 batch 501 batch_loss: 0.15489333868026733\n",
      "training: 25 batch 502 batch_loss: 0.15537238121032715\n",
      "training: 25 batch 503 batch_loss: 0.1553587019443512\n",
      "training: 25 batch 504 batch_loss: 0.1483229100704193\n",
      "training: 25 batch 505 batch_loss: 0.15334543585777283\n",
      "training: 25 batch 506 batch_loss: 0.15457883477210999\n",
      "training: 25 batch 507 batch_loss: 0.15365004539489746\n",
      "training: 25 batch 508 batch_loss: 0.1545047163963318\n",
      "training: 25 batch 509 batch_loss: 0.14952605962753296\n",
      "training: 25 batch 510 batch_loss: 0.15407690405845642\n",
      "training: 25 batch 511 batch_loss: 0.15240618586540222\n",
      "training: 25 batch 512 batch_loss: 0.15181288123130798\n",
      "training: 25 batch 513 batch_loss: 0.15350186824798584\n",
      "training: 25 batch 514 batch_loss: 0.15653163194656372\n",
      "training: 25 batch 515 batch_loss: 0.1534530222415924\n",
      "training: 25 batch 516 batch_loss: 0.1531534194946289\n",
      "training: 25 batch 517 batch_loss: 0.15032175183296204\n",
      "training: 25 batch 518 batch_loss: 0.15035992860794067\n",
      "training: 25 batch 519 batch_loss: 0.15653261542320251\n",
      "training: 25 batch 520 batch_loss: 0.15290617942810059\n",
      "training: 25 batch 521 batch_loss: 0.15615803003311157\n",
      "training: 25 batch 522 batch_loss: 0.1530008614063263\n",
      "training: 25 batch 523 batch_loss: 0.15523603558540344\n",
      "training: 25 batch 524 batch_loss: 0.15405845642089844\n",
      "training: 25 batch 525 batch_loss: 0.15351247787475586\n",
      "training: 25 batch 526 batch_loss: 0.15075889229774475\n",
      "training: 25 batch 527 batch_loss: 0.1563238799571991\n",
      "training: 25 batch 528 batch_loss: 0.1537061631679535\n",
      "training: 25 batch 529 batch_loss: 0.15421581268310547\n",
      "training: 25 batch 530 batch_loss: 0.15281662344932556\n",
      "training: 25 batch 531 batch_loss: 0.1540590524673462\n",
      "training: 25 batch 532 batch_loss: 0.15232563018798828\n",
      "training: 25 batch 533 batch_loss: 0.15294620394706726\n",
      "training: 25 batch 534 batch_loss: 0.15255984663963318\n",
      "training: 25 batch 535 batch_loss: 0.15460991859436035\n",
      "training: 25 batch 536 batch_loss: 0.15386301279067993\n",
      "training: 25 batch 537 batch_loss: 0.15230008959770203\n",
      "training: 25 batch 538 batch_loss: 0.15465444326400757\n",
      "training: 25 batch 539 batch_loss: 0.15325337648391724\n",
      "training: 25 batch 540 batch_loss: 0.15515395998954773\n",
      "training: 25 batch 541 batch_loss: 0.15488886833190918\n",
      "training: 25 batch 542 batch_loss: 0.15496450662612915\n",
      "training: 25 batch 543 batch_loss: 0.15139621496200562\n",
      "training: 25 batch 544 batch_loss: 0.15276777744293213\n",
      "training: 25 batch 545 batch_loss: 0.15718841552734375\n",
      "training: 25 batch 546 batch_loss: 0.15539157390594482\n",
      "training: 25 batch 547 batch_loss: 0.15502533316612244\n",
      "training: 25 batch 548 batch_loss: 0.15680712461471558\n",
      "training: 25 batch 549 batch_loss: 0.15406590700149536\n",
      "training: 25 batch 550 batch_loss: 0.15087789297103882\n",
      "training: 25 batch 551 batch_loss: 0.15269118547439575\n",
      "training: 25 batch 552 batch_loss: 0.1538725197315216\n",
      "training: 25 batch 553 batch_loss: 0.15319517254829407\n",
      "training: 25 batch 554 batch_loss: 0.15557458996772766\n",
      "training: 25 batch 555 batch_loss: 0.15356871485710144\n",
      "training: 25 batch 556 batch_loss: 0.1571178138256073\n",
      "training: 25 batch 557 batch_loss: 0.15417718887329102\n",
      "training: 25 batch 558 batch_loss: 0.15702638030052185\n",
      "training: 25 batch 559 batch_loss: 0.15329572558403015\n",
      "training: 25 batch 560 batch_loss: 0.152154803276062\n",
      "training: 25 batch 561 batch_loss: 0.15440839529037476\n",
      "training: 25 batch 562 batch_loss: 0.15265914797782898\n",
      "training: 25 batch 563 batch_loss: 0.15422937273979187\n",
      "training: 25 batch 564 batch_loss: 0.15202978253364563\n",
      "training: 25 batch 565 batch_loss: 0.15182334184646606\n",
      "training: 25 batch 566 batch_loss: 0.15307360887527466\n",
      "training: 25 batch 567 batch_loss: 0.15229341387748718\n",
      "training: 25 batch 568 batch_loss: 0.15407222509384155\n",
      "training: 25 batch 569 batch_loss: 0.15282267332077026\n",
      "training: 25 batch 570 batch_loss: 0.15332722663879395\n",
      "training: 25 batch 571 batch_loss: 0.15594756603240967\n",
      "training: 25 batch 572 batch_loss: 0.151075541973114\n",
      "training: 25 batch 573 batch_loss: 0.15421459078788757\n",
      "training: 25 batch 574 batch_loss: 0.15365928411483765\n",
      "training: 25 batch 575 batch_loss: 0.15445849299430847\n",
      "training: 25 batch 576 batch_loss: 0.15031227469444275\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 25, Hit Ratio:0.02837507869937007 | Precision:0.041865722992234344 | Recall:0.05722098510346509 | NDCG:0.05326754681088087\n",
      "*Best Performance* \n",
      "Epoch: 24, Hit Ratio:0.02903132318208619 | Precision:0.04283397227956355 | Recall:0.058055009572000306 | MDCG:0.054485315882160955\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 26 batch 0 batch_loss: 0.15157675743103027\n",
      "training: 26 batch 1 batch_loss: 0.15497258305549622\n",
      "training: 26 batch 2 batch_loss: 0.14981746673583984\n",
      "training: 26 batch 3 batch_loss: 0.15578103065490723\n",
      "training: 26 batch 4 batch_loss: 0.1541561782360077\n",
      "training: 26 batch 5 batch_loss: 0.14980950951576233\n",
      "training: 26 batch 6 batch_loss: 0.14981994032859802\n",
      "training: 26 batch 7 batch_loss: 0.15220704674720764\n",
      "training: 26 batch 8 batch_loss: 0.15380850434303284\n",
      "training: 26 batch 9 batch_loss: 0.1514691412448883\n",
      "training: 26 batch 10 batch_loss: 0.15125742554664612\n",
      "training: 26 batch 11 batch_loss: 0.15251988172531128\n",
      "training: 26 batch 12 batch_loss: 0.1534317433834076\n",
      "training: 26 batch 13 batch_loss: 0.15160703659057617\n",
      "training: 26 batch 14 batch_loss: 0.1529119610786438\n",
      "training: 26 batch 15 batch_loss: 0.14902499318122864\n",
      "training: 26 batch 16 batch_loss: 0.15405744314193726\n",
      "training: 26 batch 17 batch_loss: 0.14916104078292847\n",
      "training: 26 batch 18 batch_loss: 0.15137436985969543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 26 batch 19 batch_loss: 0.15618792176246643\n",
      "training: 26 batch 20 batch_loss: 0.15553691983222961\n",
      "training: 26 batch 21 batch_loss: 0.15259456634521484\n",
      "training: 26 batch 22 batch_loss: 0.154542475938797\n",
      "training: 26 batch 23 batch_loss: 0.15395864844322205\n",
      "training: 26 batch 24 batch_loss: 0.15066081285476685\n",
      "training: 26 batch 25 batch_loss: 0.15325701236724854\n",
      "training: 26 batch 26 batch_loss: 0.15214329957962036\n",
      "training: 26 batch 27 batch_loss: 0.1514253318309784\n",
      "training: 26 batch 28 batch_loss: 0.1521698236465454\n",
      "training: 26 batch 29 batch_loss: 0.15401849150657654\n",
      "training: 26 batch 30 batch_loss: 0.15111365914344788\n",
      "training: 26 batch 31 batch_loss: 0.15656417608261108\n",
      "training: 26 batch 32 batch_loss: 0.15281081199645996\n",
      "training: 26 batch 33 batch_loss: 0.15412560105323792\n",
      "training: 26 batch 34 batch_loss: 0.15241923928260803\n",
      "training: 26 batch 35 batch_loss: 0.15001943707466125\n",
      "training: 26 batch 36 batch_loss: 0.15229421854019165\n",
      "training: 26 batch 37 batch_loss: 0.15563112497329712\n",
      "training: 26 batch 38 batch_loss: 0.1555064618587494\n",
      "training: 26 batch 39 batch_loss: 0.15426692366600037\n",
      "training: 26 batch 40 batch_loss: 0.14991337060928345\n",
      "training: 26 batch 41 batch_loss: 0.14840441942214966\n",
      "training: 26 batch 42 batch_loss: 0.15656545758247375\n",
      "training: 26 batch 43 batch_loss: 0.1516798436641693\n",
      "training: 26 batch 44 batch_loss: 0.15099528431892395\n",
      "training: 26 batch 45 batch_loss: 0.15401282906532288\n",
      "training: 26 batch 46 batch_loss: 0.15433001518249512\n",
      "training: 26 batch 47 batch_loss: 0.15416976809501648\n",
      "training: 26 batch 48 batch_loss: 0.15599071979522705\n",
      "training: 26 batch 49 batch_loss: 0.15407970547676086\n",
      "training: 26 batch 50 batch_loss: 0.15401610732078552\n",
      "training: 26 batch 51 batch_loss: 0.15359866619110107\n",
      "training: 26 batch 52 batch_loss: 0.1528855562210083\n",
      "training: 26 batch 53 batch_loss: 0.1552455723285675\n",
      "training: 26 batch 54 batch_loss: 0.15299710631370544\n",
      "training: 26 batch 55 batch_loss: 0.14926284551620483\n",
      "training: 26 batch 56 batch_loss: 0.15444859862327576\n",
      "training: 26 batch 57 batch_loss: 0.1539134383201599\n",
      "training: 26 batch 58 batch_loss: 0.15388831496238708\n",
      "training: 26 batch 59 batch_loss: 0.15298917889595032\n",
      "training: 26 batch 60 batch_loss: 0.15636757016181946\n",
      "training: 26 batch 61 batch_loss: 0.15719297528266907\n",
      "training: 26 batch 62 batch_loss: 0.14979353547096252\n",
      "training: 26 batch 63 batch_loss: 0.154728502035141\n",
      "training: 26 batch 64 batch_loss: 0.15100407600402832\n",
      "training: 26 batch 65 batch_loss: 0.14997899532318115\n",
      "training: 26 batch 66 batch_loss: 0.15324658155441284\n",
      "training: 26 batch 67 batch_loss: 0.1510636806488037\n",
      "training: 26 batch 68 batch_loss: 0.15129002928733826\n",
      "training: 26 batch 69 batch_loss: 0.15630018711090088\n",
      "training: 26 batch 70 batch_loss: 0.15650895237922668\n",
      "training: 26 batch 71 batch_loss: 0.14961662888526917\n",
      "training: 26 batch 72 batch_loss: 0.15008243918418884\n",
      "training: 26 batch 73 batch_loss: 0.1490093469619751\n",
      "training: 26 batch 74 batch_loss: 0.15278387069702148\n",
      "training: 26 batch 75 batch_loss: 0.154066264629364\n",
      "training: 26 batch 76 batch_loss: 0.15456020832061768\n",
      "training: 26 batch 77 batch_loss: 0.15174704790115356\n",
      "training: 26 batch 78 batch_loss: 0.1537093222141266\n",
      "training: 26 batch 79 batch_loss: 0.15377691388130188\n",
      "training: 26 batch 80 batch_loss: 0.1541641354560852\n",
      "training: 26 batch 81 batch_loss: 0.1510632038116455\n",
      "training: 26 batch 82 batch_loss: 0.1551242470741272\n",
      "training: 26 batch 83 batch_loss: 0.15373966097831726\n",
      "training: 26 batch 84 batch_loss: 0.15404373407363892\n",
      "training: 26 batch 85 batch_loss: 0.1560588777065277\n",
      "training: 26 batch 86 batch_loss: 0.15203598141670227\n",
      "training: 26 batch 87 batch_loss: 0.15309208631515503\n",
      "training: 26 batch 88 batch_loss: 0.1536216139793396\n",
      "training: 26 batch 89 batch_loss: 0.15760040283203125\n",
      "training: 26 batch 90 batch_loss: 0.15240046381950378\n",
      "training: 26 batch 91 batch_loss: 0.1514061689376831\n",
      "training: 26 batch 92 batch_loss: 0.15354356169700623\n",
      "training: 26 batch 93 batch_loss: 0.15126103162765503\n",
      "training: 26 batch 94 batch_loss: 0.1564103662967682\n",
      "training: 26 batch 95 batch_loss: 0.15833154320716858\n",
      "training: 26 batch 96 batch_loss: 0.1559378206729889\n",
      "training: 26 batch 97 batch_loss: 0.15171945095062256\n",
      "training: 26 batch 98 batch_loss: 0.1523200273513794\n",
      "training: 26 batch 99 batch_loss: 0.15426060557365417\n",
      "training: 26 batch 100 batch_loss: 0.1493884027004242\n",
      "training: 26 batch 101 batch_loss: 0.15227168798446655\n",
      "training: 26 batch 102 batch_loss: 0.15132316946983337\n",
      "training: 26 batch 103 batch_loss: 0.15374907851219177\n",
      "training: 26 batch 104 batch_loss: 0.14944973587989807\n",
      "training: 26 batch 105 batch_loss: 0.1545935571193695\n",
      "training: 26 batch 106 batch_loss: 0.15368494391441345\n",
      "training: 26 batch 107 batch_loss: 0.1558266282081604\n",
      "training: 26 batch 108 batch_loss: 0.1560966968536377\n",
      "training: 26 batch 109 batch_loss: 0.15331712365150452\n",
      "training: 26 batch 110 batch_loss: 0.15188705921173096\n",
      "training: 26 batch 111 batch_loss: 0.15618151426315308\n",
      "training: 26 batch 112 batch_loss: 0.15297013521194458\n",
      "training: 26 batch 113 batch_loss: 0.15336501598358154\n",
      "training: 26 batch 114 batch_loss: 0.15343475341796875\n",
      "training: 26 batch 115 batch_loss: 0.15020045638084412\n",
      "training: 26 batch 116 batch_loss: 0.1493283212184906\n",
      "training: 26 batch 117 batch_loss: 0.1534886360168457\n",
      "training: 26 batch 118 batch_loss: 0.15300977230072021\n",
      "training: 26 batch 119 batch_loss: 0.15255457162857056\n",
      "training: 26 batch 120 batch_loss: 0.15594059228897095\n",
      "training: 26 batch 121 batch_loss: 0.1534980833530426\n",
      "training: 26 batch 122 batch_loss: 0.15130919218063354\n",
      "training: 26 batch 123 batch_loss: 0.15379869937896729\n",
      "training: 26 batch 124 batch_loss: 0.1532546579837799\n",
      "training: 26 batch 125 batch_loss: 0.15485918521881104\n",
      "training: 26 batch 126 batch_loss: 0.15052706003189087\n",
      "training: 26 batch 127 batch_loss: 0.15608903765678406\n",
      "training: 26 batch 128 batch_loss: 0.1515006422996521\n",
      "training: 26 batch 129 batch_loss: 0.15364190936088562\n",
      "training: 26 batch 130 batch_loss: 0.15516740083694458\n",
      "training: 26 batch 131 batch_loss: 0.15069520473480225\n",
      "training: 26 batch 132 batch_loss: 0.15334203839302063\n",
      "training: 26 batch 133 batch_loss: 0.15426886081695557\n",
      "training: 26 batch 134 batch_loss: 0.152016282081604\n",
      "training: 26 batch 135 batch_loss: 0.15195417404174805\n",
      "training: 26 batch 136 batch_loss: 0.1519356667995453\n",
      "training: 26 batch 137 batch_loss: 0.15501824021339417\n",
      "training: 26 batch 138 batch_loss: 0.1529371738433838\n",
      "training: 26 batch 139 batch_loss: 0.15304145216941833\n",
      "training: 26 batch 140 batch_loss: 0.15327265858650208\n",
      "training: 26 batch 141 batch_loss: 0.15325909852981567\n",
      "training: 26 batch 142 batch_loss: 0.15458276867866516\n",
      "training: 26 batch 143 batch_loss: 0.1508917212486267\n",
      "training: 26 batch 144 batch_loss: 0.1541757583618164\n",
      "training: 26 batch 145 batch_loss: 0.14959070086479187\n",
      "training: 26 batch 146 batch_loss: 0.15307709574699402\n",
      "training: 26 batch 147 batch_loss: 0.15344473719596863\n",
      "training: 26 batch 148 batch_loss: 0.15499234199523926\n",
      "training: 26 batch 149 batch_loss: 0.14936277270317078\n",
      "training: 26 batch 150 batch_loss: 0.15058869123458862\n",
      "training: 26 batch 151 batch_loss: 0.15671244263648987\n",
      "training: 26 batch 152 batch_loss: 0.15596288442611694\n",
      "training: 26 batch 153 batch_loss: 0.15282484889030457\n",
      "training: 26 batch 154 batch_loss: 0.15230882167816162\n",
      "training: 26 batch 155 batch_loss: 0.151566743850708\n",
      "training: 26 batch 156 batch_loss: 0.1544182002544403\n",
      "training: 26 batch 157 batch_loss: 0.1531648337841034\n",
      "training: 26 batch 158 batch_loss: 0.15258264541625977\n",
      "training: 26 batch 159 batch_loss: 0.1548115313053131\n",
      "training: 26 batch 160 batch_loss: 0.15424317121505737\n",
      "training: 26 batch 161 batch_loss: 0.1556169092655182\n",
      "training: 26 batch 162 batch_loss: 0.15409332513809204\n",
      "training: 26 batch 163 batch_loss: 0.15147614479064941\n",
      "training: 26 batch 164 batch_loss: 0.1546255350112915\n",
      "training: 26 batch 165 batch_loss: 0.15518498420715332\n",
      "training: 26 batch 166 batch_loss: 0.14939162135124207\n",
      "training: 26 batch 167 batch_loss: 0.15085530281066895\n",
      "training: 26 batch 168 batch_loss: 0.15636831521987915\n",
      "training: 26 batch 169 batch_loss: 0.15741583704948425\n",
      "training: 26 batch 170 batch_loss: 0.15188753604888916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 26 batch 171 batch_loss: 0.15203812718391418\n",
      "training: 26 batch 172 batch_loss: 0.15352031588554382\n",
      "training: 26 batch 173 batch_loss: 0.15440446138381958\n",
      "training: 26 batch 174 batch_loss: 0.15619665384292603\n",
      "training: 26 batch 175 batch_loss: 0.15468460321426392\n",
      "training: 26 batch 176 batch_loss: 0.15456917881965637\n",
      "training: 26 batch 177 batch_loss: 0.1556762456893921\n",
      "training: 26 batch 178 batch_loss: 0.15265926718711853\n",
      "training: 26 batch 179 batch_loss: 0.151826411485672\n",
      "training: 26 batch 180 batch_loss: 0.15687373280525208\n",
      "training: 26 batch 181 batch_loss: 0.15526089072227478\n",
      "training: 26 batch 182 batch_loss: 0.156223326921463\n",
      "training: 26 batch 183 batch_loss: 0.15123960375785828\n",
      "training: 26 batch 184 batch_loss: 0.1537337303161621\n",
      "training: 26 batch 185 batch_loss: 0.15832209587097168\n",
      "training: 26 batch 186 batch_loss: 0.155999094247818\n",
      "training: 26 batch 187 batch_loss: 0.15210282802581787\n",
      "training: 26 batch 188 batch_loss: 0.15092810988426208\n",
      "training: 26 batch 189 batch_loss: 0.15383535623550415\n",
      "training: 26 batch 190 batch_loss: 0.15154317021369934\n",
      "training: 26 batch 191 batch_loss: 0.15534788370132446\n",
      "training: 26 batch 192 batch_loss: 0.1570023000240326\n",
      "training: 26 batch 193 batch_loss: 0.15474048256874084\n",
      "training: 26 batch 194 batch_loss: 0.15416455268859863\n",
      "training: 26 batch 195 batch_loss: 0.1527513563632965\n",
      "training: 26 batch 196 batch_loss: 0.15715983510017395\n",
      "training: 26 batch 197 batch_loss: 0.15542784333229065\n",
      "training: 26 batch 198 batch_loss: 0.15059557557106018\n",
      "training: 26 batch 199 batch_loss: 0.15390288829803467\n",
      "training: 26 batch 200 batch_loss: 0.1577267050743103\n",
      "training: 26 batch 201 batch_loss: 0.15201371908187866\n",
      "training: 26 batch 202 batch_loss: 0.1540161371231079\n",
      "training: 26 batch 203 batch_loss: 0.15472671389579773\n",
      "training: 26 batch 204 batch_loss: 0.1548343300819397\n",
      "training: 26 batch 205 batch_loss: 0.15219151973724365\n",
      "training: 26 batch 206 batch_loss: 0.15209531784057617\n",
      "training: 26 batch 207 batch_loss: 0.15298888087272644\n",
      "training: 26 batch 208 batch_loss: 0.1580161452293396\n",
      "training: 26 batch 209 batch_loss: 0.15473437309265137\n",
      "training: 26 batch 210 batch_loss: 0.15423065423965454\n",
      "training: 26 batch 211 batch_loss: 0.15147078037261963\n",
      "training: 26 batch 212 batch_loss: 0.15206223726272583\n",
      "training: 26 batch 213 batch_loss: 0.1533317267894745\n",
      "training: 26 batch 214 batch_loss: 0.15129387378692627\n",
      "training: 26 batch 215 batch_loss: 0.15240168571472168\n",
      "training: 26 batch 216 batch_loss: 0.15328317880630493\n",
      "training: 26 batch 217 batch_loss: 0.1544092297554016\n",
      "training: 26 batch 218 batch_loss: 0.1541900932788849\n",
      "training: 26 batch 219 batch_loss: 0.15501797199249268\n",
      "training: 26 batch 220 batch_loss: 0.15144050121307373\n",
      "training: 26 batch 221 batch_loss: 0.15432527661323547\n",
      "training: 26 batch 222 batch_loss: 0.15812405943870544\n",
      "training: 26 batch 223 batch_loss: 0.1534232497215271\n",
      "training: 26 batch 224 batch_loss: 0.15358155965805054\n",
      "training: 26 batch 225 batch_loss: 0.15368568897247314\n",
      "training: 26 batch 226 batch_loss: 0.15162613987922668\n",
      "training: 26 batch 227 batch_loss: 0.15440267324447632\n",
      "training: 26 batch 228 batch_loss: 0.15737611055374146\n",
      "training: 26 batch 229 batch_loss: 0.15663230419158936\n",
      "training: 26 batch 230 batch_loss: 0.15706098079681396\n",
      "training: 26 batch 231 batch_loss: 0.15477854013442993\n",
      "training: 26 batch 232 batch_loss: 0.1594557762145996\n",
      "training: 26 batch 233 batch_loss: 0.1542668640613556\n",
      "training: 26 batch 234 batch_loss: 0.15508508682250977\n",
      "training: 26 batch 235 batch_loss: 0.15439429879188538\n",
      "training: 26 batch 236 batch_loss: 0.15374436974525452\n",
      "training: 26 batch 237 batch_loss: 0.15339133143424988\n",
      "training: 26 batch 238 batch_loss: 0.15359711647033691\n",
      "training: 26 batch 239 batch_loss: 0.15754157304763794\n",
      "training: 26 batch 240 batch_loss: 0.1528078019618988\n",
      "training: 26 batch 241 batch_loss: 0.15890634059906006\n",
      "training: 26 batch 242 batch_loss: 0.15251216292381287\n",
      "training: 26 batch 243 batch_loss: 0.15575405955314636\n",
      "training: 26 batch 244 batch_loss: 0.1541520357131958\n",
      "training: 26 batch 245 batch_loss: 0.15663546323776245\n",
      "training: 26 batch 246 batch_loss: 0.15457278490066528\n",
      "training: 26 batch 247 batch_loss: 0.15620693564414978\n",
      "training: 26 batch 248 batch_loss: 0.15333807468414307\n",
      "training: 26 batch 249 batch_loss: 0.1530100703239441\n",
      "training: 26 batch 250 batch_loss: 0.15121063590049744\n",
      "training: 26 batch 251 batch_loss: 0.15810179710388184\n",
      "training: 26 batch 252 batch_loss: 0.1514013707637787\n",
      "training: 26 batch 253 batch_loss: 0.15478339791297913\n",
      "training: 26 batch 254 batch_loss: 0.15551292896270752\n",
      "training: 26 batch 255 batch_loss: 0.15348109602928162\n",
      "training: 26 batch 256 batch_loss: 0.15233168005943298\n",
      "training: 26 batch 257 batch_loss: 0.1508394181728363\n",
      "training: 26 batch 258 batch_loss: 0.1534174680709839\n",
      "training: 26 batch 259 batch_loss: 0.1559876799583435\n",
      "training: 26 batch 260 batch_loss: 0.1557159423828125\n",
      "training: 26 batch 261 batch_loss: 0.1519310176372528\n",
      "training: 26 batch 262 batch_loss: 0.15649327635765076\n",
      "training: 26 batch 263 batch_loss: 0.15695929527282715\n",
      "training: 26 batch 264 batch_loss: 0.15732428431510925\n",
      "training: 26 batch 265 batch_loss: 0.15540623664855957\n",
      "training: 26 batch 266 batch_loss: 0.15505295991897583\n",
      "training: 26 batch 267 batch_loss: 0.15478721261024475\n",
      "training: 26 batch 268 batch_loss: 0.15499913692474365\n",
      "training: 26 batch 269 batch_loss: 0.15488895773887634\n",
      "training: 26 batch 270 batch_loss: 0.1542556881904602\n",
      "training: 26 batch 271 batch_loss: 0.15326887369155884\n",
      "training: 26 batch 272 batch_loss: 0.1568552553653717\n",
      "training: 26 batch 273 batch_loss: 0.15284472703933716\n",
      "training: 26 batch 274 batch_loss: 0.15395322442054749\n",
      "training: 26 batch 275 batch_loss: 0.15434971451759338\n",
      "training: 26 batch 276 batch_loss: 0.15648877620697021\n",
      "training: 26 batch 277 batch_loss: 0.15290969610214233\n",
      "training: 26 batch 278 batch_loss: 0.15428176522254944\n",
      "training: 26 batch 279 batch_loss: 0.15504348278045654\n",
      "training: 26 batch 280 batch_loss: 0.15233752131462097\n",
      "training: 26 batch 281 batch_loss: 0.15355509519577026\n",
      "training: 26 batch 282 batch_loss: 0.15255704522132874\n",
      "training: 26 batch 283 batch_loss: 0.15813839435577393\n",
      "training: 26 batch 284 batch_loss: 0.15881004929542542\n",
      "training: 26 batch 285 batch_loss: 0.15727537870407104\n",
      "training: 26 batch 286 batch_loss: 0.15098467469215393\n",
      "training: 26 batch 287 batch_loss: 0.15257176756858826\n",
      "training: 26 batch 288 batch_loss: 0.15461334586143494\n",
      "training: 26 batch 289 batch_loss: 0.15562936663627625\n",
      "training: 26 batch 290 batch_loss: 0.15517917275428772\n",
      "training: 26 batch 291 batch_loss: 0.1534692645072937\n",
      "training: 26 batch 292 batch_loss: 0.157196044921875\n",
      "training: 26 batch 293 batch_loss: 0.1530126929283142\n",
      "training: 26 batch 294 batch_loss: 0.15363967418670654\n",
      "training: 26 batch 295 batch_loss: 0.1544780135154724\n",
      "training: 26 batch 296 batch_loss: 0.15331441164016724\n",
      "training: 26 batch 297 batch_loss: 0.15051376819610596\n",
      "training: 26 batch 298 batch_loss: 0.15217453241348267\n",
      "training: 26 batch 299 batch_loss: 0.15650182962417603\n",
      "training: 26 batch 300 batch_loss: 0.15264493227005005\n",
      "training: 26 batch 301 batch_loss: 0.15585923194885254\n",
      "training: 26 batch 302 batch_loss: 0.1546573042869568\n",
      "training: 26 batch 303 batch_loss: 0.15264424681663513\n",
      "training: 26 batch 304 batch_loss: 0.15454819798469543\n",
      "training: 26 batch 305 batch_loss: 0.15028983354568481\n",
      "training: 26 batch 306 batch_loss: 0.153774231672287\n",
      "training: 26 batch 307 batch_loss: 0.1534212827682495\n",
      "training: 26 batch 308 batch_loss: 0.15692895650863647\n",
      "training: 26 batch 309 batch_loss: 0.1511690318584442\n",
      "training: 26 batch 310 batch_loss: 0.15319734811782837\n",
      "training: 26 batch 311 batch_loss: 0.15241417288780212\n",
      "training: 26 batch 312 batch_loss: 0.15292146801948547\n",
      "training: 26 batch 313 batch_loss: 0.1568288505077362\n",
      "training: 26 batch 314 batch_loss: 0.15562638640403748\n",
      "training: 26 batch 315 batch_loss: 0.15441817045211792\n",
      "training: 26 batch 316 batch_loss: 0.15693745017051697\n",
      "training: 26 batch 317 batch_loss: 0.15251058340072632\n",
      "training: 26 batch 318 batch_loss: 0.15520930290222168\n",
      "training: 26 batch 319 batch_loss: 0.15314018726348877\n",
      "training: 26 batch 320 batch_loss: 0.15578067302703857\n",
      "training: 26 batch 321 batch_loss: 0.15464091300964355\n",
      "training: 26 batch 322 batch_loss: 0.15499019622802734\n",
      "training: 26 batch 323 batch_loss: 0.1530284881591797\n",
      "training: 26 batch 324 batch_loss: 0.15512719750404358\n",
      "training: 26 batch 325 batch_loss: 0.154473215341568\n",
      "training: 26 batch 326 batch_loss: 0.15678620338439941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 26 batch 327 batch_loss: 0.1516561210155487\n",
      "training: 26 batch 328 batch_loss: 0.15231478214263916\n",
      "training: 26 batch 329 batch_loss: 0.1550573706626892\n",
      "training: 26 batch 330 batch_loss: 0.15384721755981445\n",
      "training: 26 batch 331 batch_loss: 0.15573036670684814\n",
      "training: 26 batch 332 batch_loss: 0.15676194429397583\n",
      "training: 26 batch 333 batch_loss: 0.15703263878822327\n",
      "training: 26 batch 334 batch_loss: 0.15662088990211487\n",
      "training: 26 batch 335 batch_loss: 0.1524469554424286\n",
      "training: 26 batch 336 batch_loss: 0.15718597173690796\n",
      "training: 26 batch 337 batch_loss: 0.15236198902130127\n",
      "training: 26 batch 338 batch_loss: 0.15397793054580688\n",
      "training: 26 batch 339 batch_loss: 0.15592318773269653\n",
      "training: 26 batch 340 batch_loss: 0.1526826024055481\n",
      "training: 26 batch 341 batch_loss: 0.1546347737312317\n",
      "training: 26 batch 342 batch_loss: 0.15458440780639648\n",
      "training: 26 batch 343 batch_loss: 0.1542261838912964\n",
      "training: 26 batch 344 batch_loss: 0.1558610498905182\n",
      "training: 26 batch 345 batch_loss: 0.15156444907188416\n",
      "training: 26 batch 346 batch_loss: 0.1527915596961975\n",
      "training: 26 batch 347 batch_loss: 0.1549191176891327\n",
      "training: 26 batch 348 batch_loss: 0.15442228317260742\n",
      "training: 26 batch 349 batch_loss: 0.15449616312980652\n",
      "training: 26 batch 350 batch_loss: 0.15892213582992554\n",
      "training: 26 batch 351 batch_loss: 0.1547999382019043\n",
      "training: 26 batch 352 batch_loss: 0.15682187676429749\n",
      "training: 26 batch 353 batch_loss: 0.15694749355316162\n",
      "training: 26 batch 354 batch_loss: 0.15724986791610718\n",
      "training: 26 batch 355 batch_loss: 0.156760573387146\n",
      "training: 26 batch 356 batch_loss: 0.15499550104141235\n",
      "training: 26 batch 357 batch_loss: 0.1534229815006256\n",
      "training: 26 batch 358 batch_loss: 0.1577107310295105\n",
      "training: 26 batch 359 batch_loss: 0.15761104226112366\n",
      "training: 26 batch 360 batch_loss: 0.15731096267700195\n",
      "training: 26 batch 361 batch_loss: 0.15322992205619812\n",
      "training: 26 batch 362 batch_loss: 0.15688282251358032\n",
      "training: 26 batch 363 batch_loss: 0.15589314699172974\n",
      "training: 26 batch 364 batch_loss: 0.15637832880020142\n",
      "training: 26 batch 365 batch_loss: 0.15094596147537231\n",
      "training: 26 batch 366 batch_loss: 0.15498235821723938\n",
      "training: 26 batch 367 batch_loss: 0.15328991413116455\n",
      "training: 26 batch 368 batch_loss: 0.15584427118301392\n",
      "training: 26 batch 369 batch_loss: 0.15436676144599915\n",
      "training: 26 batch 370 batch_loss: 0.15782490372657776\n",
      "training: 26 batch 371 batch_loss: 0.15133607387542725\n",
      "training: 26 batch 372 batch_loss: 0.1553972363471985\n",
      "training: 26 batch 373 batch_loss: 0.15426382422447205\n",
      "training: 26 batch 374 batch_loss: 0.1539635956287384\n",
      "training: 26 batch 375 batch_loss: 0.1559857726097107\n",
      "training: 26 batch 376 batch_loss: 0.15438273549079895\n",
      "training: 26 batch 377 batch_loss: 0.15765392780303955\n",
      "training: 26 batch 378 batch_loss: 0.1555877923965454\n",
      "training: 26 batch 379 batch_loss: 0.15347450971603394\n",
      "training: 26 batch 380 batch_loss: 0.1570243537425995\n",
      "training: 26 batch 381 batch_loss: 0.15393966436386108\n",
      "training: 26 batch 382 batch_loss: 0.15246683359146118\n",
      "training: 26 batch 383 batch_loss: 0.15266048908233643\n",
      "training: 26 batch 384 batch_loss: 0.1558365821838379\n",
      "training: 26 batch 385 batch_loss: 0.1534339189529419\n",
      "training: 26 batch 386 batch_loss: 0.15184694528579712\n",
      "training: 26 batch 387 batch_loss: 0.15305379033088684\n",
      "training: 26 batch 388 batch_loss: 0.15678900480270386\n",
      "training: 26 batch 389 batch_loss: 0.15606063604354858\n",
      "training: 26 batch 390 batch_loss: 0.15468692779541016\n",
      "training: 26 batch 391 batch_loss: 0.15439459681510925\n",
      "training: 26 batch 392 batch_loss: 0.15494093298912048\n",
      "training: 26 batch 393 batch_loss: 0.1553373634815216\n",
      "training: 26 batch 394 batch_loss: 0.1540345847606659\n",
      "training: 26 batch 395 batch_loss: 0.15290629863739014\n",
      "training: 26 batch 396 batch_loss: 0.15498369932174683\n",
      "training: 26 batch 397 batch_loss: 0.15405181050300598\n",
      "training: 26 batch 398 batch_loss: 0.15320169925689697\n",
      "training: 26 batch 399 batch_loss: 0.15460237860679626\n",
      "training: 26 batch 400 batch_loss: 0.1525525450706482\n",
      "training: 26 batch 401 batch_loss: 0.15515899658203125\n",
      "training: 26 batch 402 batch_loss: 0.15537568926811218\n",
      "training: 26 batch 403 batch_loss: 0.15530624985694885\n",
      "training: 26 batch 404 batch_loss: 0.15494903922080994\n",
      "training: 26 batch 405 batch_loss: 0.15336576104164124\n",
      "training: 26 batch 406 batch_loss: 0.15070855617523193\n",
      "training: 26 batch 407 batch_loss: 0.15657922625541687\n",
      "training: 26 batch 408 batch_loss: 0.1563035249710083\n",
      "training: 26 batch 409 batch_loss: 0.15248611569404602\n",
      "training: 26 batch 410 batch_loss: 0.15522843599319458\n",
      "training: 26 batch 411 batch_loss: 0.15077200531959534\n",
      "training: 26 batch 412 batch_loss: 0.15486907958984375\n",
      "training: 26 batch 413 batch_loss: 0.1525161862373352\n",
      "training: 26 batch 414 batch_loss: 0.15422099828720093\n",
      "training: 26 batch 415 batch_loss: 0.15680360794067383\n",
      "training: 26 batch 416 batch_loss: 0.15426242351531982\n",
      "training: 26 batch 417 batch_loss: 0.15469664335250854\n",
      "training: 26 batch 418 batch_loss: 0.15535560250282288\n",
      "training: 26 batch 419 batch_loss: 0.1557353436946869\n",
      "training: 26 batch 420 batch_loss: 0.1551046073436737\n",
      "training: 26 batch 421 batch_loss: 0.15409088134765625\n",
      "training: 26 batch 422 batch_loss: 0.15541315078735352\n",
      "training: 26 batch 423 batch_loss: 0.1534227728843689\n",
      "training: 26 batch 424 batch_loss: 0.15823572874069214\n",
      "training: 26 batch 425 batch_loss: 0.15525096654891968\n",
      "training: 26 batch 426 batch_loss: 0.15519332885742188\n",
      "training: 26 batch 427 batch_loss: 0.15471023321151733\n",
      "training: 26 batch 428 batch_loss: 0.15413886308670044\n",
      "training: 26 batch 429 batch_loss: 0.15269261598587036\n",
      "training: 26 batch 430 batch_loss: 0.15620297193527222\n",
      "training: 26 batch 431 batch_loss: 0.1552765667438507\n",
      "training: 26 batch 432 batch_loss: 0.1587311029434204\n",
      "training: 26 batch 433 batch_loss: 0.15657681226730347\n",
      "training: 26 batch 434 batch_loss: 0.153854101896286\n",
      "training: 26 batch 435 batch_loss: 0.15455013513565063\n",
      "training: 26 batch 436 batch_loss: 0.15464168787002563\n",
      "training: 26 batch 437 batch_loss: 0.1565353274345398\n",
      "training: 26 batch 438 batch_loss: 0.15703317523002625\n",
      "training: 26 batch 439 batch_loss: 0.15227800607681274\n",
      "training: 26 batch 440 batch_loss: 0.15610304474830627\n",
      "training: 26 batch 441 batch_loss: 0.15437015891075134\n",
      "training: 26 batch 442 batch_loss: 0.15518337488174438\n",
      "training: 26 batch 443 batch_loss: 0.15442430973052979\n",
      "training: 26 batch 444 batch_loss: 0.15710848569869995\n",
      "training: 26 batch 445 batch_loss: 0.15373745560646057\n",
      "training: 26 batch 446 batch_loss: 0.151455819606781\n",
      "training: 26 batch 447 batch_loss: 0.1567777395248413\n",
      "training: 26 batch 448 batch_loss: 0.15458300709724426\n",
      "training: 26 batch 449 batch_loss: 0.15552422404289246\n",
      "training: 26 batch 450 batch_loss: 0.1553727388381958\n",
      "training: 26 batch 451 batch_loss: 0.1575395166873932\n",
      "training: 26 batch 452 batch_loss: 0.15779587626457214\n",
      "training: 26 batch 453 batch_loss: 0.15576958656311035\n",
      "training: 26 batch 454 batch_loss: 0.15706473588943481\n",
      "training: 26 batch 455 batch_loss: 0.1544134020805359\n",
      "training: 26 batch 456 batch_loss: 0.15321072936058044\n",
      "training: 26 batch 457 batch_loss: 0.1551164984703064\n",
      "training: 26 batch 458 batch_loss: 0.14892476797103882\n",
      "training: 26 batch 459 batch_loss: 0.15091019868850708\n",
      "training: 26 batch 460 batch_loss: 0.1576867699623108\n",
      "training: 26 batch 461 batch_loss: 0.15536463260650635\n",
      "training: 26 batch 462 batch_loss: 0.15529808402061462\n",
      "training: 26 batch 463 batch_loss: 0.15248239040374756\n",
      "training: 26 batch 464 batch_loss: 0.1533348560333252\n",
      "training: 26 batch 465 batch_loss: 0.15573477745056152\n",
      "training: 26 batch 466 batch_loss: 0.15685886144638062\n",
      "training: 26 batch 467 batch_loss: 0.15425118803977966\n",
      "training: 26 batch 468 batch_loss: 0.15569865703582764\n",
      "training: 26 batch 469 batch_loss: 0.15596827864646912\n",
      "training: 26 batch 470 batch_loss: 0.1538752019405365\n",
      "training: 26 batch 471 batch_loss: 0.15654721856117249\n",
      "training: 26 batch 472 batch_loss: 0.152572900056839\n",
      "training: 26 batch 473 batch_loss: 0.15748745203018188\n",
      "training: 26 batch 474 batch_loss: 0.1564386785030365\n",
      "training: 26 batch 475 batch_loss: 0.15550905466079712\n",
      "training: 26 batch 476 batch_loss: 0.15505698323249817\n",
      "training: 26 batch 477 batch_loss: 0.15402832627296448\n",
      "training: 26 batch 478 batch_loss: 0.1565355360507965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 26 batch 479 batch_loss: 0.15624850988388062\n",
      "training: 26 batch 480 batch_loss: 0.1570398509502411\n",
      "training: 26 batch 481 batch_loss: 0.15868517756462097\n",
      "training: 26 batch 482 batch_loss: 0.15741091966629028\n",
      "training: 26 batch 483 batch_loss: 0.1565096378326416\n",
      "training: 26 batch 484 batch_loss: 0.15999871492385864\n",
      "training: 26 batch 485 batch_loss: 0.15618672966957092\n",
      "training: 26 batch 486 batch_loss: 0.1532312035560608\n",
      "training: 26 batch 487 batch_loss: 0.15423116087913513\n",
      "training: 26 batch 488 batch_loss: 0.15511292219161987\n",
      "training: 26 batch 489 batch_loss: 0.15893062949180603\n",
      "training: 26 batch 490 batch_loss: 0.15513107180595398\n",
      "training: 26 batch 491 batch_loss: 0.15488997101783752\n",
      "training: 26 batch 492 batch_loss: 0.1563795506954193\n",
      "training: 26 batch 493 batch_loss: 0.15614956617355347\n",
      "training: 26 batch 494 batch_loss: 0.1531912386417389\n",
      "training: 26 batch 495 batch_loss: 0.15451839566230774\n",
      "training: 26 batch 496 batch_loss: 0.15494737029075623\n",
      "training: 26 batch 497 batch_loss: 0.15537530183792114\n",
      "training: 26 batch 498 batch_loss: 0.15466433763504028\n",
      "training: 26 batch 499 batch_loss: 0.15500888228416443\n",
      "training: 26 batch 500 batch_loss: 0.15818938612937927\n",
      "training: 26 batch 501 batch_loss: 0.15785858035087585\n",
      "training: 26 batch 502 batch_loss: 0.15941184759140015\n",
      "training: 26 batch 503 batch_loss: 0.15711727738380432\n",
      "training: 26 batch 504 batch_loss: 0.15920108556747437\n",
      "training: 26 batch 505 batch_loss: 0.15619879961013794\n",
      "training: 26 batch 506 batch_loss: 0.15337008237838745\n",
      "training: 26 batch 507 batch_loss: 0.15475139021873474\n",
      "training: 26 batch 508 batch_loss: 0.1552026867866516\n",
      "training: 26 batch 509 batch_loss: 0.1584782600402832\n",
      "training: 26 batch 510 batch_loss: 0.15101012587547302\n",
      "training: 26 batch 511 batch_loss: 0.15672063827514648\n",
      "training: 26 batch 512 batch_loss: 0.15369901061058044\n",
      "training: 26 batch 513 batch_loss: 0.15769165754318237\n",
      "training: 26 batch 514 batch_loss: 0.15455007553100586\n",
      "training: 26 batch 515 batch_loss: 0.15223580598831177\n",
      "training: 26 batch 516 batch_loss: 0.15566039085388184\n",
      "training: 26 batch 517 batch_loss: 0.15592175722122192\n",
      "training: 26 batch 518 batch_loss: 0.15604865550994873\n",
      "training: 26 batch 519 batch_loss: 0.1574057936668396\n",
      "training: 26 batch 520 batch_loss: 0.1560678482055664\n",
      "training: 26 batch 521 batch_loss: 0.15332722663879395\n",
      "training: 26 batch 522 batch_loss: 0.15929540991783142\n",
      "training: 26 batch 523 batch_loss: 0.15127643942832947\n",
      "training: 26 batch 524 batch_loss: 0.1551557183265686\n",
      "training: 26 batch 525 batch_loss: 0.15710997581481934\n",
      "training: 26 batch 526 batch_loss: 0.1540500819683075\n",
      "training: 26 batch 527 batch_loss: 0.15506744384765625\n",
      "training: 26 batch 528 batch_loss: 0.15291213989257812\n",
      "training: 26 batch 529 batch_loss: 0.15703555941581726\n",
      "training: 26 batch 530 batch_loss: 0.15323087573051453\n",
      "training: 26 batch 531 batch_loss: 0.1595759391784668\n",
      "training: 26 batch 532 batch_loss: 0.15761703252792358\n",
      "training: 26 batch 533 batch_loss: 0.1567845642566681\n",
      "training: 26 batch 534 batch_loss: 0.15553772449493408\n",
      "training: 26 batch 535 batch_loss: 0.15521502494812012\n",
      "training: 26 batch 536 batch_loss: 0.1548314094543457\n",
      "training: 26 batch 537 batch_loss: 0.15813076496124268\n",
      "training: 26 batch 538 batch_loss: 0.1561233401298523\n",
      "training: 26 batch 539 batch_loss: 0.1546955108642578\n",
      "training: 26 batch 540 batch_loss: 0.15728920698165894\n",
      "training: 26 batch 541 batch_loss: 0.15700000524520874\n",
      "training: 26 batch 542 batch_loss: 0.15730804204940796\n",
      "training: 26 batch 543 batch_loss: 0.1550215482711792\n",
      "training: 26 batch 544 batch_loss: 0.1541326642036438\n",
      "training: 26 batch 545 batch_loss: 0.1555490791797638\n",
      "training: 26 batch 546 batch_loss: 0.15459120273590088\n",
      "training: 26 batch 547 batch_loss: 0.16029328107833862\n",
      "training: 26 batch 548 batch_loss: 0.1565207540988922\n",
      "training: 26 batch 549 batch_loss: 0.15554171800613403\n",
      "training: 26 batch 550 batch_loss: 0.1561865210533142\n",
      "training: 26 batch 551 batch_loss: 0.15593451261520386\n",
      "training: 26 batch 552 batch_loss: 0.15682625770568848\n",
      "training: 26 batch 553 batch_loss: 0.1564297080039978\n",
      "training: 26 batch 554 batch_loss: 0.1541745662689209\n",
      "training: 26 batch 555 batch_loss: 0.15798893570899963\n",
      "training: 26 batch 556 batch_loss: 0.15401402115821838\n",
      "training: 26 batch 557 batch_loss: 0.15649402141571045\n",
      "training: 26 batch 558 batch_loss: 0.15586784482002258\n",
      "training: 26 batch 559 batch_loss: 0.15631520748138428\n",
      "training: 26 batch 560 batch_loss: 0.15312612056732178\n",
      "training: 26 batch 561 batch_loss: 0.1562606394290924\n",
      "training: 26 batch 562 batch_loss: 0.154564768075943\n",
      "training: 26 batch 563 batch_loss: 0.15213745832443237\n",
      "training: 26 batch 564 batch_loss: 0.15869033336639404\n",
      "training: 26 batch 565 batch_loss: 0.15254580974578857\n",
      "training: 26 batch 566 batch_loss: 0.15870970487594604\n",
      "training: 26 batch 567 batch_loss: 0.15501028299331665\n",
      "training: 26 batch 568 batch_loss: 0.1578965187072754\n",
      "training: 26 batch 569 batch_loss: 0.1582370102405548\n",
      "training: 26 batch 570 batch_loss: 0.15335613489151\n",
      "training: 26 batch 571 batch_loss: 0.15440571308135986\n",
      "training: 26 batch 572 batch_loss: 0.15589067339897156\n",
      "training: 26 batch 573 batch_loss: 0.15424957871437073\n",
      "training: 26 batch 574 batch_loss: 0.15566495060920715\n",
      "training: 26 batch 575 batch_loss: 0.15254002809524536\n",
      "training: 26 batch 576 batch_loss: 0.16097351908683777\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 26, Hit Ratio:0.02862491796943966 | Precision:0.04223434581735968 | Recall:0.05765563386952962 | NDCG:0.05412345537959538\n",
      "*Best Performance* \n",
      "Epoch: 24, Hit Ratio:0.02903132318208619 | Precision:0.04283397227956355 | Recall:0.058055009572000306 | MDCG:0.054485315882160955\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 27 batch 0 batch_loss: 0.15197119116783142\n",
      "training: 27 batch 1 batch_loss: 0.15504813194274902\n",
      "training: 27 batch 2 batch_loss: 0.1558985710144043\n",
      "training: 27 batch 3 batch_loss: 0.15333905816078186\n",
      "training: 27 batch 4 batch_loss: 0.1542341709136963\n",
      "training: 27 batch 5 batch_loss: 0.15407606959342957\n",
      "training: 27 batch 6 batch_loss: 0.15387985110282898\n",
      "training: 27 batch 7 batch_loss: 0.15314993262290955\n",
      "training: 27 batch 8 batch_loss: 0.15318575501441956\n",
      "training: 27 batch 9 batch_loss: 0.1559850573539734\n",
      "training: 27 batch 10 batch_loss: 0.155165433883667\n",
      "training: 27 batch 11 batch_loss: 0.15442785620689392\n",
      "training: 27 batch 12 batch_loss: 0.15691068768501282\n",
      "training: 27 batch 13 batch_loss: 0.15323856472969055\n",
      "training: 27 batch 14 batch_loss: 0.1528034806251526\n",
      "training: 27 batch 15 batch_loss: 0.1556764543056488\n",
      "training: 27 batch 16 batch_loss: 0.15229922533035278\n",
      "training: 27 batch 17 batch_loss: 0.15304726362228394\n",
      "training: 27 batch 18 batch_loss: 0.15946656465530396\n",
      "training: 27 batch 19 batch_loss: 0.15512996912002563\n",
      "training: 27 batch 20 batch_loss: 0.158963143825531\n",
      "training: 27 batch 21 batch_loss: 0.15674465894699097\n",
      "training: 27 batch 22 batch_loss: 0.1551770567893982\n",
      "training: 27 batch 23 batch_loss: 0.15535980463027954\n",
      "training: 27 batch 24 batch_loss: 0.1525581181049347\n",
      "training: 27 batch 25 batch_loss: 0.15297240018844604\n",
      "training: 27 batch 26 batch_loss: 0.15862542390823364\n",
      "training: 27 batch 27 batch_loss: 0.1532750129699707\n",
      "training: 27 batch 28 batch_loss: 0.15189769864082336\n",
      "training: 27 batch 29 batch_loss: 0.15503939986228943\n",
      "training: 27 batch 30 batch_loss: 0.15337076783180237\n",
      "training: 27 batch 31 batch_loss: 0.1536867320537567\n",
      "training: 27 batch 32 batch_loss: 0.1525968313217163\n",
      "training: 27 batch 33 batch_loss: 0.15307819843292236\n",
      "training: 27 batch 34 batch_loss: 0.15468451380729675\n",
      "training: 27 batch 35 batch_loss: 0.1523507535457611\n",
      "training: 27 batch 36 batch_loss: 0.15501916408538818\n",
      "training: 27 batch 37 batch_loss: 0.1557692587375641\n",
      "training: 27 batch 38 batch_loss: 0.15372899174690247\n",
      "training: 27 batch 39 batch_loss: 0.1523236334323883\n",
      "training: 27 batch 40 batch_loss: 0.15439006686210632\n",
      "training: 27 batch 41 batch_loss: 0.15411809086799622\n",
      "training: 27 batch 42 batch_loss: 0.1531311571598053\n",
      "training: 27 batch 43 batch_loss: 0.15469619631767273\n",
      "training: 27 batch 44 batch_loss: 0.15565204620361328\n",
      "training: 27 batch 45 batch_loss: 0.15651318430900574\n",
      "training: 27 batch 46 batch_loss: 0.1573942005634308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 27 batch 47 batch_loss: 0.15885698795318604\n",
      "training: 27 batch 48 batch_loss: 0.15715691447257996\n",
      "training: 27 batch 49 batch_loss: 0.1558956801891327\n",
      "training: 27 batch 50 batch_loss: 0.15183621644973755\n",
      "training: 27 batch 51 batch_loss: 0.15693119168281555\n",
      "training: 27 batch 52 batch_loss: 0.15173837542533875\n",
      "training: 27 batch 53 batch_loss: 0.15611734986305237\n",
      "training: 27 batch 54 batch_loss: 0.15299415588378906\n",
      "training: 27 batch 55 batch_loss: 0.15612372756004333\n",
      "training: 27 batch 56 batch_loss: 0.15446633100509644\n",
      "training: 27 batch 57 batch_loss: 0.15535596013069153\n",
      "training: 27 batch 58 batch_loss: 0.15456682443618774\n",
      "training: 27 batch 59 batch_loss: 0.15590131282806396\n",
      "training: 27 batch 60 batch_loss: 0.1525459885597229\n",
      "training: 27 batch 61 batch_loss: 0.1566317081451416\n",
      "training: 27 batch 62 batch_loss: 0.1562783122062683\n",
      "training: 27 batch 63 batch_loss: 0.15833476185798645\n",
      "training: 27 batch 64 batch_loss: 0.15515965223312378\n",
      "training: 27 batch 65 batch_loss: 0.15404221415519714\n",
      "training: 27 batch 66 batch_loss: 0.1538391411304474\n",
      "training: 27 batch 67 batch_loss: 0.15364181995391846\n",
      "training: 27 batch 68 batch_loss: 0.15448519587516785\n",
      "training: 27 batch 69 batch_loss: 0.15344002842903137\n",
      "training: 27 batch 70 batch_loss: 0.15403175354003906\n",
      "training: 27 batch 71 batch_loss: 0.15314748883247375\n",
      "training: 27 batch 72 batch_loss: 0.16089054942131042\n",
      "training: 27 batch 73 batch_loss: 0.15855365991592407\n",
      "training: 27 batch 74 batch_loss: 0.1541871726512909\n",
      "training: 27 batch 75 batch_loss: 0.15412932634353638\n",
      "training: 27 batch 76 batch_loss: 0.15289542078971863\n",
      "training: 27 batch 77 batch_loss: 0.1532190442085266\n",
      "training: 27 batch 78 batch_loss: 0.15502500534057617\n",
      "training: 27 batch 79 batch_loss: 0.15561246871948242\n",
      "training: 27 batch 80 batch_loss: 0.15457165241241455\n",
      "training: 27 batch 81 batch_loss: 0.1525382399559021\n",
      "training: 27 batch 82 batch_loss: 0.15370091795921326\n",
      "training: 27 batch 83 batch_loss: 0.15531054139137268\n",
      "training: 27 batch 84 batch_loss: 0.15462824702262878\n",
      "training: 27 batch 85 batch_loss: 0.15409725904464722\n",
      "training: 27 batch 86 batch_loss: 0.1561492681503296\n",
      "training: 27 batch 87 batch_loss: 0.15506958961486816\n",
      "training: 27 batch 88 batch_loss: 0.15238985419273376\n",
      "training: 27 batch 89 batch_loss: 0.1529531478881836\n",
      "training: 27 batch 90 batch_loss: 0.1531813144683838\n",
      "training: 27 batch 91 batch_loss: 0.15298977494239807\n",
      "training: 27 batch 92 batch_loss: 0.15584197640419006\n",
      "training: 27 batch 93 batch_loss: 0.15629392862319946\n",
      "training: 27 batch 94 batch_loss: 0.15545237064361572\n",
      "training: 27 batch 95 batch_loss: 0.1603674292564392\n",
      "training: 27 batch 96 batch_loss: 0.15478423237800598\n",
      "training: 27 batch 97 batch_loss: 0.15441396832466125\n",
      "training: 27 batch 98 batch_loss: 0.15426796674728394\n",
      "training: 27 batch 99 batch_loss: 0.15554535388946533\n",
      "training: 27 batch 100 batch_loss: 0.15593120455741882\n",
      "training: 27 batch 101 batch_loss: 0.15549945831298828\n",
      "training: 27 batch 102 batch_loss: 0.15728473663330078\n",
      "training: 27 batch 103 batch_loss: 0.15389806032180786\n",
      "training: 27 batch 104 batch_loss: 0.15376421809196472\n",
      "training: 27 batch 105 batch_loss: 0.15395265817642212\n",
      "training: 27 batch 106 batch_loss: 0.15670430660247803\n",
      "training: 27 batch 107 batch_loss: 0.15755334496498108\n",
      "training: 27 batch 108 batch_loss: 0.15513360500335693\n",
      "training: 27 batch 109 batch_loss: 0.1521988809108734\n",
      "training: 27 batch 110 batch_loss: 0.15628701448440552\n",
      "training: 27 batch 111 batch_loss: 0.15976828336715698\n",
      "training: 27 batch 112 batch_loss: 0.15378695726394653\n",
      "training: 27 batch 113 batch_loss: 0.15677541494369507\n",
      "training: 27 batch 114 batch_loss: 0.15735048055648804\n",
      "training: 27 batch 115 batch_loss: 0.15672796964645386\n",
      "training: 27 batch 116 batch_loss: 0.1561446189880371\n",
      "training: 27 batch 117 batch_loss: 0.15637311339378357\n",
      "training: 27 batch 118 batch_loss: 0.15807831287384033\n",
      "training: 27 batch 119 batch_loss: 0.15677499771118164\n",
      "training: 27 batch 120 batch_loss: 0.15446117520332336\n",
      "training: 27 batch 121 batch_loss: 0.1554451882839203\n",
      "training: 27 batch 122 batch_loss: 0.15773522853851318\n",
      "training: 27 batch 123 batch_loss: 0.15458959341049194\n",
      "training: 27 batch 124 batch_loss: 0.15933191776275635\n",
      "training: 27 batch 125 batch_loss: 0.15311285853385925\n",
      "training: 27 batch 126 batch_loss: 0.15708035230636597\n",
      "training: 27 batch 127 batch_loss: 0.15754857659339905\n",
      "training: 27 batch 128 batch_loss: 0.15694665908813477\n",
      "training: 27 batch 129 batch_loss: 0.15324175357818604\n",
      "training: 27 batch 130 batch_loss: 0.15534362196922302\n",
      "training: 27 batch 131 batch_loss: 0.15707051753997803\n",
      "training: 27 batch 132 batch_loss: 0.15454477071762085\n",
      "training: 27 batch 133 batch_loss: 0.15861105918884277\n",
      "training: 27 batch 134 batch_loss: 0.1536252200603485\n",
      "training: 27 batch 135 batch_loss: 0.15644320845603943\n",
      "training: 27 batch 136 batch_loss: 0.15570035576820374\n",
      "training: 27 batch 137 batch_loss: 0.1523013710975647\n",
      "training: 27 batch 138 batch_loss: 0.15393668413162231\n",
      "training: 27 batch 139 batch_loss: 0.1533268690109253\n",
      "training: 27 batch 140 batch_loss: 0.15615543723106384\n",
      "training: 27 batch 141 batch_loss: 0.15725699067115784\n",
      "training: 27 batch 142 batch_loss: 0.15977558493614197\n",
      "training: 27 batch 143 batch_loss: 0.15342754125595093\n",
      "training: 27 batch 144 batch_loss: 0.15449538826942444\n",
      "training: 27 batch 145 batch_loss: 0.156002938747406\n",
      "training: 27 batch 146 batch_loss: 0.156269371509552\n",
      "training: 27 batch 147 batch_loss: 0.1544656753540039\n",
      "training: 27 batch 148 batch_loss: 0.15447312593460083\n",
      "training: 27 batch 149 batch_loss: 0.15640217065811157\n",
      "training: 27 batch 150 batch_loss: 0.15472346544265747\n",
      "training: 27 batch 151 batch_loss: 0.15530577301979065\n",
      "training: 27 batch 152 batch_loss: 0.15483275055885315\n",
      "training: 27 batch 153 batch_loss: 0.15330809354782104\n",
      "training: 27 batch 154 batch_loss: 0.15450364351272583\n",
      "training: 27 batch 155 batch_loss: 0.1510651707649231\n",
      "training: 27 batch 156 batch_loss: 0.15447789430618286\n",
      "training: 27 batch 157 batch_loss: 0.15813949704170227\n",
      "training: 27 batch 158 batch_loss: 0.1521184742450714\n",
      "training: 27 batch 159 batch_loss: 0.15528720617294312\n",
      "training: 27 batch 160 batch_loss: 0.15568792819976807\n",
      "training: 27 batch 161 batch_loss: 0.15844318270683289\n",
      "training: 27 batch 162 batch_loss: 0.155988872051239\n",
      "training: 27 batch 163 batch_loss: 0.15687906742095947\n",
      "training: 27 batch 164 batch_loss: 0.15515953302383423\n",
      "training: 27 batch 165 batch_loss: 0.154366135597229\n",
      "training: 27 batch 166 batch_loss: 0.15601113438606262\n",
      "training: 27 batch 167 batch_loss: 0.15089792013168335\n",
      "training: 27 batch 168 batch_loss: 0.1564319133758545\n",
      "training: 27 batch 169 batch_loss: 0.15499335527420044\n",
      "training: 27 batch 170 batch_loss: 0.15667647123336792\n",
      "training: 27 batch 171 batch_loss: 0.15386655926704407\n",
      "training: 27 batch 172 batch_loss: 0.15763750672340393\n",
      "training: 27 batch 173 batch_loss: 0.15657445788383484\n",
      "training: 27 batch 174 batch_loss: 0.15604239702224731\n",
      "training: 27 batch 175 batch_loss: 0.155795156955719\n",
      "training: 27 batch 176 batch_loss: 0.15745291113853455\n",
      "training: 27 batch 177 batch_loss: 0.15747186541557312\n",
      "training: 27 batch 178 batch_loss: 0.1574351191520691\n",
      "training: 27 batch 179 batch_loss: 0.15393662452697754\n",
      "training: 27 batch 180 batch_loss: 0.15482771396636963\n",
      "training: 27 batch 181 batch_loss: 0.15806910395622253\n",
      "training: 27 batch 182 batch_loss: 0.15173271298408508\n",
      "training: 27 batch 183 batch_loss: 0.1559809148311615\n",
      "training: 27 batch 184 batch_loss: 0.15626201033592224\n",
      "training: 27 batch 185 batch_loss: 0.1577478051185608\n",
      "training: 27 batch 186 batch_loss: 0.15801462531089783\n",
      "training: 27 batch 187 batch_loss: 0.151564359664917\n",
      "training: 27 batch 188 batch_loss: 0.15588238835334778\n",
      "training: 27 batch 189 batch_loss: 0.15399450063705444\n",
      "training: 27 batch 190 batch_loss: 0.15829899907112122\n",
      "training: 27 batch 191 batch_loss: 0.15571802854537964\n",
      "training: 27 batch 192 batch_loss: 0.15601184964179993\n",
      "training: 27 batch 193 batch_loss: 0.15629440546035767\n",
      "training: 27 batch 194 batch_loss: 0.1598798930644989\n",
      "training: 27 batch 195 batch_loss: 0.15597817301750183\n",
      "training: 27 batch 196 batch_loss: 0.15484309196472168\n",
      "training: 27 batch 197 batch_loss: 0.1535380482673645\n",
      "training: 27 batch 198 batch_loss: 0.15524759888648987\n",
      "training: 27 batch 199 batch_loss: 0.15495914220809937\n",
      "training: 27 batch 200 batch_loss: 0.1553516983985901\n",
      "training: 27 batch 201 batch_loss: 0.16410812735557556\n",
      "training: 27 batch 202 batch_loss: 0.15473631024360657\n",
      "training: 27 batch 203 batch_loss: 0.1542244553565979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 27 batch 204 batch_loss: 0.15821006894111633\n",
      "training: 27 batch 205 batch_loss: 0.158706933259964\n",
      "training: 27 batch 206 batch_loss: 0.15897861123085022\n",
      "training: 27 batch 207 batch_loss: 0.1543012261390686\n",
      "training: 27 batch 208 batch_loss: 0.15848380327224731\n",
      "training: 27 batch 209 batch_loss: 0.15599051117897034\n",
      "training: 27 batch 210 batch_loss: 0.16088786721229553\n",
      "training: 27 batch 211 batch_loss: 0.15699508786201477\n",
      "training: 27 batch 212 batch_loss: 0.15729457139968872\n",
      "training: 27 batch 213 batch_loss: 0.15743955969810486\n",
      "training: 27 batch 214 batch_loss: 0.15567639470100403\n",
      "training: 27 batch 215 batch_loss: 0.15418550372123718\n",
      "training: 27 batch 216 batch_loss: 0.15308886766433716\n",
      "training: 27 batch 217 batch_loss: 0.15729188919067383\n",
      "training: 27 batch 218 batch_loss: 0.15635377168655396\n",
      "training: 27 batch 219 batch_loss: 0.1558150053024292\n",
      "training: 27 batch 220 batch_loss: 0.15748146176338196\n",
      "training: 27 batch 221 batch_loss: 0.15782010555267334\n",
      "training: 27 batch 222 batch_loss: 0.1546826958656311\n",
      "training: 27 batch 223 batch_loss: 0.15796732902526855\n",
      "training: 27 batch 224 batch_loss: 0.15695294737815857\n",
      "training: 27 batch 225 batch_loss: 0.15453243255615234\n",
      "training: 27 batch 226 batch_loss: 0.15644162893295288\n",
      "training: 27 batch 227 batch_loss: 0.15879160165786743\n",
      "training: 27 batch 228 batch_loss: 0.15576419234275818\n",
      "training: 27 batch 229 batch_loss: 0.15635794401168823\n",
      "training: 27 batch 230 batch_loss: 0.1561603546142578\n",
      "training: 27 batch 231 batch_loss: 0.15830755233764648\n",
      "training: 27 batch 232 batch_loss: 0.1578826606273651\n",
      "training: 27 batch 233 batch_loss: 0.16015425324440002\n",
      "training: 27 batch 234 batch_loss: 0.15509134531021118\n",
      "training: 27 batch 235 batch_loss: 0.158868670463562\n",
      "training: 27 batch 236 batch_loss: 0.15613538026809692\n",
      "training: 27 batch 237 batch_loss: 0.15579652786254883\n",
      "training: 27 batch 238 batch_loss: 0.15621152520179749\n",
      "training: 27 batch 239 batch_loss: 0.1594865322113037\n",
      "training: 27 batch 240 batch_loss: 0.15583962202072144\n",
      "training: 27 batch 241 batch_loss: 0.15553265810012817\n",
      "training: 27 batch 242 batch_loss: 0.15698638558387756\n",
      "training: 27 batch 243 batch_loss: 0.15512073040008545\n",
      "training: 27 batch 244 batch_loss: 0.15504974126815796\n",
      "training: 27 batch 245 batch_loss: 0.15516012907028198\n",
      "training: 27 batch 246 batch_loss: 0.15467867255210876\n",
      "training: 27 batch 247 batch_loss: 0.15830844640731812\n",
      "training: 27 batch 248 batch_loss: 0.15298938751220703\n",
      "training: 27 batch 249 batch_loss: 0.15485554933547974\n",
      "training: 27 batch 250 batch_loss: 0.15509143471717834\n",
      "training: 27 batch 251 batch_loss: 0.15476340055465698\n",
      "training: 27 batch 252 batch_loss: 0.15530231595039368\n",
      "training: 27 batch 253 batch_loss: 0.15711352229118347\n",
      "training: 27 batch 254 batch_loss: 0.15490391850471497\n",
      "training: 27 batch 255 batch_loss: 0.1564410924911499\n",
      "training: 27 batch 256 batch_loss: 0.1551477015018463\n",
      "training: 27 batch 257 batch_loss: 0.15138819813728333\n",
      "training: 27 batch 258 batch_loss: 0.15781250596046448\n",
      "training: 27 batch 259 batch_loss: 0.15997356176376343\n",
      "training: 27 batch 260 batch_loss: 0.15847527980804443\n",
      "training: 27 batch 261 batch_loss: 0.1567155122756958\n",
      "training: 27 batch 262 batch_loss: 0.15605345368385315\n",
      "training: 27 batch 263 batch_loss: 0.15635553002357483\n",
      "training: 27 batch 264 batch_loss: 0.15766939520835876\n",
      "training: 27 batch 265 batch_loss: 0.153496652841568\n",
      "training: 27 batch 266 batch_loss: 0.1598736047744751\n",
      "training: 27 batch 267 batch_loss: 0.15937432646751404\n",
      "training: 27 batch 268 batch_loss: 0.1548735499382019\n",
      "training: 27 batch 269 batch_loss: 0.15764638781547546\n",
      "training: 27 batch 270 batch_loss: 0.1582944393157959\n",
      "training: 27 batch 271 batch_loss: 0.15465542674064636\n",
      "training: 27 batch 272 batch_loss: 0.15416216850280762\n",
      "training: 27 batch 273 batch_loss: 0.156211256980896\n",
      "training: 27 batch 274 batch_loss: 0.1554865837097168\n",
      "training: 27 batch 275 batch_loss: 0.15676197409629822\n",
      "training: 27 batch 276 batch_loss: 0.15537118911743164\n",
      "training: 27 batch 277 batch_loss: 0.15575668215751648\n",
      "training: 27 batch 278 batch_loss: 0.15626075863838196\n",
      "training: 27 batch 279 batch_loss: 0.15398594737052917\n",
      "training: 27 batch 280 batch_loss: 0.1555856168270111\n",
      "training: 27 batch 281 batch_loss: 0.15517181158065796\n",
      "training: 27 batch 282 batch_loss: 0.15646705031394958\n",
      "training: 27 batch 283 batch_loss: 0.16035804152488708\n",
      "training: 27 batch 284 batch_loss: 0.1548233926296234\n",
      "training: 27 batch 285 batch_loss: 0.15805572271347046\n",
      "training: 27 batch 286 batch_loss: 0.15565288066864014\n",
      "training: 27 batch 287 batch_loss: 0.1584448218345642\n",
      "training: 27 batch 288 batch_loss: 0.1551036834716797\n",
      "training: 27 batch 289 batch_loss: 0.1569620668888092\n",
      "training: 27 batch 290 batch_loss: 0.16141462326049805\n",
      "training: 27 batch 291 batch_loss: 0.15774187445640564\n",
      "training: 27 batch 292 batch_loss: 0.15529000759124756\n",
      "training: 27 batch 293 batch_loss: 0.15588563680648804\n",
      "training: 27 batch 294 batch_loss: 0.1592279076576233\n",
      "training: 27 batch 295 batch_loss: 0.15965288877487183\n",
      "training: 27 batch 296 batch_loss: 0.156091570854187\n",
      "training: 27 batch 297 batch_loss: 0.15703833103179932\n",
      "training: 27 batch 298 batch_loss: 0.15255993604660034\n",
      "training: 27 batch 299 batch_loss: 0.15432220697402954\n",
      "training: 27 batch 300 batch_loss: 0.1594446897506714\n",
      "training: 27 batch 301 batch_loss: 0.1565643548965454\n",
      "training: 27 batch 302 batch_loss: 0.1535029411315918\n",
      "training: 27 batch 303 batch_loss: 0.1561487317085266\n",
      "training: 27 batch 304 batch_loss: 0.15457937121391296\n",
      "training: 27 batch 305 batch_loss: 0.15594658255577087\n",
      "training: 27 batch 306 batch_loss: 0.15591013431549072\n",
      "training: 27 batch 307 batch_loss: 0.1617797315120697\n",
      "training: 27 batch 308 batch_loss: 0.1569657027721405\n",
      "training: 27 batch 309 batch_loss: 0.1569294035434723\n",
      "training: 27 batch 310 batch_loss: 0.15232035517692566\n",
      "training: 27 batch 311 batch_loss: 0.15546169877052307\n",
      "training: 27 batch 312 batch_loss: 0.15772098302841187\n",
      "training: 27 batch 313 batch_loss: 0.15932908654212952\n",
      "training: 27 batch 314 batch_loss: 0.15384119749069214\n",
      "training: 27 batch 315 batch_loss: 0.15727093815803528\n",
      "training: 27 batch 316 batch_loss: 0.15826967358589172\n",
      "training: 27 batch 317 batch_loss: 0.1569100022315979\n",
      "training: 27 batch 318 batch_loss: 0.15574806928634644\n",
      "training: 27 batch 319 batch_loss: 0.1599309742450714\n",
      "training: 27 batch 320 batch_loss: 0.15716704726219177\n",
      "training: 27 batch 321 batch_loss: 0.15782931447029114\n",
      "training: 27 batch 322 batch_loss: 0.15923845767974854\n",
      "training: 27 batch 323 batch_loss: 0.15399625897407532\n",
      "training: 27 batch 324 batch_loss: 0.1558322310447693\n",
      "training: 27 batch 325 batch_loss: 0.1587560474872589\n",
      "training: 27 batch 326 batch_loss: 0.15743854641914368\n",
      "training: 27 batch 327 batch_loss: 0.1556704044342041\n",
      "training: 27 batch 328 batch_loss: 0.15572026371955872\n",
      "training: 27 batch 329 batch_loss: 0.1558300256729126\n",
      "training: 27 batch 330 batch_loss: 0.15688008069992065\n",
      "training: 27 batch 331 batch_loss: 0.1555301547050476\n",
      "training: 27 batch 332 batch_loss: 0.15671715140342712\n",
      "training: 27 batch 333 batch_loss: 0.15886825323104858\n",
      "training: 27 batch 334 batch_loss: 0.15846720337867737\n",
      "training: 27 batch 335 batch_loss: 0.15830788016319275\n",
      "training: 27 batch 336 batch_loss: 0.1560114622116089\n",
      "training: 27 batch 337 batch_loss: 0.15343007445335388\n",
      "training: 27 batch 338 batch_loss: 0.15697583556175232\n",
      "training: 27 batch 339 batch_loss: 0.15482643246650696\n",
      "training: 27 batch 340 batch_loss: 0.15658682584762573\n",
      "training: 27 batch 341 batch_loss: 0.1583794355392456\n",
      "training: 27 batch 342 batch_loss: 0.1552247405052185\n",
      "training: 27 batch 343 batch_loss: 0.1555250585079193\n",
      "training: 27 batch 344 batch_loss: 0.15669456124305725\n",
      "training: 27 batch 345 batch_loss: 0.15343615412712097\n",
      "training: 27 batch 346 batch_loss: 0.15666240453720093\n",
      "training: 27 batch 347 batch_loss: 0.1564357876777649\n",
      "training: 27 batch 348 batch_loss: 0.1567816138267517\n",
      "training: 27 batch 349 batch_loss: 0.15719348192214966\n",
      "training: 27 batch 350 batch_loss: 0.15834927558898926\n",
      "training: 27 batch 351 batch_loss: 0.15681296586990356\n",
      "training: 27 batch 352 batch_loss: 0.1545712947845459\n",
      "training: 27 batch 353 batch_loss: 0.15772932767868042\n",
      "training: 27 batch 354 batch_loss: 0.15494325757026672\n",
      "training: 27 batch 355 batch_loss: 0.15796637535095215\n",
      "training: 27 batch 356 batch_loss: 0.15724822878837585\n",
      "training: 27 batch 357 batch_loss: 0.15739816427230835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 27 batch 358 batch_loss: 0.15781855583190918\n",
      "training: 27 batch 359 batch_loss: 0.15541431307792664\n",
      "training: 27 batch 360 batch_loss: 0.16286712884902954\n",
      "training: 27 batch 361 batch_loss: 0.15422171354293823\n",
      "training: 27 batch 362 batch_loss: 0.15542364120483398\n",
      "training: 27 batch 363 batch_loss: 0.1590081751346588\n",
      "training: 27 batch 364 batch_loss: 0.15684163570404053\n",
      "training: 27 batch 365 batch_loss: 0.15737584233283997\n",
      "training: 27 batch 366 batch_loss: 0.15537145733833313\n",
      "training: 27 batch 367 batch_loss: 0.15749508142471313\n",
      "training: 27 batch 368 batch_loss: 0.15671434998512268\n",
      "training: 27 batch 369 batch_loss: 0.15484383702278137\n",
      "training: 27 batch 370 batch_loss: 0.15982529520988464\n",
      "training: 27 batch 371 batch_loss: 0.15986153483390808\n",
      "training: 27 batch 372 batch_loss: 0.15913748741149902\n",
      "training: 27 batch 373 batch_loss: 0.1525348722934723\n",
      "training: 27 batch 374 batch_loss: 0.15777623653411865\n",
      "training: 27 batch 375 batch_loss: 0.15418267250061035\n",
      "training: 27 batch 376 batch_loss: 0.15906593203544617\n",
      "training: 27 batch 377 batch_loss: 0.1582368016242981\n",
      "training: 27 batch 378 batch_loss: 0.1594010889530182\n",
      "training: 27 batch 379 batch_loss: 0.15609174966812134\n",
      "training: 27 batch 380 batch_loss: 0.15533626079559326\n",
      "training: 27 batch 381 batch_loss: 0.15768110752105713\n",
      "training: 27 batch 382 batch_loss: 0.15694281458854675\n",
      "training: 27 batch 383 batch_loss: 0.15595445036888123\n",
      "training: 27 batch 384 batch_loss: 0.1605638861656189\n",
      "training: 27 batch 385 batch_loss: 0.15310758352279663\n",
      "training: 27 batch 386 batch_loss: 0.1577226221561432\n",
      "training: 27 batch 387 batch_loss: 0.15711098909378052\n",
      "training: 27 batch 388 batch_loss: 0.15472397208213806\n",
      "training: 27 batch 389 batch_loss: 0.15704214572906494\n",
      "training: 27 batch 390 batch_loss: 0.15632203221321106\n",
      "training: 27 batch 391 batch_loss: 0.15597373247146606\n",
      "training: 27 batch 392 batch_loss: 0.15626895427703857\n",
      "training: 27 batch 393 batch_loss: 0.1554398536682129\n",
      "training: 27 batch 394 batch_loss: 0.15700852870941162\n",
      "training: 27 batch 395 batch_loss: 0.1567780077457428\n",
      "training: 27 batch 396 batch_loss: 0.1579611599445343\n",
      "training: 27 batch 397 batch_loss: 0.15773344039916992\n",
      "training: 27 batch 398 batch_loss: 0.1611786186695099\n",
      "training: 27 batch 399 batch_loss: 0.15927430987358093\n",
      "training: 27 batch 400 batch_loss: 0.15628015995025635\n",
      "training: 27 batch 401 batch_loss: 0.1568695604801178\n",
      "training: 27 batch 402 batch_loss: 0.15711358189582825\n",
      "training: 27 batch 403 batch_loss: 0.15698033571243286\n",
      "training: 27 batch 404 batch_loss: 0.15571847558021545\n",
      "training: 27 batch 405 batch_loss: 0.15895944833755493\n",
      "training: 27 batch 406 batch_loss: 0.1557057499885559\n",
      "training: 27 batch 407 batch_loss: 0.15806832909584045\n",
      "training: 27 batch 408 batch_loss: 0.1569460928440094\n",
      "training: 27 batch 409 batch_loss: 0.15444403886795044\n",
      "training: 27 batch 410 batch_loss: 0.15570056438446045\n",
      "training: 27 batch 411 batch_loss: 0.15540435910224915\n",
      "training: 27 batch 412 batch_loss: 0.15633735060691833\n",
      "training: 27 batch 413 batch_loss: 0.1578056812286377\n",
      "training: 27 batch 414 batch_loss: 0.15616178512573242\n",
      "training: 27 batch 415 batch_loss: 0.157881498336792\n",
      "training: 27 batch 416 batch_loss: 0.15497326850891113\n",
      "training: 27 batch 417 batch_loss: 0.15605944395065308\n",
      "training: 27 batch 418 batch_loss: 0.15823185443878174\n",
      "training: 27 batch 419 batch_loss: 0.1583646535873413\n",
      "training: 27 batch 420 batch_loss: 0.15715575218200684\n",
      "training: 27 batch 421 batch_loss: 0.15600836277008057\n",
      "training: 27 batch 422 batch_loss: 0.15884900093078613\n",
      "training: 27 batch 423 batch_loss: 0.15952810645103455\n",
      "training: 27 batch 424 batch_loss: 0.1564847230911255\n",
      "training: 27 batch 425 batch_loss: 0.15640419721603394\n",
      "training: 27 batch 426 batch_loss: 0.16069930791854858\n",
      "training: 27 batch 427 batch_loss: 0.15637436509132385\n",
      "training: 27 batch 428 batch_loss: 0.15701037645339966\n",
      "training: 27 batch 429 batch_loss: 0.1601925492286682\n",
      "training: 27 batch 430 batch_loss: 0.15796667337417603\n",
      "training: 27 batch 431 batch_loss: 0.15679487586021423\n",
      "training: 27 batch 432 batch_loss: 0.16282856464385986\n",
      "training: 27 batch 433 batch_loss: 0.15535831451416016\n",
      "training: 27 batch 434 batch_loss: 0.1583915650844574\n",
      "training: 27 batch 435 batch_loss: 0.15366670489311218\n",
      "training: 27 batch 436 batch_loss: 0.15793994069099426\n",
      "training: 27 batch 437 batch_loss: 0.16031691431999207\n",
      "training: 27 batch 438 batch_loss: 0.15339604020118713\n",
      "training: 27 batch 439 batch_loss: 0.16022780537605286\n",
      "training: 27 batch 440 batch_loss: 0.158179372549057\n",
      "training: 27 batch 441 batch_loss: 0.1587236523628235\n",
      "training: 27 batch 442 batch_loss: 0.15727761387825012\n",
      "training: 27 batch 443 batch_loss: 0.15755167603492737\n",
      "training: 27 batch 444 batch_loss: 0.15465936064720154\n",
      "training: 27 batch 445 batch_loss: 0.1581389307975769\n",
      "training: 27 batch 446 batch_loss: 0.15676763653755188\n",
      "training: 27 batch 447 batch_loss: 0.1558765470981598\n",
      "training: 27 batch 448 batch_loss: 0.15641513466835022\n",
      "training: 27 batch 449 batch_loss: 0.1596646010875702\n",
      "training: 27 batch 450 batch_loss: 0.15786302089691162\n",
      "training: 27 batch 451 batch_loss: 0.1573275625705719\n",
      "training: 27 batch 452 batch_loss: 0.15643447637557983\n",
      "training: 27 batch 453 batch_loss: 0.15769445896148682\n",
      "training: 27 batch 454 batch_loss: 0.1562875509262085\n",
      "training: 27 batch 455 batch_loss: 0.15610837936401367\n",
      "training: 27 batch 456 batch_loss: 0.1546514630317688\n",
      "training: 27 batch 457 batch_loss: 0.15977340936660767\n",
      "training: 27 batch 458 batch_loss: 0.15886765718460083\n",
      "training: 27 batch 459 batch_loss: 0.1595524251461029\n",
      "training: 27 batch 460 batch_loss: 0.15921640396118164\n",
      "training: 27 batch 461 batch_loss: 0.15566575527191162\n",
      "training: 27 batch 462 batch_loss: 0.16160598397254944\n",
      "training: 27 batch 463 batch_loss: 0.1606217622756958\n",
      "training: 27 batch 464 batch_loss: 0.15743499994277954\n",
      "training: 27 batch 465 batch_loss: 0.15813344717025757\n",
      "training: 27 batch 466 batch_loss: 0.15894412994384766\n",
      "training: 27 batch 467 batch_loss: 0.15943625569343567\n",
      "training: 27 batch 468 batch_loss: 0.1562730073928833\n",
      "training: 27 batch 469 batch_loss: 0.1581621766090393\n",
      "training: 27 batch 470 batch_loss: 0.1582876443862915\n",
      "training: 27 batch 471 batch_loss: 0.15807664394378662\n",
      "training: 27 batch 472 batch_loss: 0.15764448046684265\n",
      "training: 27 batch 473 batch_loss: 0.1588107943534851\n",
      "training: 27 batch 474 batch_loss: 0.1583302915096283\n",
      "training: 27 batch 475 batch_loss: 0.1552329659461975\n",
      "training: 27 batch 476 batch_loss: 0.15844589471817017\n",
      "training: 27 batch 477 batch_loss: 0.15825802087783813\n",
      "training: 27 batch 478 batch_loss: 0.15489596128463745\n",
      "training: 27 batch 479 batch_loss: 0.1573697030544281\n",
      "training: 27 batch 480 batch_loss: 0.15375226736068726\n",
      "training: 27 batch 481 batch_loss: 0.1583312451839447\n",
      "training: 27 batch 482 batch_loss: 0.15429586172103882\n",
      "training: 27 batch 483 batch_loss: 0.15844950079917908\n",
      "training: 27 batch 484 batch_loss: 0.15705624222755432\n",
      "training: 27 batch 485 batch_loss: 0.15725484490394592\n",
      "training: 27 batch 486 batch_loss: 0.16241437196731567\n",
      "training: 27 batch 487 batch_loss: 0.15569660067558289\n",
      "training: 27 batch 488 batch_loss: 0.1577892005443573\n",
      "training: 27 batch 489 batch_loss: 0.15648654103279114\n",
      "training: 27 batch 490 batch_loss: 0.15738138556480408\n",
      "training: 27 batch 491 batch_loss: 0.1592564582824707\n",
      "training: 27 batch 492 batch_loss: 0.15783041715621948\n",
      "training: 27 batch 493 batch_loss: 0.1549833118915558\n",
      "training: 27 batch 494 batch_loss: 0.157825767993927\n",
      "training: 27 batch 495 batch_loss: 0.15743884444236755\n",
      "training: 27 batch 496 batch_loss: 0.15351593494415283\n",
      "training: 27 batch 497 batch_loss: 0.1596582531929016\n",
      "training: 27 batch 498 batch_loss: 0.16067937016487122\n",
      "training: 27 batch 499 batch_loss: 0.1550712287425995\n",
      "training: 27 batch 500 batch_loss: 0.15918883681297302\n",
      "training: 27 batch 501 batch_loss: 0.1558154821395874\n",
      "training: 27 batch 502 batch_loss: 0.15882337093353271\n",
      "training: 27 batch 503 batch_loss: 0.15610206127166748\n",
      "training: 27 batch 504 batch_loss: 0.15781673789024353\n",
      "training: 27 batch 505 batch_loss: 0.160210520029068\n",
      "training: 27 batch 506 batch_loss: 0.15676891803741455\n",
      "training: 27 batch 507 batch_loss: 0.15765678882598877\n",
      "training: 27 batch 508 batch_loss: 0.15569564700126648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 27 batch 509 batch_loss: 0.1597149670124054\n",
      "training: 27 batch 510 batch_loss: 0.15635478496551514\n",
      "training: 27 batch 511 batch_loss: 0.15723609924316406\n",
      "training: 27 batch 512 batch_loss: 0.16150450706481934\n",
      "training: 27 batch 513 batch_loss: 0.1585555076599121\n",
      "training: 27 batch 514 batch_loss: 0.15559691190719604\n",
      "training: 27 batch 515 batch_loss: 0.1603304147720337\n",
      "training: 27 batch 516 batch_loss: 0.1560378074645996\n",
      "training: 27 batch 517 batch_loss: 0.16063344478607178\n",
      "training: 27 batch 518 batch_loss: 0.15699362754821777\n",
      "training: 27 batch 519 batch_loss: 0.15818768739700317\n",
      "training: 27 batch 520 batch_loss: 0.15534049272537231\n",
      "training: 27 batch 521 batch_loss: 0.15799686312675476\n",
      "training: 27 batch 522 batch_loss: 0.15852734446525574\n",
      "training: 27 batch 523 batch_loss: 0.15666237473487854\n",
      "training: 27 batch 524 batch_loss: 0.15561464428901672\n",
      "training: 27 batch 525 batch_loss: 0.15775153040885925\n",
      "training: 27 batch 526 batch_loss: 0.159908264875412\n",
      "training: 27 batch 527 batch_loss: 0.15899240970611572\n",
      "training: 27 batch 528 batch_loss: 0.1588527262210846\n",
      "training: 27 batch 529 batch_loss: 0.15629148483276367\n",
      "training: 27 batch 530 batch_loss: 0.15881091356277466\n",
      "training: 27 batch 531 batch_loss: 0.15906983613967896\n",
      "training: 27 batch 532 batch_loss: 0.1629895269870758\n",
      "training: 27 batch 533 batch_loss: 0.1541059911251068\n",
      "training: 27 batch 534 batch_loss: 0.15544646978378296\n",
      "training: 27 batch 535 batch_loss: 0.15917983651161194\n",
      "training: 27 batch 536 batch_loss: 0.15801864862442017\n",
      "training: 27 batch 537 batch_loss: 0.15916958451271057\n",
      "training: 27 batch 538 batch_loss: 0.15624016523361206\n",
      "training: 27 batch 539 batch_loss: 0.15824300050735474\n",
      "training: 27 batch 540 batch_loss: 0.1554548144340515\n",
      "training: 27 batch 541 batch_loss: 0.15505462884902954\n",
      "training: 27 batch 542 batch_loss: 0.16085800528526306\n",
      "training: 27 batch 543 batch_loss: 0.15670350193977356\n",
      "training: 27 batch 544 batch_loss: 0.15906432271003723\n",
      "training: 27 batch 545 batch_loss: 0.1590481400489807\n",
      "training: 27 batch 546 batch_loss: 0.15652137994766235\n",
      "training: 27 batch 547 batch_loss: 0.15890228748321533\n",
      "training: 27 batch 548 batch_loss: 0.15746337175369263\n",
      "training: 27 batch 549 batch_loss: 0.1567980945110321\n",
      "training: 27 batch 550 batch_loss: 0.15955179929733276\n",
      "training: 27 batch 551 batch_loss: 0.15921297669410706\n",
      "training: 27 batch 552 batch_loss: 0.15946725010871887\n",
      "training: 27 batch 553 batch_loss: 0.16042646765708923\n",
      "training: 27 batch 554 batch_loss: 0.15779930353164673\n",
      "training: 27 batch 555 batch_loss: 0.15901100635528564\n",
      "training: 27 batch 556 batch_loss: 0.16055059432983398\n",
      "training: 27 batch 557 batch_loss: 0.15627634525299072\n",
      "training: 27 batch 558 batch_loss: 0.16171637177467346\n",
      "training: 27 batch 559 batch_loss: 0.15777698159217834\n",
      "training: 27 batch 560 batch_loss: 0.15777331590652466\n",
      "training: 27 batch 561 batch_loss: 0.1552734375\n",
      "training: 27 batch 562 batch_loss: 0.15732407569885254\n",
      "training: 27 batch 563 batch_loss: 0.15688949823379517\n",
      "training: 27 batch 564 batch_loss: 0.1550273895263672\n",
      "training: 27 batch 565 batch_loss: 0.15845847129821777\n",
      "training: 27 batch 566 batch_loss: 0.15984264016151428\n",
      "training: 27 batch 567 batch_loss: 0.16051211953163147\n",
      "training: 27 batch 568 batch_loss: 0.15816181898117065\n",
      "training: 27 batch 569 batch_loss: 0.15761104226112366\n",
      "training: 27 batch 570 batch_loss: 0.1612040400505066\n",
      "training: 27 batch 571 batch_loss: 0.15634995698928833\n",
      "training: 27 batch 572 batch_loss: 0.15985438227653503\n",
      "training: 27 batch 573 batch_loss: 0.15726056694984436\n",
      "training: 27 batch 574 batch_loss: 0.15914452075958252\n",
      "training: 27 batch 575 batch_loss: 0.1575198769569397\n",
      "training: 27 batch 576 batch_loss: 0.15927448868751526\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 27, Hit Ratio:0.02903132318208619 | Precision:0.04283397227956355 | Recall:0.057934895605156654 | NDCG:0.054512967089005264\n",
      "*Best Performance* \n",
      "Epoch: 27, Hit Ratio:0.02903132318208619 | Precision:0.04283397227956355 | Recall:0.057934895605156654 | MDCG:0.054512967089005264\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 28 batch 0 batch_loss: 0.15252524614334106\n",
      "training: 28 batch 1 batch_loss: 0.15435823798179626\n",
      "training: 28 batch 2 batch_loss: 0.1549445390701294\n",
      "training: 28 batch 3 batch_loss: 0.1552121341228485\n",
      "training: 28 batch 4 batch_loss: 0.1561192274093628\n",
      "training: 28 batch 5 batch_loss: 0.1563551127910614\n",
      "training: 28 batch 6 batch_loss: 0.15618407726287842\n",
      "training: 28 batch 7 batch_loss: 0.1576712727546692\n",
      "training: 28 batch 8 batch_loss: 0.15705278515815735\n",
      "training: 28 batch 9 batch_loss: 0.15408211946487427\n",
      "training: 28 batch 10 batch_loss: 0.1551336646080017\n",
      "training: 28 batch 11 batch_loss: 0.15808725357055664\n",
      "training: 28 batch 12 batch_loss: 0.15655556321144104\n",
      "training: 28 batch 13 batch_loss: 0.1542363166809082\n",
      "training: 28 batch 14 batch_loss: 0.16066297888755798\n",
      "training: 28 batch 15 batch_loss: 0.15750479698181152\n",
      "training: 28 batch 16 batch_loss: 0.1551244556903839\n",
      "training: 28 batch 17 batch_loss: 0.15638059377670288\n",
      "training: 28 batch 18 batch_loss: 0.1574484407901764\n",
      "training: 28 batch 19 batch_loss: 0.15545183420181274\n",
      "training: 28 batch 20 batch_loss: 0.15826758742332458\n",
      "training: 28 batch 21 batch_loss: 0.1522638499736786\n",
      "training: 28 batch 22 batch_loss: 0.1574750542640686\n",
      "training: 28 batch 23 batch_loss: 0.15887346863746643\n",
      "training: 28 batch 24 batch_loss: 0.15760192275047302\n",
      "training: 28 batch 25 batch_loss: 0.15288639068603516\n",
      "training: 28 batch 26 batch_loss: 0.1580508053302765\n",
      "training: 28 batch 27 batch_loss: 0.15470412373542786\n",
      "training: 28 batch 28 batch_loss: 0.15716737508773804\n",
      "training: 28 batch 29 batch_loss: 0.1524454951286316\n",
      "training: 28 batch 30 batch_loss: 0.15484464168548584\n",
      "training: 28 batch 31 batch_loss: 0.15693306922912598\n",
      "training: 28 batch 32 batch_loss: 0.15647423267364502\n",
      "training: 28 batch 33 batch_loss: 0.15639731287956238\n",
      "training: 28 batch 34 batch_loss: 0.1582837998867035\n",
      "training: 28 batch 35 batch_loss: 0.15657442808151245\n",
      "training: 28 batch 36 batch_loss: 0.15470710396766663\n",
      "training: 28 batch 37 batch_loss: 0.1571788787841797\n",
      "training: 28 batch 38 batch_loss: 0.15750664472579956\n",
      "training: 28 batch 39 batch_loss: 0.1558445692062378\n",
      "training: 28 batch 40 batch_loss: 0.15530264377593994\n",
      "training: 28 batch 41 batch_loss: 0.15756240487098694\n",
      "training: 28 batch 42 batch_loss: 0.15820655226707458\n",
      "training: 28 batch 43 batch_loss: 0.15426397323608398\n",
      "training: 28 batch 44 batch_loss: 0.1595057249069214\n",
      "training: 28 batch 45 batch_loss: 0.15640109777450562\n",
      "training: 28 batch 46 batch_loss: 0.15581876039505005\n",
      "training: 28 batch 47 batch_loss: 0.15611019730567932\n",
      "training: 28 batch 48 batch_loss: 0.16018912196159363\n",
      "training: 28 batch 49 batch_loss: 0.1566655933856964\n",
      "training: 28 batch 50 batch_loss: 0.1592583954334259\n",
      "training: 28 batch 51 batch_loss: 0.15573182702064514\n",
      "training: 28 batch 52 batch_loss: 0.15200814604759216\n",
      "training: 28 batch 53 batch_loss: 0.15730121731758118\n",
      "training: 28 batch 54 batch_loss: 0.15934553742408752\n",
      "training: 28 batch 55 batch_loss: 0.15666744112968445\n",
      "training: 28 batch 56 batch_loss: 0.1551496386528015\n",
      "training: 28 batch 57 batch_loss: 0.1557653844356537\n",
      "training: 28 batch 58 batch_loss: 0.15596985816955566\n",
      "training: 28 batch 59 batch_loss: 0.1584998071193695\n",
      "training: 28 batch 60 batch_loss: 0.15526434779167175\n",
      "training: 28 batch 61 batch_loss: 0.15698683261871338\n",
      "training: 28 batch 62 batch_loss: 0.16033685207366943\n",
      "training: 28 batch 63 batch_loss: 0.15547668933868408\n",
      "training: 28 batch 64 batch_loss: 0.15746748447418213\n",
      "training: 28 batch 65 batch_loss: 0.15186089277267456\n",
      "training: 28 batch 66 batch_loss: 0.15738165378570557\n",
      "training: 28 batch 67 batch_loss: 0.15604424476623535\n",
      "training: 28 batch 68 batch_loss: 0.1592383086681366\n",
      "training: 28 batch 69 batch_loss: 0.15758132934570312\n",
      "training: 28 batch 70 batch_loss: 0.1568036675453186\n",
      "training: 28 batch 71 batch_loss: 0.15580043196678162\n",
      "training: 28 batch 72 batch_loss: 0.15547892451286316\n",
      "training: 28 batch 73 batch_loss: 0.15904060006141663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 28 batch 74 batch_loss: 0.16070610284805298\n",
      "training: 28 batch 75 batch_loss: 0.15642771124839783\n",
      "training: 28 batch 76 batch_loss: 0.15393617749214172\n",
      "training: 28 batch 77 batch_loss: 0.15766698122024536\n",
      "training: 28 batch 78 batch_loss: 0.15811297297477722\n",
      "training: 28 batch 79 batch_loss: 0.15666651725769043\n",
      "training: 28 batch 80 batch_loss: 0.15510037541389465\n",
      "training: 28 batch 81 batch_loss: 0.1537378430366516\n",
      "training: 28 batch 82 batch_loss: 0.15588849782943726\n",
      "training: 28 batch 83 batch_loss: 0.15724492073059082\n",
      "training: 28 batch 84 batch_loss: 0.15698885917663574\n",
      "training: 28 batch 85 batch_loss: 0.15834394097328186\n",
      "training: 28 batch 86 batch_loss: 0.16162076592445374\n",
      "training: 28 batch 87 batch_loss: 0.1543709635734558\n",
      "training: 28 batch 88 batch_loss: 0.15466347336769104\n",
      "training: 28 batch 89 batch_loss: 0.15670263767242432\n",
      "training: 28 batch 90 batch_loss: 0.15836763381958008\n",
      "training: 28 batch 91 batch_loss: 0.1551390290260315\n",
      "training: 28 batch 92 batch_loss: 0.15861117839813232\n",
      "training: 28 batch 93 batch_loss: 0.15731307864189148\n",
      "training: 28 batch 94 batch_loss: 0.15414339303970337\n",
      "training: 28 batch 95 batch_loss: 0.158161461353302\n",
      "training: 28 batch 96 batch_loss: 0.1555681824684143\n",
      "training: 28 batch 97 batch_loss: 0.15843841433525085\n",
      "training: 28 batch 98 batch_loss: 0.1570071578025818\n",
      "training: 28 batch 99 batch_loss: 0.1551417112350464\n",
      "training: 28 batch 100 batch_loss: 0.1553092896938324\n",
      "training: 28 batch 101 batch_loss: 0.1563873589038849\n",
      "training: 28 batch 102 batch_loss: 0.1607714593410492\n",
      "training: 28 batch 103 batch_loss: 0.15725955367088318\n",
      "training: 28 batch 104 batch_loss: 0.1608322560787201\n",
      "training: 28 batch 105 batch_loss: 0.15627121925354004\n",
      "training: 28 batch 106 batch_loss: 0.1591671109199524\n",
      "training: 28 batch 107 batch_loss: 0.1559300720691681\n",
      "training: 28 batch 108 batch_loss: 0.1585741639137268\n",
      "training: 28 batch 109 batch_loss: 0.154851496219635\n",
      "training: 28 batch 110 batch_loss: 0.15942445397377014\n",
      "training: 28 batch 111 batch_loss: 0.15644973516464233\n",
      "training: 28 batch 112 batch_loss: 0.15554529428482056\n",
      "training: 28 batch 113 batch_loss: 0.1573205590248108\n",
      "training: 28 batch 114 batch_loss: 0.15566933155059814\n",
      "training: 28 batch 115 batch_loss: 0.1604495346546173\n",
      "training: 28 batch 116 batch_loss: 0.15791770815849304\n",
      "training: 28 batch 117 batch_loss: 0.15555477142333984\n",
      "training: 28 batch 118 batch_loss: 0.16034311056137085\n",
      "training: 28 batch 119 batch_loss: 0.1581154763698578\n",
      "training: 28 batch 120 batch_loss: 0.15639424324035645\n",
      "training: 28 batch 121 batch_loss: 0.15503308176994324\n",
      "training: 28 batch 122 batch_loss: 0.15904515981674194\n",
      "training: 28 batch 123 batch_loss: 0.15756943821907043\n",
      "training: 28 batch 124 batch_loss: 0.15568313002586365\n",
      "training: 28 batch 125 batch_loss: 0.15582194924354553\n",
      "training: 28 batch 126 batch_loss: 0.15598762035369873\n",
      "training: 28 batch 127 batch_loss: 0.1559470295906067\n",
      "training: 28 batch 128 batch_loss: 0.16172122955322266\n",
      "training: 28 batch 129 batch_loss: 0.15633973479270935\n",
      "training: 28 batch 130 batch_loss: 0.15574601292610168\n",
      "training: 28 batch 131 batch_loss: 0.15688249468803406\n",
      "training: 28 batch 132 batch_loss: 0.15478327870368958\n",
      "training: 28 batch 133 batch_loss: 0.15798497200012207\n",
      "training: 28 batch 134 batch_loss: 0.1583051085472107\n",
      "training: 28 batch 135 batch_loss: 0.1576693058013916\n",
      "training: 28 batch 136 batch_loss: 0.15732118487358093\n",
      "training: 28 batch 137 batch_loss: 0.16011080145835876\n",
      "training: 28 batch 138 batch_loss: 0.15505707263946533\n",
      "training: 28 batch 139 batch_loss: 0.15678465366363525\n",
      "training: 28 batch 140 batch_loss: 0.15669205784797668\n",
      "training: 28 batch 141 batch_loss: 0.1582760214805603\n",
      "training: 28 batch 142 batch_loss: 0.1565336287021637\n",
      "training: 28 batch 143 batch_loss: 0.1596621572971344\n",
      "training: 28 batch 144 batch_loss: 0.1582106649875641\n",
      "training: 28 batch 145 batch_loss: 0.1592627465724945\n",
      "training: 28 batch 146 batch_loss: 0.15735173225402832\n",
      "training: 28 batch 147 batch_loss: 0.15801167488098145\n",
      "training: 28 batch 148 batch_loss: 0.15705084800720215\n",
      "training: 28 batch 149 batch_loss: 0.1583814024925232\n",
      "training: 28 batch 150 batch_loss: 0.15586617588996887\n",
      "training: 28 batch 151 batch_loss: 0.15874606370925903\n",
      "training: 28 batch 152 batch_loss: 0.15788090229034424\n",
      "training: 28 batch 153 batch_loss: 0.15926006436347961\n",
      "training: 28 batch 154 batch_loss: 0.1576451063156128\n",
      "training: 28 batch 155 batch_loss: 0.15856590867042542\n",
      "training: 28 batch 156 batch_loss: 0.15322455763816833\n",
      "training: 28 batch 157 batch_loss: 0.15812146663665771\n",
      "training: 28 batch 158 batch_loss: 0.15487900376319885\n",
      "training: 28 batch 159 batch_loss: 0.1562238335609436\n",
      "training: 28 batch 160 batch_loss: 0.1572171449661255\n",
      "training: 28 batch 161 batch_loss: 0.15532714128494263\n",
      "training: 28 batch 162 batch_loss: 0.15875336527824402\n",
      "training: 28 batch 163 batch_loss: 0.15728813409805298\n",
      "training: 28 batch 164 batch_loss: 0.15948271751403809\n",
      "training: 28 batch 165 batch_loss: 0.15788516402244568\n",
      "training: 28 batch 166 batch_loss: 0.16221684217453003\n",
      "training: 28 batch 167 batch_loss: 0.15848970413208008\n",
      "training: 28 batch 168 batch_loss: 0.1566711962223053\n",
      "training: 28 batch 169 batch_loss: 0.15860706567764282\n",
      "training: 28 batch 170 batch_loss: 0.15823867917060852\n",
      "training: 28 batch 171 batch_loss: 0.15952631831169128\n",
      "training: 28 batch 172 batch_loss: 0.1552981436252594\n",
      "training: 28 batch 173 batch_loss: 0.15604174137115479\n",
      "training: 28 batch 174 batch_loss: 0.157069593667984\n",
      "training: 28 batch 175 batch_loss: 0.1562967300415039\n",
      "training: 28 batch 176 batch_loss: 0.15544480085372925\n",
      "training: 28 batch 177 batch_loss: 0.15991148352622986\n",
      "training: 28 batch 178 batch_loss: 0.15992966294288635\n",
      "training: 28 batch 179 batch_loss: 0.16115429997444153\n",
      "training: 28 batch 180 batch_loss: 0.15797322988510132\n",
      "training: 28 batch 181 batch_loss: 0.15829256176948547\n",
      "training: 28 batch 182 batch_loss: 0.15906721353530884\n",
      "training: 28 batch 183 batch_loss: 0.15890797972679138\n",
      "training: 28 batch 184 batch_loss: 0.16159451007843018\n",
      "training: 28 batch 185 batch_loss: 0.15747013688087463\n",
      "training: 28 batch 186 batch_loss: 0.15616804361343384\n",
      "training: 28 batch 187 batch_loss: 0.15544834733009338\n",
      "training: 28 batch 188 batch_loss: 0.15662702918052673\n",
      "training: 28 batch 189 batch_loss: 0.15744978189468384\n",
      "training: 28 batch 190 batch_loss: 0.1543089747428894\n",
      "training: 28 batch 191 batch_loss: 0.15714454650878906\n",
      "training: 28 batch 192 batch_loss: 0.1581701934337616\n",
      "training: 28 batch 193 batch_loss: 0.16042497754096985\n",
      "training: 28 batch 194 batch_loss: 0.1594543755054474\n",
      "training: 28 batch 195 batch_loss: 0.16004005074501038\n",
      "training: 28 batch 196 batch_loss: 0.16153842210769653\n",
      "training: 28 batch 197 batch_loss: 0.15639913082122803\n",
      "training: 28 batch 198 batch_loss: 0.1589604616165161\n",
      "training: 28 batch 199 batch_loss: 0.15573740005493164\n",
      "training: 28 batch 200 batch_loss: 0.15604650974273682\n",
      "training: 28 batch 201 batch_loss: 0.16021272540092468\n",
      "training: 28 batch 202 batch_loss: 0.1588621437549591\n",
      "training: 28 batch 203 batch_loss: 0.15985199809074402\n",
      "training: 28 batch 204 batch_loss: 0.15639299154281616\n",
      "training: 28 batch 205 batch_loss: 0.15789756178855896\n",
      "training: 28 batch 206 batch_loss: 0.15400850772857666\n",
      "training: 28 batch 207 batch_loss: 0.15593013167381287\n",
      "training: 28 batch 208 batch_loss: 0.15714213252067566\n",
      "training: 28 batch 209 batch_loss: 0.15795806050300598\n",
      "training: 28 batch 210 batch_loss: 0.1571716070175171\n",
      "training: 28 batch 211 batch_loss: 0.15683525800704956\n",
      "training: 28 batch 212 batch_loss: 0.15661466121673584\n",
      "training: 28 batch 213 batch_loss: 0.15984725952148438\n",
      "training: 28 batch 214 batch_loss: 0.15692457556724548\n",
      "training: 28 batch 215 batch_loss: 0.16118952631950378\n",
      "training: 28 batch 216 batch_loss: 0.1586589217185974\n",
      "training: 28 batch 217 batch_loss: 0.16027367115020752\n",
      "training: 28 batch 218 batch_loss: 0.159562885761261\n",
      "training: 28 batch 219 batch_loss: 0.15888595581054688\n",
      "training: 28 batch 220 batch_loss: 0.15534266829490662\n",
      "training: 28 batch 221 batch_loss: 0.15748971700668335\n",
      "training: 28 batch 222 batch_loss: 0.15980207920074463\n",
      "training: 28 batch 223 batch_loss: 0.15538397431373596\n",
      "training: 28 batch 224 batch_loss: 0.15607619285583496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 28 batch 225 batch_loss: 0.15648788213729858\n",
      "training: 28 batch 226 batch_loss: 0.15857887268066406\n",
      "training: 28 batch 227 batch_loss: 0.15639689564704895\n",
      "training: 28 batch 228 batch_loss: 0.15576428174972534\n",
      "training: 28 batch 229 batch_loss: 0.1589725911617279\n",
      "training: 28 batch 230 batch_loss: 0.1575840711593628\n",
      "training: 28 batch 231 batch_loss: 0.155448317527771\n",
      "training: 28 batch 232 batch_loss: 0.158160001039505\n",
      "training: 28 batch 233 batch_loss: 0.16021817922592163\n",
      "training: 28 batch 234 batch_loss: 0.15839076042175293\n",
      "training: 28 batch 235 batch_loss: 0.15670502185821533\n",
      "training: 28 batch 236 batch_loss: 0.15772533416748047\n",
      "training: 28 batch 237 batch_loss: 0.15948393940925598\n",
      "training: 28 batch 238 batch_loss: 0.15827926993370056\n",
      "training: 28 batch 239 batch_loss: 0.15834587812423706\n",
      "training: 28 batch 240 batch_loss: 0.16026151180267334\n",
      "training: 28 batch 241 batch_loss: 0.16018325090408325\n",
      "training: 28 batch 242 batch_loss: 0.15540894865989685\n",
      "training: 28 batch 243 batch_loss: 0.1561187505722046\n",
      "training: 28 batch 244 batch_loss: 0.1586378812789917\n",
      "training: 28 batch 245 batch_loss: 0.15984538197517395\n",
      "training: 28 batch 246 batch_loss: 0.15605345368385315\n",
      "training: 28 batch 247 batch_loss: 0.15618816018104553\n",
      "training: 28 batch 248 batch_loss: 0.16146963834762573\n",
      "training: 28 batch 249 batch_loss: 0.15981313586235046\n",
      "training: 28 batch 250 batch_loss: 0.15719640254974365\n",
      "training: 28 batch 251 batch_loss: 0.16240093111991882\n",
      "training: 28 batch 252 batch_loss: 0.15996354818344116\n",
      "training: 28 batch 253 batch_loss: 0.15913406014442444\n",
      "training: 28 batch 254 batch_loss: 0.15952977538108826\n",
      "training: 28 batch 255 batch_loss: 0.15641868114471436\n",
      "training: 28 batch 256 batch_loss: 0.15892112255096436\n",
      "training: 28 batch 257 batch_loss: 0.15864330530166626\n",
      "training: 28 batch 258 batch_loss: 0.15740948915481567\n",
      "training: 28 batch 259 batch_loss: 0.15779638290405273\n",
      "training: 28 batch 260 batch_loss: 0.15795904397964478\n",
      "training: 28 batch 261 batch_loss: 0.15735593438148499\n",
      "training: 28 batch 262 batch_loss: 0.15782788395881653\n",
      "training: 28 batch 263 batch_loss: 0.1569703221321106\n",
      "training: 28 batch 264 batch_loss: 0.16136613488197327\n",
      "training: 28 batch 265 batch_loss: 0.15900397300720215\n",
      "training: 28 batch 266 batch_loss: 0.1595829725265503\n",
      "training: 28 batch 267 batch_loss: 0.15971678495407104\n",
      "training: 28 batch 268 batch_loss: 0.15753057599067688\n",
      "training: 28 batch 269 batch_loss: 0.16109046339988708\n",
      "training: 28 batch 270 batch_loss: 0.16241374611854553\n",
      "training: 28 batch 271 batch_loss: 0.15900015830993652\n",
      "training: 28 batch 272 batch_loss: 0.15800562500953674\n",
      "training: 28 batch 273 batch_loss: 0.1602037250995636\n",
      "training: 28 batch 274 batch_loss: 0.15875619649887085\n",
      "training: 28 batch 275 batch_loss: 0.15759646892547607\n",
      "training: 28 batch 276 batch_loss: 0.15732106566429138\n",
      "training: 28 batch 277 batch_loss: 0.15853452682495117\n",
      "training: 28 batch 278 batch_loss: 0.15786442160606384\n",
      "training: 28 batch 279 batch_loss: 0.1572001874446869\n",
      "training: 28 batch 280 batch_loss: 0.15885797142982483\n",
      "training: 28 batch 281 batch_loss: 0.15901577472686768\n",
      "training: 28 batch 282 batch_loss: 0.1594752073287964\n",
      "training: 28 batch 283 batch_loss: 0.15911424160003662\n",
      "training: 28 batch 284 batch_loss: 0.1585351526737213\n",
      "training: 28 batch 285 batch_loss: 0.16062918305397034\n",
      "training: 28 batch 286 batch_loss: 0.15745776891708374\n",
      "training: 28 batch 287 batch_loss: 0.1587112843990326\n",
      "training: 28 batch 288 batch_loss: 0.16059079766273499\n",
      "training: 28 batch 289 batch_loss: 0.15649470686912537\n",
      "training: 28 batch 290 batch_loss: 0.15825676918029785\n",
      "training: 28 batch 291 batch_loss: 0.1563093364238739\n",
      "training: 28 batch 292 batch_loss: 0.15617990493774414\n",
      "training: 28 batch 293 batch_loss: 0.1600770652294159\n",
      "training: 28 batch 294 batch_loss: 0.15850961208343506\n",
      "training: 28 batch 295 batch_loss: 0.1591511070728302\n",
      "training: 28 batch 296 batch_loss: 0.1598963737487793\n",
      "training: 28 batch 297 batch_loss: 0.15819823741912842\n",
      "training: 28 batch 298 batch_loss: 0.15986913442611694\n",
      "training: 28 batch 299 batch_loss: 0.15699782967567444\n",
      "training: 28 batch 300 batch_loss: 0.1595214605331421\n",
      "training: 28 batch 301 batch_loss: 0.15904483199119568\n",
      "training: 28 batch 302 batch_loss: 0.1525346040725708\n",
      "training: 28 batch 303 batch_loss: 0.15947479009628296\n",
      "training: 28 batch 304 batch_loss: 0.15778639912605286\n",
      "training: 28 batch 305 batch_loss: 0.15575668215751648\n",
      "training: 28 batch 306 batch_loss: 0.15765097737312317\n",
      "training: 28 batch 307 batch_loss: 0.1578473150730133\n",
      "training: 28 batch 308 batch_loss: 0.15919852256774902\n",
      "training: 28 batch 309 batch_loss: 0.15872395038604736\n",
      "training: 28 batch 310 batch_loss: 0.16051959991455078\n",
      "training: 28 batch 311 batch_loss: 0.158015638589859\n",
      "training: 28 batch 312 batch_loss: 0.1550520658493042\n",
      "training: 28 batch 313 batch_loss: 0.16063863039016724\n",
      "training: 28 batch 314 batch_loss: 0.15854576230049133\n",
      "training: 28 batch 315 batch_loss: 0.1619146764278412\n",
      "training: 28 batch 316 batch_loss: 0.15960568189620972\n",
      "training: 28 batch 317 batch_loss: 0.16085678339004517\n",
      "training: 28 batch 318 batch_loss: 0.15834206342697144\n",
      "training: 28 batch 319 batch_loss: 0.16054561734199524\n",
      "training: 28 batch 320 batch_loss: 0.15679901838302612\n",
      "training: 28 batch 321 batch_loss: 0.15994197130203247\n",
      "training: 28 batch 322 batch_loss: 0.1570982038974762\n",
      "training: 28 batch 323 batch_loss: 0.15858185291290283\n",
      "training: 28 batch 324 batch_loss: 0.15632641315460205\n",
      "training: 28 batch 325 batch_loss: 0.15548312664031982\n",
      "training: 28 batch 326 batch_loss: 0.15547552704811096\n",
      "training: 28 batch 327 batch_loss: 0.1618773341178894\n",
      "training: 28 batch 328 batch_loss: 0.15753352642059326\n",
      "training: 28 batch 329 batch_loss: 0.1550847291946411\n",
      "training: 28 batch 330 batch_loss: 0.15533390641212463\n",
      "training: 28 batch 331 batch_loss: 0.15792235732078552\n",
      "training: 28 batch 332 batch_loss: 0.15600433945655823\n",
      "training: 28 batch 333 batch_loss: 0.16372951865196228\n",
      "training: 28 batch 334 batch_loss: 0.16174790263175964\n",
      "training: 28 batch 335 batch_loss: 0.16012734174728394\n",
      "training: 28 batch 336 batch_loss: 0.15856456756591797\n",
      "training: 28 batch 337 batch_loss: 0.15959212183952332\n",
      "training: 28 batch 338 batch_loss: 0.15669500827789307\n",
      "training: 28 batch 339 batch_loss: 0.15792301297187805\n",
      "training: 28 batch 340 batch_loss: 0.15842881798744202\n",
      "training: 28 batch 341 batch_loss: 0.16023766994476318\n",
      "training: 28 batch 342 batch_loss: 0.15912926197052002\n",
      "training: 28 batch 343 batch_loss: 0.15600445866584778\n",
      "training: 28 batch 344 batch_loss: 0.15780943632125854\n",
      "training: 28 batch 345 batch_loss: 0.1586012840270996\n",
      "training: 28 batch 346 batch_loss: 0.15936776995658875\n",
      "training: 28 batch 347 batch_loss: 0.15997540950775146\n",
      "training: 28 batch 348 batch_loss: 0.15953317284584045\n",
      "training: 28 batch 349 batch_loss: 0.16092520952224731\n",
      "training: 28 batch 350 batch_loss: 0.1599481999874115\n",
      "training: 28 batch 351 batch_loss: 0.15897950530052185\n",
      "training: 28 batch 352 batch_loss: 0.1599985957145691\n",
      "training: 28 batch 353 batch_loss: 0.15778464078903198\n",
      "training: 28 batch 354 batch_loss: 0.15685540437698364\n",
      "training: 28 batch 355 batch_loss: 0.15950322151184082\n",
      "training: 28 batch 356 batch_loss: 0.16404512524604797\n",
      "training: 28 batch 357 batch_loss: 0.15723958611488342\n",
      "training: 28 batch 358 batch_loss: 0.16009527444839478\n",
      "training: 28 batch 359 batch_loss: 0.154740571975708\n",
      "training: 28 batch 360 batch_loss: 0.16021931171417236\n",
      "training: 28 batch 361 batch_loss: 0.15897855162620544\n",
      "training: 28 batch 362 batch_loss: 0.161331444978714\n",
      "training: 28 batch 363 batch_loss: 0.15750423073768616\n",
      "training: 28 batch 364 batch_loss: 0.15951895713806152\n",
      "training: 28 batch 365 batch_loss: 0.16098257899284363\n",
      "training: 28 batch 366 batch_loss: 0.16120675206184387\n",
      "training: 28 batch 367 batch_loss: 0.16035079956054688\n",
      "training: 28 batch 368 batch_loss: 0.15969425439834595\n",
      "training: 28 batch 369 batch_loss: 0.15604758262634277\n",
      "training: 28 batch 370 batch_loss: 0.1603117287158966\n",
      "training: 28 batch 371 batch_loss: 0.15687131881713867\n",
      "training: 28 batch 372 batch_loss: 0.1584513783454895\n",
      "training: 28 batch 373 batch_loss: 0.15985891222953796\n",
      "training: 28 batch 374 batch_loss: 0.15871134400367737\n",
      "training: 28 batch 375 batch_loss: 0.16085916757583618\n",
      "training: 28 batch 376 batch_loss: 0.15848708152770996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 28 batch 377 batch_loss: 0.16024893522262573\n",
      "training: 28 batch 378 batch_loss: 0.16055968403816223\n",
      "training: 28 batch 379 batch_loss: 0.15726551413536072\n",
      "training: 28 batch 380 batch_loss: 0.15795236825942993\n",
      "training: 28 batch 381 batch_loss: 0.16014552116394043\n",
      "training: 28 batch 382 batch_loss: 0.1602422595024109\n",
      "training: 28 batch 383 batch_loss: 0.15971601009368896\n",
      "training: 28 batch 384 batch_loss: 0.15755698084831238\n",
      "training: 28 batch 385 batch_loss: 0.16031348705291748\n",
      "training: 28 batch 386 batch_loss: 0.15960979461669922\n",
      "training: 28 batch 387 batch_loss: 0.15903854370117188\n",
      "training: 28 batch 388 batch_loss: 0.1568012237548828\n",
      "training: 28 batch 389 batch_loss: 0.16142809391021729\n",
      "training: 28 batch 390 batch_loss: 0.15849000215530396\n",
      "training: 28 batch 391 batch_loss: 0.1563795506954193\n",
      "training: 28 batch 392 batch_loss: 0.15586122870445251\n",
      "training: 28 batch 393 batch_loss: 0.16092926263809204\n",
      "training: 28 batch 394 batch_loss: 0.15968045592308044\n",
      "training: 28 batch 395 batch_loss: 0.16127651929855347\n",
      "training: 28 batch 396 batch_loss: 0.16140347719192505\n",
      "training: 28 batch 397 batch_loss: 0.15856754779815674\n",
      "training: 28 batch 398 batch_loss: 0.15858575701713562\n",
      "training: 28 batch 399 batch_loss: 0.15603125095367432\n",
      "training: 28 batch 400 batch_loss: 0.1572759449481964\n",
      "training: 28 batch 401 batch_loss: 0.16194003820419312\n",
      "training: 28 batch 402 batch_loss: 0.1613781452178955\n",
      "training: 28 batch 403 batch_loss: 0.1567726731300354\n",
      "training: 28 batch 404 batch_loss: 0.1599542796611786\n",
      "training: 28 batch 405 batch_loss: 0.16144585609436035\n",
      "training: 28 batch 406 batch_loss: 0.15632364153862\n",
      "training: 28 batch 407 batch_loss: 0.1619778871536255\n",
      "training: 28 batch 408 batch_loss: 0.16553151607513428\n",
      "training: 28 batch 409 batch_loss: 0.16349846124649048\n",
      "training: 28 batch 410 batch_loss: 0.16196903586387634\n",
      "training: 28 batch 411 batch_loss: 0.15838071703910828\n",
      "training: 28 batch 412 batch_loss: 0.16016849875450134\n",
      "training: 28 batch 413 batch_loss: 0.1581965684890747\n",
      "training: 28 batch 414 batch_loss: 0.16349941492080688\n",
      "training: 28 batch 415 batch_loss: 0.16026535630226135\n",
      "training: 28 batch 416 batch_loss: 0.16023355722427368\n",
      "training: 28 batch 417 batch_loss: 0.16322281956672668\n",
      "training: 28 batch 418 batch_loss: 0.16016453504562378\n",
      "training: 28 batch 419 batch_loss: 0.16136467456817627\n",
      "training: 28 batch 420 batch_loss: 0.15759733319282532\n",
      "training: 28 batch 421 batch_loss: 0.1577368676662445\n",
      "training: 28 batch 422 batch_loss: 0.15744799375534058\n",
      "training: 28 batch 423 batch_loss: 0.16211330890655518\n",
      "training: 28 batch 424 batch_loss: 0.15992236137390137\n",
      "training: 28 batch 425 batch_loss: 0.15992218255996704\n",
      "training: 28 batch 426 batch_loss: 0.15945741534233093\n",
      "training: 28 batch 427 batch_loss: 0.1637287139892578\n",
      "training: 28 batch 428 batch_loss: 0.16095426678657532\n",
      "training: 28 batch 429 batch_loss: 0.15615126490592957\n",
      "training: 28 batch 430 batch_loss: 0.1585785448551178\n",
      "training: 28 batch 431 batch_loss: 0.16222161054611206\n",
      "training: 28 batch 432 batch_loss: 0.16255247592926025\n",
      "training: 28 batch 433 batch_loss: 0.1558038294315338\n",
      "training: 28 batch 434 batch_loss: 0.15970629453659058\n",
      "training: 28 batch 435 batch_loss: 0.1579706370830536\n",
      "training: 28 batch 436 batch_loss: 0.16023430228233337\n",
      "training: 28 batch 437 batch_loss: 0.16323944926261902\n",
      "training: 28 batch 438 batch_loss: 0.15791532397270203\n",
      "training: 28 batch 439 batch_loss: 0.15767955780029297\n",
      "training: 28 batch 440 batch_loss: 0.15888604521751404\n",
      "training: 28 batch 441 batch_loss: 0.15607133507728577\n",
      "training: 28 batch 442 batch_loss: 0.16104698181152344\n",
      "training: 28 batch 443 batch_loss: 0.16155391931533813\n",
      "training: 28 batch 444 batch_loss: 0.1558729112148285\n",
      "training: 28 batch 445 batch_loss: 0.16038423776626587\n",
      "training: 28 batch 446 batch_loss: 0.16053643822669983\n",
      "training: 28 batch 447 batch_loss: 0.15894323587417603\n",
      "training: 28 batch 448 batch_loss: 0.15898624062538147\n",
      "training: 28 batch 449 batch_loss: 0.1588512361049652\n",
      "training: 28 batch 450 batch_loss: 0.16154417395591736\n",
      "training: 28 batch 451 batch_loss: 0.16029861569404602\n",
      "training: 28 batch 452 batch_loss: 0.15978139638900757\n",
      "training: 28 batch 453 batch_loss: 0.15630388259887695\n",
      "training: 28 batch 454 batch_loss: 0.16223189234733582\n",
      "training: 28 batch 455 batch_loss: 0.16047748923301697\n",
      "training: 28 batch 456 batch_loss: 0.15883997082710266\n",
      "training: 28 batch 457 batch_loss: 0.16237899661064148\n",
      "training: 28 batch 458 batch_loss: 0.15962782502174377\n",
      "training: 28 batch 459 batch_loss: 0.16245675086975098\n",
      "training: 28 batch 460 batch_loss: 0.15868794918060303\n",
      "training: 28 batch 461 batch_loss: 0.1614360809326172\n",
      "training: 28 batch 462 batch_loss: 0.15885770320892334\n",
      "training: 28 batch 463 batch_loss: 0.16099554300308228\n",
      "training: 28 batch 464 batch_loss: 0.1589830219745636\n",
      "training: 28 batch 465 batch_loss: 0.15899813175201416\n",
      "training: 28 batch 466 batch_loss: 0.15721634030342102\n",
      "training: 28 batch 467 batch_loss: 0.1570143699645996\n",
      "training: 28 batch 468 batch_loss: 0.16012361645698547\n",
      "training: 28 batch 469 batch_loss: 0.16012820601463318\n",
      "training: 28 batch 470 batch_loss: 0.16054373979568481\n",
      "training: 28 batch 471 batch_loss: 0.1612032651901245\n",
      "training: 28 batch 472 batch_loss: 0.15747582912445068\n",
      "training: 28 batch 473 batch_loss: 0.15751469135284424\n",
      "training: 28 batch 474 batch_loss: 0.1628856062889099\n",
      "training: 28 batch 475 batch_loss: 0.16117215156555176\n",
      "training: 28 batch 476 batch_loss: 0.1590818166732788\n",
      "training: 28 batch 477 batch_loss: 0.16409564018249512\n",
      "training: 28 batch 478 batch_loss: 0.16383573412895203\n",
      "training: 28 batch 479 batch_loss: 0.15809708833694458\n",
      "training: 28 batch 480 batch_loss: 0.15975648164749146\n",
      "training: 28 batch 481 batch_loss: 0.158221036195755\n",
      "training: 28 batch 482 batch_loss: 0.15757229924201965\n",
      "training: 28 batch 483 batch_loss: 0.16106629371643066\n",
      "training: 28 batch 484 batch_loss: 0.15997135639190674\n",
      "training: 28 batch 485 batch_loss: 0.15992677211761475\n",
      "training: 28 batch 486 batch_loss: 0.16104263067245483\n",
      "training: 28 batch 487 batch_loss: 0.16003286838531494\n",
      "training: 28 batch 488 batch_loss: 0.1630443036556244\n",
      "training: 28 batch 489 batch_loss: 0.15809190273284912\n",
      "training: 28 batch 490 batch_loss: 0.15857923030853271\n",
      "training: 28 batch 491 batch_loss: 0.15920379757881165\n",
      "training: 28 batch 492 batch_loss: 0.15830296277999878\n",
      "training: 28 batch 493 batch_loss: 0.15831926465034485\n",
      "training: 28 batch 494 batch_loss: 0.15912014245986938\n",
      "training: 28 batch 495 batch_loss: 0.15907496213912964\n",
      "training: 28 batch 496 batch_loss: 0.16267210245132446\n",
      "training: 28 batch 497 batch_loss: 0.1603567898273468\n",
      "training: 28 batch 498 batch_loss: 0.15803027153015137\n",
      "training: 28 batch 499 batch_loss: 0.1614946722984314\n",
      "training: 28 batch 500 batch_loss: 0.16262513399124146\n",
      "training: 28 batch 501 batch_loss: 0.16052809357643127\n",
      "training: 28 batch 502 batch_loss: 0.1588287055492401\n",
      "training: 28 batch 503 batch_loss: 0.16222289204597473\n",
      "training: 28 batch 504 batch_loss: 0.15700846910476685\n",
      "training: 28 batch 505 batch_loss: 0.16056981682777405\n",
      "training: 28 batch 506 batch_loss: 0.1630881130695343\n",
      "training: 28 batch 507 batch_loss: 0.15983590483665466\n",
      "training: 28 batch 508 batch_loss: 0.16083061695098877\n",
      "training: 28 batch 509 batch_loss: 0.1609470248222351\n",
      "training: 28 batch 510 batch_loss: 0.15943342447280884\n",
      "training: 28 batch 511 batch_loss: 0.16246816515922546\n",
      "training: 28 batch 512 batch_loss: 0.16075634956359863\n",
      "training: 28 batch 513 batch_loss: 0.15994200110435486\n",
      "training: 28 batch 514 batch_loss: 0.15925157070159912\n",
      "training: 28 batch 515 batch_loss: 0.16245198249816895\n",
      "training: 28 batch 516 batch_loss: 0.15916553139686584\n",
      "training: 28 batch 517 batch_loss: 0.15925228595733643\n",
      "training: 28 batch 518 batch_loss: 0.16005712747573853\n",
      "training: 28 batch 519 batch_loss: 0.16002413630485535\n",
      "training: 28 batch 520 batch_loss: 0.1605829894542694\n",
      "training: 28 batch 521 batch_loss: 0.1626080870628357\n",
      "training: 28 batch 522 batch_loss: 0.16386592388153076\n",
      "training: 28 batch 523 batch_loss: 0.1617329716682434\n",
      "training: 28 batch 524 batch_loss: 0.15931129455566406\n",
      "training: 28 batch 525 batch_loss: 0.15747389197349548\n",
      "training: 28 batch 526 batch_loss: 0.1617928147315979\n",
      "training: 28 batch 527 batch_loss: 0.16539674997329712\n",
      "training: 28 batch 528 batch_loss: 0.16518259048461914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 28 batch 529 batch_loss: 0.16025429964065552\n",
      "training: 28 batch 530 batch_loss: 0.1626586616039276\n",
      "training: 28 batch 531 batch_loss: 0.16411471366882324\n",
      "training: 28 batch 532 batch_loss: 0.15996840596199036\n",
      "training: 28 batch 533 batch_loss: 0.16012302041053772\n",
      "training: 28 batch 534 batch_loss: 0.1598886251449585\n",
      "training: 28 batch 535 batch_loss: 0.15815147757530212\n",
      "training: 28 batch 536 batch_loss: 0.16015595197677612\n",
      "training: 28 batch 537 batch_loss: 0.16150805354118347\n",
      "training: 28 batch 538 batch_loss: 0.1619899868965149\n",
      "training: 28 batch 539 batch_loss: 0.15791013836860657\n",
      "training: 28 batch 540 batch_loss: 0.1599457859992981\n",
      "training: 28 batch 541 batch_loss: 0.16074591875076294\n",
      "training: 28 batch 542 batch_loss: 0.16114214062690735\n",
      "training: 28 batch 543 batch_loss: 0.15756669640541077\n",
      "training: 28 batch 544 batch_loss: 0.15631145238876343\n",
      "training: 28 batch 545 batch_loss: 0.15816178917884827\n",
      "training: 28 batch 546 batch_loss: 0.15878963470458984\n",
      "training: 28 batch 547 batch_loss: 0.1586499810218811\n",
      "training: 28 batch 548 batch_loss: 0.15875175595283508\n",
      "training: 28 batch 549 batch_loss: 0.16029900312423706\n",
      "training: 28 batch 550 batch_loss: 0.15893560647964478\n",
      "training: 28 batch 551 batch_loss: 0.15922021865844727\n",
      "training: 28 batch 552 batch_loss: 0.15898346900939941\n",
      "training: 28 batch 553 batch_loss: 0.1608324944972992\n",
      "training: 28 batch 554 batch_loss: 0.16088831424713135\n",
      "training: 28 batch 555 batch_loss: 0.15848347544670105\n",
      "training: 28 batch 556 batch_loss: 0.1614530086517334\n",
      "training: 28 batch 557 batch_loss: 0.15986749529838562\n",
      "training: 28 batch 558 batch_loss: 0.15974131226539612\n",
      "training: 28 batch 559 batch_loss: 0.1603555679321289\n",
      "training: 28 batch 560 batch_loss: 0.1598135232925415\n",
      "training: 28 batch 561 batch_loss: 0.1585189402103424\n",
      "training: 28 batch 562 batch_loss: 0.15801462531089783\n",
      "training: 28 batch 563 batch_loss: 0.1639491617679596\n",
      "training: 28 batch 564 batch_loss: 0.16146597266197205\n",
      "training: 28 batch 565 batch_loss: 0.15712127089500427\n",
      "training: 28 batch 566 batch_loss: 0.16178733110427856\n",
      "training: 28 batch 567 batch_loss: 0.16066259145736694\n",
      "training: 28 batch 568 batch_loss: 0.15866881608963013\n",
      "training: 28 batch 569 batch_loss: 0.16277876496315002\n",
      "training: 28 batch 570 batch_loss: 0.1582234501838684\n",
      "training: 28 batch 571 batch_loss: 0.162062406539917\n",
      "training: 28 batch 572 batch_loss: 0.16702383756637573\n",
      "training: 28 batch 573 batch_loss: 0.16015371680259705\n",
      "training: 28 batch 574 batch_loss: 0.1587127447128296\n",
      "training: 28 batch 575 batch_loss: 0.1599685549736023\n",
      "training: 28 batch 576 batch_loss: 0.15872347354888916\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 28, Hit Ratio:0.02971088599667547 | Precision:0.04383662636390445 | Recall:0.05922652713594889 | NDCG:0.05590556067426552\n",
      "*Best Performance* \n",
      "Epoch: 28, Hit Ratio:0.02971088599667547 | Precision:0.04383662636390445 | Recall:0.05922652713594889 | MDCG:0.05590556067426552\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 29 batch 0 batch_loss: 0.1570701003074646\n",
      "training: 29 batch 1 batch_loss: 0.15855208039283752\n",
      "training: 29 batch 2 batch_loss: 0.15504315495491028\n",
      "training: 29 batch 3 batch_loss: 0.16033613681793213\n",
      "training: 29 batch 4 batch_loss: 0.15520983934402466\n",
      "training: 29 batch 5 batch_loss: 0.1553754210472107\n",
      "training: 29 batch 6 batch_loss: 0.1589246392250061\n",
      "training: 29 batch 7 batch_loss: 0.16044098138809204\n",
      "training: 29 batch 8 batch_loss: 0.1605258285999298\n",
      "training: 29 batch 9 batch_loss: 0.15684211254119873\n",
      "training: 29 batch 10 batch_loss: 0.15794295072555542\n",
      "training: 29 batch 11 batch_loss: 0.1558787226676941\n",
      "training: 29 batch 12 batch_loss: 0.15926966071128845\n",
      "training: 29 batch 13 batch_loss: 0.1618332862854004\n",
      "training: 29 batch 14 batch_loss: 0.15832483768463135\n",
      "training: 29 batch 15 batch_loss: 0.1592581868171692\n",
      "training: 29 batch 16 batch_loss: 0.15439844131469727\n",
      "training: 29 batch 17 batch_loss: 0.15574681758880615\n",
      "training: 29 batch 18 batch_loss: 0.15642786026000977\n",
      "training: 29 batch 19 batch_loss: 0.15776664018630981\n",
      "training: 29 batch 20 batch_loss: 0.15605002641677856\n",
      "training: 29 batch 21 batch_loss: 0.15765398740768433\n",
      "training: 29 batch 22 batch_loss: 0.1548117697238922\n",
      "training: 29 batch 23 batch_loss: 0.15589383244514465\n",
      "training: 29 batch 24 batch_loss: 0.15705463290214539\n",
      "training: 29 batch 25 batch_loss: 0.1617390513420105\n",
      "training: 29 batch 26 batch_loss: 0.15821832418441772\n",
      "training: 29 batch 27 batch_loss: 0.15581613779067993\n",
      "training: 29 batch 28 batch_loss: 0.16259515285491943\n",
      "training: 29 batch 29 batch_loss: 0.15666604042053223\n",
      "training: 29 batch 30 batch_loss: 0.1595570147037506\n",
      "training: 29 batch 31 batch_loss: 0.15371114015579224\n",
      "training: 29 batch 32 batch_loss: 0.15846386551856995\n",
      "training: 29 batch 33 batch_loss: 0.1566261649131775\n",
      "training: 29 batch 34 batch_loss: 0.15532496571540833\n",
      "training: 29 batch 35 batch_loss: 0.16011080145835876\n",
      "training: 29 batch 36 batch_loss: 0.16031095385551453\n",
      "training: 29 batch 37 batch_loss: 0.15718886256217957\n",
      "training: 29 batch 38 batch_loss: 0.15605801343917847\n",
      "training: 29 batch 39 batch_loss: 0.15758246183395386\n",
      "training: 29 batch 40 batch_loss: 0.16008442640304565\n",
      "training: 29 batch 41 batch_loss: 0.15616416931152344\n",
      "training: 29 batch 42 batch_loss: 0.15654152631759644\n",
      "training: 29 batch 43 batch_loss: 0.16216468811035156\n",
      "training: 29 batch 44 batch_loss: 0.15933427214622498\n",
      "training: 29 batch 45 batch_loss: 0.15861377120018005\n",
      "training: 29 batch 46 batch_loss: 0.15982145071029663\n",
      "training: 29 batch 47 batch_loss: 0.15775105357170105\n",
      "training: 29 batch 48 batch_loss: 0.16368484497070312\n",
      "training: 29 batch 49 batch_loss: 0.1590089201927185\n",
      "training: 29 batch 50 batch_loss: 0.15933790802955627\n",
      "training: 29 batch 51 batch_loss: 0.15958255529403687\n",
      "training: 29 batch 52 batch_loss: 0.15734630823135376\n",
      "training: 29 batch 53 batch_loss: 0.16166239976882935\n",
      "training: 29 batch 54 batch_loss: 0.16044482588768005\n",
      "training: 29 batch 55 batch_loss: 0.16117224097251892\n",
      "training: 29 batch 56 batch_loss: 0.15832167863845825\n",
      "training: 29 batch 57 batch_loss: 0.16242742538452148\n",
      "training: 29 batch 58 batch_loss: 0.15885964035987854\n",
      "training: 29 batch 59 batch_loss: 0.15773671865463257\n",
      "training: 29 batch 60 batch_loss: 0.15680396556854248\n",
      "training: 29 batch 61 batch_loss: 0.15639850497245789\n",
      "training: 29 batch 62 batch_loss: 0.15857556462287903\n",
      "training: 29 batch 63 batch_loss: 0.16060209274291992\n",
      "training: 29 batch 64 batch_loss: 0.1551617681980133\n",
      "training: 29 batch 65 batch_loss: 0.15557116270065308\n",
      "training: 29 batch 66 batch_loss: 0.16009822487831116\n",
      "training: 29 batch 67 batch_loss: 0.15887299180030823\n",
      "training: 29 batch 68 batch_loss: 0.16091260313987732\n",
      "training: 29 batch 69 batch_loss: 0.15818750858306885\n",
      "training: 29 batch 70 batch_loss: 0.16296690702438354\n",
      "training: 29 batch 71 batch_loss: 0.16018635034561157\n",
      "training: 29 batch 72 batch_loss: 0.16379737854003906\n",
      "training: 29 batch 73 batch_loss: 0.15805664658546448\n",
      "training: 29 batch 74 batch_loss: 0.16019275784492493\n",
      "training: 29 batch 75 batch_loss: 0.15718576312065125\n",
      "training: 29 batch 76 batch_loss: 0.15836650133132935\n",
      "training: 29 batch 77 batch_loss: 0.1619376242160797\n",
      "training: 29 batch 78 batch_loss: 0.160369873046875\n",
      "training: 29 batch 79 batch_loss: 0.15751594305038452\n",
      "training: 29 batch 80 batch_loss: 0.16196182370185852\n",
      "training: 29 batch 81 batch_loss: 0.1581219732761383\n",
      "training: 29 batch 82 batch_loss: 0.16193020343780518\n",
      "training: 29 batch 83 batch_loss: 0.15647345781326294\n",
      "training: 29 batch 84 batch_loss: 0.15812596678733826\n",
      "training: 29 batch 85 batch_loss: 0.15875285863876343\n",
      "training: 29 batch 86 batch_loss: 0.15569144487380981\n",
      "training: 29 batch 87 batch_loss: 0.1600484549999237\n",
      "training: 29 batch 88 batch_loss: 0.15976887941360474\n",
      "training: 29 batch 89 batch_loss: 0.15895283222198486\n",
      "training: 29 batch 90 batch_loss: 0.15714552998542786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 29 batch 91 batch_loss: 0.16092172265052795\n",
      "training: 29 batch 92 batch_loss: 0.15366795659065247\n",
      "training: 29 batch 93 batch_loss: 0.15654850006103516\n",
      "training: 29 batch 94 batch_loss: 0.16275036334991455\n",
      "training: 29 batch 95 batch_loss: 0.1578226387500763\n",
      "training: 29 batch 96 batch_loss: 0.1607908010482788\n",
      "training: 29 batch 97 batch_loss: 0.15777555108070374\n",
      "training: 29 batch 98 batch_loss: 0.16036933660507202\n",
      "training: 29 batch 99 batch_loss: 0.15856018662452698\n",
      "training: 29 batch 100 batch_loss: 0.15622970461845398\n",
      "training: 29 batch 101 batch_loss: 0.15977784991264343\n",
      "training: 29 batch 102 batch_loss: 0.1593455672264099\n",
      "training: 29 batch 103 batch_loss: 0.15647801756858826\n",
      "training: 29 batch 104 batch_loss: 0.16017651557922363\n",
      "training: 29 batch 105 batch_loss: 0.16299384832382202\n",
      "training: 29 batch 106 batch_loss: 0.15722578763961792\n",
      "training: 29 batch 107 batch_loss: 0.16281551122665405\n",
      "training: 29 batch 108 batch_loss: 0.15925389528274536\n",
      "training: 29 batch 109 batch_loss: 0.16015902161598206\n",
      "training: 29 batch 110 batch_loss: 0.15972375869750977\n",
      "training: 29 batch 111 batch_loss: 0.15740013122558594\n",
      "training: 29 batch 112 batch_loss: 0.1590801477432251\n",
      "training: 29 batch 113 batch_loss: 0.16010117530822754\n",
      "training: 29 batch 114 batch_loss: 0.16043266654014587\n",
      "training: 29 batch 115 batch_loss: 0.16134318709373474\n",
      "training: 29 batch 116 batch_loss: 0.1611291766166687\n",
      "training: 29 batch 117 batch_loss: 0.15891438722610474\n",
      "training: 29 batch 118 batch_loss: 0.15745097398757935\n",
      "training: 29 batch 119 batch_loss: 0.15630516409873962\n",
      "training: 29 batch 120 batch_loss: 0.15973618626594543\n",
      "training: 29 batch 121 batch_loss: 0.15833234786987305\n",
      "training: 29 batch 122 batch_loss: 0.16009780764579773\n",
      "training: 29 batch 123 batch_loss: 0.1588219702243805\n",
      "training: 29 batch 124 batch_loss: 0.15684577822685242\n",
      "training: 29 batch 125 batch_loss: 0.16071853041648865\n",
      "training: 29 batch 126 batch_loss: 0.15849930047988892\n",
      "training: 29 batch 127 batch_loss: 0.1567043662071228\n",
      "training: 29 batch 128 batch_loss: 0.15930622816085815\n",
      "training: 29 batch 129 batch_loss: 0.1595446765422821\n",
      "training: 29 batch 130 batch_loss: 0.15712085366249084\n",
      "training: 29 batch 131 batch_loss: 0.1613990068435669\n",
      "training: 29 batch 132 batch_loss: 0.16158244013786316\n",
      "training: 29 batch 133 batch_loss: 0.1640009880065918\n",
      "training: 29 batch 134 batch_loss: 0.1594078242778778\n",
      "training: 29 batch 135 batch_loss: 0.15904617309570312\n",
      "training: 29 batch 136 batch_loss: 0.16024142503738403\n",
      "training: 29 batch 137 batch_loss: 0.15794742107391357\n",
      "training: 29 batch 138 batch_loss: 0.16056302189826965\n",
      "training: 29 batch 139 batch_loss: 0.1625155508518219\n",
      "training: 29 batch 140 batch_loss: 0.16260802745819092\n",
      "training: 29 batch 141 batch_loss: 0.16232669353485107\n",
      "training: 29 batch 142 batch_loss: 0.16031867265701294\n",
      "training: 29 batch 143 batch_loss: 0.1593555212020874\n",
      "training: 29 batch 144 batch_loss: 0.15772569179534912\n",
      "training: 29 batch 145 batch_loss: 0.1612352430820465\n",
      "training: 29 batch 146 batch_loss: 0.15984445810317993\n",
      "training: 29 batch 147 batch_loss: 0.15935295820236206\n",
      "training: 29 batch 148 batch_loss: 0.158197283744812\n",
      "training: 29 batch 149 batch_loss: 0.1598796844482422\n",
      "training: 29 batch 150 batch_loss: 0.16158413887023926\n",
      "training: 29 batch 151 batch_loss: 0.1612185835838318\n",
      "training: 29 batch 152 batch_loss: 0.15512827038764954\n",
      "training: 29 batch 153 batch_loss: 0.15704789757728577\n",
      "training: 29 batch 154 batch_loss: 0.15820804238319397\n",
      "training: 29 batch 155 batch_loss: 0.1590348780155182\n",
      "training: 29 batch 156 batch_loss: 0.15811753273010254\n",
      "training: 29 batch 157 batch_loss: 0.1616051197052002\n",
      "training: 29 batch 158 batch_loss: 0.15872129797935486\n",
      "training: 29 batch 159 batch_loss: 0.15871679782867432\n",
      "training: 29 batch 160 batch_loss: 0.16105049848556519\n",
      "training: 29 batch 161 batch_loss: 0.1571638584136963\n",
      "training: 29 batch 162 batch_loss: 0.15700289607048035\n",
      "training: 29 batch 163 batch_loss: 0.15891948342323303\n",
      "training: 29 batch 164 batch_loss: 0.1627194583415985\n",
      "training: 29 batch 165 batch_loss: 0.15996119379997253\n",
      "training: 29 batch 166 batch_loss: 0.1597059965133667\n",
      "training: 29 batch 167 batch_loss: 0.15941005945205688\n",
      "training: 29 batch 168 batch_loss: 0.15687042474746704\n",
      "training: 29 batch 169 batch_loss: 0.15891867876052856\n",
      "training: 29 batch 170 batch_loss: 0.15993845462799072\n",
      "training: 29 batch 171 batch_loss: 0.16090381145477295\n",
      "training: 29 batch 172 batch_loss: 0.16048893332481384\n",
      "training: 29 batch 173 batch_loss: 0.1617523431777954\n",
      "training: 29 batch 174 batch_loss: 0.15898990631103516\n",
      "training: 29 batch 175 batch_loss: 0.15914741158485413\n",
      "training: 29 batch 176 batch_loss: 0.16075792908668518\n",
      "training: 29 batch 177 batch_loss: 0.16170614957809448\n",
      "training: 29 batch 178 batch_loss: 0.16081279516220093\n",
      "training: 29 batch 179 batch_loss: 0.15858647227287292\n",
      "training: 29 batch 180 batch_loss: 0.16205430030822754\n",
      "training: 29 batch 181 batch_loss: 0.16262590885162354\n",
      "training: 29 batch 182 batch_loss: 0.15883365273475647\n",
      "training: 29 batch 183 batch_loss: 0.15718993544578552\n",
      "training: 29 batch 184 batch_loss: 0.15852117538452148\n",
      "training: 29 batch 185 batch_loss: 0.16125434637069702\n",
      "training: 29 batch 186 batch_loss: 0.16043394804000854\n",
      "training: 29 batch 187 batch_loss: 0.16464242339134216\n",
      "training: 29 batch 188 batch_loss: 0.15582698583602905\n",
      "training: 29 batch 189 batch_loss: 0.15854305028915405\n",
      "training: 29 batch 190 batch_loss: 0.1581728160381317\n",
      "training: 29 batch 191 batch_loss: 0.159817636013031\n",
      "training: 29 batch 192 batch_loss: 0.1590615212917328\n",
      "training: 29 batch 193 batch_loss: 0.15667256712913513\n",
      "training: 29 batch 194 batch_loss: 0.16164329648017883\n",
      "training: 29 batch 195 batch_loss: 0.16066879034042358\n",
      "training: 29 batch 196 batch_loss: 0.16219359636306763\n",
      "training: 29 batch 197 batch_loss: 0.16040247678756714\n",
      "training: 29 batch 198 batch_loss: 0.16139930486679077\n",
      "training: 29 batch 199 batch_loss: 0.16289359331130981\n",
      "training: 29 batch 200 batch_loss: 0.15975940227508545\n",
      "training: 29 batch 201 batch_loss: 0.16084253787994385\n",
      "training: 29 batch 202 batch_loss: 0.16359537839889526\n",
      "training: 29 batch 203 batch_loss: 0.1622203290462494\n",
      "training: 29 batch 204 batch_loss: 0.16207468509674072\n",
      "training: 29 batch 205 batch_loss: 0.16162794828414917\n",
      "training: 29 batch 206 batch_loss: 0.16303479671478271\n",
      "training: 29 batch 207 batch_loss: 0.16088202595710754\n",
      "training: 29 batch 208 batch_loss: 0.1613624095916748\n",
      "training: 29 batch 209 batch_loss: 0.1614503264427185\n",
      "training: 29 batch 210 batch_loss: 0.1581188440322876\n",
      "training: 29 batch 211 batch_loss: 0.16070488095283508\n",
      "training: 29 batch 212 batch_loss: 0.15986526012420654\n",
      "training: 29 batch 213 batch_loss: 0.16223806142807007\n",
      "training: 29 batch 214 batch_loss: 0.15700596570968628\n",
      "training: 29 batch 215 batch_loss: 0.1580468714237213\n",
      "training: 29 batch 216 batch_loss: 0.16019922494888306\n",
      "training: 29 batch 217 batch_loss: 0.15705090761184692\n",
      "training: 29 batch 218 batch_loss: 0.15776410698890686\n",
      "training: 29 batch 219 batch_loss: 0.15950796008110046\n",
      "training: 29 batch 220 batch_loss: 0.15970131754875183\n",
      "training: 29 batch 221 batch_loss: 0.1595383882522583\n",
      "training: 29 batch 222 batch_loss: 0.1612515151500702\n",
      "training: 29 batch 223 batch_loss: 0.1618851125240326\n",
      "training: 29 batch 224 batch_loss: 0.1605369746685028\n",
      "training: 29 batch 225 batch_loss: 0.1581735908985138\n",
      "training: 29 batch 226 batch_loss: 0.16229817271232605\n",
      "training: 29 batch 227 batch_loss: 0.164946049451828\n",
      "training: 29 batch 228 batch_loss: 0.15863987803459167\n",
      "training: 29 batch 229 batch_loss: 0.15930959582328796\n",
      "training: 29 batch 230 batch_loss: 0.1571137011051178\n",
      "training: 29 batch 231 batch_loss: 0.16159403324127197\n",
      "training: 29 batch 232 batch_loss: 0.16158199310302734\n",
      "training: 29 batch 233 batch_loss: 0.16035088896751404\n",
      "training: 29 batch 234 batch_loss: 0.16255879402160645\n",
      "training: 29 batch 235 batch_loss: 0.15775969624519348\n",
      "training: 29 batch 236 batch_loss: 0.16155439615249634\n",
      "training: 29 batch 237 batch_loss: 0.1590726375579834\n",
      "training: 29 batch 238 batch_loss: 0.15702807903289795\n",
      "training: 29 batch 239 batch_loss: 0.1560060977935791\n",
      "training: 29 batch 240 batch_loss: 0.15877193212509155\n",
      "training: 29 batch 241 batch_loss: 0.1620873212814331\n",
      "training: 29 batch 242 batch_loss: 0.16060376167297363\n",
      "training: 29 batch 243 batch_loss: 0.15962707996368408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 29 batch 244 batch_loss: 0.161685049533844\n",
      "training: 29 batch 245 batch_loss: 0.1621500849723816\n",
      "training: 29 batch 246 batch_loss: 0.1621108055114746\n",
      "training: 29 batch 247 batch_loss: 0.16320180892944336\n",
      "training: 29 batch 248 batch_loss: 0.15829414129257202\n",
      "training: 29 batch 249 batch_loss: 0.16077396273612976\n",
      "training: 29 batch 250 batch_loss: 0.1593700349330902\n",
      "training: 29 batch 251 batch_loss: 0.16333141922950745\n",
      "training: 29 batch 252 batch_loss: 0.15734317898750305\n",
      "training: 29 batch 253 batch_loss: 0.1599435806274414\n",
      "training: 29 batch 254 batch_loss: 0.1624930202960968\n",
      "training: 29 batch 255 batch_loss: 0.1588558852672577\n",
      "training: 29 batch 256 batch_loss: 0.16214552521705627\n",
      "training: 29 batch 257 batch_loss: 0.16276240348815918\n",
      "training: 29 batch 258 batch_loss: 0.16184687614440918\n",
      "training: 29 batch 259 batch_loss: 0.15608298778533936\n",
      "training: 29 batch 260 batch_loss: 0.15933331847190857\n",
      "training: 29 batch 261 batch_loss: 0.15896904468536377\n",
      "training: 29 batch 262 batch_loss: 0.1608096957206726\n",
      "training: 29 batch 263 batch_loss: 0.16309523582458496\n",
      "training: 29 batch 264 batch_loss: 0.16093873977661133\n",
      "training: 29 batch 265 batch_loss: 0.1670435667037964\n",
      "training: 29 batch 266 batch_loss: 0.15970945358276367\n",
      "training: 29 batch 267 batch_loss: 0.15976810455322266\n",
      "training: 29 batch 268 batch_loss: 0.15912142395973206\n",
      "training: 29 batch 269 batch_loss: 0.15872818231582642\n",
      "training: 29 batch 270 batch_loss: 0.15974152088165283\n",
      "training: 29 batch 271 batch_loss: 0.15939688682556152\n",
      "training: 29 batch 272 batch_loss: 0.15757206082344055\n",
      "training: 29 batch 273 batch_loss: 0.16038617491722107\n",
      "training: 29 batch 274 batch_loss: 0.1593863070011139\n",
      "training: 29 batch 275 batch_loss: 0.1595975160598755\n",
      "training: 29 batch 276 batch_loss: 0.15747123956680298\n",
      "training: 29 batch 277 batch_loss: 0.16461491584777832\n",
      "training: 29 batch 278 batch_loss: 0.15859472751617432\n",
      "training: 29 batch 279 batch_loss: 0.16179734468460083\n",
      "training: 29 batch 280 batch_loss: 0.16319531202316284\n",
      "training: 29 batch 281 batch_loss: 0.15972676873207092\n",
      "training: 29 batch 282 batch_loss: 0.1631801426410675\n",
      "training: 29 batch 283 batch_loss: 0.16159695386886597\n",
      "training: 29 batch 284 batch_loss: 0.1616593599319458\n",
      "training: 29 batch 285 batch_loss: 0.16214889287948608\n",
      "training: 29 batch 286 batch_loss: 0.16300955414772034\n",
      "training: 29 batch 287 batch_loss: 0.16123464703559875\n",
      "training: 29 batch 288 batch_loss: 0.15876755118370056\n",
      "training: 29 batch 289 batch_loss: 0.16055351495742798\n",
      "training: 29 batch 290 batch_loss: 0.160437673330307\n",
      "training: 29 batch 291 batch_loss: 0.15899574756622314\n",
      "training: 29 batch 292 batch_loss: 0.16167426109313965\n",
      "training: 29 batch 293 batch_loss: 0.16133838891983032\n",
      "training: 29 batch 294 batch_loss: 0.16329440474510193\n",
      "training: 29 batch 295 batch_loss: 0.1647556722164154\n",
      "training: 29 batch 296 batch_loss: 0.16043975949287415\n",
      "training: 29 batch 297 batch_loss: 0.1607533097267151\n",
      "training: 29 batch 298 batch_loss: 0.16476568579673767\n",
      "training: 29 batch 299 batch_loss: 0.16271698474884033\n",
      "training: 29 batch 300 batch_loss: 0.1634092926979065\n",
      "training: 29 batch 301 batch_loss: 0.16288965940475464\n",
      "training: 29 batch 302 batch_loss: 0.16154253482818604\n",
      "training: 29 batch 303 batch_loss: 0.15922290086746216\n",
      "training: 29 batch 304 batch_loss: 0.16316351294517517\n",
      "training: 29 batch 305 batch_loss: 0.1611970067024231\n",
      "training: 29 batch 306 batch_loss: 0.16061949729919434\n",
      "training: 29 batch 307 batch_loss: 0.16333112120628357\n",
      "training: 29 batch 308 batch_loss: 0.15723299980163574\n",
      "training: 29 batch 309 batch_loss: 0.16413792967796326\n",
      "training: 29 batch 310 batch_loss: 0.16049620509147644\n",
      "training: 29 batch 311 batch_loss: 0.16106870770454407\n",
      "training: 29 batch 312 batch_loss: 0.16236037015914917\n",
      "training: 29 batch 313 batch_loss: 0.16288474202156067\n",
      "training: 29 batch 314 batch_loss: 0.1645771861076355\n",
      "training: 29 batch 315 batch_loss: 0.16092070937156677\n",
      "training: 29 batch 316 batch_loss: 0.16047021746635437\n",
      "training: 29 batch 317 batch_loss: 0.16148385405540466\n",
      "training: 29 batch 318 batch_loss: 0.16099900007247925\n",
      "training: 29 batch 319 batch_loss: 0.15854796767234802\n",
      "training: 29 batch 320 batch_loss: 0.16076073050498962\n",
      "training: 29 batch 321 batch_loss: 0.16201171278953552\n",
      "training: 29 batch 322 batch_loss: 0.1628609299659729\n",
      "training: 29 batch 323 batch_loss: 0.15857219696044922\n",
      "training: 29 batch 324 batch_loss: 0.1632704734802246\n",
      "training: 29 batch 325 batch_loss: 0.16012555360794067\n",
      "training: 29 batch 326 batch_loss: 0.1608404815196991\n",
      "training: 29 batch 327 batch_loss: 0.161831796169281\n",
      "training: 29 batch 328 batch_loss: 0.158613383769989\n",
      "training: 29 batch 329 batch_loss: 0.16159525513648987\n",
      "training: 29 batch 330 batch_loss: 0.16187965869903564\n",
      "training: 29 batch 331 batch_loss: 0.16184252500534058\n",
      "training: 29 batch 332 batch_loss: 0.15985029935836792\n",
      "training: 29 batch 333 batch_loss: 0.1610564887523651\n",
      "training: 29 batch 334 batch_loss: 0.15472543239593506\n",
      "training: 29 batch 335 batch_loss: 0.16084721684455872\n",
      "training: 29 batch 336 batch_loss: 0.16297826170921326\n",
      "training: 29 batch 337 batch_loss: 0.16030815243721008\n",
      "training: 29 batch 338 batch_loss: 0.1585424840450287\n",
      "training: 29 batch 339 batch_loss: 0.15805548429489136\n",
      "training: 29 batch 340 batch_loss: 0.16216593980789185\n",
      "training: 29 batch 341 batch_loss: 0.16368496417999268\n",
      "training: 29 batch 342 batch_loss: 0.160244882106781\n",
      "training: 29 batch 343 batch_loss: 0.16327744722366333\n",
      "training: 29 batch 344 batch_loss: 0.15952253341674805\n",
      "training: 29 batch 345 batch_loss: 0.15743103623390198\n",
      "training: 29 batch 346 batch_loss: 0.15905162692070007\n",
      "training: 29 batch 347 batch_loss: 0.16155603528022766\n",
      "training: 29 batch 348 batch_loss: 0.1592828333377838\n",
      "training: 29 batch 349 batch_loss: 0.155535489320755\n",
      "training: 29 batch 350 batch_loss: 0.1617276668548584\n",
      "training: 29 batch 351 batch_loss: 0.16192921996116638\n",
      "training: 29 batch 352 batch_loss: 0.16064944863319397\n",
      "training: 29 batch 353 batch_loss: 0.15890070796012878\n",
      "training: 29 batch 354 batch_loss: 0.16367608308792114\n",
      "training: 29 batch 355 batch_loss: 0.15872597694396973\n",
      "training: 29 batch 356 batch_loss: 0.16225600242614746\n",
      "training: 29 batch 357 batch_loss: 0.1621030569076538\n",
      "training: 29 batch 358 batch_loss: 0.1611168086528778\n",
      "training: 29 batch 359 batch_loss: 0.1602221131324768\n",
      "training: 29 batch 360 batch_loss: 0.16193145513534546\n",
      "training: 29 batch 361 batch_loss: 0.16383329033851624\n",
      "training: 29 batch 362 batch_loss: 0.16257303953170776\n",
      "training: 29 batch 363 batch_loss: 0.15996652841567993\n",
      "training: 29 batch 364 batch_loss: 0.1620655357837677\n",
      "training: 29 batch 365 batch_loss: 0.16147065162658691\n",
      "training: 29 batch 366 batch_loss: 0.15971389412879944\n",
      "training: 29 batch 367 batch_loss: 0.15923252701759338\n",
      "training: 29 batch 368 batch_loss: 0.16167598962783813\n",
      "training: 29 batch 369 batch_loss: 0.1630299687385559\n",
      "training: 29 batch 370 batch_loss: 0.16064363718032837\n",
      "training: 29 batch 371 batch_loss: 0.1613663136959076\n",
      "training: 29 batch 372 batch_loss: 0.16368338465690613\n",
      "training: 29 batch 373 batch_loss: 0.1623496413230896\n",
      "training: 29 batch 374 batch_loss: 0.16017359495162964\n",
      "training: 29 batch 375 batch_loss: 0.15830910205841064\n",
      "training: 29 batch 376 batch_loss: 0.1570991575717926\n",
      "training: 29 batch 377 batch_loss: 0.1604084074497223\n",
      "training: 29 batch 378 batch_loss: 0.16292408108711243\n",
      "training: 29 batch 379 batch_loss: 0.15946292877197266\n",
      "training: 29 batch 380 batch_loss: 0.15935099124908447\n",
      "training: 29 batch 381 batch_loss: 0.16266879439353943\n",
      "training: 29 batch 382 batch_loss: 0.16106346249580383\n",
      "training: 29 batch 383 batch_loss: 0.16141024231910706\n",
      "training: 29 batch 384 batch_loss: 0.16049158573150635\n",
      "training: 29 batch 385 batch_loss: 0.16209161281585693\n",
      "training: 29 batch 386 batch_loss: 0.1624753177165985\n",
      "training: 29 batch 387 batch_loss: 0.16163548827171326\n",
      "training: 29 batch 388 batch_loss: 0.1618388593196869\n",
      "training: 29 batch 389 batch_loss: 0.1604321002960205\n",
      "training: 29 batch 390 batch_loss: 0.1628991961479187\n",
      "training: 29 batch 391 batch_loss: 0.15864887833595276\n",
      "training: 29 batch 392 batch_loss: 0.1585984230041504\n",
      "training: 29 batch 393 batch_loss: 0.16669899225234985\n",
      "training: 29 batch 394 batch_loss: 0.16229432821273804\n",
      "training: 29 batch 395 batch_loss: 0.16329345107078552\n",
      "training: 29 batch 396 batch_loss: 0.1612720787525177\n",
      "training: 29 batch 397 batch_loss: 0.1629207730293274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 29 batch 398 batch_loss: 0.15733814239501953\n",
      "training: 29 batch 399 batch_loss: 0.161522775888443\n",
      "training: 29 batch 400 batch_loss: 0.16272896528244019\n",
      "training: 29 batch 401 batch_loss: 0.16110137104988098\n",
      "training: 29 batch 402 batch_loss: 0.1629789173603058\n",
      "training: 29 batch 403 batch_loss: 0.16170307993888855\n",
      "training: 29 batch 404 batch_loss: 0.16146665811538696\n",
      "training: 29 batch 405 batch_loss: 0.16291645169258118\n",
      "training: 29 batch 406 batch_loss: 0.161848247051239\n",
      "training: 29 batch 407 batch_loss: 0.16153886914253235\n",
      "training: 29 batch 408 batch_loss: 0.15944379568099976\n",
      "training: 29 batch 409 batch_loss: 0.1622164249420166\n",
      "training: 29 batch 410 batch_loss: 0.1594175100326538\n",
      "training: 29 batch 411 batch_loss: 0.16368752717971802\n",
      "training: 29 batch 412 batch_loss: 0.1655924916267395\n",
      "training: 29 batch 413 batch_loss: 0.16238951683044434\n",
      "training: 29 batch 414 batch_loss: 0.16295066475868225\n",
      "training: 29 batch 415 batch_loss: 0.16224676370620728\n",
      "training: 29 batch 416 batch_loss: 0.16285663843154907\n",
      "training: 29 batch 417 batch_loss: 0.16375157237052917\n",
      "training: 29 batch 418 batch_loss: 0.1634092926979065\n",
      "training: 29 batch 419 batch_loss: 0.16023021936416626\n",
      "training: 29 batch 420 batch_loss: 0.1624869704246521\n",
      "training: 29 batch 421 batch_loss: 0.1623939573764801\n",
      "training: 29 batch 422 batch_loss: 0.15839922428131104\n",
      "training: 29 batch 423 batch_loss: 0.16076156497001648\n",
      "training: 29 batch 424 batch_loss: 0.15963172912597656\n",
      "training: 29 batch 425 batch_loss: 0.16269826889038086\n",
      "training: 29 batch 426 batch_loss: 0.1594790816307068\n",
      "training: 29 batch 427 batch_loss: 0.16017740964889526\n",
      "training: 29 batch 428 batch_loss: 0.16262730956077576\n",
      "training: 29 batch 429 batch_loss: 0.16099581122398376\n",
      "training: 29 batch 430 batch_loss: 0.1612132489681244\n",
      "training: 29 batch 431 batch_loss: 0.15879464149475098\n",
      "training: 29 batch 432 batch_loss: 0.16041123867034912\n",
      "training: 29 batch 433 batch_loss: 0.15999865531921387\n",
      "training: 29 batch 434 batch_loss: 0.16302314400672913\n",
      "training: 29 batch 435 batch_loss: 0.16222542524337769\n",
      "training: 29 batch 436 batch_loss: 0.16295355558395386\n",
      "training: 29 batch 437 batch_loss: 0.16174444556236267\n",
      "training: 29 batch 438 batch_loss: 0.16176968812942505\n",
      "training: 29 batch 439 batch_loss: 0.16454851627349854\n",
      "training: 29 batch 440 batch_loss: 0.1619049310684204\n",
      "training: 29 batch 441 batch_loss: 0.15961501002311707\n",
      "training: 29 batch 442 batch_loss: 0.1652446687221527\n",
      "training: 29 batch 443 batch_loss: 0.16234907507896423\n",
      "training: 29 batch 444 batch_loss: 0.16259852051734924\n",
      "training: 29 batch 445 batch_loss: 0.16411814093589783\n",
      "training: 29 batch 446 batch_loss: 0.1644974946975708\n",
      "training: 29 batch 447 batch_loss: 0.16048651933670044\n",
      "training: 29 batch 448 batch_loss: 0.1644546091556549\n",
      "training: 29 batch 449 batch_loss: 0.1624051332473755\n",
      "training: 29 batch 450 batch_loss: 0.1620568335056305\n",
      "training: 29 batch 451 batch_loss: 0.16447368264198303\n",
      "training: 29 batch 452 batch_loss: 0.15744951367378235\n",
      "training: 29 batch 453 batch_loss: 0.16405454277992249\n",
      "training: 29 batch 454 batch_loss: 0.16001960635185242\n",
      "training: 29 batch 455 batch_loss: 0.16134420037269592\n",
      "training: 29 batch 456 batch_loss: 0.16151273250579834\n",
      "training: 29 batch 457 batch_loss: 0.15784808993339539\n",
      "training: 29 batch 458 batch_loss: 0.16282805800437927\n",
      "training: 29 batch 459 batch_loss: 0.16030186414718628\n",
      "training: 29 batch 460 batch_loss: 0.16254723072052002\n",
      "training: 29 batch 461 batch_loss: 0.16286617517471313\n",
      "training: 29 batch 462 batch_loss: 0.16309994459152222\n",
      "training: 29 batch 463 batch_loss: 0.16434788703918457\n",
      "training: 29 batch 464 batch_loss: 0.160902202129364\n",
      "training: 29 batch 465 batch_loss: 0.16151359677314758\n",
      "training: 29 batch 466 batch_loss: 0.16329234838485718\n",
      "training: 29 batch 467 batch_loss: 0.16123828291893005\n",
      "training: 29 batch 468 batch_loss: 0.16194111108779907\n",
      "training: 29 batch 469 batch_loss: 0.15811383724212646\n",
      "training: 29 batch 470 batch_loss: 0.16183149814605713\n",
      "training: 29 batch 471 batch_loss: 0.1626657247543335\n",
      "training: 29 batch 472 batch_loss: 0.16377973556518555\n",
      "training: 29 batch 473 batch_loss: 0.16019654273986816\n",
      "training: 29 batch 474 batch_loss: 0.1570107340812683\n",
      "training: 29 batch 475 batch_loss: 0.15845051407814026\n",
      "training: 29 batch 476 batch_loss: 0.1616763174533844\n",
      "training: 29 batch 477 batch_loss: 0.1648501455783844\n",
      "training: 29 batch 478 batch_loss: 0.1584400236606598\n",
      "training: 29 batch 479 batch_loss: 0.1623804271221161\n",
      "training: 29 batch 480 batch_loss: 0.16205748915672302\n",
      "training: 29 batch 481 batch_loss: 0.1609368622303009\n",
      "training: 29 batch 482 batch_loss: 0.15925142168998718\n",
      "training: 29 batch 483 batch_loss: 0.16138267517089844\n",
      "training: 29 batch 484 batch_loss: 0.16312333941459656\n",
      "training: 29 batch 485 batch_loss: 0.1615174114704132\n",
      "training: 29 batch 486 batch_loss: 0.16232270002365112\n",
      "training: 29 batch 487 batch_loss: 0.16151201725006104\n",
      "training: 29 batch 488 batch_loss: 0.159224271774292\n",
      "training: 29 batch 489 batch_loss: 0.16298508644104004\n",
      "training: 29 batch 490 batch_loss: 0.1616508960723877\n",
      "training: 29 batch 491 batch_loss: 0.16255807876586914\n",
      "training: 29 batch 492 batch_loss: 0.158390074968338\n",
      "training: 29 batch 493 batch_loss: 0.16355976462364197\n",
      "training: 29 batch 494 batch_loss: 0.16149765253067017\n",
      "training: 29 batch 495 batch_loss: 0.16637185215950012\n",
      "training: 29 batch 496 batch_loss: 0.16188639402389526\n",
      "training: 29 batch 497 batch_loss: 0.16498562693595886\n",
      "training: 29 batch 498 batch_loss: 0.16037315130233765\n",
      "training: 29 batch 499 batch_loss: 0.16562196612358093\n",
      "training: 29 batch 500 batch_loss: 0.1606360673904419\n",
      "training: 29 batch 501 batch_loss: 0.15951308608055115\n",
      "training: 29 batch 502 batch_loss: 0.16136696934700012\n",
      "training: 29 batch 503 batch_loss: 0.1598571240901947\n",
      "training: 29 batch 504 batch_loss: 0.16230711340904236\n",
      "training: 29 batch 505 batch_loss: 0.16348040103912354\n",
      "training: 29 batch 506 batch_loss: 0.160910964012146\n",
      "training: 29 batch 507 batch_loss: 0.15974175930023193\n",
      "training: 29 batch 508 batch_loss: 0.16518735885620117\n",
      "training: 29 batch 509 batch_loss: 0.15886929631233215\n",
      "training: 29 batch 510 batch_loss: 0.16205304861068726\n",
      "training: 29 batch 511 batch_loss: 0.16356584429740906\n",
      "training: 29 batch 512 batch_loss: 0.1641715168952942\n",
      "training: 29 batch 513 batch_loss: 0.16304552555084229\n",
      "training: 29 batch 514 batch_loss: 0.16122189164161682\n",
      "training: 29 batch 515 batch_loss: 0.1635456681251526\n",
      "training: 29 batch 516 batch_loss: 0.15980926156044006\n",
      "training: 29 batch 517 batch_loss: 0.16210395097732544\n",
      "training: 29 batch 518 batch_loss: 0.16149136424064636\n",
      "training: 29 batch 519 batch_loss: 0.16226211190223694\n",
      "training: 29 batch 520 batch_loss: 0.16214153170585632\n",
      "training: 29 batch 521 batch_loss: 0.16163146495819092\n",
      "training: 29 batch 522 batch_loss: 0.16157189011573792\n",
      "training: 29 batch 523 batch_loss: 0.15977609157562256\n",
      "training: 29 batch 524 batch_loss: 0.15821439027786255\n",
      "training: 29 batch 525 batch_loss: 0.1595269739627838\n",
      "training: 29 batch 526 batch_loss: 0.1607242226600647\n",
      "training: 29 batch 527 batch_loss: 0.16212207078933716\n",
      "training: 29 batch 528 batch_loss: 0.16483676433563232\n",
      "training: 29 batch 529 batch_loss: 0.16586515307426453\n",
      "training: 29 batch 530 batch_loss: 0.16083014011383057\n",
      "training: 29 batch 531 batch_loss: 0.16205036640167236\n",
      "training: 29 batch 532 batch_loss: 0.1627964973449707\n",
      "training: 29 batch 533 batch_loss: 0.16190680861473083\n",
      "training: 29 batch 534 batch_loss: 0.16481491923332214\n",
      "training: 29 batch 535 batch_loss: 0.16241714358329773\n",
      "training: 29 batch 536 batch_loss: 0.15837809443473816\n",
      "training: 29 batch 537 batch_loss: 0.16185897588729858\n",
      "training: 29 batch 538 batch_loss: 0.1639271378517151\n",
      "training: 29 batch 539 batch_loss: 0.15865880250930786\n",
      "training: 29 batch 540 batch_loss: 0.1619366705417633\n",
      "training: 29 batch 541 batch_loss: 0.15866002440452576\n",
      "training: 29 batch 542 batch_loss: 0.1611848771572113\n",
      "training: 29 batch 543 batch_loss: 0.16302970051765442\n",
      "training: 29 batch 544 batch_loss: 0.16432854533195496\n",
      "training: 29 batch 545 batch_loss: 0.16415798664093018\n",
      "training: 29 batch 546 batch_loss: 0.16698932647705078\n",
      "training: 29 batch 547 batch_loss: 0.16307508945465088\n",
      "training: 29 batch 548 batch_loss: 0.16198769211769104\n",
      "training: 29 batch 549 batch_loss: 0.16530919075012207\n",
      "training: 29 batch 550 batch_loss: 0.16192829608917236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 29 batch 551 batch_loss: 0.16373920440673828\n",
      "training: 29 batch 552 batch_loss: 0.16342490911483765\n",
      "training: 29 batch 553 batch_loss: 0.15872693061828613\n",
      "training: 29 batch 554 batch_loss: 0.16310611367225647\n",
      "training: 29 batch 555 batch_loss: 0.16259264945983887\n",
      "training: 29 batch 556 batch_loss: 0.1624777913093567\n",
      "training: 29 batch 557 batch_loss: 0.161304771900177\n",
      "training: 29 batch 558 batch_loss: 0.16005387902259827\n",
      "training: 29 batch 559 batch_loss: 0.16311579942703247\n",
      "training: 29 batch 560 batch_loss: 0.15871483087539673\n",
      "training: 29 batch 561 batch_loss: 0.1591191589832306\n",
      "training: 29 batch 562 batch_loss: 0.16186892986297607\n",
      "training: 29 batch 563 batch_loss: 0.1642690896987915\n",
      "training: 29 batch 564 batch_loss: 0.16313576698303223\n",
      "training: 29 batch 565 batch_loss: 0.16122281551361084\n",
      "training: 29 batch 566 batch_loss: 0.1680532991886139\n",
      "training: 29 batch 567 batch_loss: 0.1602858603000641\n",
      "training: 29 batch 568 batch_loss: 0.1639997959136963\n",
      "training: 29 batch 569 batch_loss: 0.1667374074459076\n",
      "training: 29 batch 570 batch_loss: 0.16204798221588135\n",
      "training: 29 batch 571 batch_loss: 0.16407787799835205\n",
      "training: 29 batch 572 batch_loss: 0.1612013280391693\n",
      "training: 29 batch 573 batch_loss: 0.16216593980789185\n",
      "training: 29 batch 574 batch_loss: 0.16245990991592407\n",
      "training: 29 batch 575 batch_loss: 0.16165539622306824\n",
      "training: 29 batch 576 batch_loss: 0.16578775644302368\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 29, Hit Ratio:0.029750860279886607 | Precision:0.04389560601592451 | Recall:0.05920231395963654 | NDCG:0.05607279909224679\n",
      "*Best Performance* \n",
      "Epoch: 29, Hit Ratio:0.029750860279886607 | Precision:0.04389560601592451 | Recall:0.05920231395963654 | MDCG:0.05607279909224679\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 30 batch 0 batch_loss: 0.16220182180404663\n",
      "training: 30 batch 1 batch_loss: 0.15900719165802002\n",
      "training: 30 batch 2 batch_loss: 0.15939435362815857\n",
      "training: 30 batch 3 batch_loss: 0.16151797771453857\n",
      "training: 30 batch 4 batch_loss: 0.15999066829681396\n",
      "training: 30 batch 5 batch_loss: 0.16191533207893372\n",
      "training: 30 batch 6 batch_loss: 0.16312721371650696\n",
      "training: 30 batch 7 batch_loss: 0.15689575672149658\n",
      "training: 30 batch 8 batch_loss: 0.15935194492340088\n",
      "training: 30 batch 9 batch_loss: 0.16046860814094543\n",
      "training: 30 batch 10 batch_loss: 0.15953665971755981\n",
      "training: 30 batch 11 batch_loss: 0.1601528823375702\n",
      "training: 30 batch 12 batch_loss: 0.16095644235610962\n",
      "training: 30 batch 13 batch_loss: 0.16110235452651978\n",
      "training: 30 batch 14 batch_loss: 0.16021481156349182\n",
      "training: 30 batch 15 batch_loss: 0.16053038835525513\n",
      "training: 30 batch 16 batch_loss: 0.1605854630470276\n",
      "training: 30 batch 17 batch_loss: 0.16232779622077942\n",
      "training: 30 batch 18 batch_loss: 0.16051062941551208\n",
      "training: 30 batch 19 batch_loss: 0.16368776559829712\n",
      "training: 30 batch 20 batch_loss: 0.1615717113018036\n",
      "training: 30 batch 21 batch_loss: 0.1606418490409851\n",
      "training: 30 batch 22 batch_loss: 0.16082099080085754\n",
      "training: 30 batch 23 batch_loss: 0.16414234042167664\n",
      "training: 30 batch 24 batch_loss: 0.16066446900367737\n",
      "training: 30 batch 25 batch_loss: 0.1627252697944641\n",
      "training: 30 batch 26 batch_loss: 0.16090402007102966\n",
      "training: 30 batch 27 batch_loss: 0.15908929705619812\n",
      "training: 30 batch 28 batch_loss: 0.16048979759216309\n",
      "training: 30 batch 29 batch_loss: 0.1587848961353302\n",
      "training: 30 batch 30 batch_loss: 0.1593170166015625\n",
      "training: 30 batch 31 batch_loss: 0.16245931386947632\n",
      "training: 30 batch 32 batch_loss: 0.16147738695144653\n",
      "training: 30 batch 33 batch_loss: 0.16275525093078613\n",
      "training: 30 batch 34 batch_loss: 0.16047489643096924\n",
      "training: 30 batch 35 batch_loss: 0.1618565022945404\n",
      "training: 30 batch 36 batch_loss: 0.16168656945228577\n",
      "training: 30 batch 37 batch_loss: 0.1606273055076599\n",
      "training: 30 batch 38 batch_loss: 0.16237425804138184\n",
      "training: 30 batch 39 batch_loss: 0.15790051221847534\n",
      "training: 30 batch 40 batch_loss: 0.16134512424468994\n",
      "training: 30 batch 41 batch_loss: 0.1597687005996704\n",
      "training: 30 batch 42 batch_loss: 0.16104602813720703\n",
      "training: 30 batch 43 batch_loss: 0.1638794243335724\n",
      "training: 30 batch 44 batch_loss: 0.16265881061553955\n",
      "training: 30 batch 45 batch_loss: 0.15750336647033691\n",
      "training: 30 batch 46 batch_loss: 0.1590161919593811\n",
      "training: 30 batch 47 batch_loss: 0.16077524423599243\n",
      "training: 30 batch 48 batch_loss: 0.160878986120224\n",
      "training: 30 batch 49 batch_loss: 0.1613810956478119\n",
      "training: 30 batch 50 batch_loss: 0.15784725546836853\n",
      "training: 30 batch 51 batch_loss: 0.16166892647743225\n",
      "training: 30 batch 52 batch_loss: 0.16077402234077454\n",
      "training: 30 batch 53 batch_loss: 0.16134095191955566\n",
      "training: 30 batch 54 batch_loss: 0.16078323125839233\n",
      "training: 30 batch 55 batch_loss: 0.15745028853416443\n",
      "training: 30 batch 56 batch_loss: 0.15854156017303467\n",
      "training: 30 batch 57 batch_loss: 0.1601618230342865\n",
      "training: 30 batch 58 batch_loss: 0.16445112228393555\n",
      "training: 30 batch 59 batch_loss: 0.16222113370895386\n",
      "training: 30 batch 60 batch_loss: 0.15939772129058838\n",
      "training: 30 batch 61 batch_loss: 0.163752019405365\n",
      "training: 30 batch 62 batch_loss: 0.15806874632835388\n",
      "training: 30 batch 63 batch_loss: 0.15770870447158813\n",
      "training: 30 batch 64 batch_loss: 0.15919649600982666\n",
      "training: 30 batch 65 batch_loss: 0.16100344061851501\n",
      "training: 30 batch 66 batch_loss: 0.16012811660766602\n",
      "training: 30 batch 67 batch_loss: 0.1630399525165558\n",
      "training: 30 batch 68 batch_loss: 0.16369035840034485\n",
      "training: 30 batch 69 batch_loss: 0.16210833191871643\n",
      "training: 30 batch 70 batch_loss: 0.15884682536125183\n",
      "training: 30 batch 71 batch_loss: 0.16284143924713135\n",
      "training: 30 batch 72 batch_loss: 0.16146299242973328\n",
      "training: 30 batch 73 batch_loss: 0.15893396735191345\n",
      "training: 30 batch 74 batch_loss: 0.1607174277305603\n",
      "training: 30 batch 75 batch_loss: 0.15984171628952026\n",
      "training: 30 batch 76 batch_loss: 0.1610839068889618\n",
      "training: 30 batch 77 batch_loss: 0.1612522006034851\n",
      "training: 30 batch 78 batch_loss: 0.16315507888793945\n",
      "training: 30 batch 79 batch_loss: 0.1636882722377777\n",
      "training: 30 batch 80 batch_loss: 0.1587110161781311\n",
      "training: 30 batch 81 batch_loss: 0.16031846404075623\n",
      "training: 30 batch 82 batch_loss: 0.16211870312690735\n",
      "training: 30 batch 83 batch_loss: 0.160625159740448\n",
      "training: 30 batch 84 batch_loss: 0.16065457463264465\n",
      "training: 30 batch 85 batch_loss: 0.1622435450553894\n",
      "training: 30 batch 86 batch_loss: 0.16239184141159058\n",
      "training: 30 batch 87 batch_loss: 0.16076689958572388\n",
      "training: 30 batch 88 batch_loss: 0.16068559885025024\n",
      "training: 30 batch 89 batch_loss: 0.16150814294815063\n",
      "training: 30 batch 90 batch_loss: 0.1631787121295929\n",
      "training: 30 batch 91 batch_loss: 0.15984201431274414\n",
      "training: 30 batch 92 batch_loss: 0.16283079981803894\n",
      "training: 30 batch 93 batch_loss: 0.1589255928993225\n",
      "training: 30 batch 94 batch_loss: 0.16074353456497192\n",
      "training: 30 batch 95 batch_loss: 0.15820711851119995\n",
      "training: 30 batch 96 batch_loss: 0.16407731175422668\n",
      "training: 30 batch 97 batch_loss: 0.15916329622268677\n",
      "training: 30 batch 98 batch_loss: 0.16221866011619568\n",
      "training: 30 batch 99 batch_loss: 0.15931373834609985\n",
      "training: 30 batch 100 batch_loss: 0.15975067019462585\n",
      "training: 30 batch 101 batch_loss: 0.16261941194534302\n",
      "training: 30 batch 102 batch_loss: 0.16427159309387207\n",
      "training: 30 batch 103 batch_loss: 0.16258105635643005\n",
      "training: 30 batch 104 batch_loss: 0.15999835729599\n",
      "training: 30 batch 105 batch_loss: 0.1596698760986328\n",
      "training: 30 batch 106 batch_loss: 0.15785250067710876\n",
      "training: 30 batch 107 batch_loss: 0.15453806519508362\n",
      "training: 30 batch 108 batch_loss: 0.16073763370513916\n",
      "training: 30 batch 109 batch_loss: 0.16147804260253906\n",
      "training: 30 batch 110 batch_loss: 0.15847092866897583\n",
      "training: 30 batch 111 batch_loss: 0.16130635142326355\n",
      "training: 30 batch 112 batch_loss: 0.16016793251037598\n",
      "training: 30 batch 113 batch_loss: 0.1590258777141571\n",
      "training: 30 batch 114 batch_loss: 0.16188770532608032\n",
      "training: 30 batch 115 batch_loss: 0.15962588787078857\n",
      "training: 30 batch 116 batch_loss: 0.16064894199371338\n",
      "training: 30 batch 117 batch_loss: 0.15912657976150513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 30 batch 118 batch_loss: 0.16373991966247559\n",
      "training: 30 batch 119 batch_loss: 0.16461080312728882\n",
      "training: 30 batch 120 batch_loss: 0.16002097725868225\n",
      "training: 30 batch 121 batch_loss: 0.16067811846733093\n",
      "training: 30 batch 122 batch_loss: 0.16440963745117188\n",
      "training: 30 batch 123 batch_loss: 0.15982869267463684\n",
      "training: 30 batch 124 batch_loss: 0.16153505444526672\n",
      "training: 30 batch 125 batch_loss: 0.16169732809066772\n",
      "training: 30 batch 126 batch_loss: 0.16043445467948914\n",
      "training: 30 batch 127 batch_loss: 0.15959995985031128\n",
      "training: 30 batch 128 batch_loss: 0.16339203715324402\n",
      "training: 30 batch 129 batch_loss: 0.15756607055664062\n",
      "training: 30 batch 130 batch_loss: 0.16065019369125366\n",
      "training: 30 batch 131 batch_loss: 0.16249015927314758\n",
      "training: 30 batch 132 batch_loss: 0.1609210968017578\n",
      "training: 30 batch 133 batch_loss: 0.16272199153900146\n",
      "training: 30 batch 134 batch_loss: 0.16096794605255127\n",
      "training: 30 batch 135 batch_loss: 0.15731331706047058\n",
      "training: 30 batch 136 batch_loss: 0.1620822548866272\n",
      "training: 30 batch 137 batch_loss: 0.15798264741897583\n",
      "training: 30 batch 138 batch_loss: 0.15910747647285461\n",
      "training: 30 batch 139 batch_loss: 0.16092395782470703\n",
      "training: 30 batch 140 batch_loss: 0.15893396735191345\n",
      "training: 30 batch 141 batch_loss: 0.16379159688949585\n",
      "training: 30 batch 142 batch_loss: 0.15963131189346313\n",
      "training: 30 batch 143 batch_loss: 0.16153189539909363\n",
      "training: 30 batch 144 batch_loss: 0.16017228364944458\n",
      "training: 30 batch 145 batch_loss: 0.1632479429244995\n",
      "training: 30 batch 146 batch_loss: 0.16154205799102783\n",
      "training: 30 batch 147 batch_loss: 0.15912610292434692\n",
      "training: 30 batch 148 batch_loss: 0.16170525550842285\n",
      "training: 30 batch 149 batch_loss: 0.16520118713378906\n",
      "training: 30 batch 150 batch_loss: 0.1632072925567627\n",
      "training: 30 batch 151 batch_loss: 0.16247856616973877\n",
      "training: 30 batch 152 batch_loss: 0.15665757656097412\n",
      "training: 30 batch 153 batch_loss: 0.16144567728042603\n",
      "training: 30 batch 154 batch_loss: 0.1598910093307495\n",
      "training: 30 batch 155 batch_loss: 0.16409984230995178\n",
      "training: 30 batch 156 batch_loss: 0.16477984189987183\n",
      "training: 30 batch 157 batch_loss: 0.1603798270225525\n",
      "training: 30 batch 158 batch_loss: 0.16225886344909668\n",
      "training: 30 batch 159 batch_loss: 0.1626587212085724\n",
      "training: 30 batch 160 batch_loss: 0.15949594974517822\n",
      "training: 30 batch 161 batch_loss: 0.16200926899909973\n",
      "training: 30 batch 162 batch_loss: 0.16230356693267822\n",
      "training: 30 batch 163 batch_loss: 0.15973281860351562\n",
      "training: 30 batch 164 batch_loss: 0.16358843445777893\n",
      "training: 30 batch 165 batch_loss: 0.16034412384033203\n",
      "training: 30 batch 166 batch_loss: 0.15741342306137085\n",
      "training: 30 batch 167 batch_loss: 0.16193503141403198\n",
      "training: 30 batch 168 batch_loss: 0.1630827784538269\n",
      "training: 30 batch 169 batch_loss: 0.16230508685112\n",
      "training: 30 batch 170 batch_loss: 0.16570037603378296\n",
      "training: 30 batch 171 batch_loss: 0.161190927028656\n",
      "training: 30 batch 172 batch_loss: 0.16088253259658813\n",
      "training: 30 batch 173 batch_loss: 0.16008949279785156\n",
      "training: 30 batch 174 batch_loss: 0.1627802848815918\n",
      "training: 30 batch 175 batch_loss: 0.16104567050933838\n",
      "training: 30 batch 176 batch_loss: 0.16116386651992798\n",
      "training: 30 batch 177 batch_loss: 0.1622486114501953\n",
      "training: 30 batch 178 batch_loss: 0.1579616665840149\n",
      "training: 30 batch 179 batch_loss: 0.16131636500358582\n",
      "training: 30 batch 180 batch_loss: 0.15873929858207703\n",
      "training: 30 batch 181 batch_loss: 0.16124248504638672\n",
      "training: 30 batch 182 batch_loss: 0.16075041890144348\n",
      "training: 30 batch 183 batch_loss: 0.16079744696617126\n",
      "training: 30 batch 184 batch_loss: 0.1673215627670288\n",
      "training: 30 batch 185 batch_loss: 0.16584113240242004\n",
      "training: 30 batch 186 batch_loss: 0.16726171970367432\n",
      "training: 30 batch 187 batch_loss: 0.15880167484283447\n",
      "training: 30 batch 188 batch_loss: 0.16046255826950073\n",
      "training: 30 batch 189 batch_loss: 0.1614198088645935\n",
      "training: 30 batch 190 batch_loss: 0.16018745303153992\n",
      "training: 30 batch 191 batch_loss: 0.16474279761314392\n",
      "training: 30 batch 192 batch_loss: 0.16049087047576904\n",
      "training: 30 batch 193 batch_loss: 0.1626911461353302\n",
      "training: 30 batch 194 batch_loss: 0.16158777475357056\n",
      "training: 30 batch 195 batch_loss: 0.1623028814792633\n",
      "training: 30 batch 196 batch_loss: 0.16261261701583862\n",
      "training: 30 batch 197 batch_loss: 0.16428306698799133\n",
      "training: 30 batch 198 batch_loss: 0.16155213117599487\n",
      "training: 30 batch 199 batch_loss: 0.16100353002548218\n",
      "training: 30 batch 200 batch_loss: 0.15959295630455017\n",
      "training: 30 batch 201 batch_loss: 0.16701167821884155\n",
      "training: 30 batch 202 batch_loss: 0.1623464822769165\n",
      "training: 30 batch 203 batch_loss: 0.16268235445022583\n",
      "training: 30 batch 204 batch_loss: 0.16300755739212036\n",
      "training: 30 batch 205 batch_loss: 0.1598445475101471\n",
      "training: 30 batch 206 batch_loss: 0.16525855660438538\n",
      "training: 30 batch 207 batch_loss: 0.16198813915252686\n",
      "training: 30 batch 208 batch_loss: 0.1633118987083435\n",
      "training: 30 batch 209 batch_loss: 0.16116175055503845\n",
      "training: 30 batch 210 batch_loss: 0.16060373187065125\n",
      "training: 30 batch 211 batch_loss: 0.16425010561943054\n",
      "training: 30 batch 212 batch_loss: 0.16398316621780396\n",
      "training: 30 batch 213 batch_loss: 0.15896520018577576\n",
      "training: 30 batch 214 batch_loss: 0.16175875067710876\n",
      "training: 30 batch 215 batch_loss: 0.16315001249313354\n",
      "training: 30 batch 216 batch_loss: 0.1627112627029419\n",
      "training: 30 batch 217 batch_loss: 0.16527411341667175\n",
      "training: 30 batch 218 batch_loss: 0.16299769282341003\n",
      "training: 30 batch 219 batch_loss: 0.1610078513622284\n",
      "training: 30 batch 220 batch_loss: 0.16470515727996826\n",
      "training: 30 batch 221 batch_loss: 0.16114774346351624\n",
      "training: 30 batch 222 batch_loss: 0.16106152534484863\n",
      "training: 30 batch 223 batch_loss: 0.16801536083221436\n",
      "training: 30 batch 224 batch_loss: 0.1594669222831726\n",
      "training: 30 batch 225 batch_loss: 0.16045987606048584\n",
      "training: 30 batch 226 batch_loss: 0.16259580850601196\n",
      "training: 30 batch 227 batch_loss: 0.16044285893440247\n",
      "training: 30 batch 228 batch_loss: 0.16421940922737122\n",
      "training: 30 batch 229 batch_loss: 0.16240385174751282\n",
      "training: 30 batch 230 batch_loss: 0.1634385585784912\n",
      "training: 30 batch 231 batch_loss: 0.16037601232528687\n",
      "training: 30 batch 232 batch_loss: 0.15973317623138428\n",
      "training: 30 batch 233 batch_loss: 0.16049373149871826\n",
      "training: 30 batch 234 batch_loss: 0.16371464729309082\n",
      "training: 30 batch 235 batch_loss: 0.16419565677642822\n",
      "training: 30 batch 236 batch_loss: 0.16528064012527466\n",
      "training: 30 batch 237 batch_loss: 0.16416996717453003\n",
      "training: 30 batch 238 batch_loss: 0.16465023159980774\n",
      "training: 30 batch 239 batch_loss: 0.1645326316356659\n",
      "training: 30 batch 240 batch_loss: 0.15938127040863037\n",
      "training: 30 batch 241 batch_loss: 0.1606602668762207\n",
      "training: 30 batch 242 batch_loss: 0.1617598533630371\n",
      "training: 30 batch 243 batch_loss: 0.16273194551467896\n",
      "training: 30 batch 244 batch_loss: 0.16338559985160828\n",
      "training: 30 batch 245 batch_loss: 0.16186285018920898\n",
      "training: 30 batch 246 batch_loss: 0.16272646188735962\n",
      "training: 30 batch 247 batch_loss: 0.16069534420967102\n",
      "training: 30 batch 248 batch_loss: 0.16163045167922974\n",
      "training: 30 batch 249 batch_loss: 0.16004672646522522\n",
      "training: 30 batch 250 batch_loss: 0.16421756148338318\n",
      "training: 30 batch 251 batch_loss: 0.1608690619468689\n",
      "training: 30 batch 252 batch_loss: 0.1659180223941803\n",
      "training: 30 batch 253 batch_loss: 0.16420677304267883\n",
      "training: 30 batch 254 batch_loss: 0.16244933009147644\n",
      "training: 30 batch 255 batch_loss: 0.16405156254768372\n",
      "training: 30 batch 256 batch_loss: 0.16452723741531372\n",
      "training: 30 batch 257 batch_loss: 0.16350620985031128\n",
      "training: 30 batch 258 batch_loss: 0.16283994913101196\n",
      "training: 30 batch 259 batch_loss: 0.1647435426712036\n",
      "training: 30 batch 260 batch_loss: 0.16305691003799438\n",
      "training: 30 batch 261 batch_loss: 0.16157689690589905\n",
      "training: 30 batch 262 batch_loss: 0.1588628888130188\n",
      "training: 30 batch 263 batch_loss: 0.16288596391677856\n",
      "training: 30 batch 264 batch_loss: 0.16439682245254517\n",
      "training: 30 batch 265 batch_loss: 0.163813054561615\n",
      "training: 30 batch 266 batch_loss: 0.16366368532180786\n",
      "training: 30 batch 267 batch_loss: 0.16335874795913696\n",
      "training: 30 batch 268 batch_loss: 0.16095346212387085\n",
      "training: 30 batch 269 batch_loss: 0.16179713606834412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 30 batch 270 batch_loss: 0.16584551334381104\n",
      "training: 30 batch 271 batch_loss: 0.16060826182365417\n",
      "training: 30 batch 272 batch_loss: 0.16223731637001038\n",
      "training: 30 batch 273 batch_loss: 0.16100773215293884\n",
      "training: 30 batch 274 batch_loss: 0.16016817092895508\n",
      "training: 30 batch 275 batch_loss: 0.16377541422843933\n",
      "training: 30 batch 276 batch_loss: 0.16110438108444214\n",
      "training: 30 batch 277 batch_loss: 0.16308945417404175\n",
      "training: 30 batch 278 batch_loss: 0.16478127241134644\n",
      "training: 30 batch 279 batch_loss: 0.16236063838005066\n",
      "training: 30 batch 280 batch_loss: 0.16100069880485535\n",
      "training: 30 batch 281 batch_loss: 0.16033583879470825\n",
      "training: 30 batch 282 batch_loss: 0.161554753780365\n",
      "training: 30 batch 283 batch_loss: 0.1642862856388092\n",
      "training: 30 batch 284 batch_loss: 0.15972569584846497\n",
      "training: 30 batch 285 batch_loss: 0.16234290599822998\n",
      "training: 30 batch 286 batch_loss: 0.16419100761413574\n",
      "training: 30 batch 287 batch_loss: 0.16089850664138794\n",
      "training: 30 batch 288 batch_loss: 0.16301798820495605\n",
      "training: 30 batch 289 batch_loss: 0.15639027953147888\n",
      "training: 30 batch 290 batch_loss: 0.1606350839138031\n",
      "training: 30 batch 291 batch_loss: 0.16347509622573853\n",
      "training: 30 batch 292 batch_loss: 0.15987157821655273\n",
      "training: 30 batch 293 batch_loss: 0.16400423645973206\n",
      "training: 30 batch 294 batch_loss: 0.16465628147125244\n",
      "training: 30 batch 295 batch_loss: 0.159308522939682\n",
      "training: 30 batch 296 batch_loss: 0.16057974100112915\n",
      "training: 30 batch 297 batch_loss: 0.1653546690940857\n",
      "training: 30 batch 298 batch_loss: 0.1628023386001587\n",
      "training: 30 batch 299 batch_loss: 0.1629675328731537\n",
      "training: 30 batch 300 batch_loss: 0.1607396900653839\n",
      "training: 30 batch 301 batch_loss: 0.1627993881702423\n",
      "training: 30 batch 302 batch_loss: 0.16219624876976013\n",
      "training: 30 batch 303 batch_loss: 0.16255590319633484\n",
      "training: 30 batch 304 batch_loss: 0.16167286038398743\n",
      "training: 30 batch 305 batch_loss: 0.16283661127090454\n",
      "training: 30 batch 306 batch_loss: 0.1630871295928955\n",
      "training: 30 batch 307 batch_loss: 0.16636884212493896\n",
      "training: 30 batch 308 batch_loss: 0.163563072681427\n",
      "training: 30 batch 309 batch_loss: 0.1670171618461609\n",
      "training: 30 batch 310 batch_loss: 0.1650993824005127\n",
      "training: 30 batch 311 batch_loss: 0.16358351707458496\n",
      "training: 30 batch 312 batch_loss: 0.16148638725280762\n",
      "training: 30 batch 313 batch_loss: 0.16182240843772888\n",
      "training: 30 batch 314 batch_loss: 0.16400447487831116\n",
      "training: 30 batch 315 batch_loss: 0.16407769918441772\n",
      "training: 30 batch 316 batch_loss: 0.16457810997962952\n",
      "training: 30 batch 317 batch_loss: 0.15964707732200623\n",
      "training: 30 batch 318 batch_loss: 0.16266313195228577\n",
      "training: 30 batch 319 batch_loss: 0.15997156500816345\n",
      "training: 30 batch 320 batch_loss: 0.16198265552520752\n",
      "training: 30 batch 321 batch_loss: 0.16208940744400024\n",
      "training: 30 batch 322 batch_loss: 0.16369453072547913\n",
      "training: 30 batch 323 batch_loss: 0.16073772311210632\n",
      "training: 30 batch 324 batch_loss: 0.16105791926383972\n",
      "training: 30 batch 325 batch_loss: 0.1618914008140564\n",
      "training: 30 batch 326 batch_loss: 0.1630871295928955\n",
      "training: 30 batch 327 batch_loss: 0.16661426424980164\n",
      "training: 30 batch 328 batch_loss: 0.1625637710094452\n",
      "training: 30 batch 329 batch_loss: 0.16106590628623962\n",
      "training: 30 batch 330 batch_loss: 0.15966284275054932\n",
      "training: 30 batch 331 batch_loss: 0.16191726922988892\n",
      "training: 30 batch 332 batch_loss: 0.1652289628982544\n",
      "training: 30 batch 333 batch_loss: 0.16282394528388977\n",
      "training: 30 batch 334 batch_loss: 0.16255536675453186\n",
      "training: 30 batch 335 batch_loss: 0.16590255498886108\n",
      "training: 30 batch 336 batch_loss: 0.1612270474433899\n",
      "training: 30 batch 337 batch_loss: 0.16586834192276\n",
      "training: 30 batch 338 batch_loss: 0.16235381364822388\n",
      "training: 30 batch 339 batch_loss: 0.16305115818977356\n",
      "training: 30 batch 340 batch_loss: 0.16338354349136353\n",
      "training: 30 batch 341 batch_loss: 0.16187340021133423\n",
      "training: 30 batch 342 batch_loss: 0.16332069039344788\n",
      "training: 30 batch 343 batch_loss: 0.16247761249542236\n",
      "training: 30 batch 344 batch_loss: 0.16053739190101624\n",
      "training: 30 batch 345 batch_loss: 0.1638958752155304\n",
      "training: 30 batch 346 batch_loss: 0.1612548828125\n",
      "training: 30 batch 347 batch_loss: 0.16355004906654358\n",
      "training: 30 batch 348 batch_loss: 0.16211262345314026\n",
      "training: 30 batch 349 batch_loss: 0.16044369339942932\n",
      "training: 30 batch 350 batch_loss: 0.16517159342765808\n",
      "training: 30 batch 351 batch_loss: 0.1645175814628601\n",
      "training: 30 batch 352 batch_loss: 0.16121706366539001\n",
      "training: 30 batch 353 batch_loss: 0.16154977679252625\n",
      "training: 30 batch 354 batch_loss: 0.16039574146270752\n",
      "training: 30 batch 355 batch_loss: 0.1659519374370575\n",
      "training: 30 batch 356 batch_loss: 0.16246730089187622\n",
      "training: 30 batch 357 batch_loss: 0.1636727750301361\n",
      "training: 30 batch 358 batch_loss: 0.16293561458587646\n",
      "training: 30 batch 359 batch_loss: 0.16203713417053223\n",
      "training: 30 batch 360 batch_loss: 0.16634663939476013\n",
      "training: 30 batch 361 batch_loss: 0.16548287868499756\n",
      "training: 30 batch 362 batch_loss: 0.15803644061088562\n",
      "training: 30 batch 363 batch_loss: 0.16542774438858032\n",
      "training: 30 batch 364 batch_loss: 0.16396117210388184\n",
      "training: 30 batch 365 batch_loss: 0.16481244564056396\n",
      "training: 30 batch 366 batch_loss: 0.1646636426448822\n",
      "training: 30 batch 367 batch_loss: 0.16248321533203125\n",
      "training: 30 batch 368 batch_loss: 0.16389840841293335\n",
      "training: 30 batch 369 batch_loss: 0.16341787576675415\n",
      "training: 30 batch 370 batch_loss: 0.16577941179275513\n",
      "training: 30 batch 371 batch_loss: 0.1631229817867279\n",
      "training: 30 batch 372 batch_loss: 0.16504767537117004\n",
      "training: 30 batch 373 batch_loss: 0.1625334620475769\n",
      "training: 30 batch 374 batch_loss: 0.16610974073410034\n",
      "training: 30 batch 375 batch_loss: 0.16570135951042175\n",
      "training: 30 batch 376 batch_loss: 0.16450998187065125\n",
      "training: 30 batch 377 batch_loss: 0.16290932893753052\n",
      "training: 30 batch 378 batch_loss: 0.1617199182510376\n",
      "training: 30 batch 379 batch_loss: 0.1659550964832306\n",
      "training: 30 batch 380 batch_loss: 0.16330218315124512\n",
      "training: 30 batch 381 batch_loss: 0.16264790296554565\n",
      "training: 30 batch 382 batch_loss: 0.16223737597465515\n",
      "training: 30 batch 383 batch_loss: 0.16311860084533691\n",
      "training: 30 batch 384 batch_loss: 0.16020727157592773\n",
      "training: 30 batch 385 batch_loss: 0.1622897982597351\n",
      "training: 30 batch 386 batch_loss: 0.16535651683807373\n",
      "training: 30 batch 387 batch_loss: 0.163148432970047\n",
      "training: 30 batch 388 batch_loss: 0.16153490543365479\n",
      "training: 30 batch 389 batch_loss: 0.16259342432022095\n",
      "training: 30 batch 390 batch_loss: 0.16514325141906738\n",
      "training: 30 batch 391 batch_loss: 0.1621471345424652\n",
      "training: 30 batch 392 batch_loss: 0.1635952591896057\n",
      "training: 30 batch 393 batch_loss: 0.165225088596344\n",
      "training: 30 batch 394 batch_loss: 0.16107049584388733\n",
      "training: 30 batch 395 batch_loss: 0.16233006119728088\n",
      "training: 30 batch 396 batch_loss: 0.16155654191970825\n",
      "training: 30 batch 397 batch_loss: 0.16333189606666565\n",
      "training: 30 batch 398 batch_loss: 0.16555172204971313\n",
      "training: 30 batch 399 batch_loss: 0.1605648398399353\n",
      "training: 30 batch 400 batch_loss: 0.16312989592552185\n",
      "training: 30 batch 401 batch_loss: 0.16793251037597656\n",
      "training: 30 batch 402 batch_loss: 0.1653037965297699\n",
      "training: 30 batch 403 batch_loss: 0.1611233353614807\n",
      "training: 30 batch 404 batch_loss: 0.16462942957878113\n",
      "training: 30 batch 405 batch_loss: 0.16457322239875793\n",
      "training: 30 batch 406 batch_loss: 0.1640319526195526\n",
      "training: 30 batch 407 batch_loss: 0.1634158194065094\n",
      "training: 30 batch 408 batch_loss: 0.16451787948608398\n",
      "training: 30 batch 409 batch_loss: 0.16342544555664062\n",
      "training: 30 batch 410 batch_loss: 0.16282343864440918\n",
      "training: 30 batch 411 batch_loss: 0.1639142632484436\n",
      "training: 30 batch 412 batch_loss: 0.15973177552223206\n",
      "training: 30 batch 413 batch_loss: 0.16606390476226807\n",
      "training: 30 batch 414 batch_loss: 0.1686241328716278\n",
      "training: 30 batch 415 batch_loss: 0.16313129663467407\n",
      "training: 30 batch 416 batch_loss: 0.16556069254875183\n",
      "training: 30 batch 417 batch_loss: 0.16442358493804932\n",
      "training: 30 batch 418 batch_loss: 0.16531801223754883\n",
      "training: 30 batch 419 batch_loss: 0.1607917845249176\n",
      "training: 30 batch 420 batch_loss: 0.16222110390663147\n",
      "training: 30 batch 421 batch_loss: 0.16406875848770142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 30 batch 422 batch_loss: 0.16447335481643677\n",
      "training: 30 batch 423 batch_loss: 0.16621968150138855\n",
      "training: 30 batch 424 batch_loss: 0.1660061478614807\n",
      "training: 30 batch 425 batch_loss: 0.1633903682231903\n",
      "training: 30 batch 426 batch_loss: 0.1639845371246338\n",
      "training: 30 batch 427 batch_loss: 0.16597336530685425\n",
      "training: 30 batch 428 batch_loss: 0.1634734570980072\n",
      "training: 30 batch 429 batch_loss: 0.16723382472991943\n",
      "training: 30 batch 430 batch_loss: 0.1599838137626648\n",
      "training: 30 batch 431 batch_loss: 0.16453000903129578\n",
      "training: 30 batch 432 batch_loss: 0.16181284189224243\n",
      "training: 30 batch 433 batch_loss: 0.16314288973808289\n",
      "training: 30 batch 434 batch_loss: 0.16206786036491394\n",
      "training: 30 batch 435 batch_loss: 0.16659724712371826\n",
      "training: 30 batch 436 batch_loss: 0.161474347114563\n",
      "training: 30 batch 437 batch_loss: 0.16369369626045227\n",
      "training: 30 batch 438 batch_loss: 0.16103258728981018\n",
      "training: 30 batch 439 batch_loss: 0.16252300143241882\n",
      "training: 30 batch 440 batch_loss: 0.16415980458259583\n",
      "training: 30 batch 441 batch_loss: 0.16183358430862427\n",
      "training: 30 batch 442 batch_loss: 0.16347581148147583\n",
      "training: 30 batch 443 batch_loss: 0.1625930368900299\n",
      "training: 30 batch 444 batch_loss: 0.167425274848938\n",
      "training: 30 batch 445 batch_loss: 0.16273069381713867\n",
      "training: 30 batch 446 batch_loss: 0.16660916805267334\n",
      "training: 30 batch 447 batch_loss: 0.1603306531906128\n",
      "training: 30 batch 448 batch_loss: 0.16858342289924622\n",
      "training: 30 batch 449 batch_loss: 0.16462388634681702\n",
      "training: 30 batch 450 batch_loss: 0.16348975896835327\n",
      "training: 30 batch 451 batch_loss: 0.16285637021064758\n",
      "training: 30 batch 452 batch_loss: 0.16525113582611084\n",
      "training: 30 batch 453 batch_loss: 0.160880446434021\n",
      "training: 30 batch 454 batch_loss: 0.16374200582504272\n",
      "training: 30 batch 455 batch_loss: 0.16443145275115967\n",
      "training: 30 batch 456 batch_loss: 0.1624448299407959\n",
      "training: 30 batch 457 batch_loss: 0.1634058952331543\n",
      "training: 30 batch 458 batch_loss: 0.16526144742965698\n",
      "training: 30 batch 459 batch_loss: 0.16516956686973572\n",
      "training: 30 batch 460 batch_loss: 0.1621965765953064\n",
      "training: 30 batch 461 batch_loss: 0.1597057580947876\n",
      "training: 30 batch 462 batch_loss: 0.16275817155838013\n",
      "training: 30 batch 463 batch_loss: 0.16264015436172485\n",
      "training: 30 batch 464 batch_loss: 0.16113123297691345\n",
      "training: 30 batch 465 batch_loss: 0.1643725335597992\n",
      "training: 30 batch 466 batch_loss: 0.16527482867240906\n",
      "training: 30 batch 467 batch_loss: 0.16538658738136292\n",
      "training: 30 batch 468 batch_loss: 0.16736868023872375\n",
      "training: 30 batch 469 batch_loss: 0.16391894221305847\n",
      "training: 30 batch 470 batch_loss: 0.16155904531478882\n",
      "training: 30 batch 471 batch_loss: 0.16398757696151733\n",
      "training: 30 batch 472 batch_loss: 0.16365015506744385\n",
      "training: 30 batch 473 batch_loss: 0.16295433044433594\n",
      "training: 30 batch 474 batch_loss: 0.16522273421287537\n",
      "training: 30 batch 475 batch_loss: 0.1642494797706604\n",
      "training: 30 batch 476 batch_loss: 0.16277670860290527\n",
      "training: 30 batch 477 batch_loss: 0.16752111911773682\n",
      "training: 30 batch 478 batch_loss: 0.16205275058746338\n",
      "training: 30 batch 479 batch_loss: 0.1603439450263977\n",
      "training: 30 batch 480 batch_loss: 0.16432476043701172\n",
      "training: 30 batch 481 batch_loss: 0.16321051120758057\n",
      "training: 30 batch 482 batch_loss: 0.16513821482658386\n",
      "training: 30 batch 483 batch_loss: 0.16053974628448486\n",
      "training: 30 batch 484 batch_loss: 0.16490542888641357\n",
      "training: 30 batch 485 batch_loss: 0.16409078240394592\n",
      "training: 30 batch 486 batch_loss: 0.16621941328048706\n",
      "training: 30 batch 487 batch_loss: 0.16549032926559448\n",
      "training: 30 batch 488 batch_loss: 0.16380846500396729\n",
      "training: 30 batch 489 batch_loss: 0.16282853484153748\n",
      "training: 30 batch 490 batch_loss: 0.1672876477241516\n",
      "training: 30 batch 491 batch_loss: 0.1632767617702484\n",
      "training: 30 batch 492 batch_loss: 0.1662941575050354\n",
      "training: 30 batch 493 batch_loss: 0.16346171498298645\n",
      "training: 30 batch 494 batch_loss: 0.16383618116378784\n",
      "training: 30 batch 495 batch_loss: 0.1672242283821106\n",
      "training: 30 batch 496 batch_loss: 0.1642763614654541\n",
      "training: 30 batch 497 batch_loss: 0.16825884580612183\n",
      "training: 30 batch 498 batch_loss: 0.16145700216293335\n",
      "training: 30 batch 499 batch_loss: 0.16335615515708923\n",
      "training: 30 batch 500 batch_loss: 0.16541260480880737\n",
      "training: 30 batch 501 batch_loss: 0.16213691234588623\n",
      "training: 30 batch 502 batch_loss: 0.1635528802871704\n",
      "training: 30 batch 503 batch_loss: 0.16487076878547668\n",
      "training: 30 batch 504 batch_loss: 0.16420277953147888\n",
      "training: 30 batch 505 batch_loss: 0.1648518145084381\n",
      "training: 30 batch 506 batch_loss: 0.1657460331916809\n",
      "training: 30 batch 507 batch_loss: 0.1699834167957306\n",
      "training: 30 batch 508 batch_loss: 0.16147899627685547\n",
      "training: 30 batch 509 batch_loss: 0.16507098078727722\n",
      "training: 30 batch 510 batch_loss: 0.16214299201965332\n",
      "training: 30 batch 511 batch_loss: 0.16167515516281128\n",
      "training: 30 batch 512 batch_loss: 0.166293203830719\n",
      "training: 30 batch 513 batch_loss: 0.17010465264320374\n",
      "training: 30 batch 514 batch_loss: 0.1664683222770691\n",
      "training: 30 batch 515 batch_loss: 0.16629528999328613\n",
      "training: 30 batch 516 batch_loss: 0.16279396414756775\n",
      "training: 30 batch 517 batch_loss: 0.16369298100471497\n",
      "training: 30 batch 518 batch_loss: 0.1631776988506317\n",
      "training: 30 batch 519 batch_loss: 0.16116511821746826\n",
      "training: 30 batch 520 batch_loss: 0.16514098644256592\n",
      "training: 30 batch 521 batch_loss: 0.16463491320610046\n",
      "training: 30 batch 522 batch_loss: 0.16552627086639404\n",
      "training: 30 batch 523 batch_loss: 0.16517847776412964\n",
      "training: 30 batch 524 batch_loss: 0.16549673676490784\n",
      "training: 30 batch 525 batch_loss: 0.16261759400367737\n",
      "training: 30 batch 526 batch_loss: 0.16322195529937744\n",
      "training: 30 batch 527 batch_loss: 0.16645771265029907\n",
      "training: 30 batch 528 batch_loss: 0.1644180417060852\n",
      "training: 30 batch 529 batch_loss: 0.16406410932540894\n",
      "training: 30 batch 530 batch_loss: 0.1626531481742859\n",
      "training: 30 batch 531 batch_loss: 0.1606680154800415\n",
      "training: 30 batch 532 batch_loss: 0.16376936435699463\n",
      "training: 30 batch 533 batch_loss: 0.16095545887947083\n",
      "training: 30 batch 534 batch_loss: 0.1617714762687683\n",
      "training: 30 batch 535 batch_loss: 0.16421297192573547\n",
      "training: 30 batch 536 batch_loss: 0.1639949083328247\n",
      "training: 30 batch 537 batch_loss: 0.16647964715957642\n",
      "training: 30 batch 538 batch_loss: 0.16302549839019775\n",
      "training: 30 batch 539 batch_loss: 0.16090968251228333\n",
      "training: 30 batch 540 batch_loss: 0.1620607078075409\n",
      "training: 30 batch 541 batch_loss: 0.1617063283920288\n",
      "training: 30 batch 542 batch_loss: 0.16555598378181458\n",
      "training: 30 batch 543 batch_loss: 0.16109001636505127\n",
      "training: 30 batch 544 batch_loss: 0.162637859582901\n",
      "training: 30 batch 545 batch_loss: 0.162700355052948\n",
      "training: 30 batch 546 batch_loss: 0.16248759627342224\n",
      "training: 30 batch 547 batch_loss: 0.16380363702774048\n",
      "training: 30 batch 548 batch_loss: 0.16423100233078003\n",
      "training: 30 batch 549 batch_loss: 0.16593563556671143\n",
      "training: 30 batch 550 batch_loss: 0.16344553232192993\n",
      "training: 30 batch 551 batch_loss: 0.16437605023384094\n",
      "training: 30 batch 552 batch_loss: 0.16189008951187134\n",
      "training: 30 batch 553 batch_loss: 0.166995108127594\n",
      "training: 30 batch 554 batch_loss: 0.16360393166542053\n",
      "training: 30 batch 555 batch_loss: 0.15905651450157166\n",
      "training: 30 batch 556 batch_loss: 0.1632360816001892\n",
      "training: 30 batch 557 batch_loss: 0.16217002272605896\n",
      "training: 30 batch 558 batch_loss: 0.1623072326183319\n",
      "training: 30 batch 559 batch_loss: 0.16469144821166992\n",
      "training: 30 batch 560 batch_loss: 0.1630687117576599\n",
      "training: 30 batch 561 batch_loss: 0.1655103862285614\n",
      "training: 30 batch 562 batch_loss: 0.1655263900756836\n",
      "training: 30 batch 563 batch_loss: 0.16169226169586182\n",
      "training: 30 batch 564 batch_loss: 0.1649179458618164\n",
      "training: 30 batch 565 batch_loss: 0.16381296515464783\n",
      "training: 30 batch 566 batch_loss: 0.1627522110939026\n",
      "training: 30 batch 567 batch_loss: 0.16304588317871094\n",
      "training: 30 batch 568 batch_loss: 0.16304242610931396\n",
      "training: 30 batch 569 batch_loss: 0.16417241096496582\n",
      "training: 30 batch 570 batch_loss: 0.1652071177959442\n",
      "training: 30 batch 571 batch_loss: 0.16383042931556702\n",
      "training: 30 batch 572 batch_loss: 0.16427820920944214\n",
      "training: 30 batch 573 batch_loss: 0.16182145476341248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 30 batch 574 batch_loss: 0.1620037853717804\n",
      "training: 30 batch 575 batch_loss: 0.1656474769115448\n",
      "training: 30 batch 576 batch_loss: 0.16367504000663757\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 30, Hit Ratio:0.02982081527550609 | Precision:0.0439988204069596 | Recall:0.05920473193928391 | NDCG:0.05634308849316303\n",
      "*Best Performance* \n",
      "Epoch: 30, Hit Ratio:0.02982081527550609 | Precision:0.0439988204069596 | Recall:0.05920473193928391 | MDCG:0.05634308849316303\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 31 batch 0 batch_loss: 0.16528171300888062\n",
      "training: 31 batch 1 batch_loss: 0.15933895111083984\n",
      "training: 31 batch 2 batch_loss: 0.1651151180267334\n",
      "training: 31 batch 3 batch_loss: 0.16135060787200928\n",
      "training: 31 batch 4 batch_loss: 0.16222965717315674\n",
      "training: 31 batch 5 batch_loss: 0.16206994652748108\n",
      "training: 31 batch 6 batch_loss: 0.16271468997001648\n",
      "training: 31 batch 7 batch_loss: 0.16347584128379822\n",
      "training: 31 batch 8 batch_loss: 0.16300374269485474\n",
      "training: 31 batch 9 batch_loss: 0.1644013524055481\n",
      "training: 31 batch 10 batch_loss: 0.16215473413467407\n",
      "training: 31 batch 11 batch_loss: 0.16272735595703125\n",
      "training: 31 batch 12 batch_loss: 0.1626276969909668\n",
      "training: 31 batch 13 batch_loss: 0.16223397850990295\n",
      "training: 31 batch 14 batch_loss: 0.16422533988952637\n",
      "training: 31 batch 15 batch_loss: 0.1616477370262146\n",
      "training: 31 batch 16 batch_loss: 0.16205427050590515\n",
      "training: 31 batch 17 batch_loss: 0.16323313117027283\n",
      "training: 31 batch 18 batch_loss: 0.16408240795135498\n",
      "training: 31 batch 19 batch_loss: 0.1600348949432373\n",
      "training: 31 batch 20 batch_loss: 0.16628098487854004\n",
      "training: 31 batch 21 batch_loss: 0.1607973575592041\n",
      "training: 31 batch 22 batch_loss: 0.1591954231262207\n",
      "training: 31 batch 23 batch_loss: 0.1676202416419983\n",
      "training: 31 batch 24 batch_loss: 0.16593915224075317\n",
      "training: 31 batch 25 batch_loss: 0.16328835487365723\n",
      "training: 31 batch 26 batch_loss: 0.16355305910110474\n",
      "training: 31 batch 27 batch_loss: 0.1600552201271057\n",
      "training: 31 batch 28 batch_loss: 0.16250449419021606\n",
      "training: 31 batch 29 batch_loss: 0.16265153884887695\n",
      "training: 31 batch 30 batch_loss: 0.16468098759651184\n",
      "training: 31 batch 31 batch_loss: 0.16268813610076904\n",
      "training: 31 batch 32 batch_loss: 0.15917125344276428\n",
      "training: 31 batch 33 batch_loss: 0.16169893741607666\n",
      "training: 31 batch 34 batch_loss: 0.16471800208091736\n",
      "training: 31 batch 35 batch_loss: 0.16377785801887512\n",
      "training: 31 batch 36 batch_loss: 0.16109228134155273\n",
      "training: 31 batch 37 batch_loss: 0.16357535123825073\n",
      "training: 31 batch 38 batch_loss: 0.1603429913520813\n",
      "training: 31 batch 39 batch_loss: 0.1616259217262268\n",
      "training: 31 batch 40 batch_loss: 0.16343986988067627\n",
      "training: 31 batch 41 batch_loss: 0.1620108187198639\n",
      "training: 31 batch 42 batch_loss: 0.1645098626613617\n",
      "training: 31 batch 43 batch_loss: 0.16471797227859497\n",
      "training: 31 batch 44 batch_loss: 0.1642475426197052\n",
      "training: 31 batch 45 batch_loss: 0.16380661725997925\n",
      "training: 31 batch 46 batch_loss: 0.166390061378479\n",
      "training: 31 batch 47 batch_loss: 0.1628105342388153\n",
      "training: 31 batch 48 batch_loss: 0.15970569849014282\n",
      "training: 31 batch 49 batch_loss: 0.16599097847938538\n",
      "training: 31 batch 50 batch_loss: 0.16311094164848328\n",
      "training: 31 batch 51 batch_loss: 0.16651052236557007\n",
      "training: 31 batch 52 batch_loss: 0.16227543354034424\n",
      "training: 31 batch 53 batch_loss: 0.1667144000530243\n",
      "training: 31 batch 54 batch_loss: 0.16323432326316833\n",
      "training: 31 batch 55 batch_loss: 0.16326043009757996\n",
      "training: 31 batch 56 batch_loss: 0.16526377201080322\n",
      "training: 31 batch 57 batch_loss: 0.15955966711044312\n",
      "training: 31 batch 58 batch_loss: 0.16667920351028442\n",
      "training: 31 batch 59 batch_loss: 0.16382527351379395\n",
      "training: 31 batch 60 batch_loss: 0.16227754950523376\n",
      "training: 31 batch 61 batch_loss: 0.1650640368461609\n",
      "training: 31 batch 62 batch_loss: 0.16230374574661255\n",
      "training: 31 batch 63 batch_loss: 0.16014611721038818\n",
      "training: 31 batch 64 batch_loss: 0.1617163121700287\n",
      "training: 31 batch 65 batch_loss: 0.1651313602924347\n",
      "training: 31 batch 66 batch_loss: 0.16204404830932617\n",
      "training: 31 batch 67 batch_loss: 0.16427025198936462\n",
      "training: 31 batch 68 batch_loss: 0.1626918911933899\n",
      "training: 31 batch 69 batch_loss: 0.16398650407791138\n",
      "training: 31 batch 70 batch_loss: 0.16377592086791992\n",
      "training: 31 batch 71 batch_loss: 0.16237404942512512\n",
      "training: 31 batch 72 batch_loss: 0.1620911955833435\n",
      "training: 31 batch 73 batch_loss: 0.1633504033088684\n",
      "training: 31 batch 74 batch_loss: 0.16354292631149292\n",
      "training: 31 batch 75 batch_loss: 0.16285866498947144\n",
      "training: 31 batch 76 batch_loss: 0.163690447807312\n",
      "training: 31 batch 77 batch_loss: 0.16427358984947205\n",
      "training: 31 batch 78 batch_loss: 0.16568687558174133\n",
      "training: 31 batch 79 batch_loss: 0.16300761699676514\n",
      "training: 31 batch 80 batch_loss: 0.16540372371673584\n",
      "training: 31 batch 81 batch_loss: 0.16453400254249573\n",
      "training: 31 batch 82 batch_loss: 0.1618344783782959\n",
      "training: 31 batch 83 batch_loss: 0.16621974110603333\n",
      "training: 31 batch 84 batch_loss: 0.163665771484375\n",
      "training: 31 batch 85 batch_loss: 0.16411635279655457\n",
      "training: 31 batch 86 batch_loss: 0.16272246837615967\n",
      "training: 31 batch 87 batch_loss: 0.16270089149475098\n",
      "training: 31 batch 88 batch_loss: 0.16344627737998962\n",
      "training: 31 batch 89 batch_loss: 0.1622123122215271\n",
      "training: 31 batch 90 batch_loss: 0.16559353470802307\n",
      "training: 31 batch 91 batch_loss: 0.16174980998039246\n",
      "training: 31 batch 92 batch_loss: 0.16423842310905457\n",
      "training: 31 batch 93 batch_loss: 0.15852713584899902\n",
      "training: 31 batch 94 batch_loss: 0.16073915362358093\n",
      "training: 31 batch 95 batch_loss: 0.16452619433403015\n",
      "training: 31 batch 96 batch_loss: 0.16548743844032288\n",
      "training: 31 batch 97 batch_loss: 0.16140779852867126\n",
      "training: 31 batch 98 batch_loss: 0.1611054539680481\n",
      "training: 31 batch 99 batch_loss: 0.16174069046974182\n",
      "training: 31 batch 100 batch_loss: 0.16368836164474487\n",
      "training: 31 batch 101 batch_loss: 0.16209861636161804\n",
      "training: 31 batch 102 batch_loss: 0.16223278641700745\n",
      "training: 31 batch 103 batch_loss: 0.1625937521457672\n",
      "training: 31 batch 104 batch_loss: 0.16534239053726196\n",
      "training: 31 batch 105 batch_loss: 0.16362711787223816\n",
      "training: 31 batch 106 batch_loss: 0.1632869839668274\n",
      "training: 31 batch 107 batch_loss: 0.16280966997146606\n",
      "training: 31 batch 108 batch_loss: 0.1621919870376587\n",
      "training: 31 batch 109 batch_loss: 0.1636897623538971\n",
      "training: 31 batch 110 batch_loss: 0.16175895929336548\n",
      "training: 31 batch 111 batch_loss: 0.1623193919658661\n",
      "training: 31 batch 112 batch_loss: 0.16416919231414795\n",
      "training: 31 batch 113 batch_loss: 0.15952739119529724\n",
      "training: 31 batch 114 batch_loss: 0.16846826672554016\n",
      "training: 31 batch 115 batch_loss: 0.16393229365348816\n",
      "training: 31 batch 116 batch_loss: 0.16273844242095947\n",
      "training: 31 batch 117 batch_loss: 0.16093194484710693\n",
      "training: 31 batch 118 batch_loss: 0.16369616985321045\n",
      "training: 31 batch 119 batch_loss: 0.1649220585823059\n",
      "training: 31 batch 120 batch_loss: 0.16543954610824585\n",
      "training: 31 batch 121 batch_loss: 0.16204535961151123\n",
      "training: 31 batch 122 batch_loss: 0.16212013363838196\n",
      "training: 31 batch 123 batch_loss: 0.16248512268066406\n",
      "training: 31 batch 124 batch_loss: 0.16252127289772034\n",
      "training: 31 batch 125 batch_loss: 0.16280046105384827\n",
      "training: 31 batch 126 batch_loss: 0.16356056928634644\n",
      "training: 31 batch 127 batch_loss: 0.16426628828048706\n",
      "training: 31 batch 128 batch_loss: 0.16172167658805847\n",
      "training: 31 batch 129 batch_loss: 0.1647418737411499\n",
      "training: 31 batch 130 batch_loss: 0.16087418794631958\n",
      "training: 31 batch 131 batch_loss: 0.1640532910823822\n",
      "training: 31 batch 132 batch_loss: 0.16510742902755737\n",
      "training: 31 batch 133 batch_loss: 0.1636871099472046\n",
      "training: 31 batch 134 batch_loss: 0.16215574741363525\n",
      "training: 31 batch 135 batch_loss: 0.1650541126728058\n",
      "training: 31 batch 136 batch_loss: 0.1629599630832672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 31 batch 137 batch_loss: 0.1679520606994629\n",
      "training: 31 batch 138 batch_loss: 0.16630905866622925\n",
      "training: 31 batch 139 batch_loss: 0.16565561294555664\n",
      "training: 31 batch 140 batch_loss: 0.16356146335601807\n",
      "training: 31 batch 141 batch_loss: 0.15934330224990845\n",
      "training: 31 batch 142 batch_loss: 0.16460081934928894\n",
      "training: 31 batch 143 batch_loss: 0.16387543082237244\n",
      "training: 31 batch 144 batch_loss: 0.16508716344833374\n",
      "training: 31 batch 145 batch_loss: 0.16304758191108704\n",
      "training: 31 batch 146 batch_loss: 0.1635502278804779\n",
      "training: 31 batch 147 batch_loss: 0.16116321086883545\n",
      "training: 31 batch 148 batch_loss: 0.1641794741153717\n",
      "training: 31 batch 149 batch_loss: 0.16486480832099915\n",
      "training: 31 batch 150 batch_loss: 0.16394281387329102\n",
      "training: 31 batch 151 batch_loss: 0.16700685024261475\n",
      "training: 31 batch 152 batch_loss: 0.1656886339187622\n",
      "training: 31 batch 153 batch_loss: 0.1622534990310669\n",
      "training: 31 batch 154 batch_loss: 0.16116103529930115\n",
      "training: 31 batch 155 batch_loss: 0.16433000564575195\n",
      "training: 31 batch 156 batch_loss: 0.16360273957252502\n",
      "training: 31 batch 157 batch_loss: 0.1644395887851715\n",
      "training: 31 batch 158 batch_loss: 0.16238859295845032\n",
      "training: 31 batch 159 batch_loss: 0.16647568345069885\n",
      "training: 31 batch 160 batch_loss: 0.16206133365631104\n",
      "training: 31 batch 161 batch_loss: 0.16467320919036865\n",
      "training: 31 batch 162 batch_loss: 0.1639036238193512\n",
      "training: 31 batch 163 batch_loss: 0.1615925133228302\n",
      "training: 31 batch 164 batch_loss: 0.1678704023361206\n",
      "training: 31 batch 165 batch_loss: 0.16354325413703918\n",
      "training: 31 batch 166 batch_loss: 0.16403430700302124\n",
      "training: 31 batch 167 batch_loss: 0.16021311283111572\n",
      "training: 31 batch 168 batch_loss: 0.16109919548034668\n",
      "training: 31 batch 169 batch_loss: 0.16537165641784668\n",
      "training: 31 batch 170 batch_loss: 0.166068434715271\n",
      "training: 31 batch 171 batch_loss: 0.16206303238868713\n",
      "training: 31 batch 172 batch_loss: 0.16367390751838684\n",
      "training: 31 batch 173 batch_loss: 0.16565093398094177\n",
      "training: 31 batch 174 batch_loss: 0.1641547977924347\n",
      "training: 31 batch 175 batch_loss: 0.16389748454093933\n",
      "training: 31 batch 176 batch_loss: 0.164119154214859\n",
      "training: 31 batch 177 batch_loss: 0.16348713636398315\n",
      "training: 31 batch 178 batch_loss: 0.16239282488822937\n",
      "training: 31 batch 179 batch_loss: 0.163010835647583\n",
      "training: 31 batch 180 batch_loss: 0.16282466053962708\n",
      "training: 31 batch 181 batch_loss: 0.16401460766792297\n",
      "training: 31 batch 182 batch_loss: 0.16294720768928528\n",
      "training: 31 batch 183 batch_loss: 0.1642562747001648\n",
      "training: 31 batch 184 batch_loss: 0.16274374723434448\n",
      "training: 31 batch 185 batch_loss: 0.16223731637001038\n",
      "training: 31 batch 186 batch_loss: 0.1635172963142395\n",
      "training: 31 batch 187 batch_loss: 0.1617140769958496\n",
      "training: 31 batch 188 batch_loss: 0.16291913390159607\n",
      "training: 31 batch 189 batch_loss: 0.16070148348808289\n",
      "training: 31 batch 190 batch_loss: 0.1626070737838745\n",
      "training: 31 batch 191 batch_loss: 0.1629975140094757\n",
      "training: 31 batch 192 batch_loss: 0.16355854272842407\n",
      "training: 31 batch 193 batch_loss: 0.1620902419090271\n",
      "training: 31 batch 194 batch_loss: 0.16255447268486023\n",
      "training: 31 batch 195 batch_loss: 0.1636909544467926\n",
      "training: 31 batch 196 batch_loss: 0.16242939233779907\n",
      "training: 31 batch 197 batch_loss: 0.16476312279701233\n",
      "training: 31 batch 198 batch_loss: 0.1637093722820282\n",
      "training: 31 batch 199 batch_loss: 0.16577839851379395\n",
      "training: 31 batch 200 batch_loss: 0.1620856523513794\n",
      "training: 31 batch 201 batch_loss: 0.16214239597320557\n",
      "training: 31 batch 202 batch_loss: 0.15935754776000977\n",
      "training: 31 batch 203 batch_loss: 0.16790097951889038\n",
      "training: 31 batch 204 batch_loss: 0.16222736239433289\n",
      "training: 31 batch 205 batch_loss: 0.16456544399261475\n",
      "training: 31 batch 206 batch_loss: 0.16300532221794128\n",
      "training: 31 batch 207 batch_loss: 0.1636143922805786\n",
      "training: 31 batch 208 batch_loss: 0.1638043224811554\n",
      "training: 31 batch 209 batch_loss: 0.16124635934829712\n",
      "training: 31 batch 210 batch_loss: 0.16475972533226013\n",
      "training: 31 batch 211 batch_loss: 0.16149002313613892\n",
      "training: 31 batch 212 batch_loss: 0.16301095485687256\n",
      "training: 31 batch 213 batch_loss: 0.1624678373336792\n",
      "training: 31 batch 214 batch_loss: 0.15786510705947876\n",
      "training: 31 batch 215 batch_loss: 0.16260942816734314\n",
      "training: 31 batch 216 batch_loss: 0.16208642721176147\n",
      "training: 31 batch 217 batch_loss: 0.16207998991012573\n",
      "training: 31 batch 218 batch_loss: 0.16291752457618713\n",
      "training: 31 batch 219 batch_loss: 0.16396984457969666\n",
      "training: 31 batch 220 batch_loss: 0.16479715704917908\n",
      "training: 31 batch 221 batch_loss: 0.16503900289535522\n",
      "training: 31 batch 222 batch_loss: 0.16454491019248962\n",
      "training: 31 batch 223 batch_loss: 0.16248628497123718\n",
      "training: 31 batch 224 batch_loss: 0.1626814305782318\n",
      "training: 31 batch 225 batch_loss: 0.16394126415252686\n",
      "training: 31 batch 226 batch_loss: 0.16657298803329468\n",
      "training: 31 batch 227 batch_loss: 0.16568180918693542\n",
      "training: 31 batch 228 batch_loss: 0.16339662671089172\n",
      "training: 31 batch 229 batch_loss: 0.16492149233818054\n",
      "training: 31 batch 230 batch_loss: 0.16324147582054138\n",
      "training: 31 batch 231 batch_loss: 0.16413581371307373\n",
      "training: 31 batch 232 batch_loss: 0.16405051946640015\n",
      "training: 31 batch 233 batch_loss: 0.16326689720153809\n",
      "training: 31 batch 234 batch_loss: 0.1638382375240326\n",
      "training: 31 batch 235 batch_loss: 0.16165226697921753\n",
      "training: 31 batch 236 batch_loss: 0.16363388299942017\n",
      "training: 31 batch 237 batch_loss: 0.16301769018173218\n",
      "training: 31 batch 238 batch_loss: 0.16371268033981323\n",
      "training: 31 batch 239 batch_loss: 0.1661277711391449\n",
      "training: 31 batch 240 batch_loss: 0.1624162495136261\n",
      "training: 31 batch 241 batch_loss: 0.1623905599117279\n",
      "training: 31 batch 242 batch_loss: 0.16142064332962036\n",
      "training: 31 batch 243 batch_loss: 0.1663130521774292\n",
      "training: 31 batch 244 batch_loss: 0.16256597638130188\n",
      "training: 31 batch 245 batch_loss: 0.1632872223854065\n",
      "training: 31 batch 246 batch_loss: 0.16594284772872925\n",
      "training: 31 batch 247 batch_loss: 0.16739201545715332\n",
      "training: 31 batch 248 batch_loss: 0.16473335027694702\n",
      "training: 31 batch 249 batch_loss: 0.16709822416305542\n",
      "training: 31 batch 250 batch_loss: 0.16349047422409058\n",
      "training: 31 batch 251 batch_loss: 0.1651020646095276\n",
      "training: 31 batch 252 batch_loss: 0.16538822650909424\n",
      "training: 31 batch 253 batch_loss: 0.16480380296707153\n",
      "training: 31 batch 254 batch_loss: 0.1688937544822693\n",
      "training: 31 batch 255 batch_loss: 0.16621613502502441\n",
      "training: 31 batch 256 batch_loss: 0.16300702095031738\n",
      "training: 31 batch 257 batch_loss: 0.16081684827804565\n",
      "training: 31 batch 258 batch_loss: 0.1638001799583435\n",
      "training: 31 batch 259 batch_loss: 0.16572457551956177\n",
      "training: 31 batch 260 batch_loss: 0.16245582699775696\n",
      "training: 31 batch 261 batch_loss: 0.16227328777313232\n",
      "training: 31 batch 262 batch_loss: 0.16327711939811707\n",
      "training: 31 batch 263 batch_loss: 0.16380354762077332\n",
      "training: 31 batch 264 batch_loss: 0.16627538204193115\n",
      "training: 31 batch 265 batch_loss: 0.16131943464279175\n",
      "training: 31 batch 266 batch_loss: 0.1672176718711853\n",
      "training: 31 batch 267 batch_loss: 0.1665102243423462\n",
      "training: 31 batch 268 batch_loss: 0.16086792945861816\n",
      "training: 31 batch 269 batch_loss: 0.16313183307647705\n",
      "training: 31 batch 270 batch_loss: 0.16628384590148926\n",
      "training: 31 batch 271 batch_loss: 0.1663961410522461\n",
      "training: 31 batch 272 batch_loss: 0.1660187542438507\n",
      "training: 31 batch 273 batch_loss: 0.1637660264968872\n",
      "training: 31 batch 274 batch_loss: 0.16267463564872742\n",
      "training: 31 batch 275 batch_loss: 0.16996705532073975\n",
      "training: 31 batch 276 batch_loss: 0.1635800004005432\n",
      "training: 31 batch 277 batch_loss: 0.1645176112651825\n",
      "training: 31 batch 278 batch_loss: 0.16521701216697693\n",
      "training: 31 batch 279 batch_loss: 0.16589102149009705\n",
      "training: 31 batch 280 batch_loss: 0.16903048753738403\n",
      "training: 31 batch 281 batch_loss: 0.1618134081363678\n",
      "training: 31 batch 282 batch_loss: 0.16310113668441772\n",
      "training: 31 batch 283 batch_loss: 0.16249626874923706\n",
      "training: 31 batch 284 batch_loss: 0.16187673807144165\n",
      "training: 31 batch 285 batch_loss: 0.16218337416648865\n",
      "training: 31 batch 286 batch_loss: 0.1643572449684143\n",
      "training: 31 batch 287 batch_loss: 0.16525062918663025\n",
      "training: 31 batch 288 batch_loss: 0.16123664379119873\n",
      "training: 31 batch 289 batch_loss: 0.1645466387271881\n",
      "training: 31 batch 290 batch_loss: 0.16514039039611816\n",
      "training: 31 batch 291 batch_loss: 0.16821393370628357\n",
      "training: 31 batch 292 batch_loss: 0.15974122285842896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 31 batch 293 batch_loss: 0.1627776026725769\n",
      "training: 31 batch 294 batch_loss: 0.16729843616485596\n",
      "training: 31 batch 295 batch_loss: 0.16226327419281006\n",
      "training: 31 batch 296 batch_loss: 0.16908150911331177\n",
      "training: 31 batch 297 batch_loss: 0.16766715049743652\n",
      "training: 31 batch 298 batch_loss: 0.1648324728012085\n",
      "training: 31 batch 299 batch_loss: 0.16406571865081787\n",
      "training: 31 batch 300 batch_loss: 0.1649896204471588\n",
      "training: 31 batch 301 batch_loss: 0.16593334078788757\n",
      "training: 31 batch 302 batch_loss: 0.1645258665084839\n",
      "training: 31 batch 303 batch_loss: 0.16299057006835938\n",
      "training: 31 batch 304 batch_loss: 0.16539335250854492\n",
      "training: 31 batch 305 batch_loss: 0.1647973358631134\n",
      "training: 31 batch 306 batch_loss: 0.16878840327262878\n",
      "training: 31 batch 307 batch_loss: 0.16195300221443176\n",
      "training: 31 batch 308 batch_loss: 0.16437703371047974\n",
      "training: 31 batch 309 batch_loss: 0.16426584124565125\n",
      "training: 31 batch 310 batch_loss: 0.16683635115623474\n",
      "training: 31 batch 311 batch_loss: 0.16565001010894775\n",
      "training: 31 batch 312 batch_loss: 0.16497373580932617\n",
      "training: 31 batch 313 batch_loss: 0.16566890478134155\n",
      "training: 31 batch 314 batch_loss: 0.16407611966133118\n",
      "training: 31 batch 315 batch_loss: 0.165657639503479\n",
      "training: 31 batch 316 batch_loss: 0.1655464768409729\n",
      "training: 31 batch 317 batch_loss: 0.16890165209770203\n",
      "training: 31 batch 318 batch_loss: 0.16634789109230042\n",
      "training: 31 batch 319 batch_loss: 0.1609055995941162\n",
      "training: 31 batch 320 batch_loss: 0.1667284369468689\n",
      "training: 31 batch 321 batch_loss: 0.16661739349365234\n",
      "training: 31 batch 322 batch_loss: 0.16511327028274536\n",
      "training: 31 batch 323 batch_loss: 0.16156047582626343\n",
      "training: 31 batch 324 batch_loss: 0.16500869393348694\n",
      "training: 31 batch 325 batch_loss: 0.1653617024421692\n",
      "training: 31 batch 326 batch_loss: 0.16781046986579895\n",
      "training: 31 batch 327 batch_loss: 0.16267940402030945\n",
      "training: 31 batch 328 batch_loss: 0.16394656896591187\n",
      "training: 31 batch 329 batch_loss: 0.1643959879875183\n",
      "training: 31 batch 330 batch_loss: 0.16961157321929932\n",
      "training: 31 batch 331 batch_loss: 0.16752871870994568\n",
      "training: 31 batch 332 batch_loss: 0.16211563348770142\n",
      "training: 31 batch 333 batch_loss: 0.1666722297668457\n",
      "training: 31 batch 334 batch_loss: 0.1648131012916565\n",
      "training: 31 batch 335 batch_loss: 0.16338616609573364\n",
      "training: 31 batch 336 batch_loss: 0.1633915901184082\n",
      "training: 31 batch 337 batch_loss: 0.1624011993408203\n",
      "training: 31 batch 338 batch_loss: 0.1643097698688507\n",
      "training: 31 batch 339 batch_loss: 0.16676568984985352\n",
      "training: 31 batch 340 batch_loss: 0.16265547275543213\n",
      "training: 31 batch 341 batch_loss: 0.1657751202583313\n",
      "training: 31 batch 342 batch_loss: 0.16230863332748413\n",
      "training: 31 batch 343 batch_loss: 0.16240030527114868\n",
      "training: 31 batch 344 batch_loss: 0.16351178288459778\n",
      "training: 31 batch 345 batch_loss: 0.16333311796188354\n",
      "training: 31 batch 346 batch_loss: 0.16528773307800293\n",
      "training: 31 batch 347 batch_loss: 0.1647634506225586\n",
      "training: 31 batch 348 batch_loss: 0.1674029529094696\n",
      "training: 31 batch 349 batch_loss: 0.16636204719543457\n",
      "training: 31 batch 350 batch_loss: 0.16273611783981323\n",
      "training: 31 batch 351 batch_loss: 0.16132929921150208\n",
      "training: 31 batch 352 batch_loss: 0.16627737879753113\n",
      "training: 31 batch 353 batch_loss: 0.1634734869003296\n",
      "training: 31 batch 354 batch_loss: 0.1630266010761261\n",
      "training: 31 batch 355 batch_loss: 0.16715165972709656\n",
      "training: 31 batch 356 batch_loss: 0.16441193222999573\n",
      "training: 31 batch 357 batch_loss: 0.16478300094604492\n",
      "training: 31 batch 358 batch_loss: 0.1686604917049408\n",
      "training: 31 batch 359 batch_loss: 0.16423776745796204\n",
      "training: 31 batch 360 batch_loss: 0.1630595624446869\n",
      "training: 31 batch 361 batch_loss: 0.1651412844657898\n",
      "training: 31 batch 362 batch_loss: 0.16521495580673218\n",
      "training: 31 batch 363 batch_loss: 0.1638506054878235\n",
      "training: 31 batch 364 batch_loss: 0.16547727584838867\n",
      "training: 31 batch 365 batch_loss: 0.16474273800849915\n",
      "training: 31 batch 366 batch_loss: 0.16686809062957764\n",
      "training: 31 batch 367 batch_loss: 0.16734421253204346\n",
      "training: 31 batch 368 batch_loss: 0.16481217741966248\n",
      "training: 31 batch 369 batch_loss: 0.1696377992630005\n",
      "training: 31 batch 370 batch_loss: 0.16580110788345337\n",
      "training: 31 batch 371 batch_loss: 0.16660144925117493\n",
      "training: 31 batch 372 batch_loss: 0.16580313444137573\n",
      "training: 31 batch 373 batch_loss: 0.16624760627746582\n",
      "training: 31 batch 374 batch_loss: 0.16906991600990295\n",
      "training: 31 batch 375 batch_loss: 0.1650356948375702\n",
      "training: 31 batch 376 batch_loss: 0.16755133867263794\n",
      "training: 31 batch 377 batch_loss: 0.1698530614376068\n",
      "training: 31 batch 378 batch_loss: 0.16332918405532837\n",
      "training: 31 batch 379 batch_loss: 0.16371551156044006\n",
      "training: 31 batch 380 batch_loss: 0.1629478633403778\n",
      "training: 31 batch 381 batch_loss: 0.16645795106887817\n",
      "training: 31 batch 382 batch_loss: 0.1699601113796234\n",
      "training: 31 batch 383 batch_loss: 0.1667974889278412\n",
      "training: 31 batch 384 batch_loss: 0.16397112607955933\n",
      "training: 31 batch 385 batch_loss: 0.16909438371658325\n",
      "training: 31 batch 386 batch_loss: 0.16513246297836304\n",
      "training: 31 batch 387 batch_loss: 0.1663203239440918\n",
      "training: 31 batch 388 batch_loss: 0.16545900702476501\n",
      "training: 31 batch 389 batch_loss: 0.16652172803878784\n",
      "training: 31 batch 390 batch_loss: 0.16534918546676636\n",
      "training: 31 batch 391 batch_loss: 0.1622636914253235\n",
      "training: 31 batch 392 batch_loss: 0.1664339005947113\n",
      "training: 31 batch 393 batch_loss: 0.16493827104568481\n",
      "training: 31 batch 394 batch_loss: 0.16480699181556702\n",
      "training: 31 batch 395 batch_loss: 0.16475942730903625\n",
      "training: 31 batch 396 batch_loss: 0.16632384061813354\n",
      "training: 31 batch 397 batch_loss: 0.1623193323612213\n",
      "training: 31 batch 398 batch_loss: 0.16041025519371033\n",
      "training: 31 batch 399 batch_loss: 0.16548922657966614\n",
      "training: 31 batch 400 batch_loss: 0.16345560550689697\n",
      "training: 31 batch 401 batch_loss: 0.1652350127696991\n",
      "training: 31 batch 402 batch_loss: 0.1626790165901184\n",
      "training: 31 batch 403 batch_loss: 0.16713294386863708\n",
      "training: 31 batch 404 batch_loss: 0.16465285420417786\n",
      "training: 31 batch 405 batch_loss: 0.16632702946662903\n",
      "training: 31 batch 406 batch_loss: 0.16357719898223877\n",
      "training: 31 batch 407 batch_loss: 0.16646432876586914\n",
      "training: 31 batch 408 batch_loss: 0.16692933440208435\n",
      "training: 31 batch 409 batch_loss: 0.16604715585708618\n",
      "training: 31 batch 410 batch_loss: 0.16467216610908508\n",
      "training: 31 batch 411 batch_loss: 0.16826599836349487\n",
      "training: 31 batch 412 batch_loss: 0.16316461563110352\n",
      "training: 31 batch 413 batch_loss: 0.1643843650817871\n",
      "training: 31 batch 414 batch_loss: 0.16866663098335266\n",
      "training: 31 batch 415 batch_loss: 0.1677427887916565\n",
      "training: 31 batch 416 batch_loss: 0.16424322128295898\n",
      "training: 31 batch 417 batch_loss: 0.16305941343307495\n",
      "training: 31 batch 418 batch_loss: 0.1654006838798523\n",
      "training: 31 batch 419 batch_loss: 0.1636102795600891\n",
      "training: 31 batch 420 batch_loss: 0.1639881432056427\n",
      "training: 31 batch 421 batch_loss: 0.16558438539505005\n",
      "training: 31 batch 422 batch_loss: 0.1641443967819214\n",
      "training: 31 batch 423 batch_loss: 0.16682660579681396\n",
      "training: 31 batch 424 batch_loss: 0.16477110981941223\n",
      "training: 31 batch 425 batch_loss: 0.16148483753204346\n",
      "training: 31 batch 426 batch_loss: 0.1637222170829773\n",
      "training: 31 batch 427 batch_loss: 0.16353538632392883\n",
      "training: 31 batch 428 batch_loss: 0.16600772738456726\n",
      "training: 31 batch 429 batch_loss: 0.16417616605758667\n",
      "training: 31 batch 430 batch_loss: 0.1680741310119629\n",
      "training: 31 batch 431 batch_loss: 0.16452834010124207\n",
      "training: 31 batch 432 batch_loss: 0.1650104820728302\n",
      "training: 31 batch 433 batch_loss: 0.16606810688972473\n",
      "training: 31 batch 434 batch_loss: 0.16474169492721558\n",
      "training: 31 batch 435 batch_loss: 0.16936174035072327\n",
      "training: 31 batch 436 batch_loss: 0.16302400827407837\n",
      "training: 31 batch 437 batch_loss: 0.1636698842048645\n",
      "training: 31 batch 438 batch_loss: 0.16623255610466003\n",
      "training: 31 batch 439 batch_loss: 0.16377830505371094\n",
      "training: 31 batch 440 batch_loss: 0.1667761206626892\n",
      "training: 31 batch 441 batch_loss: 0.16622883081436157\n",
      "training: 31 batch 442 batch_loss: 0.16573548316955566\n",
      "training: 31 batch 443 batch_loss: 0.16940215229988098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 31 batch 444 batch_loss: 0.16555511951446533\n",
      "training: 31 batch 445 batch_loss: 0.16608983278274536\n",
      "training: 31 batch 446 batch_loss: 0.16802233457565308\n",
      "training: 31 batch 447 batch_loss: 0.1661723554134369\n",
      "training: 31 batch 448 batch_loss: 0.16377121210098267\n",
      "training: 31 batch 449 batch_loss: 0.16644585132598877\n",
      "training: 31 batch 450 batch_loss: 0.16120344400405884\n",
      "training: 31 batch 451 batch_loss: 0.16946333646774292\n",
      "training: 31 batch 452 batch_loss: 0.16258695721626282\n",
      "training: 31 batch 453 batch_loss: 0.16415780782699585\n",
      "training: 31 batch 454 batch_loss: 0.1658628284931183\n",
      "training: 31 batch 455 batch_loss: 0.16218680143356323\n",
      "training: 31 batch 456 batch_loss: 0.16831958293914795\n",
      "training: 31 batch 457 batch_loss: 0.16570332646369934\n",
      "training: 31 batch 458 batch_loss: 0.16902905702590942\n",
      "training: 31 batch 459 batch_loss: 0.16932716965675354\n",
      "training: 31 batch 460 batch_loss: 0.1614656150341034\n",
      "training: 31 batch 461 batch_loss: 0.1646794080734253\n",
      "training: 31 batch 462 batch_loss: 0.165041983127594\n",
      "training: 31 batch 463 batch_loss: 0.16735154390335083\n",
      "training: 31 batch 464 batch_loss: 0.16183724999427795\n",
      "training: 31 batch 465 batch_loss: 0.16628620028495789\n",
      "training: 31 batch 466 batch_loss: 0.16227465867996216\n",
      "training: 31 batch 467 batch_loss: 0.16382086277008057\n",
      "training: 31 batch 468 batch_loss: 0.1660107970237732\n",
      "training: 31 batch 469 batch_loss: 0.1688145399093628\n",
      "training: 31 batch 470 batch_loss: 0.16351628303527832\n",
      "training: 31 batch 471 batch_loss: 0.1669067144393921\n",
      "training: 31 batch 472 batch_loss: 0.159373939037323\n",
      "training: 31 batch 473 batch_loss: 0.17004725337028503\n",
      "training: 31 batch 474 batch_loss: 0.16593432426452637\n",
      "training: 31 batch 475 batch_loss: 0.16639530658721924\n",
      "training: 31 batch 476 batch_loss: 0.16502264142036438\n",
      "training: 31 batch 477 batch_loss: 0.16867002844810486\n",
      "training: 31 batch 478 batch_loss: 0.1669885218143463\n",
      "training: 31 batch 479 batch_loss: 0.16892889142036438\n",
      "training: 31 batch 480 batch_loss: 0.16541963815689087\n",
      "training: 31 batch 481 batch_loss: 0.16389042139053345\n",
      "training: 31 batch 482 batch_loss: 0.16724860668182373\n",
      "training: 31 batch 483 batch_loss: 0.16198718547821045\n",
      "training: 31 batch 484 batch_loss: 0.1671421229839325\n",
      "training: 31 batch 485 batch_loss: 0.16626375913619995\n",
      "training: 31 batch 486 batch_loss: 0.16475903987884521\n",
      "training: 31 batch 487 batch_loss: 0.16687539219856262\n",
      "training: 31 batch 488 batch_loss: 0.16486123204231262\n",
      "training: 31 batch 489 batch_loss: 0.1637125313282013\n",
      "training: 31 batch 490 batch_loss: 0.167596697807312\n",
      "training: 31 batch 491 batch_loss: 0.16489142179489136\n",
      "training: 31 batch 492 batch_loss: 0.16415995359420776\n",
      "training: 31 batch 493 batch_loss: 0.1660289168357849\n",
      "training: 31 batch 494 batch_loss: 0.164016991853714\n",
      "training: 31 batch 495 batch_loss: 0.1628548800945282\n",
      "training: 31 batch 496 batch_loss: 0.16537559032440186\n",
      "training: 31 batch 497 batch_loss: 0.16631093621253967\n",
      "training: 31 batch 498 batch_loss: 0.16773363947868347\n",
      "training: 31 batch 499 batch_loss: 0.16497528553009033\n",
      "training: 31 batch 500 batch_loss: 0.1640649437904358\n",
      "training: 31 batch 501 batch_loss: 0.16573578119277954\n",
      "training: 31 batch 502 batch_loss: 0.16490542888641357\n",
      "training: 31 batch 503 batch_loss: 0.16587626934051514\n",
      "training: 31 batch 504 batch_loss: 0.16151711344718933\n",
      "training: 31 batch 505 batch_loss: 0.16617503762245178\n",
      "training: 31 batch 506 batch_loss: 0.16650518774986267\n",
      "training: 31 batch 507 batch_loss: 0.16395342350006104\n",
      "training: 31 batch 508 batch_loss: 0.1658991575241089\n",
      "training: 31 batch 509 batch_loss: 0.1668473482131958\n",
      "training: 31 batch 510 batch_loss: 0.16423636674880981\n",
      "training: 31 batch 511 batch_loss: 0.16429108381271362\n",
      "training: 31 batch 512 batch_loss: 0.1676640808582306\n",
      "training: 31 batch 513 batch_loss: 0.16524025797843933\n",
      "training: 31 batch 514 batch_loss: 0.16353371739387512\n",
      "training: 31 batch 515 batch_loss: 0.1659696400165558\n",
      "training: 31 batch 516 batch_loss: 0.16247215867042542\n",
      "training: 31 batch 517 batch_loss: 0.16527768969535828\n",
      "training: 31 batch 518 batch_loss: 0.1664963662624359\n",
      "training: 31 batch 519 batch_loss: 0.16323089599609375\n",
      "training: 31 batch 520 batch_loss: 0.16694867610931396\n",
      "training: 31 batch 521 batch_loss: 0.16677480936050415\n",
      "training: 31 batch 522 batch_loss: 0.16686642169952393\n",
      "training: 31 batch 523 batch_loss: 0.16557186841964722\n",
      "training: 31 batch 524 batch_loss: 0.16496020555496216\n",
      "training: 31 batch 525 batch_loss: 0.1662655472755432\n",
      "training: 31 batch 526 batch_loss: 0.16459649801254272\n",
      "training: 31 batch 527 batch_loss: 0.16656607389450073\n",
      "training: 31 batch 528 batch_loss: 0.16704243421554565\n",
      "training: 31 batch 529 batch_loss: 0.1682443618774414\n",
      "training: 31 batch 530 batch_loss: 0.16387072205543518\n",
      "training: 31 batch 531 batch_loss: 0.16428327560424805\n",
      "training: 31 batch 532 batch_loss: 0.1664915382862091\n",
      "training: 31 batch 533 batch_loss: 0.16416072845458984\n",
      "training: 31 batch 534 batch_loss: 0.1675744652748108\n",
      "training: 31 batch 535 batch_loss: 0.1651327908039093\n",
      "training: 31 batch 536 batch_loss: 0.1657177209854126\n",
      "training: 31 batch 537 batch_loss: 0.16440841555595398\n",
      "training: 31 batch 538 batch_loss: 0.15981432795524597\n",
      "training: 31 batch 539 batch_loss: 0.1653802990913391\n",
      "training: 31 batch 540 batch_loss: 0.16648456454277039\n",
      "training: 31 batch 541 batch_loss: 0.16967326402664185\n",
      "training: 31 batch 542 batch_loss: 0.1691402792930603\n",
      "training: 31 batch 543 batch_loss: 0.1694139838218689\n",
      "training: 31 batch 544 batch_loss: 0.16458094120025635\n",
      "training: 31 batch 545 batch_loss: 0.16419818997383118\n",
      "training: 31 batch 546 batch_loss: 0.16640764474868774\n",
      "training: 31 batch 547 batch_loss: 0.16327786445617676\n",
      "training: 31 batch 548 batch_loss: 0.16677820682525635\n",
      "training: 31 batch 549 batch_loss: 0.16666263341903687\n",
      "training: 31 batch 550 batch_loss: 0.1645471453666687\n",
      "training: 31 batch 551 batch_loss: 0.16621482372283936\n",
      "training: 31 batch 552 batch_loss: 0.165037602186203\n",
      "training: 31 batch 553 batch_loss: 0.16567867994308472\n",
      "training: 31 batch 554 batch_loss: 0.1664435863494873\n",
      "training: 31 batch 555 batch_loss: 0.16244915127754211\n",
      "training: 31 batch 556 batch_loss: 0.1684184968471527\n",
      "training: 31 batch 557 batch_loss: 0.16619887948036194\n",
      "training: 31 batch 558 batch_loss: 0.16458165645599365\n",
      "training: 31 batch 559 batch_loss: 0.16450893878936768\n",
      "training: 31 batch 560 batch_loss: 0.16418671607971191\n",
      "training: 31 batch 561 batch_loss: 0.1685534417629242\n",
      "training: 31 batch 562 batch_loss: 0.16499757766723633\n",
      "training: 31 batch 563 batch_loss: 0.167159765958786\n",
      "training: 31 batch 564 batch_loss: 0.16776221990585327\n",
      "training: 31 batch 565 batch_loss: 0.1657753586769104\n",
      "training: 31 batch 566 batch_loss: 0.16712728142738342\n",
      "training: 31 batch 567 batch_loss: 0.16359049081802368\n",
      "training: 31 batch 568 batch_loss: 0.16343578696250916\n",
      "training: 31 batch 569 batch_loss: 0.16426846385002136\n",
      "training: 31 batch 570 batch_loss: 0.16475671529769897\n",
      "training: 31 batch 571 batch_loss: 0.16187834739685059\n",
      "training: 31 batch 572 batch_loss: 0.166736900806427\n",
      "training: 31 batch 573 batch_loss: 0.16614016890525818\n",
      "training: 31 batch 574 batch_loss: 0.1691027283668518\n",
      "training: 31 batch 575 batch_loss: 0.1683611273765564\n",
      "training: 31 batch 576 batch_loss: 0.16528931260108948\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 31, Hit Ratio:0.029491027439014234 | Precision:0.043512238277794164 | Recall:0.05893196152868161 | NDCG:0.05568386441320187\n",
      "*Best Performance* \n",
      "Epoch: 30, Hit Ratio:0.02982081527550609 | Precision:0.0439988204069596 | Recall:0.05920473193928391 | MDCG:0.05634308849316303\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 32 batch 0 batch_loss: 0.16467267274856567\n",
      "training: 32 batch 1 batch_loss: 0.1671307384967804\n",
      "training: 32 batch 2 batch_loss: 0.16029831767082214\n",
      "training: 32 batch 3 batch_loss: 0.163429856300354\n",
      "training: 32 batch 4 batch_loss: 0.16101986169815063\n",
      "training: 32 batch 5 batch_loss: 0.16405457258224487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 32 batch 6 batch_loss: 0.16139304637908936\n",
      "training: 32 batch 7 batch_loss: 0.1674245297908783\n",
      "training: 32 batch 8 batch_loss: 0.1643807291984558\n",
      "training: 32 batch 9 batch_loss: 0.16551131010055542\n",
      "training: 32 batch 10 batch_loss: 0.16471830010414124\n",
      "training: 32 batch 11 batch_loss: 0.16216498613357544\n",
      "training: 32 batch 12 batch_loss: 0.16300565004348755\n",
      "training: 32 batch 13 batch_loss: 0.16446402668952942\n",
      "training: 32 batch 14 batch_loss: 0.1599852442741394\n",
      "training: 32 batch 15 batch_loss: 0.1638583540916443\n",
      "training: 32 batch 16 batch_loss: 0.16742545366287231\n",
      "training: 32 batch 17 batch_loss: 0.1643814742565155\n",
      "training: 32 batch 18 batch_loss: 0.16282084584236145\n",
      "training: 32 batch 19 batch_loss: 0.1667444109916687\n",
      "training: 32 batch 20 batch_loss: 0.16415950655937195\n",
      "training: 32 batch 21 batch_loss: 0.16360247135162354\n",
      "training: 32 batch 22 batch_loss: 0.16332489252090454\n",
      "training: 32 batch 23 batch_loss: 0.16340646147727966\n",
      "training: 32 batch 24 batch_loss: 0.16597703099250793\n",
      "training: 32 batch 25 batch_loss: 0.16320857405662537\n",
      "training: 32 batch 26 batch_loss: 0.16211390495300293\n",
      "training: 32 batch 27 batch_loss: 0.16639164090156555\n",
      "training: 32 batch 28 batch_loss: 0.1630040407180786\n",
      "training: 32 batch 29 batch_loss: 0.16549897193908691\n",
      "training: 32 batch 30 batch_loss: 0.16756737232208252\n",
      "training: 32 batch 31 batch_loss: 0.16349217295646667\n",
      "training: 32 batch 32 batch_loss: 0.1623310148715973\n",
      "training: 32 batch 33 batch_loss: 0.16520792245864868\n",
      "training: 32 batch 34 batch_loss: 0.1615304946899414\n",
      "training: 32 batch 35 batch_loss: 0.16617697477340698\n",
      "training: 32 batch 36 batch_loss: 0.16650176048278809\n",
      "training: 32 batch 37 batch_loss: 0.16387328505516052\n",
      "training: 32 batch 38 batch_loss: 0.16439715027809143\n",
      "training: 32 batch 39 batch_loss: 0.16337904334068298\n",
      "training: 32 batch 40 batch_loss: 0.16142728924751282\n",
      "training: 32 batch 41 batch_loss: 0.16741377115249634\n",
      "training: 32 batch 42 batch_loss: 0.16422897577285767\n",
      "training: 32 batch 43 batch_loss: 0.16186195611953735\n",
      "training: 32 batch 44 batch_loss: 0.16857370734214783\n",
      "training: 32 batch 45 batch_loss: 0.15997624397277832\n",
      "training: 32 batch 46 batch_loss: 0.16505321860313416\n",
      "training: 32 batch 47 batch_loss: 0.16539156436920166\n",
      "training: 32 batch 48 batch_loss: 0.16427645087242126\n",
      "training: 32 batch 49 batch_loss: 0.1630536913871765\n",
      "training: 32 batch 50 batch_loss: 0.16603273153305054\n",
      "training: 32 batch 51 batch_loss: 0.16092422604560852\n",
      "training: 32 batch 52 batch_loss: 0.16656336188316345\n",
      "training: 32 batch 53 batch_loss: 0.16600924730300903\n",
      "training: 32 batch 54 batch_loss: 0.1649511456489563\n",
      "training: 32 batch 55 batch_loss: 0.1631394922733307\n",
      "training: 32 batch 56 batch_loss: 0.16424483060836792\n",
      "training: 32 batch 57 batch_loss: 0.16241157054901123\n",
      "training: 32 batch 58 batch_loss: 0.16457384824752808\n",
      "training: 32 batch 59 batch_loss: 0.16632720828056335\n",
      "training: 32 batch 60 batch_loss: 0.15794846415519714\n",
      "training: 32 batch 61 batch_loss: 0.16549891233444214\n",
      "training: 32 batch 62 batch_loss: 0.1609303057193756\n",
      "training: 32 batch 63 batch_loss: 0.16521725058555603\n",
      "training: 32 batch 64 batch_loss: 0.1653212308883667\n",
      "training: 32 batch 65 batch_loss: 0.16381776332855225\n",
      "training: 32 batch 66 batch_loss: 0.16535687446594238\n",
      "training: 32 batch 67 batch_loss: 0.16623139381408691\n",
      "training: 32 batch 68 batch_loss: 0.164878249168396\n",
      "training: 32 batch 69 batch_loss: 0.1641382873058319\n",
      "training: 32 batch 70 batch_loss: 0.16354069113731384\n",
      "training: 32 batch 71 batch_loss: 0.1646939218044281\n",
      "training: 32 batch 72 batch_loss: 0.1624070405960083\n",
      "training: 32 batch 73 batch_loss: 0.16769057512283325\n",
      "training: 32 batch 74 batch_loss: 0.1651650071144104\n",
      "training: 32 batch 75 batch_loss: 0.1640256643295288\n",
      "training: 32 batch 76 batch_loss: 0.1683340072631836\n",
      "training: 32 batch 77 batch_loss: 0.16202202439308167\n",
      "training: 32 batch 78 batch_loss: 0.16597828269004822\n",
      "training: 32 batch 79 batch_loss: 0.16006028652191162\n",
      "training: 32 batch 80 batch_loss: 0.1674429178237915\n",
      "training: 32 batch 81 batch_loss: 0.16530263423919678\n",
      "training: 32 batch 82 batch_loss: 0.16324201226234436\n",
      "training: 32 batch 83 batch_loss: 0.16261368989944458\n",
      "training: 32 batch 84 batch_loss: 0.1652064323425293\n",
      "training: 32 batch 85 batch_loss: 0.1668403148651123\n",
      "training: 32 batch 86 batch_loss: 0.16600388288497925\n",
      "training: 32 batch 87 batch_loss: 0.1631162166595459\n",
      "training: 32 batch 88 batch_loss: 0.16663873195648193\n",
      "training: 32 batch 89 batch_loss: 0.16522809863090515\n",
      "training: 32 batch 90 batch_loss: 0.1664249300956726\n",
      "training: 32 batch 91 batch_loss: 0.16709798574447632\n",
      "training: 32 batch 92 batch_loss: 0.1657320261001587\n",
      "training: 32 batch 93 batch_loss: 0.16650965809822083\n",
      "training: 32 batch 94 batch_loss: 0.1677412986755371\n",
      "training: 32 batch 95 batch_loss: 0.16405078768730164\n",
      "training: 32 batch 96 batch_loss: 0.1604943871498108\n",
      "training: 32 batch 97 batch_loss: 0.16588568687438965\n",
      "training: 32 batch 98 batch_loss: 0.16465362906455994\n",
      "training: 32 batch 99 batch_loss: 0.16527193784713745\n",
      "training: 32 batch 100 batch_loss: 0.16357606649398804\n",
      "training: 32 batch 101 batch_loss: 0.16984224319458008\n",
      "training: 32 batch 102 batch_loss: 0.1631198227405548\n",
      "training: 32 batch 103 batch_loss: 0.1641499102115631\n",
      "training: 32 batch 104 batch_loss: 0.16192173957824707\n",
      "training: 32 batch 105 batch_loss: 0.16387763619422913\n",
      "training: 32 batch 106 batch_loss: 0.16182491183280945\n",
      "training: 32 batch 107 batch_loss: 0.1694052815437317\n",
      "training: 32 batch 108 batch_loss: 0.16529107093811035\n",
      "training: 32 batch 109 batch_loss: 0.16380080580711365\n",
      "training: 32 batch 110 batch_loss: 0.16502606868743896\n",
      "training: 32 batch 111 batch_loss: 0.1632837951183319\n",
      "training: 32 batch 112 batch_loss: 0.16778239607810974\n",
      "training: 32 batch 113 batch_loss: 0.16861656308174133\n",
      "training: 32 batch 114 batch_loss: 0.16592395305633545\n",
      "training: 32 batch 115 batch_loss: 0.16281798481941223\n",
      "training: 32 batch 116 batch_loss: 0.16470393538475037\n",
      "training: 32 batch 117 batch_loss: 0.16709214448928833\n",
      "training: 32 batch 118 batch_loss: 0.16396787762641907\n",
      "training: 32 batch 119 batch_loss: 0.166934072971344\n",
      "training: 32 batch 120 batch_loss: 0.16524004936218262\n",
      "training: 32 batch 121 batch_loss: 0.16426420211791992\n",
      "training: 32 batch 122 batch_loss: 0.16691046953201294\n",
      "training: 32 batch 123 batch_loss: 0.1660391390323639\n",
      "training: 32 batch 124 batch_loss: 0.16636937856674194\n",
      "training: 32 batch 125 batch_loss: 0.16372895240783691\n",
      "training: 32 batch 126 batch_loss: 0.16550570726394653\n",
      "training: 32 batch 127 batch_loss: 0.16385877132415771\n",
      "training: 32 batch 128 batch_loss: 0.16554415225982666\n",
      "training: 32 batch 129 batch_loss: 0.1674952507019043\n",
      "training: 32 batch 130 batch_loss: 0.16959071159362793\n",
      "training: 32 batch 131 batch_loss: 0.16436386108398438\n",
      "training: 32 batch 132 batch_loss: 0.16747772693634033\n",
      "training: 32 batch 133 batch_loss: 0.16456589102745056\n",
      "training: 32 batch 134 batch_loss: 0.16518884897232056\n",
      "training: 32 batch 135 batch_loss: 0.16475218534469604\n",
      "training: 32 batch 136 batch_loss: 0.16623741388320923\n",
      "training: 32 batch 137 batch_loss: 0.16276735067367554\n",
      "training: 32 batch 138 batch_loss: 0.16393664479255676\n",
      "training: 32 batch 139 batch_loss: 0.16794437170028687\n",
      "training: 32 batch 140 batch_loss: 0.16508901119232178\n",
      "training: 32 batch 141 batch_loss: 0.1683938205242157\n",
      "training: 32 batch 142 batch_loss: 0.1612478494644165\n",
      "training: 32 batch 143 batch_loss: 0.16669440269470215\n",
      "training: 32 batch 144 batch_loss: 0.1656116247177124\n",
      "training: 32 batch 145 batch_loss: 0.16672605276107788\n",
      "training: 32 batch 146 batch_loss: 0.16702669858932495\n",
      "training: 32 batch 147 batch_loss: 0.16477066278457642\n",
      "training: 32 batch 148 batch_loss: 0.16684788465499878\n",
      "training: 32 batch 149 batch_loss: 0.1681128442287445\n",
      "training: 32 batch 150 batch_loss: 0.16963618993759155\n",
      "training: 32 batch 151 batch_loss: 0.16578829288482666\n",
      "training: 32 batch 152 batch_loss: 0.16527977585792542\n",
      "training: 32 batch 153 batch_loss: 0.16146621108055115\n",
      "training: 32 batch 154 batch_loss: 0.1638025939464569\n",
      "training: 32 batch 155 batch_loss: 0.16448715329170227\n",
      "training: 32 batch 156 batch_loss: 0.1639430820941925\n",
      "training: 32 batch 157 batch_loss: 0.16531062126159668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 32 batch 158 batch_loss: 0.16541820764541626\n",
      "training: 32 batch 159 batch_loss: 0.16387474536895752\n",
      "training: 32 batch 160 batch_loss: 0.16824889183044434\n",
      "training: 32 batch 161 batch_loss: 0.16658863425254822\n",
      "training: 32 batch 162 batch_loss: 0.16641968488693237\n",
      "training: 32 batch 163 batch_loss: 0.16346585750579834\n",
      "training: 32 batch 164 batch_loss: 0.16715455055236816\n",
      "training: 32 batch 165 batch_loss: 0.1628478765487671\n",
      "training: 32 batch 166 batch_loss: 0.1651841104030609\n",
      "training: 32 batch 167 batch_loss: 0.16623973846435547\n",
      "training: 32 batch 168 batch_loss: 0.1656513214111328\n",
      "training: 32 batch 169 batch_loss: 0.16266736388206482\n",
      "training: 32 batch 170 batch_loss: 0.16313350200653076\n",
      "training: 32 batch 171 batch_loss: 0.16323783993721008\n",
      "training: 32 batch 172 batch_loss: 0.1666540801525116\n",
      "training: 32 batch 173 batch_loss: 0.16409865021705627\n",
      "training: 32 batch 174 batch_loss: 0.16714811325073242\n",
      "training: 32 batch 175 batch_loss: 0.1662680208683014\n",
      "training: 32 batch 176 batch_loss: 0.1620565950870514\n",
      "training: 32 batch 177 batch_loss: 0.16867101192474365\n",
      "training: 32 batch 178 batch_loss: 0.16397666931152344\n",
      "training: 32 batch 179 batch_loss: 0.16677549481391907\n",
      "training: 32 batch 180 batch_loss: 0.1643925905227661\n",
      "training: 32 batch 181 batch_loss: 0.16503408551216125\n",
      "training: 32 batch 182 batch_loss: 0.16544127464294434\n",
      "training: 32 batch 183 batch_loss: 0.16434302926063538\n",
      "training: 32 batch 184 batch_loss: 0.1665155291557312\n",
      "training: 32 batch 185 batch_loss: 0.16402381658554077\n",
      "training: 32 batch 186 batch_loss: 0.16868090629577637\n",
      "training: 32 batch 187 batch_loss: 0.1668599247932434\n",
      "training: 32 batch 188 batch_loss: 0.16642281413078308\n",
      "training: 32 batch 189 batch_loss: 0.16776472330093384\n",
      "training: 32 batch 190 batch_loss: 0.16488313674926758\n",
      "training: 32 batch 191 batch_loss: 0.1662110984325409\n",
      "training: 32 batch 192 batch_loss: 0.1632116436958313\n",
      "training: 32 batch 193 batch_loss: 0.16320586204528809\n",
      "training: 32 batch 194 batch_loss: 0.1650330126285553\n",
      "training: 32 batch 195 batch_loss: 0.16705584526062012\n",
      "training: 32 batch 196 batch_loss: 0.1667480766773224\n",
      "training: 32 batch 197 batch_loss: 0.16447892785072327\n",
      "training: 32 batch 198 batch_loss: 0.16477349400520325\n",
      "training: 32 batch 199 batch_loss: 0.1640438437461853\n",
      "training: 32 batch 200 batch_loss: 0.1643357276916504\n",
      "training: 32 batch 201 batch_loss: 0.1668781340122223\n",
      "training: 32 batch 202 batch_loss: 0.16530531644821167\n",
      "training: 32 batch 203 batch_loss: 0.16830632090568542\n",
      "training: 32 batch 204 batch_loss: 0.16674581170082092\n",
      "training: 32 batch 205 batch_loss: 0.16473063826560974\n",
      "training: 32 batch 206 batch_loss: 0.165032297372818\n",
      "training: 32 batch 207 batch_loss: 0.17004677653312683\n",
      "training: 32 batch 208 batch_loss: 0.16523513197898865\n",
      "training: 32 batch 209 batch_loss: 0.16756120324134827\n",
      "training: 32 batch 210 batch_loss: 0.16601479053497314\n",
      "training: 32 batch 211 batch_loss: 0.16247057914733887\n",
      "training: 32 batch 212 batch_loss: 0.16725397109985352\n",
      "training: 32 batch 213 batch_loss: 0.16447657346725464\n",
      "training: 32 batch 214 batch_loss: 0.1649564802646637\n",
      "training: 32 batch 215 batch_loss: 0.16607201099395752\n",
      "training: 32 batch 216 batch_loss: 0.16469857096672058\n",
      "training: 32 batch 217 batch_loss: 0.16463226079940796\n",
      "training: 32 batch 218 batch_loss: 0.16460412740707397\n",
      "training: 32 batch 219 batch_loss: 0.16978272795677185\n",
      "training: 32 batch 220 batch_loss: 0.1641099750995636\n",
      "training: 32 batch 221 batch_loss: 0.16303980350494385\n",
      "training: 32 batch 222 batch_loss: 0.16671067476272583\n",
      "training: 32 batch 223 batch_loss: 0.16718047857284546\n",
      "training: 32 batch 224 batch_loss: 0.16603699326515198\n",
      "training: 32 batch 225 batch_loss: 0.16568487882614136\n",
      "training: 32 batch 226 batch_loss: 0.16607072949409485\n",
      "training: 32 batch 227 batch_loss: 0.16262733936309814\n",
      "training: 32 batch 228 batch_loss: 0.1662239134311676\n",
      "training: 32 batch 229 batch_loss: 0.16678303480148315\n",
      "training: 32 batch 230 batch_loss: 0.169327974319458\n",
      "training: 32 batch 231 batch_loss: 0.16569939255714417\n",
      "training: 32 batch 232 batch_loss: 0.17019391059875488\n",
      "training: 32 batch 233 batch_loss: 0.16533076763153076\n",
      "training: 32 batch 234 batch_loss: 0.1632869839668274\n",
      "training: 32 batch 235 batch_loss: 0.16345590353012085\n",
      "training: 32 batch 236 batch_loss: 0.16703733801841736\n",
      "training: 32 batch 237 batch_loss: 0.16812384128570557\n",
      "training: 32 batch 238 batch_loss: 0.16588401794433594\n",
      "training: 32 batch 239 batch_loss: 0.1644105315208435\n",
      "training: 32 batch 240 batch_loss: 0.16571453213691711\n",
      "training: 32 batch 241 batch_loss: 0.16431942582130432\n",
      "training: 32 batch 242 batch_loss: 0.16832461953163147\n",
      "training: 32 batch 243 batch_loss: 0.16453999280929565\n",
      "training: 32 batch 244 batch_loss: 0.17134538292884827\n",
      "training: 32 batch 245 batch_loss: 0.1621415615081787\n",
      "training: 32 batch 246 batch_loss: 0.16176986694335938\n",
      "training: 32 batch 247 batch_loss: 0.1631423532962799\n",
      "training: 32 batch 248 batch_loss: 0.16386041045188904\n",
      "training: 32 batch 249 batch_loss: 0.16657492518424988\n",
      "training: 32 batch 250 batch_loss: 0.16451755166053772\n",
      "training: 32 batch 251 batch_loss: 0.16504555940628052\n",
      "training: 32 batch 252 batch_loss: 0.1672133207321167\n",
      "training: 32 batch 253 batch_loss: 0.17297309637069702\n",
      "training: 32 batch 254 batch_loss: 0.16793859004974365\n",
      "training: 32 batch 255 batch_loss: 0.16449680924415588\n",
      "training: 32 batch 256 batch_loss: 0.16804596781730652\n",
      "training: 32 batch 257 batch_loss: 0.16710472106933594\n",
      "training: 32 batch 258 batch_loss: 0.16598469018936157\n",
      "training: 32 batch 259 batch_loss: 0.16629689931869507\n",
      "training: 32 batch 260 batch_loss: 0.16426068544387817\n",
      "training: 32 batch 261 batch_loss: 0.17004290223121643\n",
      "training: 32 batch 262 batch_loss: 0.16597160696983337\n",
      "training: 32 batch 263 batch_loss: 0.1650809645652771\n",
      "training: 32 batch 264 batch_loss: 0.16439470648765564\n",
      "training: 32 batch 265 batch_loss: 0.1661873161792755\n",
      "training: 32 batch 266 batch_loss: 0.16832756996154785\n",
      "training: 32 batch 267 batch_loss: 0.17026036977767944\n",
      "training: 32 batch 268 batch_loss: 0.16466844081878662\n",
      "training: 32 batch 269 batch_loss: 0.16528111696243286\n",
      "training: 32 batch 270 batch_loss: 0.16472101211547852\n",
      "training: 32 batch 271 batch_loss: 0.16741731762886047\n",
      "training: 32 batch 272 batch_loss: 0.16578134894371033\n",
      "training: 32 batch 273 batch_loss: 0.16377082467079163\n",
      "training: 32 batch 274 batch_loss: 0.16818967461585999\n",
      "training: 32 batch 275 batch_loss: 0.1664506196975708\n",
      "training: 32 batch 276 batch_loss: 0.16611617803573608\n",
      "training: 32 batch 277 batch_loss: 0.1676824390888214\n",
      "training: 32 batch 278 batch_loss: 0.16668415069580078\n",
      "training: 32 batch 279 batch_loss: 0.16613814234733582\n",
      "training: 32 batch 280 batch_loss: 0.1669052243232727\n",
      "training: 32 batch 281 batch_loss: 0.16902658343315125\n",
      "training: 32 batch 282 batch_loss: 0.16646426916122437\n",
      "training: 32 batch 283 batch_loss: 0.16951826214790344\n",
      "training: 32 batch 284 batch_loss: 0.16612139344215393\n",
      "training: 32 batch 285 batch_loss: 0.16829952597618103\n",
      "training: 32 batch 286 batch_loss: 0.1678953766822815\n",
      "training: 32 batch 287 batch_loss: 0.16727066040039062\n",
      "training: 32 batch 288 batch_loss: 0.1618921160697937\n",
      "training: 32 batch 289 batch_loss: 0.16827839612960815\n",
      "training: 32 batch 290 batch_loss: 0.1688482165336609\n",
      "training: 32 batch 291 batch_loss: 0.16671624779701233\n",
      "training: 32 batch 292 batch_loss: 0.16882574558258057\n",
      "training: 32 batch 293 batch_loss: 0.16879284381866455\n",
      "training: 32 batch 294 batch_loss: 0.1632465124130249\n",
      "training: 32 batch 295 batch_loss: 0.1655711829662323\n",
      "training: 32 batch 296 batch_loss: 0.16677942872047424\n",
      "training: 32 batch 297 batch_loss: 0.16724321246147156\n",
      "training: 32 batch 298 batch_loss: 0.169362872838974\n",
      "training: 32 batch 299 batch_loss: 0.16966044902801514\n",
      "training: 32 batch 300 batch_loss: 0.16517296433448792\n",
      "training: 32 batch 301 batch_loss: 0.16351765394210815\n",
      "training: 32 batch 302 batch_loss: 0.16751545667648315\n",
      "training: 32 batch 303 batch_loss: 0.16883468627929688\n",
      "training: 32 batch 304 batch_loss: 0.16568773984909058\n",
      "training: 32 batch 305 batch_loss: 0.17025232315063477\n",
      "training: 32 batch 306 batch_loss: 0.16113436222076416\n",
      "training: 32 batch 307 batch_loss: 0.16528621315956116\n",
      "training: 32 batch 308 batch_loss: 0.16552424430847168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 32 batch 309 batch_loss: 0.1651255488395691\n",
      "training: 32 batch 310 batch_loss: 0.1657528281211853\n",
      "training: 32 batch 311 batch_loss: 0.16844278573989868\n",
      "training: 32 batch 312 batch_loss: 0.16604125499725342\n",
      "training: 32 batch 313 batch_loss: 0.16802603006362915\n",
      "training: 32 batch 314 batch_loss: 0.16915902495384216\n",
      "training: 32 batch 315 batch_loss: 0.1670861840248108\n",
      "training: 32 batch 316 batch_loss: 0.16818544268608093\n",
      "training: 32 batch 317 batch_loss: 0.1664770245552063\n",
      "training: 32 batch 318 batch_loss: 0.16280734539031982\n",
      "training: 32 batch 319 batch_loss: 0.16166943311691284\n",
      "training: 32 batch 320 batch_loss: 0.16899919509887695\n",
      "training: 32 batch 321 batch_loss: 0.16802117228507996\n",
      "training: 32 batch 322 batch_loss: 0.16747310757637024\n",
      "training: 32 batch 323 batch_loss: 0.16847649216651917\n",
      "training: 32 batch 324 batch_loss: 0.16798299551010132\n",
      "training: 32 batch 325 batch_loss: 0.1642540693283081\n",
      "training: 32 batch 326 batch_loss: 0.16755443811416626\n",
      "training: 32 batch 327 batch_loss: 0.16951429843902588\n",
      "training: 32 batch 328 batch_loss: 0.16450929641723633\n",
      "training: 32 batch 329 batch_loss: 0.16700896620750427\n",
      "training: 32 batch 330 batch_loss: 0.16628515720367432\n",
      "training: 32 batch 331 batch_loss: 0.16779440641403198\n",
      "training: 32 batch 332 batch_loss: 0.1672467589378357\n",
      "training: 32 batch 333 batch_loss: 0.16458070278167725\n",
      "training: 32 batch 334 batch_loss: 0.16767281293869019\n",
      "training: 32 batch 335 batch_loss: 0.16584345698356628\n",
      "training: 32 batch 336 batch_loss: 0.16568127274513245\n",
      "training: 32 batch 337 batch_loss: 0.16369915008544922\n",
      "training: 32 batch 338 batch_loss: 0.16867679357528687\n",
      "training: 32 batch 339 batch_loss: 0.1663576066493988\n",
      "training: 32 batch 340 batch_loss: 0.16412800550460815\n",
      "training: 32 batch 341 batch_loss: 0.16762816905975342\n",
      "training: 32 batch 342 batch_loss: 0.16831636428833008\n",
      "training: 32 batch 343 batch_loss: 0.16788524389266968\n",
      "training: 32 batch 344 batch_loss: 0.16858655214309692\n",
      "training: 32 batch 345 batch_loss: 0.16518276929855347\n",
      "training: 32 batch 346 batch_loss: 0.1720324158668518\n",
      "training: 32 batch 347 batch_loss: 0.16371509432792664\n",
      "training: 32 batch 348 batch_loss: 0.1626368761062622\n",
      "training: 32 batch 349 batch_loss: 0.1630995273590088\n",
      "training: 32 batch 350 batch_loss: 0.16744449734687805\n",
      "training: 32 batch 351 batch_loss: 0.16620102524757385\n",
      "training: 32 batch 352 batch_loss: 0.1682339608669281\n",
      "training: 32 batch 353 batch_loss: 0.16792866587638855\n",
      "training: 32 batch 354 batch_loss: 0.1691020131111145\n",
      "training: 32 batch 355 batch_loss: 0.165943443775177\n",
      "training: 32 batch 356 batch_loss: 0.16454100608825684\n",
      "training: 32 batch 357 batch_loss: 0.16543921828269958\n",
      "training: 32 batch 358 batch_loss: 0.17049336433410645\n",
      "training: 32 batch 359 batch_loss: 0.1659993827342987\n",
      "training: 32 batch 360 batch_loss: 0.1694965362548828\n",
      "training: 32 batch 361 batch_loss: 0.1657322645187378\n",
      "training: 32 batch 362 batch_loss: 0.16362613439559937\n",
      "training: 32 batch 363 batch_loss: 0.1658707559108734\n",
      "training: 32 batch 364 batch_loss: 0.17013657093048096\n",
      "training: 32 batch 365 batch_loss: 0.16632992029190063\n",
      "training: 32 batch 366 batch_loss: 0.166914701461792\n",
      "training: 32 batch 367 batch_loss: 0.16862007975578308\n",
      "training: 32 batch 368 batch_loss: 0.16822299361228943\n",
      "training: 32 batch 369 batch_loss: 0.16843780875205994\n",
      "training: 32 batch 370 batch_loss: 0.1654984951019287\n",
      "training: 32 batch 371 batch_loss: 0.17226314544677734\n",
      "training: 32 batch 372 batch_loss: 0.16698560118675232\n",
      "training: 32 batch 373 batch_loss: 0.1660262942314148\n",
      "training: 32 batch 374 batch_loss: 0.16696178913116455\n",
      "training: 32 batch 375 batch_loss: 0.1710032820701599\n",
      "training: 32 batch 376 batch_loss: 0.17181888222694397\n",
      "training: 32 batch 377 batch_loss: 0.1690608263015747\n",
      "training: 32 batch 378 batch_loss: 0.16766592860221863\n",
      "training: 32 batch 379 batch_loss: 0.16803660988807678\n",
      "training: 32 batch 380 batch_loss: 0.1660739779472351\n",
      "training: 32 batch 381 batch_loss: 0.16819971799850464\n",
      "training: 32 batch 382 batch_loss: 0.17020446062088013\n",
      "training: 32 batch 383 batch_loss: 0.1665603220462799\n",
      "training: 32 batch 384 batch_loss: 0.16459408402442932\n",
      "training: 32 batch 385 batch_loss: 0.16437530517578125\n",
      "training: 32 batch 386 batch_loss: 0.16442862153053284\n",
      "training: 32 batch 387 batch_loss: 0.1667667031288147\n",
      "training: 32 batch 388 batch_loss: 0.16827362775802612\n",
      "training: 32 batch 389 batch_loss: 0.167201429605484\n",
      "training: 32 batch 390 batch_loss: 0.16344738006591797\n",
      "training: 32 batch 391 batch_loss: 0.17035406827926636\n",
      "training: 32 batch 392 batch_loss: 0.16480448842048645\n",
      "training: 32 batch 393 batch_loss: 0.1635579764842987\n",
      "training: 32 batch 394 batch_loss: 0.16629517078399658\n",
      "training: 32 batch 395 batch_loss: 0.16705003380775452\n",
      "training: 32 batch 396 batch_loss: 0.1678025722503662\n",
      "training: 32 batch 397 batch_loss: 0.16819417476654053\n",
      "training: 32 batch 398 batch_loss: 0.1672779619693756\n",
      "training: 32 batch 399 batch_loss: 0.1664677858352661\n",
      "training: 32 batch 400 batch_loss: 0.16672497987747192\n",
      "training: 32 batch 401 batch_loss: 0.16666173934936523\n",
      "training: 32 batch 402 batch_loss: 0.1712353229522705\n",
      "training: 32 batch 403 batch_loss: 0.1633768379688263\n",
      "training: 32 batch 404 batch_loss: 0.1653975546360016\n",
      "training: 32 batch 405 batch_loss: 0.1669253706932068\n",
      "training: 32 batch 406 batch_loss: 0.1677992343902588\n",
      "training: 32 batch 407 batch_loss: 0.16620883345603943\n",
      "training: 32 batch 408 batch_loss: 0.166893869638443\n",
      "training: 32 batch 409 batch_loss: 0.1623353362083435\n",
      "training: 32 batch 410 batch_loss: 0.1678226888179779\n",
      "training: 32 batch 411 batch_loss: 0.1640261709690094\n",
      "training: 32 batch 412 batch_loss: 0.16522067785263062\n",
      "training: 32 batch 413 batch_loss: 0.16796714067459106\n",
      "training: 32 batch 414 batch_loss: 0.16692376136779785\n",
      "training: 32 batch 415 batch_loss: 0.1636744737625122\n",
      "training: 32 batch 416 batch_loss: 0.16877830028533936\n",
      "training: 32 batch 417 batch_loss: 0.16956308484077454\n",
      "training: 32 batch 418 batch_loss: 0.16632404923439026\n",
      "training: 32 batch 419 batch_loss: 0.168580561876297\n",
      "training: 32 batch 420 batch_loss: 0.16810187697410583\n",
      "training: 32 batch 421 batch_loss: 0.16725867986679077\n",
      "training: 32 batch 422 batch_loss: 0.1655062735080719\n",
      "training: 32 batch 423 batch_loss: 0.1671319305896759\n",
      "training: 32 batch 424 batch_loss: 0.16207900643348694\n",
      "training: 32 batch 425 batch_loss: 0.1687133014202118\n",
      "training: 32 batch 426 batch_loss: 0.16652631759643555\n",
      "training: 32 batch 427 batch_loss: 0.16783177852630615\n",
      "training: 32 batch 428 batch_loss: 0.17070022225379944\n",
      "training: 32 batch 429 batch_loss: 0.16568946838378906\n",
      "training: 32 batch 430 batch_loss: 0.1676558256149292\n",
      "training: 32 batch 431 batch_loss: 0.16527003049850464\n",
      "training: 32 batch 432 batch_loss: 0.16632726788520813\n",
      "training: 32 batch 433 batch_loss: 0.1659172773361206\n",
      "training: 32 batch 434 batch_loss: 0.16691187024116516\n",
      "training: 32 batch 435 batch_loss: 0.17039808630943298\n",
      "training: 32 batch 436 batch_loss: 0.16547942161560059\n",
      "training: 32 batch 437 batch_loss: 0.16969892382621765\n",
      "training: 32 batch 438 batch_loss: 0.1681048572063446\n",
      "training: 32 batch 439 batch_loss: 0.16799646615982056\n",
      "training: 32 batch 440 batch_loss: 0.16721129417419434\n",
      "training: 32 batch 441 batch_loss: 0.16341501474380493\n",
      "training: 32 batch 442 batch_loss: 0.16895121335983276\n",
      "training: 32 batch 443 batch_loss: 0.16514921188354492\n",
      "training: 32 batch 444 batch_loss: 0.1673477292060852\n",
      "training: 32 batch 445 batch_loss: 0.16448938846588135\n",
      "training: 32 batch 446 batch_loss: 0.16594624519348145\n",
      "training: 32 batch 447 batch_loss: 0.1686406433582306\n",
      "training: 32 batch 448 batch_loss: 0.16672587394714355\n",
      "training: 32 batch 449 batch_loss: 0.1650409698486328\n",
      "training: 32 batch 450 batch_loss: 0.1668984293937683\n",
      "training: 32 batch 451 batch_loss: 0.16394934058189392\n",
      "training: 32 batch 452 batch_loss: 0.16713610291481018\n",
      "training: 32 batch 453 batch_loss: 0.16769227385520935\n",
      "training: 32 batch 454 batch_loss: 0.16791972517967224\n",
      "training: 32 batch 455 batch_loss: 0.16533216834068298\n",
      "training: 32 batch 456 batch_loss: 0.16863760352134705\n",
      "training: 32 batch 457 batch_loss: 0.16825655102729797\n",
      "training: 32 batch 458 batch_loss: 0.16381898522377014\n",
      "training: 32 batch 459 batch_loss: 0.1694643497467041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 32 batch 460 batch_loss: 0.1666150689125061\n",
      "training: 32 batch 461 batch_loss: 0.1671517789363861\n",
      "training: 32 batch 462 batch_loss: 0.16432645916938782\n",
      "training: 32 batch 463 batch_loss: 0.16649797558784485\n",
      "training: 32 batch 464 batch_loss: 0.16685977578163147\n",
      "training: 32 batch 465 batch_loss: 0.16831821203231812\n",
      "training: 32 batch 466 batch_loss: 0.1704043447971344\n",
      "training: 32 batch 467 batch_loss: 0.1658562421798706\n",
      "training: 32 batch 468 batch_loss: 0.16833072900772095\n",
      "training: 32 batch 469 batch_loss: 0.1669316589832306\n",
      "training: 32 batch 470 batch_loss: 0.16781282424926758\n",
      "training: 32 batch 471 batch_loss: 0.1654582917690277\n",
      "training: 32 batch 472 batch_loss: 0.16625404357910156\n",
      "training: 32 batch 473 batch_loss: 0.16567060351371765\n",
      "training: 32 batch 474 batch_loss: 0.1655086874961853\n",
      "training: 32 batch 475 batch_loss: 0.1680344045162201\n",
      "training: 32 batch 476 batch_loss: 0.16521313786506653\n",
      "training: 32 batch 477 batch_loss: 0.17172282934188843\n",
      "training: 32 batch 478 batch_loss: 0.16621899604797363\n",
      "training: 32 batch 479 batch_loss: 0.1676529049873352\n",
      "training: 32 batch 480 batch_loss: 0.16536596417427063\n",
      "training: 32 batch 481 batch_loss: 0.16913354396820068\n",
      "training: 32 batch 482 batch_loss: 0.17198467254638672\n",
      "training: 32 batch 483 batch_loss: 0.16923049092292786\n",
      "training: 32 batch 484 batch_loss: 0.1690295934677124\n",
      "training: 32 batch 485 batch_loss: 0.16832730174064636\n",
      "training: 32 batch 486 batch_loss: 0.16480618715286255\n",
      "training: 32 batch 487 batch_loss: 0.16747760772705078\n",
      "training: 32 batch 488 batch_loss: 0.1667393445968628\n",
      "training: 32 batch 489 batch_loss: 0.17089378833770752\n",
      "training: 32 batch 490 batch_loss: 0.16602492332458496\n",
      "training: 32 batch 491 batch_loss: 0.16505393385887146\n",
      "training: 32 batch 492 batch_loss: 0.1701332926750183\n",
      "training: 32 batch 493 batch_loss: 0.16854023933410645\n",
      "training: 32 batch 494 batch_loss: 0.16457566618919373\n",
      "training: 32 batch 495 batch_loss: 0.1657489836215973\n",
      "training: 32 batch 496 batch_loss: 0.16883093118667603\n",
      "training: 32 batch 497 batch_loss: 0.16737744212150574\n",
      "training: 32 batch 498 batch_loss: 0.16350725293159485\n",
      "training: 32 batch 499 batch_loss: 0.1654089391231537\n",
      "training: 32 batch 500 batch_loss: 0.16325104236602783\n",
      "training: 32 batch 501 batch_loss: 0.16696709394454956\n",
      "training: 32 batch 502 batch_loss: 0.16623389720916748\n",
      "training: 32 batch 503 batch_loss: 0.16228151321411133\n",
      "training: 32 batch 504 batch_loss: 0.1650625765323639\n",
      "training: 32 batch 505 batch_loss: 0.16849762201309204\n",
      "training: 32 batch 506 batch_loss: 0.1713712215423584\n",
      "training: 32 batch 507 batch_loss: 0.16414201259613037\n",
      "training: 32 batch 508 batch_loss: 0.16590964794158936\n",
      "training: 32 batch 509 batch_loss: 0.16509568691253662\n",
      "training: 32 batch 510 batch_loss: 0.1698172688484192\n",
      "training: 32 batch 511 batch_loss: 0.1666949987411499\n",
      "training: 32 batch 512 batch_loss: 0.16501352190971375\n",
      "training: 32 batch 513 batch_loss: 0.17130103707313538\n",
      "training: 32 batch 514 batch_loss: 0.16711294651031494\n",
      "training: 32 batch 515 batch_loss: 0.16850140690803528\n",
      "training: 32 batch 516 batch_loss: 0.16655734181404114\n",
      "training: 32 batch 517 batch_loss: 0.16619348526000977\n",
      "training: 32 batch 518 batch_loss: 0.16553330421447754\n",
      "training: 32 batch 519 batch_loss: 0.16737347841262817\n",
      "training: 32 batch 520 batch_loss: 0.16865646839141846\n",
      "training: 32 batch 521 batch_loss: 0.16696012020111084\n",
      "training: 32 batch 522 batch_loss: 0.16993603110313416\n",
      "training: 32 batch 523 batch_loss: 0.1645846962928772\n",
      "training: 32 batch 524 batch_loss: 0.1692585051059723\n",
      "training: 32 batch 525 batch_loss: 0.1708313226699829\n",
      "training: 32 batch 526 batch_loss: 0.16536667943000793\n",
      "training: 32 batch 527 batch_loss: 0.16946688294410706\n",
      "training: 32 batch 528 batch_loss: 0.16885706782341003\n",
      "training: 32 batch 529 batch_loss: 0.16705328226089478\n",
      "training: 32 batch 530 batch_loss: 0.1685364842414856\n",
      "training: 32 batch 531 batch_loss: 0.16919809579849243\n",
      "training: 32 batch 532 batch_loss: 0.16827186942100525\n",
      "training: 32 batch 533 batch_loss: 0.16725429892539978\n",
      "training: 32 batch 534 batch_loss: 0.16490581631660461\n",
      "training: 32 batch 535 batch_loss: 0.16736900806427002\n",
      "training: 32 batch 536 batch_loss: 0.16678082942962646\n",
      "training: 32 batch 537 batch_loss: 0.1678767204284668\n",
      "training: 32 batch 538 batch_loss: 0.16940131783485413\n",
      "training: 32 batch 539 batch_loss: 0.16612929105758667\n",
      "training: 32 batch 540 batch_loss: 0.16560098528862\n",
      "training: 32 batch 541 batch_loss: 0.16672345995903015\n",
      "training: 32 batch 542 batch_loss: 0.16624337434768677\n",
      "training: 32 batch 543 batch_loss: 0.16714823246002197\n",
      "training: 32 batch 544 batch_loss: 0.16821521520614624\n",
      "training: 32 batch 545 batch_loss: 0.1692270040512085\n",
      "training: 32 batch 546 batch_loss: 0.1680777668952942\n",
      "training: 32 batch 547 batch_loss: 0.16508811712265015\n",
      "training: 32 batch 548 batch_loss: 0.16721826791763306\n",
      "training: 32 batch 549 batch_loss: 0.16900694370269775\n",
      "training: 32 batch 550 batch_loss: 0.16941294074058533\n",
      "training: 32 batch 551 batch_loss: 0.170546293258667\n",
      "training: 32 batch 552 batch_loss: 0.16809427738189697\n",
      "training: 32 batch 553 batch_loss: 0.17095237970352173\n",
      "training: 32 batch 554 batch_loss: 0.16622328758239746\n",
      "training: 32 batch 555 batch_loss: 0.16647857427597046\n",
      "training: 32 batch 556 batch_loss: 0.17088556289672852\n",
      "training: 32 batch 557 batch_loss: 0.16664916276931763\n",
      "training: 32 batch 558 batch_loss: 0.16671550273895264\n",
      "training: 32 batch 559 batch_loss: 0.16940629482269287\n",
      "training: 32 batch 560 batch_loss: 0.16678577661514282\n",
      "training: 32 batch 561 batch_loss: 0.16681936383247375\n",
      "training: 32 batch 562 batch_loss: 0.16789990663528442\n",
      "training: 32 batch 563 batch_loss: 0.16662105917930603\n",
      "training: 32 batch 564 batch_loss: 0.16880536079406738\n",
      "training: 32 batch 565 batch_loss: 0.1647363007068634\n",
      "training: 32 batch 566 batch_loss: 0.16927021741867065\n",
      "training: 32 batch 567 batch_loss: 0.1669253706932068\n",
      "training: 32 batch 568 batch_loss: 0.1691177785396576\n",
      "training: 32 batch 569 batch_loss: 0.17045184969902039\n",
      "training: 32 batch 570 batch_loss: 0.16654926538467407\n",
      "training: 32 batch 571 batch_loss: 0.16716179251670837\n",
      "training: 32 batch 572 batch_loss: 0.16769182682037354\n",
      "training: 32 batch 573 batch_loss: 0.16894686222076416\n",
      "training: 32 batch 574 batch_loss: 0.16757652163505554\n",
      "training: 32 batch 575 batch_loss: 0.1654295027256012\n",
      "training: 32 batch 576 batch_loss: 0.1804540753364563\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 32, Hit Ratio:0.03027052596163135 | Precision:0.0446623414921852 | Recall:0.059919020302043914 | NDCG:0.05736168811883431\n",
      "*Best Performance* \n",
      "Epoch: 32, Hit Ratio:0.03027052596163135 | Precision:0.0446623414921852 | Recall:0.059919020302043914 | MDCG:0.05736168811883431\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 33 batch 0 batch_loss: 0.1668851375579834\n",
      "training: 33 batch 1 batch_loss: 0.16659191250801086\n",
      "training: 33 batch 2 batch_loss: 0.16795507073402405\n",
      "training: 33 batch 3 batch_loss: 0.1680648922920227\n",
      "training: 33 batch 4 batch_loss: 0.16605910658836365\n",
      "training: 33 batch 5 batch_loss: 0.1643945872783661\n",
      "training: 33 batch 6 batch_loss: 0.16325417160987854\n",
      "training: 33 batch 7 batch_loss: 0.164740651845932\n",
      "training: 33 batch 8 batch_loss: 0.16456562280654907\n",
      "training: 33 batch 9 batch_loss: 0.1622452735900879\n",
      "training: 33 batch 10 batch_loss: 0.16633737087249756\n",
      "training: 33 batch 11 batch_loss: 0.1634579300880432\n",
      "training: 33 batch 12 batch_loss: 0.1616930067539215\n",
      "training: 33 batch 13 batch_loss: 0.16745808720588684\n",
      "training: 33 batch 14 batch_loss: 0.16325584053993225\n",
      "training: 33 batch 15 batch_loss: 0.16314712166786194\n",
      "training: 33 batch 16 batch_loss: 0.1649419069290161\n",
      "training: 33 batch 17 batch_loss: 0.16595780849456787\n",
      "training: 33 batch 18 batch_loss: 0.1686529517173767\n",
      "training: 33 batch 19 batch_loss: 0.16410785913467407\n",
      "training: 33 batch 20 batch_loss: 0.16611120104789734\n",
      "training: 33 batch 21 batch_loss: 0.16451919078826904\n",
      "training: 33 batch 22 batch_loss: 0.16303688287734985\n",
      "training: 33 batch 23 batch_loss: 0.16924923658370972\n",
      "training: 33 batch 24 batch_loss: 0.1702343225479126\n",
      "training: 33 batch 25 batch_loss: 0.1651442050933838\n",
      "training: 33 batch 26 batch_loss: 0.1643827259540558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 33 batch 27 batch_loss: 0.17011666297912598\n",
      "training: 33 batch 28 batch_loss: 0.16791075468063354\n",
      "training: 33 batch 29 batch_loss: 0.16789329051971436\n",
      "training: 33 batch 30 batch_loss: 0.16581594944000244\n",
      "training: 33 batch 31 batch_loss: 0.16672509908676147\n",
      "training: 33 batch 32 batch_loss: 0.16452112793922424\n",
      "training: 33 batch 33 batch_loss: 0.1669880747795105\n",
      "training: 33 batch 34 batch_loss: 0.1643979251384735\n",
      "training: 33 batch 35 batch_loss: 0.16841933131217957\n",
      "training: 33 batch 36 batch_loss: 0.16830161213874817\n",
      "training: 33 batch 37 batch_loss: 0.16414642333984375\n",
      "training: 33 batch 38 batch_loss: 0.16362017393112183\n",
      "training: 33 batch 39 batch_loss: 0.16249507665634155\n",
      "training: 33 batch 40 batch_loss: 0.16615036129951477\n",
      "training: 33 batch 41 batch_loss: 0.16317224502563477\n",
      "training: 33 batch 42 batch_loss: 0.1670188009738922\n",
      "training: 33 batch 43 batch_loss: 0.16559964418411255\n",
      "training: 33 batch 44 batch_loss: 0.17044949531555176\n",
      "training: 33 batch 45 batch_loss: 0.1678868532180786\n",
      "training: 33 batch 46 batch_loss: 0.16634297370910645\n",
      "training: 33 batch 47 batch_loss: 0.16304945945739746\n",
      "training: 33 batch 48 batch_loss: 0.1689121127128601\n",
      "training: 33 batch 49 batch_loss: 0.16815966367721558\n",
      "training: 33 batch 50 batch_loss: 0.16537082195281982\n",
      "training: 33 batch 51 batch_loss: 0.16941681504249573\n",
      "training: 33 batch 52 batch_loss: 0.16684994101524353\n",
      "training: 33 batch 53 batch_loss: 0.1673048734664917\n",
      "training: 33 batch 54 batch_loss: 0.16456475853919983\n",
      "training: 33 batch 55 batch_loss: 0.16384661197662354\n",
      "training: 33 batch 56 batch_loss: 0.16894298791885376\n",
      "training: 33 batch 57 batch_loss: 0.16609981656074524\n",
      "training: 33 batch 58 batch_loss: 0.16696330904960632\n",
      "training: 33 batch 59 batch_loss: 0.16818946599960327\n",
      "training: 33 batch 60 batch_loss: 0.16786924004554749\n",
      "training: 33 batch 61 batch_loss: 0.16903340816497803\n",
      "training: 33 batch 62 batch_loss: 0.16737988591194153\n",
      "training: 33 batch 63 batch_loss: 0.168301522731781\n",
      "training: 33 batch 64 batch_loss: 0.16866862773895264\n",
      "training: 33 batch 65 batch_loss: 0.16914120316505432\n",
      "training: 33 batch 66 batch_loss: 0.1668597161769867\n",
      "training: 33 batch 67 batch_loss: 0.16469711065292358\n",
      "training: 33 batch 68 batch_loss: 0.16688448190689087\n",
      "training: 33 batch 69 batch_loss: 0.16487666964530945\n",
      "training: 33 batch 70 batch_loss: 0.16700905561447144\n",
      "training: 33 batch 71 batch_loss: 0.1666182577610016\n",
      "training: 33 batch 72 batch_loss: 0.1674221158027649\n",
      "training: 33 batch 73 batch_loss: 0.1660013496875763\n",
      "training: 33 batch 74 batch_loss: 0.16914644837379456\n",
      "training: 33 batch 75 batch_loss: 0.1629963517189026\n",
      "training: 33 batch 76 batch_loss: 0.1678714156150818\n",
      "training: 33 batch 77 batch_loss: 0.17161059379577637\n",
      "training: 33 batch 78 batch_loss: 0.16759571433067322\n",
      "training: 33 batch 79 batch_loss: 0.16783103346824646\n",
      "training: 33 batch 80 batch_loss: 0.1689261794090271\n",
      "training: 33 batch 81 batch_loss: 0.16739046573638916\n",
      "training: 33 batch 82 batch_loss: 0.163389652967453\n",
      "training: 33 batch 83 batch_loss: 0.168099045753479\n",
      "training: 33 batch 84 batch_loss: 0.1681782305240631\n",
      "training: 33 batch 85 batch_loss: 0.16708984971046448\n",
      "training: 33 batch 86 batch_loss: 0.16816779971122742\n",
      "training: 33 batch 87 batch_loss: 0.16781076788902283\n",
      "training: 33 batch 88 batch_loss: 0.16751447319984436\n",
      "training: 33 batch 89 batch_loss: 0.16453856229782104\n",
      "training: 33 batch 90 batch_loss: 0.15991723537445068\n",
      "training: 33 batch 91 batch_loss: 0.16447386145591736\n",
      "training: 33 batch 92 batch_loss: 0.16643482446670532\n",
      "training: 33 batch 93 batch_loss: 0.16618037223815918\n",
      "training: 33 batch 94 batch_loss: 0.16397196054458618\n",
      "training: 33 batch 95 batch_loss: 0.17018845677375793\n",
      "training: 33 batch 96 batch_loss: 0.16504818201065063\n",
      "training: 33 batch 97 batch_loss: 0.16728836297988892\n",
      "training: 33 batch 98 batch_loss: 0.16369441151618958\n",
      "training: 33 batch 99 batch_loss: 0.1664266288280487\n",
      "training: 33 batch 100 batch_loss: 0.168760746717453\n",
      "training: 33 batch 101 batch_loss: 0.17051902413368225\n",
      "training: 33 batch 102 batch_loss: 0.1665116548538208\n",
      "training: 33 batch 103 batch_loss: 0.16769826412200928\n",
      "training: 33 batch 104 batch_loss: 0.16730302572250366\n",
      "training: 33 batch 105 batch_loss: 0.1662711501121521\n",
      "training: 33 batch 106 batch_loss: 0.1655173897743225\n",
      "training: 33 batch 107 batch_loss: 0.16505855321884155\n",
      "training: 33 batch 108 batch_loss: 0.16827493906021118\n",
      "training: 33 batch 109 batch_loss: 0.16683802008628845\n",
      "training: 33 batch 110 batch_loss: 0.16512906551361084\n",
      "training: 33 batch 111 batch_loss: 0.16755419969558716\n",
      "training: 33 batch 112 batch_loss: 0.16783320903778076\n",
      "training: 33 batch 113 batch_loss: 0.16819363832473755\n",
      "training: 33 batch 114 batch_loss: 0.1661161482334137\n",
      "training: 33 batch 115 batch_loss: 0.17014235258102417\n",
      "training: 33 batch 116 batch_loss: 0.16581344604492188\n",
      "training: 33 batch 117 batch_loss: 0.1673576831817627\n",
      "training: 33 batch 118 batch_loss: 0.16830891370773315\n",
      "training: 33 batch 119 batch_loss: 0.16516125202178955\n",
      "training: 33 batch 120 batch_loss: 0.16502773761749268\n",
      "training: 33 batch 121 batch_loss: 0.1680608093738556\n",
      "training: 33 batch 122 batch_loss: 0.16745567321777344\n",
      "training: 33 batch 123 batch_loss: 0.16710913181304932\n",
      "training: 33 batch 124 batch_loss: 0.16704216599464417\n",
      "training: 33 batch 125 batch_loss: 0.16752657294273376\n",
      "training: 33 batch 126 batch_loss: 0.1672496497631073\n",
      "training: 33 batch 127 batch_loss: 0.1674131453037262\n",
      "training: 33 batch 128 batch_loss: 0.16977262496948242\n",
      "training: 33 batch 129 batch_loss: 0.17131641507148743\n",
      "training: 33 batch 130 batch_loss: 0.16667675971984863\n",
      "training: 33 batch 131 batch_loss: 0.16776835918426514\n",
      "training: 33 batch 132 batch_loss: 0.1676303744316101\n",
      "training: 33 batch 133 batch_loss: 0.16120100021362305\n",
      "training: 33 batch 134 batch_loss: 0.1684841513633728\n",
      "training: 33 batch 135 batch_loss: 0.16591936349868774\n",
      "training: 33 batch 136 batch_loss: 0.16824224591255188\n",
      "training: 33 batch 137 batch_loss: 0.1645103394985199\n",
      "training: 33 batch 138 batch_loss: 0.16687840223312378\n",
      "training: 33 batch 139 batch_loss: 0.16489148139953613\n",
      "training: 33 batch 140 batch_loss: 0.16910505294799805\n",
      "training: 33 batch 141 batch_loss: 0.16958075761795044\n",
      "training: 33 batch 142 batch_loss: 0.16777485609054565\n",
      "training: 33 batch 143 batch_loss: 0.16527390480041504\n",
      "training: 33 batch 144 batch_loss: 0.16523000597953796\n",
      "training: 33 batch 145 batch_loss: 0.1682259738445282\n",
      "training: 33 batch 146 batch_loss: 0.1675170660018921\n",
      "training: 33 batch 147 batch_loss: 0.16642466187477112\n",
      "training: 33 batch 148 batch_loss: 0.164497971534729\n",
      "training: 33 batch 149 batch_loss: 0.16870296001434326\n",
      "training: 33 batch 150 batch_loss: 0.1692466139793396\n",
      "training: 33 batch 151 batch_loss: 0.16218557953834534\n",
      "training: 33 batch 152 batch_loss: 0.1683332920074463\n",
      "training: 33 batch 153 batch_loss: 0.16340994834899902\n",
      "training: 33 batch 154 batch_loss: 0.16833576560020447\n",
      "training: 33 batch 155 batch_loss: 0.16328555345535278\n",
      "training: 33 batch 156 batch_loss: 0.16761711239814758\n",
      "training: 33 batch 157 batch_loss: 0.16621917486190796\n",
      "training: 33 batch 158 batch_loss: 0.17004910111427307\n",
      "training: 33 batch 159 batch_loss: 0.16547223925590515\n",
      "training: 33 batch 160 batch_loss: 0.16892263293266296\n",
      "training: 33 batch 161 batch_loss: 0.17053300142288208\n",
      "training: 33 batch 162 batch_loss: 0.16590455174446106\n",
      "training: 33 batch 163 batch_loss: 0.16646230220794678\n",
      "training: 33 batch 164 batch_loss: 0.16725650429725647\n",
      "training: 33 batch 165 batch_loss: 0.16830527782440186\n",
      "training: 33 batch 166 batch_loss: 0.17071449756622314\n",
      "training: 33 batch 167 batch_loss: 0.16998305916786194\n",
      "training: 33 batch 168 batch_loss: 0.16877445578575134\n",
      "training: 33 batch 169 batch_loss: 0.16747641563415527\n",
      "training: 33 batch 170 batch_loss: 0.16390398144721985\n",
      "training: 33 batch 171 batch_loss: 0.16728666424751282\n",
      "training: 33 batch 172 batch_loss: 0.17131036520004272\n",
      "training: 33 batch 173 batch_loss: 0.1666722297668457\n",
      "training: 33 batch 174 batch_loss: 0.16671445965766907\n",
      "training: 33 batch 175 batch_loss: 0.16654914617538452\n",
      "training: 33 batch 176 batch_loss: 0.16748708486557007\n",
      "training: 33 batch 177 batch_loss: 0.16411703824996948\n",
      "training: 33 batch 178 batch_loss: 0.16489505767822266\n",
      "training: 33 batch 179 batch_loss: 0.16998597979545593\n",
      "training: 33 batch 180 batch_loss: 0.16834789514541626\n",
      "training: 33 batch 181 batch_loss: 0.16738539934158325\n",
      "training: 33 batch 182 batch_loss: 0.16950753331184387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 33 batch 183 batch_loss: 0.16707497835159302\n",
      "training: 33 batch 184 batch_loss: 0.16746094822883606\n",
      "training: 33 batch 185 batch_loss: 0.16387277841567993\n",
      "training: 33 batch 186 batch_loss: 0.16600564122200012\n",
      "training: 33 batch 187 batch_loss: 0.16687679290771484\n",
      "training: 33 batch 188 batch_loss: 0.16541552543640137\n",
      "training: 33 batch 189 batch_loss: 0.1681104302406311\n",
      "training: 33 batch 190 batch_loss: 0.16880494356155396\n",
      "training: 33 batch 191 batch_loss: 0.16732510924339294\n",
      "training: 33 batch 192 batch_loss: 0.1685369312763214\n",
      "training: 33 batch 193 batch_loss: 0.1688474416732788\n",
      "training: 33 batch 194 batch_loss: 0.16284611821174622\n",
      "training: 33 batch 195 batch_loss: 0.1676660180091858\n",
      "training: 33 batch 196 batch_loss: 0.1700006127357483\n",
      "training: 33 batch 197 batch_loss: 0.16431140899658203\n",
      "training: 33 batch 198 batch_loss: 0.1659330129623413\n",
      "training: 33 batch 199 batch_loss: 0.1688230037689209\n",
      "training: 33 batch 200 batch_loss: 0.1664772629737854\n",
      "training: 33 batch 201 batch_loss: 0.16738152503967285\n",
      "training: 33 batch 202 batch_loss: 0.16692939400672913\n",
      "training: 33 batch 203 batch_loss: 0.17129108309745789\n",
      "training: 33 batch 204 batch_loss: 0.16419899463653564\n",
      "training: 33 batch 205 batch_loss: 0.16864371299743652\n",
      "training: 33 batch 206 batch_loss: 0.16592296957969666\n",
      "training: 33 batch 207 batch_loss: 0.16475141048431396\n",
      "training: 33 batch 208 batch_loss: 0.16581249237060547\n",
      "training: 33 batch 209 batch_loss: 0.16973093152046204\n",
      "training: 33 batch 210 batch_loss: 0.16883015632629395\n",
      "training: 33 batch 211 batch_loss: 0.1707424521446228\n",
      "training: 33 batch 212 batch_loss: 0.17086061835289001\n",
      "training: 33 batch 213 batch_loss: 0.1637236475944519\n",
      "training: 33 batch 214 batch_loss: 0.16503259539604187\n",
      "training: 33 batch 215 batch_loss: 0.16824904084205627\n",
      "training: 33 batch 216 batch_loss: 0.16525983810424805\n",
      "training: 33 batch 217 batch_loss: 0.16994261741638184\n",
      "training: 33 batch 218 batch_loss: 0.16733837127685547\n",
      "training: 33 batch 219 batch_loss: 0.16461700201034546\n",
      "training: 33 batch 220 batch_loss: 0.1667611002922058\n",
      "training: 33 batch 221 batch_loss: 0.1696339249610901\n",
      "training: 33 batch 222 batch_loss: 0.1652485728263855\n",
      "training: 33 batch 223 batch_loss: 0.16473034024238586\n",
      "training: 33 batch 224 batch_loss: 0.16598600149154663\n",
      "training: 33 batch 225 batch_loss: 0.1724545955657959\n",
      "training: 33 batch 226 batch_loss: 0.1654413938522339\n",
      "training: 33 batch 227 batch_loss: 0.1667765974998474\n",
      "training: 33 batch 228 batch_loss: 0.16840854287147522\n",
      "training: 33 batch 229 batch_loss: 0.1677059829235077\n",
      "training: 33 batch 230 batch_loss: 0.1705651581287384\n",
      "training: 33 batch 231 batch_loss: 0.16692441701889038\n",
      "training: 33 batch 232 batch_loss: 0.17091166973114014\n",
      "training: 33 batch 233 batch_loss: 0.16695848107337952\n",
      "training: 33 batch 234 batch_loss: 0.1686762571334839\n",
      "training: 33 batch 235 batch_loss: 0.1706114411354065\n",
      "training: 33 batch 236 batch_loss: 0.16473865509033203\n",
      "training: 33 batch 237 batch_loss: 0.16732144355773926\n",
      "training: 33 batch 238 batch_loss: 0.16871872544288635\n",
      "training: 33 batch 239 batch_loss: 0.16780292987823486\n",
      "training: 33 batch 240 batch_loss: 0.16634374856948853\n",
      "training: 33 batch 241 batch_loss: 0.1663506031036377\n",
      "training: 33 batch 242 batch_loss: 0.17077967524528503\n",
      "training: 33 batch 243 batch_loss: 0.16496148705482483\n",
      "training: 33 batch 244 batch_loss: 0.16783124208450317\n",
      "training: 33 batch 245 batch_loss: 0.1693781614303589\n",
      "training: 33 batch 246 batch_loss: 0.1670229434967041\n",
      "training: 33 batch 247 batch_loss: 0.1624469757080078\n",
      "training: 33 batch 248 batch_loss: 0.1705610156059265\n",
      "training: 33 batch 249 batch_loss: 0.16653382778167725\n",
      "training: 33 batch 250 batch_loss: 0.16949620842933655\n",
      "training: 33 batch 251 batch_loss: 0.16725695133209229\n",
      "training: 33 batch 252 batch_loss: 0.16861090064048767\n",
      "training: 33 batch 253 batch_loss: 0.16872555017471313\n",
      "training: 33 batch 254 batch_loss: 0.16645604372024536\n",
      "training: 33 batch 255 batch_loss: 0.169948548078537\n",
      "training: 33 batch 256 batch_loss: 0.16949951648712158\n",
      "training: 33 batch 257 batch_loss: 0.164296954870224\n",
      "training: 33 batch 258 batch_loss: 0.16596773266792297\n",
      "training: 33 batch 259 batch_loss: 0.16843649744987488\n",
      "training: 33 batch 260 batch_loss: 0.16918030381202698\n",
      "training: 33 batch 261 batch_loss: 0.16643792390823364\n",
      "training: 33 batch 262 batch_loss: 0.1657390296459198\n",
      "training: 33 batch 263 batch_loss: 0.16394901275634766\n",
      "training: 33 batch 264 batch_loss: 0.16713550686836243\n",
      "training: 33 batch 265 batch_loss: 0.1685277223587036\n",
      "training: 33 batch 266 batch_loss: 0.17005541920661926\n",
      "training: 33 batch 267 batch_loss: 0.16658499836921692\n",
      "training: 33 batch 268 batch_loss: 0.16585186123847961\n",
      "training: 33 batch 269 batch_loss: 0.16781169176101685\n",
      "training: 33 batch 270 batch_loss: 0.16571083664894104\n",
      "training: 33 batch 271 batch_loss: 0.16891515254974365\n",
      "training: 33 batch 272 batch_loss: 0.16540858149528503\n",
      "training: 33 batch 273 batch_loss: 0.16843491792678833\n",
      "training: 33 batch 274 batch_loss: 0.1657640039920807\n",
      "training: 33 batch 275 batch_loss: 0.16876929998397827\n",
      "training: 33 batch 276 batch_loss: 0.16433194279670715\n",
      "training: 33 batch 277 batch_loss: 0.1669400930404663\n",
      "training: 33 batch 278 batch_loss: 0.16810700297355652\n",
      "training: 33 batch 279 batch_loss: 0.17085808515548706\n",
      "training: 33 batch 280 batch_loss: 0.16979444026947021\n",
      "training: 33 batch 281 batch_loss: 0.16755914688110352\n",
      "training: 33 batch 282 batch_loss: 0.16618776321411133\n",
      "training: 33 batch 283 batch_loss: 0.17091649770736694\n",
      "training: 33 batch 284 batch_loss: 0.1695629060268402\n",
      "training: 33 batch 285 batch_loss: 0.16694596409797668\n",
      "training: 33 batch 286 batch_loss: 0.167056143283844\n",
      "training: 33 batch 287 batch_loss: 0.17495611310005188\n",
      "training: 33 batch 288 batch_loss: 0.17121541500091553\n",
      "training: 33 batch 289 batch_loss: 0.1692103147506714\n",
      "training: 33 batch 290 batch_loss: 0.1684223711490631\n",
      "training: 33 batch 291 batch_loss: 0.16777655482292175\n",
      "training: 33 batch 292 batch_loss: 0.1687416136264801\n",
      "training: 33 batch 293 batch_loss: 0.17015087604522705\n",
      "training: 33 batch 294 batch_loss: 0.16993069648742676\n",
      "training: 33 batch 295 batch_loss: 0.1703089475631714\n",
      "training: 33 batch 296 batch_loss: 0.16974881291389465\n",
      "training: 33 batch 297 batch_loss: 0.1697421371936798\n",
      "training: 33 batch 298 batch_loss: 0.1666204333305359\n",
      "training: 33 batch 299 batch_loss: 0.16512972116470337\n",
      "training: 33 batch 300 batch_loss: 0.16978290677070618\n",
      "training: 33 batch 301 batch_loss: 0.1686500906944275\n",
      "training: 33 batch 302 batch_loss: 0.16806694865226746\n",
      "training: 33 batch 303 batch_loss: 0.16662898659706116\n",
      "training: 33 batch 304 batch_loss: 0.16665399074554443\n",
      "training: 33 batch 305 batch_loss: 0.16962546110153198\n",
      "training: 33 batch 306 batch_loss: 0.16895991563796997\n",
      "training: 33 batch 307 batch_loss: 0.1673966646194458\n",
      "training: 33 batch 308 batch_loss: 0.16584107279777527\n",
      "training: 33 batch 309 batch_loss: 0.1664881408214569\n",
      "training: 33 batch 310 batch_loss: 0.17009463906288147\n",
      "training: 33 batch 311 batch_loss: 0.16890105605125427\n",
      "training: 33 batch 312 batch_loss: 0.1686381995677948\n",
      "training: 33 batch 313 batch_loss: 0.16915887594223022\n",
      "training: 33 batch 314 batch_loss: 0.1685912013053894\n",
      "training: 33 batch 315 batch_loss: 0.1698886752128601\n",
      "training: 33 batch 316 batch_loss: 0.1728896200656891\n",
      "training: 33 batch 317 batch_loss: 0.16757556796073914\n",
      "training: 33 batch 318 batch_loss: 0.16811180114746094\n",
      "training: 33 batch 319 batch_loss: 0.16694283485412598\n",
      "training: 33 batch 320 batch_loss: 0.16469472646713257\n",
      "training: 33 batch 321 batch_loss: 0.16933953762054443\n",
      "training: 33 batch 322 batch_loss: 0.16563689708709717\n",
      "training: 33 batch 323 batch_loss: 0.1658414602279663\n",
      "training: 33 batch 324 batch_loss: 0.16562765836715698\n",
      "training: 33 batch 325 batch_loss: 0.16788101196289062\n",
      "training: 33 batch 326 batch_loss: 0.16801202297210693\n",
      "training: 33 batch 327 batch_loss: 0.1692051887512207\n",
      "training: 33 batch 328 batch_loss: 0.1631534993648529\n",
      "training: 33 batch 329 batch_loss: 0.16827723383903503\n",
      "training: 33 batch 330 batch_loss: 0.17030173540115356\n",
      "training: 33 batch 331 batch_loss: 0.16929766535758972\n",
      "training: 33 batch 332 batch_loss: 0.1695694625377655\n",
      "training: 33 batch 333 batch_loss: 0.16994932293891907\n",
      "training: 33 batch 334 batch_loss: 0.16963928937911987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 33 batch 335 batch_loss: 0.1682882308959961\n",
      "training: 33 batch 336 batch_loss: 0.16973796486854553\n",
      "training: 33 batch 337 batch_loss: 0.166738361120224\n",
      "training: 33 batch 338 batch_loss: 0.1638234257698059\n",
      "training: 33 batch 339 batch_loss: 0.16814416646957397\n",
      "training: 33 batch 340 batch_loss: 0.16869109869003296\n",
      "training: 33 batch 341 batch_loss: 0.16879189014434814\n",
      "training: 33 batch 342 batch_loss: 0.16821417212486267\n",
      "training: 33 batch 343 batch_loss: 0.1677495241165161\n",
      "training: 33 batch 344 batch_loss: 0.16869008541107178\n",
      "training: 33 batch 345 batch_loss: 0.1704188585281372\n",
      "training: 33 batch 346 batch_loss: 0.1659151017665863\n",
      "training: 33 batch 347 batch_loss: 0.16706743836402893\n",
      "training: 33 batch 348 batch_loss: 0.16694465279579163\n",
      "training: 33 batch 349 batch_loss: 0.17039141058921814\n",
      "training: 33 batch 350 batch_loss: 0.17122003436088562\n",
      "training: 33 batch 351 batch_loss: 0.16752693057060242\n",
      "training: 33 batch 352 batch_loss: 0.170781672000885\n",
      "training: 33 batch 353 batch_loss: 0.16719698905944824\n",
      "training: 33 batch 354 batch_loss: 0.16884052753448486\n",
      "training: 33 batch 355 batch_loss: 0.1692097783088684\n",
      "training: 33 batch 356 batch_loss: 0.1697775423526764\n",
      "training: 33 batch 357 batch_loss: 0.16541308164596558\n",
      "training: 33 batch 358 batch_loss: 0.1685364842414856\n",
      "training: 33 batch 359 batch_loss: 0.1686214804649353\n",
      "training: 33 batch 360 batch_loss: 0.16793981194496155\n",
      "training: 33 batch 361 batch_loss: 0.16691315174102783\n",
      "training: 33 batch 362 batch_loss: 0.16501063108444214\n",
      "training: 33 batch 363 batch_loss: 0.1702079176902771\n",
      "training: 33 batch 364 batch_loss: 0.166917085647583\n",
      "training: 33 batch 365 batch_loss: 0.16785657405853271\n",
      "training: 33 batch 366 batch_loss: 0.16821593046188354\n",
      "training: 33 batch 367 batch_loss: 0.1664678156375885\n",
      "training: 33 batch 368 batch_loss: 0.16984981298446655\n",
      "training: 33 batch 369 batch_loss: 0.16924503445625305\n",
      "training: 33 batch 370 batch_loss: 0.16539129614830017\n",
      "training: 33 batch 371 batch_loss: 0.16864648461341858\n",
      "training: 33 batch 372 batch_loss: 0.16631945967674255\n",
      "training: 33 batch 373 batch_loss: 0.16788849234580994\n",
      "training: 33 batch 374 batch_loss: 0.1656649112701416\n",
      "training: 33 batch 375 batch_loss: 0.16697373986244202\n",
      "training: 33 batch 376 batch_loss: 0.16900765895843506\n",
      "training: 33 batch 377 batch_loss: 0.16409358382225037\n",
      "training: 33 batch 378 batch_loss: 0.16871890425682068\n",
      "training: 33 batch 379 batch_loss: 0.16558748483657837\n",
      "training: 33 batch 380 batch_loss: 0.16567280888557434\n",
      "training: 33 batch 381 batch_loss: 0.17186278104782104\n",
      "training: 33 batch 382 batch_loss: 0.16713738441467285\n",
      "training: 33 batch 383 batch_loss: 0.16863030195236206\n",
      "training: 33 batch 384 batch_loss: 0.16833117604255676\n",
      "training: 33 batch 385 batch_loss: 0.16737645864486694\n",
      "training: 33 batch 386 batch_loss: 0.16959330439567566\n",
      "training: 33 batch 387 batch_loss: 0.16635799407958984\n",
      "training: 33 batch 388 batch_loss: 0.1711309254169464\n",
      "training: 33 batch 389 batch_loss: 0.1727871596813202\n",
      "training: 33 batch 390 batch_loss: 0.17096585035324097\n",
      "training: 33 batch 391 batch_loss: 0.16889479756355286\n",
      "training: 33 batch 392 batch_loss: 0.17517408728599548\n",
      "training: 33 batch 393 batch_loss: 0.16802239418029785\n",
      "training: 33 batch 394 batch_loss: 0.17199036478996277\n",
      "training: 33 batch 395 batch_loss: 0.16954830288887024\n",
      "training: 33 batch 396 batch_loss: 0.16908538341522217\n",
      "training: 33 batch 397 batch_loss: 0.1688593626022339\n",
      "training: 33 batch 398 batch_loss: 0.16988590359687805\n",
      "training: 33 batch 399 batch_loss: 0.16980817914009094\n",
      "training: 33 batch 400 batch_loss: 0.17037183046340942\n",
      "training: 33 batch 401 batch_loss: 0.16971588134765625\n",
      "training: 33 batch 402 batch_loss: 0.16775909066200256\n",
      "training: 33 batch 403 batch_loss: 0.16703426837921143\n",
      "training: 33 batch 404 batch_loss: 0.1667163372039795\n",
      "training: 33 batch 405 batch_loss: 0.1714707612991333\n",
      "training: 33 batch 406 batch_loss: 0.16984626650810242\n",
      "training: 33 batch 407 batch_loss: 0.1676464080810547\n",
      "training: 33 batch 408 batch_loss: 0.17068159580230713\n",
      "training: 33 batch 409 batch_loss: 0.16900640726089478\n",
      "training: 33 batch 410 batch_loss: 0.169375479221344\n",
      "training: 33 batch 411 batch_loss: 0.1704883873462677\n",
      "training: 33 batch 412 batch_loss: 0.1713019609451294\n",
      "training: 33 batch 413 batch_loss: 0.1677107810974121\n",
      "training: 33 batch 414 batch_loss: 0.1642301082611084\n",
      "training: 33 batch 415 batch_loss: 0.1684184968471527\n",
      "training: 33 batch 416 batch_loss: 0.1729131042957306\n",
      "training: 33 batch 417 batch_loss: 0.17186880111694336\n",
      "training: 33 batch 418 batch_loss: 0.1664043664932251\n",
      "training: 33 batch 419 batch_loss: 0.17202168703079224\n",
      "training: 33 batch 420 batch_loss: 0.17245861887931824\n",
      "training: 33 batch 421 batch_loss: 0.16979071497917175\n",
      "training: 33 batch 422 batch_loss: 0.16659152507781982\n",
      "training: 33 batch 423 batch_loss: 0.16694241762161255\n",
      "training: 33 batch 424 batch_loss: 0.16702324151992798\n",
      "training: 33 batch 425 batch_loss: 0.1678677797317505\n",
      "training: 33 batch 426 batch_loss: 0.17159315943717957\n",
      "training: 33 batch 427 batch_loss: 0.17003417015075684\n",
      "training: 33 batch 428 batch_loss: 0.16773879528045654\n",
      "training: 33 batch 429 batch_loss: 0.16567981243133545\n",
      "training: 33 batch 430 batch_loss: 0.1698213517665863\n",
      "training: 33 batch 431 batch_loss: 0.16778159141540527\n",
      "training: 33 batch 432 batch_loss: 0.1686924397945404\n",
      "training: 33 batch 433 batch_loss: 0.16954070329666138\n",
      "training: 33 batch 434 batch_loss: 0.1665608286857605\n",
      "training: 33 batch 435 batch_loss: 0.1688295304775238\n",
      "training: 33 batch 436 batch_loss: 0.17079874873161316\n",
      "training: 33 batch 437 batch_loss: 0.1666245460510254\n",
      "training: 33 batch 438 batch_loss: 0.171603262424469\n",
      "training: 33 batch 439 batch_loss: 0.16803622245788574\n",
      "training: 33 batch 440 batch_loss: 0.17160075902938843\n",
      "training: 33 batch 441 batch_loss: 0.16990846395492554\n",
      "training: 33 batch 442 batch_loss: 0.17092439532279968\n",
      "training: 33 batch 443 batch_loss: 0.16907721757888794\n",
      "training: 33 batch 444 batch_loss: 0.17052334547042847\n",
      "training: 33 batch 445 batch_loss: 0.16989058256149292\n",
      "training: 33 batch 446 batch_loss: 0.1650524139404297\n",
      "training: 33 batch 447 batch_loss: 0.17012330889701843\n",
      "training: 33 batch 448 batch_loss: 0.16587725281715393\n",
      "training: 33 batch 449 batch_loss: 0.16647160053253174\n",
      "training: 33 batch 450 batch_loss: 0.1714501976966858\n",
      "training: 33 batch 451 batch_loss: 0.16943025588989258\n",
      "training: 33 batch 452 batch_loss: 0.1659083068370819\n",
      "training: 33 batch 453 batch_loss: 0.17309755086898804\n",
      "training: 33 batch 454 batch_loss: 0.1667271852493286\n",
      "training: 33 batch 455 batch_loss: 0.17294636368751526\n",
      "training: 33 batch 456 batch_loss: 0.16953468322753906\n",
      "training: 33 batch 457 batch_loss: 0.16667687892913818\n",
      "training: 33 batch 458 batch_loss: 0.1705007255077362\n",
      "training: 33 batch 459 batch_loss: 0.16291919350624084\n",
      "training: 33 batch 460 batch_loss: 0.17258453369140625\n",
      "training: 33 batch 461 batch_loss: 0.16745808720588684\n",
      "training: 33 batch 462 batch_loss: 0.16784465312957764\n",
      "training: 33 batch 463 batch_loss: 0.16909745335578918\n",
      "training: 33 batch 464 batch_loss: 0.17030969262123108\n",
      "training: 33 batch 465 batch_loss: 0.16798654198646545\n",
      "training: 33 batch 466 batch_loss: 0.16814467310905457\n",
      "training: 33 batch 467 batch_loss: 0.16645434498786926\n",
      "training: 33 batch 468 batch_loss: 0.16958016157150269\n",
      "training: 33 batch 469 batch_loss: 0.17183199524879456\n",
      "training: 33 batch 470 batch_loss: 0.1680900752544403\n",
      "training: 33 batch 471 batch_loss: 0.16599905490875244\n",
      "training: 33 batch 472 batch_loss: 0.167768657207489\n",
      "training: 33 batch 473 batch_loss: 0.16864615678787231\n",
      "training: 33 batch 474 batch_loss: 0.17077285051345825\n",
      "training: 33 batch 475 batch_loss: 0.17362117767333984\n",
      "training: 33 batch 476 batch_loss: 0.16817960143089294\n",
      "training: 33 batch 477 batch_loss: 0.17186397314071655\n",
      "training: 33 batch 478 batch_loss: 0.16879069805145264\n",
      "training: 33 batch 479 batch_loss: 0.1669548749923706\n",
      "training: 33 batch 480 batch_loss: 0.1706373691558838\n",
      "training: 33 batch 481 batch_loss: 0.16882747411727905\n",
      "training: 33 batch 482 batch_loss: 0.16733574867248535\n",
      "training: 33 batch 483 batch_loss: 0.1686977744102478\n",
      "training: 33 batch 484 batch_loss: 0.1707853078842163\n",
      "training: 33 batch 485 batch_loss: 0.16935127973556519\n",
      "training: 33 batch 486 batch_loss: 0.16730603575706482\n",
      "training: 33 batch 487 batch_loss: 0.16492080688476562\n",
      "training: 33 batch 488 batch_loss: 0.17122966051101685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 33 batch 489 batch_loss: 0.16836029291152954\n",
      "training: 33 batch 490 batch_loss: 0.16947025060653687\n",
      "training: 33 batch 491 batch_loss: 0.16756415367126465\n",
      "training: 33 batch 492 batch_loss: 0.1708977222442627\n",
      "training: 33 batch 493 batch_loss: 0.16999098658561707\n",
      "training: 33 batch 494 batch_loss: 0.17031532526016235\n",
      "training: 33 batch 495 batch_loss: 0.16998791694641113\n",
      "training: 33 batch 496 batch_loss: 0.1710812747478485\n",
      "training: 33 batch 497 batch_loss: 0.1679595708847046\n",
      "training: 33 batch 498 batch_loss: 0.1730017066001892\n",
      "training: 33 batch 499 batch_loss: 0.16537132859230042\n",
      "training: 33 batch 500 batch_loss: 0.16698962450027466\n",
      "training: 33 batch 501 batch_loss: 0.174148291349411\n",
      "training: 33 batch 502 batch_loss: 0.17211338877677917\n",
      "training: 33 batch 503 batch_loss: 0.1699715256690979\n",
      "training: 33 batch 504 batch_loss: 0.1691773235797882\n",
      "training: 33 batch 505 batch_loss: 0.16745835542678833\n",
      "training: 33 batch 506 batch_loss: 0.16826888918876648\n",
      "training: 33 batch 507 batch_loss: 0.1712895929813385\n",
      "training: 33 batch 508 batch_loss: 0.16737261414527893\n",
      "training: 33 batch 509 batch_loss: 0.16919606924057007\n",
      "training: 33 batch 510 batch_loss: 0.16942951083183289\n",
      "training: 33 batch 511 batch_loss: 0.16959401965141296\n",
      "training: 33 batch 512 batch_loss: 0.16835972666740417\n",
      "training: 33 batch 513 batch_loss: 0.16808021068572998\n",
      "training: 33 batch 514 batch_loss: 0.16713672876358032\n",
      "training: 33 batch 515 batch_loss: 0.1699998378753662\n",
      "training: 33 batch 516 batch_loss: 0.17324218153953552\n",
      "training: 33 batch 517 batch_loss: 0.17285266518592834\n",
      "training: 33 batch 518 batch_loss: 0.16779166460037231\n",
      "training: 33 batch 519 batch_loss: 0.17317897081375122\n",
      "training: 33 batch 520 batch_loss: 0.16896313428878784\n",
      "training: 33 batch 521 batch_loss: 0.17033651471138\n",
      "training: 33 batch 522 batch_loss: 0.16692352294921875\n",
      "training: 33 batch 523 batch_loss: 0.16925761103630066\n",
      "training: 33 batch 524 batch_loss: 0.1685706079006195\n",
      "training: 33 batch 525 batch_loss: 0.16829243302345276\n",
      "training: 33 batch 526 batch_loss: 0.16919714212417603\n",
      "training: 33 batch 527 batch_loss: 0.16823092103004456\n",
      "training: 33 batch 528 batch_loss: 0.17087280750274658\n",
      "training: 33 batch 529 batch_loss: 0.16916120052337646\n",
      "training: 33 batch 530 batch_loss: 0.17010009288787842\n",
      "training: 33 batch 531 batch_loss: 0.16882485151290894\n",
      "training: 33 batch 532 batch_loss: 0.16897252202033997\n",
      "training: 33 batch 533 batch_loss: 0.1701907515525818\n",
      "training: 33 batch 534 batch_loss: 0.17309138178825378\n",
      "training: 33 batch 535 batch_loss: 0.16570118069648743\n",
      "training: 33 batch 536 batch_loss: 0.16609859466552734\n",
      "training: 33 batch 537 batch_loss: 0.1688295304775238\n",
      "training: 33 batch 538 batch_loss: 0.16311046481132507\n",
      "training: 33 batch 539 batch_loss: 0.1680019497871399\n",
      "training: 33 batch 540 batch_loss: 0.1771356463432312\n",
      "training: 33 batch 541 batch_loss: 0.16794008016586304\n",
      "training: 33 batch 542 batch_loss: 0.16559290885925293\n",
      "training: 33 batch 543 batch_loss: 0.16953948140144348\n",
      "training: 33 batch 544 batch_loss: 0.17117708921432495\n",
      "training: 33 batch 545 batch_loss: 0.16794931888580322\n",
      "training: 33 batch 546 batch_loss: 0.16880196332931519\n",
      "training: 33 batch 547 batch_loss: 0.16796422004699707\n",
      "training: 33 batch 548 batch_loss: 0.16840404272079468\n",
      "training: 33 batch 549 batch_loss: 0.1693459153175354\n",
      "training: 33 batch 550 batch_loss: 0.16759058833122253\n",
      "training: 33 batch 551 batch_loss: 0.16883552074432373\n",
      "training: 33 batch 552 batch_loss: 0.16996124386787415\n",
      "training: 33 batch 553 batch_loss: 0.1674644649028778\n",
      "training: 33 batch 554 batch_loss: 0.16840046644210815\n",
      "training: 33 batch 555 batch_loss: 0.16605940461158752\n",
      "training: 33 batch 556 batch_loss: 0.1716480553150177\n",
      "training: 33 batch 557 batch_loss: 0.16977879405021667\n",
      "training: 33 batch 558 batch_loss: 0.16962042450904846\n",
      "training: 33 batch 559 batch_loss: 0.169572651386261\n",
      "training: 33 batch 560 batch_loss: 0.17128479480743408\n",
      "training: 33 batch 561 batch_loss: 0.16872122883796692\n",
      "training: 33 batch 562 batch_loss: 0.17300963401794434\n",
      "training: 33 batch 563 batch_loss: 0.17525744438171387\n",
      "training: 33 batch 564 batch_loss: 0.16534119844436646\n",
      "training: 33 batch 565 batch_loss: 0.16768008470535278\n",
      "training: 33 batch 566 batch_loss: 0.16641345620155334\n",
      "training: 33 batch 567 batch_loss: 0.17066144943237305\n",
      "training: 33 batch 568 batch_loss: 0.16932353377342224\n",
      "training: 33 batch 569 batch_loss: 0.1676534116268158\n",
      "training: 33 batch 570 batch_loss: 0.16905319690704346\n",
      "training: 33 batch 571 batch_loss: 0.17150264978408813\n",
      "training: 33 batch 572 batch_loss: 0.17287635803222656\n",
      "training: 33 batch 573 batch_loss: 0.16861718893051147\n",
      "training: 33 batch 574 batch_loss: 0.16910648345947266\n",
      "training: 33 batch 575 batch_loss: 0.1689872145652771\n",
      "training: 33 batch 576 batch_loss: 0.17697209119796753\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 33, Hit Ratio:0.03023388286868781 | Precision:0.04460827681116682 | Recall:0.05975545836331163 | NDCG:0.057187998006602514\n",
      "*Best Performance* \n",
      "Epoch: 32, Hit Ratio:0.03027052596163135 | Precision:0.0446623414921852 | Recall:0.059919020302043914 | MDCG:0.05736168811883431\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 34 batch 0 batch_loss: 0.16496038436889648\n",
      "training: 34 batch 1 batch_loss: 0.16644597053527832\n",
      "training: 34 batch 2 batch_loss: 0.16489872336387634\n",
      "training: 34 batch 3 batch_loss: 0.16685894131660461\n",
      "training: 34 batch 4 batch_loss: 0.1707814335823059\n",
      "training: 34 batch 5 batch_loss: 0.16654789447784424\n",
      "training: 34 batch 6 batch_loss: 0.1685979664325714\n",
      "training: 34 batch 7 batch_loss: 0.16658467054367065\n",
      "training: 34 batch 8 batch_loss: 0.16827625036239624\n",
      "training: 34 batch 9 batch_loss: 0.16293948888778687\n",
      "training: 34 batch 10 batch_loss: 0.16444602608680725\n",
      "training: 34 batch 11 batch_loss: 0.16692131757736206\n",
      "training: 34 batch 12 batch_loss: 0.1703125238418579\n",
      "training: 34 batch 13 batch_loss: 0.17135077714920044\n",
      "training: 34 batch 14 batch_loss: 0.1691182255744934\n",
      "training: 34 batch 15 batch_loss: 0.1679566204547882\n",
      "training: 34 batch 16 batch_loss: 0.1694309413433075\n",
      "training: 34 batch 17 batch_loss: 0.16822189092636108\n",
      "training: 34 batch 18 batch_loss: 0.16887140274047852\n",
      "training: 34 batch 19 batch_loss: 0.1680317521095276\n",
      "training: 34 batch 20 batch_loss: 0.16876262426376343\n",
      "training: 34 batch 21 batch_loss: 0.16847121715545654\n",
      "training: 34 batch 22 batch_loss: 0.1699495017528534\n",
      "training: 34 batch 23 batch_loss: 0.1688079833984375\n",
      "training: 34 batch 24 batch_loss: 0.1682373583316803\n",
      "training: 34 batch 25 batch_loss: 0.16899985074996948\n",
      "training: 34 batch 26 batch_loss: 0.16882222890853882\n",
      "training: 34 batch 27 batch_loss: 0.16960173845291138\n",
      "training: 34 batch 28 batch_loss: 0.16893625259399414\n",
      "training: 34 batch 29 batch_loss: 0.16472521424293518\n",
      "training: 34 batch 30 batch_loss: 0.16800954937934875\n",
      "training: 34 batch 31 batch_loss: 0.1688133180141449\n",
      "training: 34 batch 32 batch_loss: 0.17079570889472961\n",
      "training: 34 batch 33 batch_loss: 0.17083096504211426\n",
      "training: 34 batch 34 batch_loss: 0.17087805271148682\n",
      "training: 34 batch 35 batch_loss: 0.16873979568481445\n",
      "training: 34 batch 36 batch_loss: 0.16961079835891724\n",
      "training: 34 batch 37 batch_loss: 0.16811442375183105\n",
      "training: 34 batch 38 batch_loss: 0.16487008333206177\n",
      "training: 34 batch 39 batch_loss: 0.1677999496459961\n",
      "training: 34 batch 40 batch_loss: 0.16844037175178528\n",
      "training: 34 batch 41 batch_loss: 0.16420018672943115\n",
      "training: 34 batch 42 batch_loss: 0.16597861051559448\n",
      "training: 34 batch 43 batch_loss: 0.17087429761886597\n",
      "training: 34 batch 44 batch_loss: 0.16756972670555115\n",
      "training: 34 batch 45 batch_loss: 0.1680055558681488\n",
      "training: 34 batch 46 batch_loss: 0.1656026542186737\n",
      "training: 34 batch 47 batch_loss: 0.16900521516799927\n",
      "training: 34 batch 48 batch_loss: 0.16898950934410095\n",
      "training: 34 batch 49 batch_loss: 0.167874276638031\n",
      "training: 34 batch 50 batch_loss: 0.1673705279827118\n",
      "training: 34 batch 51 batch_loss: 0.16396909952163696\n",
      "training: 34 batch 52 batch_loss: 0.17258936166763306\n",
      "training: 34 batch 53 batch_loss: 0.16910246014595032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 34 batch 54 batch_loss: 0.16945478320121765\n",
      "training: 34 batch 55 batch_loss: 0.16523435711860657\n",
      "training: 34 batch 56 batch_loss: 0.16633257269859314\n",
      "training: 34 batch 57 batch_loss: 0.16520369052886963\n",
      "training: 34 batch 58 batch_loss: 0.16316929459571838\n",
      "training: 34 batch 59 batch_loss: 0.16606220602989197\n",
      "training: 34 batch 60 batch_loss: 0.16705548763275146\n",
      "training: 34 batch 61 batch_loss: 0.1657513678073883\n",
      "training: 34 batch 62 batch_loss: 0.1716192066669464\n",
      "training: 34 batch 63 batch_loss: 0.1680597960948944\n",
      "training: 34 batch 64 batch_loss: 0.1672593355178833\n",
      "training: 34 batch 65 batch_loss: 0.17042043805122375\n",
      "training: 34 batch 66 batch_loss: 0.16826346516609192\n",
      "training: 34 batch 67 batch_loss: 0.16782283782958984\n",
      "training: 34 batch 68 batch_loss: 0.17052510380744934\n",
      "training: 34 batch 69 batch_loss: 0.16690987348556519\n",
      "training: 34 batch 70 batch_loss: 0.17043206095695496\n",
      "training: 34 batch 71 batch_loss: 0.16933974623680115\n",
      "training: 34 batch 72 batch_loss: 0.16849803924560547\n",
      "training: 34 batch 73 batch_loss: 0.1672774851322174\n",
      "training: 34 batch 74 batch_loss: 0.16727116703987122\n",
      "training: 34 batch 75 batch_loss: 0.16733995079994202\n",
      "training: 34 batch 76 batch_loss: 0.17029857635498047\n",
      "training: 34 batch 77 batch_loss: 0.16937711834907532\n",
      "training: 34 batch 78 batch_loss: 0.16746622323989868\n",
      "training: 34 batch 79 batch_loss: 0.1702238917350769\n",
      "training: 34 batch 80 batch_loss: 0.1666494607925415\n",
      "training: 34 batch 81 batch_loss: 0.17335674166679382\n",
      "training: 34 batch 82 batch_loss: 0.1676599681377411\n",
      "training: 34 batch 83 batch_loss: 0.16820544004440308\n",
      "training: 34 batch 84 batch_loss: 0.16772183775901794\n",
      "training: 34 batch 85 batch_loss: 0.16225391626358032\n",
      "training: 34 batch 86 batch_loss: 0.17020484805107117\n",
      "training: 34 batch 87 batch_loss: 0.16729575395584106\n",
      "training: 34 batch 88 batch_loss: 0.16621047258377075\n",
      "training: 34 batch 89 batch_loss: 0.16779544949531555\n",
      "training: 34 batch 90 batch_loss: 0.16766998171806335\n",
      "training: 34 batch 91 batch_loss: 0.16549953818321228\n",
      "training: 34 batch 92 batch_loss: 0.16823548078536987\n",
      "training: 34 batch 93 batch_loss: 0.1701449453830719\n",
      "training: 34 batch 94 batch_loss: 0.1699846088886261\n",
      "training: 34 batch 95 batch_loss: 0.17261117696762085\n",
      "training: 34 batch 96 batch_loss: 0.16814738512039185\n",
      "training: 34 batch 97 batch_loss: 0.16881564259529114\n",
      "training: 34 batch 98 batch_loss: 0.16628366708755493\n",
      "training: 34 batch 99 batch_loss: 0.17020153999328613\n",
      "training: 34 batch 100 batch_loss: 0.16794532537460327\n",
      "training: 34 batch 101 batch_loss: 0.1707967221736908\n",
      "training: 34 batch 102 batch_loss: 0.16767597198486328\n",
      "training: 34 batch 103 batch_loss: 0.170710027217865\n",
      "training: 34 batch 104 batch_loss: 0.16786152124404907\n",
      "training: 34 batch 105 batch_loss: 0.17201822996139526\n",
      "training: 34 batch 106 batch_loss: 0.1660042405128479\n",
      "training: 34 batch 107 batch_loss: 0.16717535257339478\n",
      "training: 34 batch 108 batch_loss: 0.169478178024292\n",
      "training: 34 batch 109 batch_loss: 0.16664215922355652\n",
      "training: 34 batch 110 batch_loss: 0.17175522446632385\n",
      "training: 34 batch 111 batch_loss: 0.16941097378730774\n",
      "training: 34 batch 112 batch_loss: 0.16963273286819458\n",
      "training: 34 batch 113 batch_loss: 0.1658233404159546\n",
      "training: 34 batch 114 batch_loss: 0.16679546236991882\n",
      "training: 34 batch 115 batch_loss: 0.16642221808433533\n",
      "training: 34 batch 116 batch_loss: 0.16577240824699402\n",
      "training: 34 batch 117 batch_loss: 0.16686540842056274\n",
      "training: 34 batch 118 batch_loss: 0.16601580381393433\n",
      "training: 34 batch 119 batch_loss: 0.16846007108688354\n",
      "training: 34 batch 120 batch_loss: 0.17257031798362732\n",
      "training: 34 batch 121 batch_loss: 0.16808253526687622\n",
      "training: 34 batch 122 batch_loss: 0.16887283325195312\n",
      "training: 34 batch 123 batch_loss: 0.1718176007270813\n",
      "training: 34 batch 124 batch_loss: 0.1673269271850586\n",
      "training: 34 batch 125 batch_loss: 0.16795164346694946\n",
      "training: 34 batch 126 batch_loss: 0.16770756244659424\n",
      "training: 34 batch 127 batch_loss: 0.16867360472679138\n",
      "training: 34 batch 128 batch_loss: 0.1647617518901825\n",
      "training: 34 batch 129 batch_loss: 0.16979962587356567\n",
      "training: 34 batch 130 batch_loss: 0.16326987743377686\n",
      "training: 34 batch 131 batch_loss: 0.17027056217193604\n",
      "training: 34 batch 132 batch_loss: 0.16809695959091187\n",
      "training: 34 batch 133 batch_loss: 0.16904997825622559\n",
      "training: 34 batch 134 batch_loss: 0.17030903697013855\n",
      "training: 34 batch 135 batch_loss: 0.16519466042518616\n",
      "training: 34 batch 136 batch_loss: 0.1699451208114624\n",
      "training: 34 batch 137 batch_loss: 0.16855400800704956\n",
      "training: 34 batch 138 batch_loss: 0.16858553886413574\n",
      "training: 34 batch 139 batch_loss: 0.16932639479637146\n",
      "training: 34 batch 140 batch_loss: 0.16958129405975342\n",
      "training: 34 batch 141 batch_loss: 0.16892707347869873\n",
      "training: 34 batch 142 batch_loss: 0.1703474223613739\n",
      "training: 34 batch 143 batch_loss: 0.16892889142036438\n",
      "training: 34 batch 144 batch_loss: 0.16773203015327454\n",
      "training: 34 batch 145 batch_loss: 0.1719387173652649\n",
      "training: 34 batch 146 batch_loss: 0.16893088817596436\n",
      "training: 34 batch 147 batch_loss: 0.16896086931228638\n",
      "training: 34 batch 148 batch_loss: 0.17446231842041016\n",
      "training: 34 batch 149 batch_loss: 0.1659962236881256\n",
      "training: 34 batch 150 batch_loss: 0.16706103086471558\n",
      "training: 34 batch 151 batch_loss: 0.17005330324172974\n",
      "training: 34 batch 152 batch_loss: 0.1684858500957489\n",
      "training: 34 batch 153 batch_loss: 0.16470497846603394\n",
      "training: 34 batch 154 batch_loss: 0.16781958937644958\n",
      "training: 34 batch 155 batch_loss: 0.16996034979820251\n",
      "training: 34 batch 156 batch_loss: 0.17184168100357056\n",
      "training: 34 batch 157 batch_loss: 0.1723036766052246\n",
      "training: 34 batch 158 batch_loss: 0.17159897089004517\n",
      "training: 34 batch 159 batch_loss: 0.1697203814983368\n",
      "training: 34 batch 160 batch_loss: 0.17123880982398987\n",
      "training: 34 batch 161 batch_loss: 0.16729247570037842\n",
      "training: 34 batch 162 batch_loss: 0.16604062914848328\n",
      "training: 34 batch 163 batch_loss: 0.1711258888244629\n",
      "training: 34 batch 164 batch_loss: 0.17284131050109863\n",
      "training: 34 batch 165 batch_loss: 0.16811636090278625\n",
      "training: 34 batch 166 batch_loss: 0.1667259931564331\n",
      "training: 34 batch 167 batch_loss: 0.16864940524101257\n",
      "training: 34 batch 168 batch_loss: 0.16748052835464478\n",
      "training: 34 batch 169 batch_loss: 0.1683538556098938\n",
      "training: 34 batch 170 batch_loss: 0.16900521516799927\n",
      "training: 34 batch 171 batch_loss: 0.1739567518234253\n",
      "training: 34 batch 172 batch_loss: 0.172382652759552\n",
      "training: 34 batch 173 batch_loss: 0.16926094889640808\n",
      "training: 34 batch 174 batch_loss: 0.1677986979484558\n",
      "training: 34 batch 175 batch_loss: 0.16734278202056885\n",
      "training: 34 batch 176 batch_loss: 0.17045775055885315\n",
      "training: 34 batch 177 batch_loss: 0.17475220561027527\n",
      "training: 34 batch 178 batch_loss: 0.1700860857963562\n",
      "training: 34 batch 179 batch_loss: 0.1678406000137329\n",
      "training: 34 batch 180 batch_loss: 0.1694590449333191\n",
      "training: 34 batch 181 batch_loss: 0.17106753587722778\n",
      "training: 34 batch 182 batch_loss: 0.16990026831626892\n",
      "training: 34 batch 183 batch_loss: 0.16851788759231567\n",
      "training: 34 batch 184 batch_loss: 0.1660536527633667\n",
      "training: 34 batch 185 batch_loss: 0.17090603709220886\n",
      "training: 34 batch 186 batch_loss: 0.17099148035049438\n",
      "training: 34 batch 187 batch_loss: 0.17154300212860107\n",
      "training: 34 batch 188 batch_loss: 0.169452965259552\n",
      "training: 34 batch 189 batch_loss: 0.1724683940410614\n",
      "training: 34 batch 190 batch_loss: 0.16365814208984375\n",
      "training: 34 batch 191 batch_loss: 0.171619713306427\n",
      "training: 34 batch 192 batch_loss: 0.17023205757141113\n",
      "training: 34 batch 193 batch_loss: 0.16958272457122803\n",
      "training: 34 batch 194 batch_loss: 0.1672895848751068\n",
      "training: 34 batch 195 batch_loss: 0.16893422603607178\n",
      "training: 34 batch 196 batch_loss: 0.16677731275558472\n",
      "training: 34 batch 197 batch_loss: 0.16708523035049438\n",
      "training: 34 batch 198 batch_loss: 0.17232820391654968\n",
      "training: 34 batch 199 batch_loss: 0.16847512125968933\n",
      "training: 34 batch 200 batch_loss: 0.16687795519828796\n",
      "training: 34 batch 201 batch_loss: 0.16761282086372375\n",
      "training: 34 batch 202 batch_loss: 0.16802972555160522\n",
      "training: 34 batch 203 batch_loss: 0.1710606813430786\n",
      "training: 34 batch 204 batch_loss: 0.1704382598400116\n",
      "training: 34 batch 205 batch_loss: 0.16780388355255127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 34 batch 206 batch_loss: 0.17008894681930542\n",
      "training: 34 batch 207 batch_loss: 0.16783946752548218\n",
      "training: 34 batch 208 batch_loss: 0.168165385723114\n",
      "training: 34 batch 209 batch_loss: 0.16891935467720032\n",
      "training: 34 batch 210 batch_loss: 0.1709359586238861\n",
      "training: 34 batch 211 batch_loss: 0.17053484916687012\n",
      "training: 34 batch 212 batch_loss: 0.171980082988739\n",
      "training: 34 batch 213 batch_loss: 0.17030102014541626\n",
      "training: 34 batch 214 batch_loss: 0.16879671812057495\n",
      "training: 34 batch 215 batch_loss: 0.17036446928977966\n",
      "training: 34 batch 216 batch_loss: 0.16913336515426636\n",
      "training: 34 batch 217 batch_loss: 0.1731376051902771\n",
      "training: 34 batch 218 batch_loss: 0.16752421855926514\n",
      "training: 34 batch 219 batch_loss: 0.17196592688560486\n",
      "training: 34 batch 220 batch_loss: 0.17196595668792725\n",
      "training: 34 batch 221 batch_loss: 0.16908228397369385\n",
      "training: 34 batch 222 batch_loss: 0.16946065425872803\n",
      "training: 34 batch 223 batch_loss: 0.1687905490398407\n",
      "training: 34 batch 224 batch_loss: 0.1688128411769867\n",
      "training: 34 batch 225 batch_loss: 0.16915911436080933\n",
      "training: 34 batch 226 batch_loss: 0.16901111602783203\n",
      "training: 34 batch 227 batch_loss: 0.16780942678451538\n",
      "training: 34 batch 228 batch_loss: 0.1673772931098938\n",
      "training: 34 batch 229 batch_loss: 0.17009592056274414\n",
      "training: 34 batch 230 batch_loss: 0.1687099039554596\n",
      "training: 34 batch 231 batch_loss: 0.1726747453212738\n",
      "training: 34 batch 232 batch_loss: 0.17396122217178345\n",
      "training: 34 batch 233 batch_loss: 0.16889157891273499\n",
      "training: 34 batch 234 batch_loss: 0.17138782143592834\n",
      "training: 34 batch 235 batch_loss: 0.17033380270004272\n",
      "training: 34 batch 236 batch_loss: 0.1670876145362854\n",
      "training: 34 batch 237 batch_loss: 0.16718247532844543\n",
      "training: 34 batch 238 batch_loss: 0.1682862639427185\n",
      "training: 34 batch 239 batch_loss: 0.16941481828689575\n",
      "training: 34 batch 240 batch_loss: 0.16953036189079285\n",
      "training: 34 batch 241 batch_loss: 0.17276442050933838\n",
      "training: 34 batch 242 batch_loss: 0.17027893662452698\n",
      "training: 34 batch 243 batch_loss: 0.16806650161743164\n",
      "training: 34 batch 244 batch_loss: 0.17002856731414795\n",
      "training: 34 batch 245 batch_loss: 0.17213797569274902\n",
      "training: 34 batch 246 batch_loss: 0.16831618547439575\n",
      "training: 34 batch 247 batch_loss: 0.16674387454986572\n",
      "training: 34 batch 248 batch_loss: 0.1733647584915161\n",
      "training: 34 batch 249 batch_loss: 0.17155009508132935\n",
      "training: 34 batch 250 batch_loss: 0.17092201113700867\n",
      "training: 34 batch 251 batch_loss: 0.1707625389099121\n",
      "training: 34 batch 252 batch_loss: 0.16727158427238464\n",
      "training: 34 batch 253 batch_loss: 0.16886714100837708\n",
      "training: 34 batch 254 batch_loss: 0.16935056447982788\n",
      "training: 34 batch 255 batch_loss: 0.16964533925056458\n",
      "training: 34 batch 256 batch_loss: 0.1680353581905365\n",
      "training: 34 batch 257 batch_loss: 0.16884630918502808\n",
      "training: 34 batch 258 batch_loss: 0.168840229511261\n",
      "training: 34 batch 259 batch_loss: 0.16806980967521667\n",
      "training: 34 batch 260 batch_loss: 0.1674361228942871\n",
      "training: 34 batch 261 batch_loss: 0.17105865478515625\n",
      "training: 34 batch 262 batch_loss: 0.1714843213558197\n",
      "training: 34 batch 263 batch_loss: 0.17008313536643982\n",
      "training: 34 batch 264 batch_loss: 0.16707393527030945\n",
      "training: 34 batch 265 batch_loss: 0.1664612591266632\n",
      "training: 34 batch 266 batch_loss: 0.16997766494750977\n",
      "training: 34 batch 267 batch_loss: 0.16968867182731628\n",
      "training: 34 batch 268 batch_loss: 0.16921749711036682\n",
      "training: 34 batch 269 batch_loss: 0.1706382930278778\n",
      "training: 34 batch 270 batch_loss: 0.1707960069179535\n",
      "training: 34 batch 271 batch_loss: 0.17087286710739136\n",
      "training: 34 batch 272 batch_loss: 0.16943871974945068\n",
      "training: 34 batch 273 batch_loss: 0.16579985618591309\n",
      "training: 34 batch 274 batch_loss: 0.1724550724029541\n",
      "training: 34 batch 275 batch_loss: 0.1695351004600525\n",
      "training: 34 batch 276 batch_loss: 0.16911989450454712\n",
      "training: 34 batch 277 batch_loss: 0.1716444194316864\n",
      "training: 34 batch 278 batch_loss: 0.16689294576644897\n",
      "training: 34 batch 279 batch_loss: 0.16881322860717773\n",
      "training: 34 batch 280 batch_loss: 0.1685827076435089\n",
      "training: 34 batch 281 batch_loss: 0.17179030179977417\n",
      "training: 34 batch 282 batch_loss: 0.17013847827911377\n",
      "training: 34 batch 283 batch_loss: 0.16888976097106934\n",
      "training: 34 batch 284 batch_loss: 0.16840404272079468\n",
      "training: 34 batch 285 batch_loss: 0.1691395342350006\n",
      "training: 34 batch 286 batch_loss: 0.16738900542259216\n",
      "training: 34 batch 287 batch_loss: 0.17185619473457336\n",
      "training: 34 batch 288 batch_loss: 0.1710788905620575\n",
      "training: 34 batch 289 batch_loss: 0.16827967762947083\n",
      "training: 34 batch 290 batch_loss: 0.16999971866607666\n",
      "training: 34 batch 291 batch_loss: 0.17202842235565186\n",
      "training: 34 batch 292 batch_loss: 0.17384964227676392\n",
      "training: 34 batch 293 batch_loss: 0.17443406581878662\n",
      "training: 34 batch 294 batch_loss: 0.17094922065734863\n",
      "training: 34 batch 295 batch_loss: 0.16601595282554626\n",
      "training: 34 batch 296 batch_loss: 0.1696453094482422\n",
      "training: 34 batch 297 batch_loss: 0.16838961839675903\n",
      "training: 34 batch 298 batch_loss: 0.17195898294448853\n",
      "training: 34 batch 299 batch_loss: 0.16928505897521973\n",
      "training: 34 batch 300 batch_loss: 0.16727164387702942\n",
      "training: 34 batch 301 batch_loss: 0.16654029488563538\n",
      "training: 34 batch 302 batch_loss: 0.16825398802757263\n",
      "training: 34 batch 303 batch_loss: 0.17447787523269653\n",
      "training: 34 batch 304 batch_loss: 0.1680266261100769\n",
      "training: 34 batch 305 batch_loss: 0.17002013325691223\n",
      "training: 34 batch 306 batch_loss: 0.17035627365112305\n",
      "training: 34 batch 307 batch_loss: 0.16694676876068115\n",
      "training: 34 batch 308 batch_loss: 0.16719380021095276\n",
      "training: 34 batch 309 batch_loss: 0.16980624198913574\n",
      "training: 34 batch 310 batch_loss: 0.17106890678405762\n",
      "training: 34 batch 311 batch_loss: 0.16803118586540222\n",
      "training: 34 batch 312 batch_loss: 0.16835904121398926\n",
      "training: 34 batch 313 batch_loss: 0.17091888189315796\n",
      "training: 34 batch 314 batch_loss: 0.16932278871536255\n",
      "training: 34 batch 315 batch_loss: 0.16688916087150574\n",
      "training: 34 batch 316 batch_loss: 0.17064169049263\n",
      "training: 34 batch 317 batch_loss: 0.17473292350769043\n",
      "training: 34 batch 318 batch_loss: 0.17192500829696655\n",
      "training: 34 batch 319 batch_loss: 0.17203906178474426\n",
      "training: 34 batch 320 batch_loss: 0.16735222935676575\n",
      "training: 34 batch 321 batch_loss: 0.1715208888053894\n",
      "training: 34 batch 322 batch_loss: 0.1688011884689331\n",
      "training: 34 batch 323 batch_loss: 0.17320317029953003\n",
      "training: 34 batch 324 batch_loss: 0.16738510131835938\n",
      "training: 34 batch 325 batch_loss: 0.1735849678516388\n",
      "training: 34 batch 326 batch_loss: 0.17010796070098877\n",
      "training: 34 batch 327 batch_loss: 0.17342907190322876\n",
      "training: 34 batch 328 batch_loss: 0.1713460087776184\n",
      "training: 34 batch 329 batch_loss: 0.16966363787651062\n",
      "training: 34 batch 330 batch_loss: 0.1707991659641266\n",
      "training: 34 batch 331 batch_loss: 0.1684875786304474\n",
      "training: 34 batch 332 batch_loss: 0.17132055759429932\n",
      "training: 34 batch 333 batch_loss: 0.1694890260696411\n",
      "training: 34 batch 334 batch_loss: 0.1726360023021698\n",
      "training: 34 batch 335 batch_loss: 0.1665543019771576\n",
      "training: 34 batch 336 batch_loss: 0.1669381856918335\n",
      "training: 34 batch 337 batch_loss: 0.16873964667320251\n",
      "training: 34 batch 338 batch_loss: 0.16758081316947937\n",
      "training: 34 batch 339 batch_loss: 0.17291194200515747\n",
      "training: 34 batch 340 batch_loss: 0.16936787962913513\n",
      "training: 34 batch 341 batch_loss: 0.16752350330352783\n",
      "training: 34 batch 342 batch_loss: 0.17039689421653748\n",
      "training: 34 batch 343 batch_loss: 0.16973593831062317\n",
      "training: 34 batch 344 batch_loss: 0.16947969794273376\n",
      "training: 34 batch 345 batch_loss: 0.1718960702419281\n",
      "training: 34 batch 346 batch_loss: 0.1705523431301117\n",
      "training: 34 batch 347 batch_loss: 0.16843020915985107\n",
      "training: 34 batch 348 batch_loss: 0.17284047603607178\n",
      "training: 34 batch 349 batch_loss: 0.17211094498634338\n",
      "training: 34 batch 350 batch_loss: 0.16954872012138367\n",
      "training: 34 batch 351 batch_loss: 0.16615119576454163\n",
      "training: 34 batch 352 batch_loss: 0.17073893547058105\n",
      "training: 34 batch 353 batch_loss: 0.17141425609588623\n",
      "training: 34 batch 354 batch_loss: 0.168952077627182\n",
      "training: 34 batch 355 batch_loss: 0.1706780195236206\n",
      "training: 34 batch 356 batch_loss: 0.1703680157661438\n",
      "training: 34 batch 357 batch_loss: 0.16837775707244873\n",
      "training: 34 batch 358 batch_loss: 0.16951119899749756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 34 batch 359 batch_loss: 0.1692538857460022\n",
      "training: 34 batch 360 batch_loss: 0.1691220998764038\n",
      "training: 34 batch 361 batch_loss: 0.1690850555896759\n",
      "training: 34 batch 362 batch_loss: 0.1684981882572174\n",
      "training: 34 batch 363 batch_loss: 0.17243531346321106\n",
      "training: 34 batch 364 batch_loss: 0.1713193953037262\n",
      "training: 34 batch 365 batch_loss: 0.16632887721061707\n",
      "training: 34 batch 366 batch_loss: 0.17138051986694336\n",
      "training: 34 batch 367 batch_loss: 0.1722956895828247\n",
      "training: 34 batch 368 batch_loss: 0.1696794629096985\n",
      "training: 34 batch 369 batch_loss: 0.17105084657669067\n",
      "training: 34 batch 370 batch_loss: 0.17023053765296936\n",
      "training: 34 batch 371 batch_loss: 0.17234688997268677\n",
      "training: 34 batch 372 batch_loss: 0.1716119647026062\n",
      "training: 34 batch 373 batch_loss: 0.17074054479599\n",
      "training: 34 batch 374 batch_loss: 0.17291632294654846\n",
      "training: 34 batch 375 batch_loss: 0.16999101638793945\n",
      "training: 34 batch 376 batch_loss: 0.17022013664245605\n",
      "training: 34 batch 377 batch_loss: 0.16847044229507446\n",
      "training: 34 batch 378 batch_loss: 0.17303723096847534\n",
      "training: 34 batch 379 batch_loss: 0.16657677292823792\n",
      "training: 34 batch 380 batch_loss: 0.17057111859321594\n",
      "training: 34 batch 381 batch_loss: 0.1711428463459015\n",
      "training: 34 batch 382 batch_loss: 0.169131338596344\n",
      "training: 34 batch 383 batch_loss: 0.16807648539543152\n",
      "training: 34 batch 384 batch_loss: 0.16907629370689392\n",
      "training: 34 batch 385 batch_loss: 0.1734197735786438\n",
      "training: 34 batch 386 batch_loss: 0.16742971539497375\n",
      "training: 34 batch 387 batch_loss: 0.17062318325042725\n",
      "training: 34 batch 388 batch_loss: 0.17207807302474976\n",
      "training: 34 batch 389 batch_loss: 0.17320460081100464\n",
      "training: 34 batch 390 batch_loss: 0.171382874250412\n",
      "training: 34 batch 391 batch_loss: 0.16945520043373108\n",
      "training: 34 batch 392 batch_loss: 0.17153257131576538\n",
      "training: 34 batch 393 batch_loss: 0.16810905933380127\n",
      "training: 34 batch 394 batch_loss: 0.16636702418327332\n",
      "training: 34 batch 395 batch_loss: 0.17086264491081238\n",
      "training: 34 batch 396 batch_loss: 0.17125260829925537\n",
      "training: 34 batch 397 batch_loss: 0.17126783728599548\n",
      "training: 34 batch 398 batch_loss: 0.1704593300819397\n",
      "training: 34 batch 399 batch_loss: 0.17333096265792847\n",
      "training: 34 batch 400 batch_loss: 0.16859817504882812\n",
      "training: 34 batch 401 batch_loss: 0.17113977670669556\n",
      "training: 34 batch 402 batch_loss: 0.1718904972076416\n",
      "training: 34 batch 403 batch_loss: 0.17518338561058044\n",
      "training: 34 batch 404 batch_loss: 0.17030340433120728\n",
      "training: 34 batch 405 batch_loss: 0.17131775617599487\n",
      "training: 34 batch 406 batch_loss: 0.17297232151031494\n",
      "training: 34 batch 407 batch_loss: 0.16998976469039917\n",
      "training: 34 batch 408 batch_loss: 0.17008966207504272\n",
      "training: 34 batch 409 batch_loss: 0.17161917686462402\n",
      "training: 34 batch 410 batch_loss: 0.17137762904167175\n",
      "training: 34 batch 411 batch_loss: 0.16964173316955566\n",
      "training: 34 batch 412 batch_loss: 0.16961193084716797\n",
      "training: 34 batch 413 batch_loss: 0.1748996078968048\n",
      "training: 34 batch 414 batch_loss: 0.1703455150127411\n",
      "training: 34 batch 415 batch_loss: 0.1672235131263733\n",
      "training: 34 batch 416 batch_loss: 0.172012060880661\n",
      "training: 34 batch 417 batch_loss: 0.16799789667129517\n",
      "training: 34 batch 418 batch_loss: 0.16835981607437134\n",
      "training: 34 batch 419 batch_loss: 0.1695849597454071\n",
      "training: 34 batch 420 batch_loss: 0.1708337664604187\n",
      "training: 34 batch 421 batch_loss: 0.16962113976478577\n",
      "training: 34 batch 422 batch_loss: 0.16925176978111267\n",
      "training: 34 batch 423 batch_loss: 0.1696157455444336\n",
      "training: 34 batch 424 batch_loss: 0.17226672172546387\n",
      "training: 34 batch 425 batch_loss: 0.16869783401489258\n",
      "training: 34 batch 426 batch_loss: 0.16907575726509094\n",
      "training: 34 batch 427 batch_loss: 0.1699918806552887\n",
      "training: 34 batch 428 batch_loss: 0.17369616031646729\n",
      "training: 34 batch 429 batch_loss: 0.1710556149482727\n",
      "training: 34 batch 430 batch_loss: 0.1685601770877838\n",
      "training: 34 batch 431 batch_loss: 0.16822680830955505\n",
      "training: 34 batch 432 batch_loss: 0.16938409209251404\n",
      "training: 34 batch 433 batch_loss: 0.17429375648498535\n",
      "training: 34 batch 434 batch_loss: 0.17354506254196167\n",
      "training: 34 batch 435 batch_loss: 0.1700831949710846\n",
      "training: 34 batch 436 batch_loss: 0.16906973719596863\n",
      "training: 34 batch 437 batch_loss: 0.17087984085083008\n",
      "training: 34 batch 438 batch_loss: 0.17260810732841492\n",
      "training: 34 batch 439 batch_loss: 0.1725383996963501\n",
      "training: 34 batch 440 batch_loss: 0.17109155654907227\n",
      "training: 34 batch 441 batch_loss: 0.16948196291923523\n",
      "training: 34 batch 442 batch_loss: 0.1690322756767273\n",
      "training: 34 batch 443 batch_loss: 0.1673414409160614\n",
      "training: 34 batch 444 batch_loss: 0.173793226480484\n",
      "training: 34 batch 445 batch_loss: 0.1712280511856079\n",
      "training: 34 batch 446 batch_loss: 0.17117822170257568\n",
      "training: 34 batch 447 batch_loss: 0.1720239520072937\n",
      "training: 34 batch 448 batch_loss: 0.17112118005752563\n",
      "training: 34 batch 449 batch_loss: 0.16751429438591003\n",
      "training: 34 batch 450 batch_loss: 0.16564762592315674\n",
      "training: 34 batch 451 batch_loss: 0.16853126883506775\n",
      "training: 34 batch 452 batch_loss: 0.1750670075416565\n",
      "training: 34 batch 453 batch_loss: 0.16847890615463257\n",
      "training: 34 batch 454 batch_loss: 0.16873013973236084\n",
      "training: 34 batch 455 batch_loss: 0.17370879650115967\n",
      "training: 34 batch 456 batch_loss: 0.17262303829193115\n",
      "training: 34 batch 457 batch_loss: 0.1672857105731964\n",
      "training: 34 batch 458 batch_loss: 0.174719899892807\n",
      "training: 34 batch 459 batch_loss: 0.1697472333908081\n",
      "training: 34 batch 460 batch_loss: 0.17216122150421143\n",
      "training: 34 batch 461 batch_loss: 0.1677234172821045\n",
      "training: 34 batch 462 batch_loss: 0.16784873604774475\n",
      "training: 34 batch 463 batch_loss: 0.17379847168922424\n",
      "training: 34 batch 464 batch_loss: 0.1685447096824646\n",
      "training: 34 batch 465 batch_loss: 0.1693422794342041\n",
      "training: 34 batch 466 batch_loss: 0.16774249076843262\n",
      "training: 34 batch 467 batch_loss: 0.17223164439201355\n",
      "training: 34 batch 468 batch_loss: 0.17166882753372192\n",
      "training: 34 batch 469 batch_loss: 0.17001289129257202\n",
      "training: 34 batch 470 batch_loss: 0.1707039475440979\n",
      "training: 34 batch 471 batch_loss: 0.16919654607772827\n",
      "training: 34 batch 472 batch_loss: 0.17248085141181946\n",
      "training: 34 batch 473 batch_loss: 0.1700977087020874\n",
      "training: 34 batch 474 batch_loss: 0.16945499181747437\n",
      "training: 34 batch 475 batch_loss: 0.1742539405822754\n",
      "training: 34 batch 476 batch_loss: 0.16897377371788025\n",
      "training: 34 batch 477 batch_loss: 0.17148059606552124\n",
      "training: 34 batch 478 batch_loss: 0.17107996344566345\n",
      "training: 34 batch 479 batch_loss: 0.17383301258087158\n",
      "training: 34 batch 480 batch_loss: 0.1698605716228485\n",
      "training: 34 batch 481 batch_loss: 0.16864779591560364\n",
      "training: 34 batch 482 batch_loss: 0.1722920835018158\n",
      "training: 34 batch 483 batch_loss: 0.1721266508102417\n",
      "training: 34 batch 484 batch_loss: 0.17260092496871948\n",
      "training: 34 batch 485 batch_loss: 0.16938677430152893\n",
      "training: 34 batch 486 batch_loss: 0.1689205765724182\n",
      "training: 34 batch 487 batch_loss: 0.17062896490097046\n",
      "training: 34 batch 488 batch_loss: 0.16805437207221985\n",
      "training: 34 batch 489 batch_loss: 0.17179888486862183\n",
      "training: 34 batch 490 batch_loss: 0.17336547374725342\n",
      "training: 34 batch 491 batch_loss: 0.17009809613227844\n",
      "training: 34 batch 492 batch_loss: 0.1705385148525238\n",
      "training: 34 batch 493 batch_loss: 0.1679045557975769\n",
      "training: 34 batch 494 batch_loss: 0.17017659544944763\n",
      "training: 34 batch 495 batch_loss: 0.17256581783294678\n",
      "training: 34 batch 496 batch_loss: 0.16638794541358948\n",
      "training: 34 batch 497 batch_loss: 0.17481794953346252\n",
      "training: 34 batch 498 batch_loss: 0.17136815190315247\n",
      "training: 34 batch 499 batch_loss: 0.16735610365867615\n",
      "training: 34 batch 500 batch_loss: 0.17347416281700134\n",
      "training: 34 batch 501 batch_loss: 0.17162403464317322\n",
      "training: 34 batch 502 batch_loss: 0.17191219329833984\n",
      "training: 34 batch 503 batch_loss: 0.17237204313278198\n",
      "training: 34 batch 504 batch_loss: 0.16775837540626526\n",
      "training: 34 batch 505 batch_loss: 0.17057976126670837\n",
      "training: 34 batch 506 batch_loss: 0.1736936867237091\n",
      "training: 34 batch 507 batch_loss: 0.17309623956680298\n",
      "training: 34 batch 508 batch_loss: 0.17106401920318604\n",
      "training: 34 batch 509 batch_loss: 0.17293605208396912\n",
      "training: 34 batch 510 batch_loss: 0.16955804824829102\n",
      "training: 34 batch 511 batch_loss: 0.17415621876716614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 34 batch 512 batch_loss: 0.1722365915775299\n",
      "training: 34 batch 513 batch_loss: 0.1737743318080902\n",
      "training: 34 batch 514 batch_loss: 0.16951879858970642\n",
      "training: 34 batch 515 batch_loss: 0.17069366574287415\n",
      "training: 34 batch 516 batch_loss: 0.16831931471824646\n",
      "training: 34 batch 517 batch_loss: 0.17113709449768066\n",
      "training: 34 batch 518 batch_loss: 0.1685171127319336\n",
      "training: 34 batch 519 batch_loss: 0.17185452580451965\n",
      "training: 34 batch 520 batch_loss: 0.1710371971130371\n",
      "training: 34 batch 521 batch_loss: 0.17340132594108582\n",
      "training: 34 batch 522 batch_loss: 0.16838839650154114\n",
      "training: 34 batch 523 batch_loss: 0.17110025882720947\n",
      "training: 34 batch 524 batch_loss: 0.17117750644683838\n",
      "training: 34 batch 525 batch_loss: 0.17054036259651184\n",
      "training: 34 batch 526 batch_loss: 0.1706041395664215\n",
      "training: 34 batch 527 batch_loss: 0.1717950701713562\n",
      "training: 34 batch 528 batch_loss: 0.17108505964279175\n",
      "training: 34 batch 529 batch_loss: 0.1735876202583313\n",
      "training: 34 batch 530 batch_loss: 0.1758321225643158\n",
      "training: 34 batch 531 batch_loss: 0.1725512146949768\n",
      "training: 34 batch 532 batch_loss: 0.17046386003494263\n",
      "training: 34 batch 533 batch_loss: 0.16877764463424683\n",
      "training: 34 batch 534 batch_loss: 0.16973114013671875\n",
      "training: 34 batch 535 batch_loss: 0.1722089946269989\n",
      "training: 34 batch 536 batch_loss: 0.17047259211540222\n",
      "training: 34 batch 537 batch_loss: 0.17399293184280396\n",
      "training: 34 batch 538 batch_loss: 0.17252874374389648\n",
      "training: 34 batch 539 batch_loss: 0.17087021470069885\n",
      "training: 34 batch 540 batch_loss: 0.17027044296264648\n",
      "training: 34 batch 541 batch_loss: 0.17014485597610474\n",
      "training: 34 batch 542 batch_loss: 0.1695888638496399\n",
      "training: 34 batch 543 batch_loss: 0.1709415316581726\n",
      "training: 34 batch 544 batch_loss: 0.17161226272583008\n",
      "training: 34 batch 545 batch_loss: 0.17298555374145508\n",
      "training: 34 batch 546 batch_loss: 0.17358198761940002\n",
      "training: 34 batch 547 batch_loss: 0.17117851972579956\n",
      "training: 34 batch 548 batch_loss: 0.16792899370193481\n",
      "training: 34 batch 549 batch_loss: 0.1704464852809906\n",
      "training: 34 batch 550 batch_loss: 0.17193329334259033\n",
      "training: 34 batch 551 batch_loss: 0.17183610796928406\n",
      "training: 34 batch 552 batch_loss: 0.1735665500164032\n",
      "training: 34 batch 553 batch_loss: 0.17351442575454712\n",
      "training: 34 batch 554 batch_loss: 0.16882410645484924\n",
      "training: 34 batch 555 batch_loss: 0.17044207453727722\n",
      "training: 34 batch 556 batch_loss: 0.17179381847381592\n",
      "training: 34 batch 557 batch_loss: 0.17264911532402039\n",
      "training: 34 batch 558 batch_loss: 0.17170119285583496\n",
      "training: 34 batch 559 batch_loss: 0.17038071155548096\n",
      "training: 34 batch 560 batch_loss: 0.1728838086128235\n",
      "training: 34 batch 561 batch_loss: 0.17218732833862305\n",
      "training: 34 batch 562 batch_loss: 0.1681891679763794\n",
      "training: 34 batch 563 batch_loss: 0.1721045970916748\n",
      "training: 34 batch 564 batch_loss: 0.17196297645568848\n",
      "training: 34 batch 565 batch_loss: 0.17095109820365906\n",
      "training: 34 batch 566 batch_loss: 0.17059192061424255\n",
      "training: 34 batch 567 batch_loss: 0.16929903626441956\n",
      "training: 34 batch 568 batch_loss: 0.16874545812606812\n",
      "training: 34 batch 569 batch_loss: 0.1738535463809967\n",
      "training: 34 batch 570 batch_loss: 0.17169621586799622\n",
      "training: 34 batch 571 batch_loss: 0.17141377925872803\n",
      "training: 34 batch 572 batch_loss: 0.16927078366279602\n",
      "training: 34 batch 573 batch_loss: 0.1678370237350464\n",
      "training: 34 batch 574 batch_loss: 0.17088764905929565\n",
      "training: 34 batch 575 batch_loss: 0.17546150088310242\n",
      "training: 34 batch 576 batch_loss: 0.1688879430294037\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 34, Hit Ratio:0.03058032665651764 | Precision:0.045119433795340606 | Recall:0.06037056963494253 | NDCG:0.05801775654298079\n",
      "*Best Performance* \n",
      "Epoch: 34, Hit Ratio:0.03058032665651764 | Precision:0.045119433795340606 | Recall:0.06037056963494253 | MDCG:0.05801775654298079\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 35 batch 0 batch_loss: 0.16982492804527283\n",
      "training: 35 batch 1 batch_loss: 0.1726810336112976\n",
      "training: 35 batch 2 batch_loss: 0.16634279489517212\n",
      "training: 35 batch 3 batch_loss: 0.17027926445007324\n",
      "training: 35 batch 4 batch_loss: 0.1712357997894287\n",
      "training: 35 batch 5 batch_loss: 0.1715986132621765\n",
      "training: 35 batch 6 batch_loss: 0.17240610718727112\n",
      "training: 35 batch 7 batch_loss: 0.16858315467834473\n",
      "training: 35 batch 8 batch_loss: 0.17372223734855652\n",
      "training: 35 batch 9 batch_loss: 0.16795748472213745\n",
      "training: 35 batch 10 batch_loss: 0.16776329278945923\n",
      "training: 35 batch 11 batch_loss: 0.16881275177001953\n",
      "training: 35 batch 12 batch_loss: 0.1684090793132782\n",
      "training: 35 batch 13 batch_loss: 0.17170202732086182\n",
      "training: 35 batch 14 batch_loss: 0.16842108964920044\n",
      "training: 35 batch 15 batch_loss: 0.17205998301506042\n",
      "training: 35 batch 16 batch_loss: 0.16900768876075745\n",
      "training: 35 batch 17 batch_loss: 0.17039728164672852\n",
      "training: 35 batch 18 batch_loss: 0.16875314712524414\n",
      "training: 35 batch 19 batch_loss: 0.16638439893722534\n",
      "training: 35 batch 20 batch_loss: 0.16662418842315674\n",
      "training: 35 batch 21 batch_loss: 0.17073610424995422\n",
      "training: 35 batch 22 batch_loss: 0.17096656560897827\n",
      "training: 35 batch 23 batch_loss: 0.1698710322380066\n",
      "training: 35 batch 24 batch_loss: 0.1694459319114685\n",
      "training: 35 batch 25 batch_loss: 0.16759687662124634\n",
      "training: 35 batch 26 batch_loss: 0.16795876622200012\n",
      "training: 35 batch 27 batch_loss: 0.17226028442382812\n",
      "training: 35 batch 28 batch_loss: 0.16945534944534302\n",
      "training: 35 batch 29 batch_loss: 0.17253845930099487\n",
      "training: 35 batch 30 batch_loss: 0.16620159149169922\n",
      "training: 35 batch 31 batch_loss: 0.17012563347816467\n",
      "training: 35 batch 32 batch_loss: 0.17040693759918213\n",
      "training: 35 batch 33 batch_loss: 0.1699700653553009\n",
      "training: 35 batch 34 batch_loss: 0.16844263672828674\n",
      "training: 35 batch 35 batch_loss: 0.17001071572303772\n",
      "training: 35 batch 36 batch_loss: 0.16728296875953674\n",
      "training: 35 batch 37 batch_loss: 0.16754120588302612\n",
      "training: 35 batch 38 batch_loss: 0.171274334192276\n",
      "training: 35 batch 39 batch_loss: 0.1651776134967804\n",
      "training: 35 batch 40 batch_loss: 0.1707545518875122\n",
      "training: 35 batch 41 batch_loss: 0.16827845573425293\n",
      "training: 35 batch 42 batch_loss: 0.16790622472763062\n",
      "training: 35 batch 43 batch_loss: 0.16977077722549438\n",
      "training: 35 batch 44 batch_loss: 0.16939586400985718\n",
      "training: 35 batch 45 batch_loss: 0.16884547472000122\n",
      "training: 35 batch 46 batch_loss: 0.17013129591941833\n",
      "training: 35 batch 47 batch_loss: 0.170780748128891\n",
      "training: 35 batch 48 batch_loss: 0.16794922947883606\n",
      "training: 35 batch 49 batch_loss: 0.17142245173454285\n",
      "training: 35 batch 50 batch_loss: 0.1724778711795807\n",
      "training: 35 batch 51 batch_loss: 0.17070400714874268\n",
      "training: 35 batch 52 batch_loss: 0.170037180185318\n",
      "training: 35 batch 53 batch_loss: 0.1708475649356842\n",
      "training: 35 batch 54 batch_loss: 0.17039060592651367\n",
      "training: 35 batch 55 batch_loss: 0.17163342237472534\n",
      "training: 35 batch 56 batch_loss: 0.17020317912101746\n",
      "training: 35 batch 57 batch_loss: 0.1721491813659668\n",
      "training: 35 batch 58 batch_loss: 0.16897809505462646\n",
      "training: 35 batch 59 batch_loss: 0.17055973410606384\n",
      "training: 35 batch 60 batch_loss: 0.1711869239807129\n",
      "training: 35 batch 61 batch_loss: 0.16783177852630615\n",
      "training: 35 batch 62 batch_loss: 0.1695764660835266\n",
      "training: 35 batch 63 batch_loss: 0.1658819317817688\n",
      "training: 35 batch 64 batch_loss: 0.17040801048278809\n",
      "training: 35 batch 65 batch_loss: 0.16808760166168213\n",
      "training: 35 batch 66 batch_loss: 0.1706582009792328\n",
      "training: 35 batch 67 batch_loss: 0.16943460702896118\n",
      "training: 35 batch 68 batch_loss: 0.1711343228816986\n",
      "training: 35 batch 69 batch_loss: 0.17127549648284912\n",
      "training: 35 batch 70 batch_loss: 0.1701487898826599\n",
      "training: 35 batch 71 batch_loss: 0.16928628087043762\n",
      "training: 35 batch 72 batch_loss: 0.16858351230621338\n",
      "training: 35 batch 73 batch_loss: 0.16843479871749878\n",
      "training: 35 batch 74 batch_loss: 0.16873639822006226\n",
      "training: 35 batch 75 batch_loss: 0.1723807454109192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 35 batch 76 batch_loss: 0.17263221740722656\n",
      "training: 35 batch 77 batch_loss: 0.17094552516937256\n",
      "training: 35 batch 78 batch_loss: 0.16972815990447998\n",
      "training: 35 batch 79 batch_loss: 0.17291831970214844\n",
      "training: 35 batch 80 batch_loss: 0.17192897200584412\n",
      "training: 35 batch 81 batch_loss: 0.16923606395721436\n",
      "training: 35 batch 82 batch_loss: 0.1707059144973755\n",
      "training: 35 batch 83 batch_loss: 0.17167606949806213\n",
      "training: 35 batch 84 batch_loss: 0.17149457335472107\n",
      "training: 35 batch 85 batch_loss: 0.17080077528953552\n",
      "training: 35 batch 86 batch_loss: 0.16916298866271973\n",
      "training: 35 batch 87 batch_loss: 0.17430484294891357\n",
      "training: 35 batch 88 batch_loss: 0.1705436110496521\n",
      "training: 35 batch 89 batch_loss: 0.17038947343826294\n",
      "training: 35 batch 90 batch_loss: 0.1704663336277008\n",
      "training: 35 batch 91 batch_loss: 0.16998878121376038\n",
      "training: 35 batch 92 batch_loss: 0.16792109608650208\n",
      "training: 35 batch 93 batch_loss: 0.17517897486686707\n",
      "training: 35 batch 94 batch_loss: 0.17211824655532837\n",
      "training: 35 batch 95 batch_loss: 0.17070135474205017\n",
      "training: 35 batch 96 batch_loss: 0.16917288303375244\n",
      "training: 35 batch 97 batch_loss: 0.17042696475982666\n",
      "training: 35 batch 98 batch_loss: 0.16701751947402954\n",
      "training: 35 batch 99 batch_loss: 0.1705002784729004\n",
      "training: 35 batch 100 batch_loss: 0.16567125916481018\n",
      "training: 35 batch 101 batch_loss: 0.17092594504356384\n",
      "training: 35 batch 102 batch_loss: 0.1730824112892151\n",
      "training: 35 batch 103 batch_loss: 0.16797614097595215\n",
      "training: 35 batch 104 batch_loss: 0.1657593548297882\n",
      "training: 35 batch 105 batch_loss: 0.1722273826599121\n",
      "training: 35 batch 106 batch_loss: 0.16883254051208496\n",
      "training: 35 batch 107 batch_loss: 0.17030447721481323\n",
      "training: 35 batch 108 batch_loss: 0.16797426342964172\n",
      "training: 35 batch 109 batch_loss: 0.16992223262786865\n",
      "training: 35 batch 110 batch_loss: 0.16895034909248352\n",
      "training: 35 batch 111 batch_loss: 0.1705215573310852\n",
      "training: 35 batch 112 batch_loss: 0.17256850004196167\n",
      "training: 35 batch 113 batch_loss: 0.17315831780433655\n",
      "training: 35 batch 114 batch_loss: 0.17094740271568298\n",
      "training: 35 batch 115 batch_loss: 0.16944873332977295\n",
      "training: 35 batch 116 batch_loss: 0.16968542337417603\n",
      "training: 35 batch 117 batch_loss: 0.17241939902305603\n",
      "training: 35 batch 118 batch_loss: 0.17048344016075134\n",
      "training: 35 batch 119 batch_loss: 0.17187029123306274\n",
      "training: 35 batch 120 batch_loss: 0.17071202397346497\n",
      "training: 35 batch 121 batch_loss: 0.16529178619384766\n",
      "training: 35 batch 122 batch_loss: 0.16933280229568481\n",
      "training: 35 batch 123 batch_loss: 0.17392697930335999\n",
      "training: 35 batch 124 batch_loss: 0.17038589715957642\n",
      "training: 35 batch 125 batch_loss: 0.16967594623565674\n",
      "training: 35 batch 126 batch_loss: 0.16839277744293213\n",
      "training: 35 batch 127 batch_loss: 0.16895577311515808\n",
      "training: 35 batch 128 batch_loss: 0.17265105247497559\n",
      "training: 35 batch 129 batch_loss: 0.16940206289291382\n",
      "training: 35 batch 130 batch_loss: 0.17207065224647522\n",
      "training: 35 batch 131 batch_loss: 0.1690065860748291\n",
      "training: 35 batch 132 batch_loss: 0.17562872171401978\n",
      "training: 35 batch 133 batch_loss: 0.17135953903198242\n",
      "training: 35 batch 134 batch_loss: 0.16746366024017334\n",
      "training: 35 batch 135 batch_loss: 0.1682978868484497\n",
      "training: 35 batch 136 batch_loss: 0.1688348650932312\n",
      "training: 35 batch 137 batch_loss: 0.16855090856552124\n",
      "training: 35 batch 138 batch_loss: 0.17199400067329407\n",
      "training: 35 batch 139 batch_loss: 0.1726210117340088\n",
      "training: 35 batch 140 batch_loss: 0.16745078563690186\n",
      "training: 35 batch 141 batch_loss: 0.17142248153686523\n",
      "training: 35 batch 142 batch_loss: 0.1732081174850464\n",
      "training: 35 batch 143 batch_loss: 0.17092570662498474\n",
      "training: 35 batch 144 batch_loss: 0.16784971952438354\n",
      "training: 35 batch 145 batch_loss: 0.17147868871688843\n",
      "training: 35 batch 146 batch_loss: 0.1698736846446991\n",
      "training: 35 batch 147 batch_loss: 0.17162802815437317\n",
      "training: 35 batch 148 batch_loss: 0.17338666319847107\n",
      "training: 35 batch 149 batch_loss: 0.16960588097572327\n",
      "training: 35 batch 150 batch_loss: 0.1699654757976532\n",
      "training: 35 batch 151 batch_loss: 0.16864526271820068\n",
      "training: 35 batch 152 batch_loss: 0.17093193531036377\n",
      "training: 35 batch 153 batch_loss: 0.16972482204437256\n",
      "training: 35 batch 154 batch_loss: 0.1720922589302063\n",
      "training: 35 batch 155 batch_loss: 0.16815733909606934\n",
      "training: 35 batch 156 batch_loss: 0.17239272594451904\n",
      "training: 35 batch 157 batch_loss: 0.17119044065475464\n",
      "training: 35 batch 158 batch_loss: 0.16957715153694153\n",
      "training: 35 batch 159 batch_loss: 0.17056119441986084\n",
      "training: 35 batch 160 batch_loss: 0.16998958587646484\n",
      "training: 35 batch 161 batch_loss: 0.17165130376815796\n",
      "training: 35 batch 162 batch_loss: 0.17271003127098083\n",
      "training: 35 batch 163 batch_loss: 0.17153266072273254\n",
      "training: 35 batch 164 batch_loss: 0.17388173937797546\n",
      "training: 35 batch 165 batch_loss: 0.17228290438652039\n",
      "training: 35 batch 166 batch_loss: 0.17245271801948547\n",
      "training: 35 batch 167 batch_loss: 0.172461599111557\n",
      "training: 35 batch 168 batch_loss: 0.17402586340904236\n",
      "training: 35 batch 169 batch_loss: 0.17322340607643127\n",
      "training: 35 batch 170 batch_loss: 0.17127913236618042\n",
      "training: 35 batch 171 batch_loss: 0.17076066136360168\n",
      "training: 35 batch 172 batch_loss: 0.1683916449546814\n",
      "training: 35 batch 173 batch_loss: 0.17210403084754944\n",
      "training: 35 batch 174 batch_loss: 0.16682207584381104\n",
      "training: 35 batch 175 batch_loss: 0.17231407761573792\n",
      "training: 35 batch 176 batch_loss: 0.1687365174293518\n",
      "training: 35 batch 177 batch_loss: 0.1712799370288849\n",
      "training: 35 batch 178 batch_loss: 0.17123794555664062\n",
      "training: 35 batch 179 batch_loss: 0.16948580741882324\n",
      "training: 35 batch 180 batch_loss: 0.1679999828338623\n",
      "training: 35 batch 181 batch_loss: 0.171517014503479\n",
      "training: 35 batch 182 batch_loss: 0.16729536652565002\n",
      "training: 35 batch 183 batch_loss: 0.16848504543304443\n",
      "training: 35 batch 184 batch_loss: 0.1718342900276184\n",
      "training: 35 batch 185 batch_loss: 0.1734875738620758\n",
      "training: 35 batch 186 batch_loss: 0.1697331666946411\n",
      "training: 35 batch 187 batch_loss: 0.16974374651908875\n",
      "training: 35 batch 188 batch_loss: 0.17039260268211365\n",
      "training: 35 batch 189 batch_loss: 0.1677788496017456\n",
      "training: 35 batch 190 batch_loss: 0.17193815112113953\n",
      "training: 35 batch 191 batch_loss: 0.17325499653816223\n",
      "training: 35 batch 192 batch_loss: 0.1741873025894165\n",
      "training: 35 batch 193 batch_loss: 0.16815167665481567\n",
      "training: 35 batch 194 batch_loss: 0.17351692914962769\n",
      "training: 35 batch 195 batch_loss: 0.17023935914039612\n",
      "training: 35 batch 196 batch_loss: 0.17003434896469116\n",
      "training: 35 batch 197 batch_loss: 0.1716095507144928\n",
      "training: 35 batch 198 batch_loss: 0.17368841171264648\n",
      "training: 35 batch 199 batch_loss: 0.17386287450790405\n",
      "training: 35 batch 200 batch_loss: 0.17329266667366028\n",
      "training: 35 batch 201 batch_loss: 0.1710135042667389\n",
      "training: 35 batch 202 batch_loss: 0.17142707109451294\n",
      "training: 35 batch 203 batch_loss: 0.17029890418052673\n",
      "training: 35 batch 204 batch_loss: 0.16883397102355957\n",
      "training: 35 batch 205 batch_loss: 0.17283102869987488\n",
      "training: 35 batch 206 batch_loss: 0.17149001359939575\n",
      "training: 35 batch 207 batch_loss: 0.171647310256958\n",
      "training: 35 batch 208 batch_loss: 0.17253592610359192\n",
      "training: 35 batch 209 batch_loss: 0.17071282863616943\n",
      "training: 35 batch 210 batch_loss: 0.16902732849121094\n",
      "training: 35 batch 211 batch_loss: 0.16970282793045044\n",
      "training: 35 batch 212 batch_loss: 0.170085608959198\n",
      "training: 35 batch 213 batch_loss: 0.17012810707092285\n",
      "training: 35 batch 214 batch_loss: 0.1725936233997345\n",
      "training: 35 batch 215 batch_loss: 0.1726571023464203\n",
      "training: 35 batch 216 batch_loss: 0.17041417956352234\n",
      "training: 35 batch 217 batch_loss: 0.17061686515808105\n",
      "training: 35 batch 218 batch_loss: 0.16870570182800293\n",
      "training: 35 batch 219 batch_loss: 0.16949322819709778\n",
      "training: 35 batch 220 batch_loss: 0.17227071523666382\n",
      "training: 35 batch 221 batch_loss: 0.17237311601638794\n",
      "training: 35 batch 222 batch_loss: 0.17135539650917053\n",
      "training: 35 batch 223 batch_loss: 0.16943401098251343\n",
      "training: 35 batch 224 batch_loss: 0.1701759397983551\n",
      "training: 35 batch 225 batch_loss: 0.17107674479484558\n",
      "training: 35 batch 226 batch_loss: 0.16867101192474365\n",
      "training: 35 batch 227 batch_loss: 0.16925275325775146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 35 batch 228 batch_loss: 0.17260873317718506\n",
      "training: 35 batch 229 batch_loss: 0.1714077591896057\n",
      "training: 35 batch 230 batch_loss: 0.17256790399551392\n",
      "training: 35 batch 231 batch_loss: 0.17551812529563904\n",
      "training: 35 batch 232 batch_loss: 0.17007309198379517\n",
      "training: 35 batch 233 batch_loss: 0.17133402824401855\n",
      "training: 35 batch 234 batch_loss: 0.1684575080871582\n",
      "training: 35 batch 235 batch_loss: 0.1714613437652588\n",
      "training: 35 batch 236 batch_loss: 0.17178240418434143\n",
      "training: 35 batch 237 batch_loss: 0.16992902755737305\n",
      "training: 35 batch 238 batch_loss: 0.17183178663253784\n",
      "training: 35 batch 239 batch_loss: 0.17086109519004822\n",
      "training: 35 batch 240 batch_loss: 0.17166942358016968\n",
      "training: 35 batch 241 batch_loss: 0.16924402117729187\n",
      "training: 35 batch 242 batch_loss: 0.17194509506225586\n",
      "training: 35 batch 243 batch_loss: 0.17110759019851685\n",
      "training: 35 batch 244 batch_loss: 0.16894572973251343\n",
      "training: 35 batch 245 batch_loss: 0.1703195571899414\n",
      "training: 35 batch 246 batch_loss: 0.17689961194992065\n",
      "training: 35 batch 247 batch_loss: 0.1713041365146637\n",
      "training: 35 batch 248 batch_loss: 0.17242184281349182\n",
      "training: 35 batch 249 batch_loss: 0.17370855808258057\n",
      "training: 35 batch 250 batch_loss: 0.16943904757499695\n",
      "training: 35 batch 251 batch_loss: 0.17325478792190552\n",
      "training: 35 batch 252 batch_loss: 0.17142188549041748\n",
      "training: 35 batch 253 batch_loss: 0.17114117741584778\n",
      "training: 35 batch 254 batch_loss: 0.17141461372375488\n",
      "training: 35 batch 255 batch_loss: 0.16948705911636353\n",
      "training: 35 batch 256 batch_loss: 0.16823169589042664\n",
      "training: 35 batch 257 batch_loss: 0.1754099726676941\n",
      "training: 35 batch 258 batch_loss: 0.16860422492027283\n",
      "training: 35 batch 259 batch_loss: 0.17291638255119324\n",
      "training: 35 batch 260 batch_loss: 0.17091429233551025\n",
      "training: 35 batch 261 batch_loss: 0.17062655091285706\n",
      "training: 35 batch 262 batch_loss: 0.16979938745498657\n",
      "training: 35 batch 263 batch_loss: 0.17258289456367493\n",
      "training: 35 batch 264 batch_loss: 0.17310434579849243\n",
      "training: 35 batch 265 batch_loss: 0.17431366443634033\n",
      "training: 35 batch 266 batch_loss: 0.17205798625946045\n",
      "training: 35 batch 267 batch_loss: 0.17277061939239502\n",
      "training: 35 batch 268 batch_loss: 0.1721709966659546\n",
      "training: 35 batch 269 batch_loss: 0.17368078231811523\n",
      "training: 35 batch 270 batch_loss: 0.17077606916427612\n",
      "training: 35 batch 271 batch_loss: 0.17069393396377563\n",
      "training: 35 batch 272 batch_loss: 0.17288023233413696\n",
      "training: 35 batch 273 batch_loss: 0.1727774739265442\n",
      "training: 35 batch 274 batch_loss: 0.17108452320098877\n",
      "training: 35 batch 275 batch_loss: 0.1715085506439209\n",
      "training: 35 batch 276 batch_loss: 0.1710149347782135\n",
      "training: 35 batch 277 batch_loss: 0.17329272627830505\n",
      "training: 35 batch 278 batch_loss: 0.16863924264907837\n",
      "training: 35 batch 279 batch_loss: 0.16823482513427734\n",
      "training: 35 batch 280 batch_loss: 0.16971614956855774\n",
      "training: 35 batch 281 batch_loss: 0.1704123318195343\n",
      "training: 35 batch 282 batch_loss: 0.16916194558143616\n",
      "training: 35 batch 283 batch_loss: 0.1723061203956604\n",
      "training: 35 batch 284 batch_loss: 0.17527249455451965\n",
      "training: 35 batch 285 batch_loss: 0.17330670356750488\n",
      "training: 35 batch 286 batch_loss: 0.17172980308532715\n",
      "training: 35 batch 287 batch_loss: 0.1719796061515808\n",
      "training: 35 batch 288 batch_loss: 0.17208647727966309\n",
      "training: 35 batch 289 batch_loss: 0.1730855107307434\n",
      "training: 35 batch 290 batch_loss: 0.1737949550151825\n",
      "training: 35 batch 291 batch_loss: 0.1722174882888794\n",
      "training: 35 batch 292 batch_loss: 0.17141321301460266\n",
      "training: 35 batch 293 batch_loss: 0.17388591170310974\n",
      "training: 35 batch 294 batch_loss: 0.17006444931030273\n",
      "training: 35 batch 295 batch_loss: 0.170206218957901\n",
      "training: 35 batch 296 batch_loss: 0.17163816094398499\n",
      "training: 35 batch 297 batch_loss: 0.1768762469291687\n",
      "training: 35 batch 298 batch_loss: 0.16924238204956055\n",
      "training: 35 batch 299 batch_loss: 0.16510885953903198\n",
      "training: 35 batch 300 batch_loss: 0.1721908450126648\n",
      "training: 35 batch 301 batch_loss: 0.17069774866104126\n",
      "training: 35 batch 302 batch_loss: 0.17037004232406616\n",
      "training: 35 batch 303 batch_loss: 0.17280006408691406\n",
      "training: 35 batch 304 batch_loss: 0.16912409663200378\n",
      "training: 35 batch 305 batch_loss: 0.17235690355300903\n",
      "training: 35 batch 306 batch_loss: 0.1754865050315857\n",
      "training: 35 batch 307 batch_loss: 0.17132824659347534\n",
      "training: 35 batch 308 batch_loss: 0.17214742302894592\n",
      "training: 35 batch 309 batch_loss: 0.17068415880203247\n",
      "training: 35 batch 310 batch_loss: 0.17128720879554749\n",
      "training: 35 batch 311 batch_loss: 0.17021659016609192\n",
      "training: 35 batch 312 batch_loss: 0.17401280999183655\n",
      "training: 35 batch 313 batch_loss: 0.17031681537628174\n",
      "training: 35 batch 314 batch_loss: 0.1731734275817871\n",
      "training: 35 batch 315 batch_loss: 0.1696951687335968\n",
      "training: 35 batch 316 batch_loss: 0.17126187682151794\n",
      "training: 35 batch 317 batch_loss: 0.17266643047332764\n",
      "training: 35 batch 318 batch_loss: 0.1707073450088501\n",
      "training: 35 batch 319 batch_loss: 0.17123058438301086\n",
      "training: 35 batch 320 batch_loss: 0.17283940315246582\n",
      "training: 35 batch 321 batch_loss: 0.1765325665473938\n",
      "training: 35 batch 322 batch_loss: 0.17064738273620605\n",
      "training: 35 batch 323 batch_loss: 0.17344409227371216\n",
      "training: 35 batch 324 batch_loss: 0.1672462821006775\n",
      "training: 35 batch 325 batch_loss: 0.17178550362586975\n",
      "training: 35 batch 326 batch_loss: 0.17191082239151\n",
      "training: 35 batch 327 batch_loss: 0.17021101713180542\n",
      "training: 35 batch 328 batch_loss: 0.17044740915298462\n",
      "training: 35 batch 329 batch_loss: 0.17141196131706238\n",
      "training: 35 batch 330 batch_loss: 0.17018699645996094\n",
      "training: 35 batch 331 batch_loss: 0.17643240094184875\n",
      "training: 35 batch 332 batch_loss: 0.1667686402797699\n",
      "training: 35 batch 333 batch_loss: 0.17016783356666565\n",
      "training: 35 batch 334 batch_loss: 0.1717095971107483\n",
      "training: 35 batch 335 batch_loss: 0.17350134253501892\n",
      "training: 35 batch 336 batch_loss: 0.17071202397346497\n",
      "training: 35 batch 337 batch_loss: 0.17693358659744263\n",
      "training: 35 batch 338 batch_loss: 0.17371779680252075\n",
      "training: 35 batch 339 batch_loss: 0.17388096451759338\n",
      "training: 35 batch 340 batch_loss: 0.17123624682426453\n",
      "training: 35 batch 341 batch_loss: 0.17050957679748535\n",
      "training: 35 batch 342 batch_loss: 0.1743505597114563\n",
      "training: 35 batch 343 batch_loss: 0.17387831211090088\n",
      "training: 35 batch 344 batch_loss: 0.17128726840019226\n",
      "training: 35 batch 345 batch_loss: 0.1735800802707672\n",
      "training: 35 batch 346 batch_loss: 0.17067456245422363\n",
      "training: 35 batch 347 batch_loss: 0.1701381504535675\n",
      "training: 35 batch 348 batch_loss: 0.17154031991958618\n",
      "training: 35 batch 349 batch_loss: 0.17379945516586304\n",
      "training: 35 batch 350 batch_loss: 0.1656365692615509\n",
      "training: 35 batch 351 batch_loss: 0.17162930965423584\n",
      "training: 35 batch 352 batch_loss: 0.1738426387310028\n",
      "training: 35 batch 353 batch_loss: 0.17415592074394226\n",
      "training: 35 batch 354 batch_loss: 0.17186707258224487\n",
      "training: 35 batch 355 batch_loss: 0.1722927689552307\n",
      "training: 35 batch 356 batch_loss: 0.172503262758255\n",
      "training: 35 batch 357 batch_loss: 0.1753799021244049\n",
      "training: 35 batch 358 batch_loss: 0.17020058631896973\n",
      "training: 35 batch 359 batch_loss: 0.17379239201545715\n",
      "training: 35 batch 360 batch_loss: 0.1726958453655243\n",
      "training: 35 batch 361 batch_loss: 0.17104071378707886\n",
      "training: 35 batch 362 batch_loss: 0.17001456022262573\n",
      "training: 35 batch 363 batch_loss: 0.16902950406074524\n",
      "training: 35 batch 364 batch_loss: 0.17185235023498535\n",
      "training: 35 batch 365 batch_loss: 0.17088833451271057\n",
      "training: 35 batch 366 batch_loss: 0.17006954550743103\n",
      "training: 35 batch 367 batch_loss: 0.17268505692481995\n",
      "training: 35 batch 368 batch_loss: 0.17417076230049133\n",
      "training: 35 batch 369 batch_loss: 0.17098426818847656\n",
      "training: 35 batch 370 batch_loss: 0.1736389398574829\n",
      "training: 35 batch 371 batch_loss: 0.1693815290927887\n",
      "training: 35 batch 372 batch_loss: 0.17482370138168335\n",
      "training: 35 batch 373 batch_loss: 0.17157608270645142\n",
      "training: 35 batch 374 batch_loss: 0.1709490716457367\n",
      "training: 35 batch 375 batch_loss: 0.17042765021324158\n",
      "training: 35 batch 376 batch_loss: 0.1753954291343689\n",
      "training: 35 batch 377 batch_loss: 0.17231082916259766\n",
      "training: 35 batch 378 batch_loss: 0.1662784218788147\n",
      "training: 35 batch 379 batch_loss: 0.17021441459655762\n",
      "training: 35 batch 380 batch_loss: 0.16857552528381348\n",
      "training: 35 batch 381 batch_loss: 0.1704140603542328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 35 batch 382 batch_loss: 0.17356786131858826\n",
      "training: 35 batch 383 batch_loss: 0.1712794303894043\n",
      "training: 35 batch 384 batch_loss: 0.16865798830986023\n",
      "training: 35 batch 385 batch_loss: 0.17359328269958496\n",
      "training: 35 batch 386 batch_loss: 0.17330914735794067\n",
      "training: 35 batch 387 batch_loss: 0.17262643575668335\n",
      "training: 35 batch 388 batch_loss: 0.17526650428771973\n",
      "training: 35 batch 389 batch_loss: 0.17303767800331116\n",
      "training: 35 batch 390 batch_loss: 0.17481830716133118\n",
      "training: 35 batch 391 batch_loss: 0.17302966117858887\n",
      "training: 35 batch 392 batch_loss: 0.1710735261440277\n",
      "training: 35 batch 393 batch_loss: 0.17380082607269287\n",
      "training: 35 batch 394 batch_loss: 0.16967922449111938\n",
      "training: 35 batch 395 batch_loss: 0.17239165306091309\n",
      "training: 35 batch 396 batch_loss: 0.1757557988166809\n",
      "training: 35 batch 397 batch_loss: 0.1670590341091156\n",
      "training: 35 batch 398 batch_loss: 0.17000988125801086\n",
      "training: 35 batch 399 batch_loss: 0.1722000241279602\n",
      "training: 35 batch 400 batch_loss: 0.17576122283935547\n",
      "training: 35 batch 401 batch_loss: 0.1741313934326172\n",
      "training: 35 batch 402 batch_loss: 0.17140048742294312\n",
      "training: 35 batch 403 batch_loss: 0.17279285192489624\n",
      "training: 35 batch 404 batch_loss: 0.16987541317939758\n",
      "training: 35 batch 405 batch_loss: 0.17157378792762756\n",
      "training: 35 batch 406 batch_loss: 0.17228129506111145\n",
      "training: 35 batch 407 batch_loss: 0.17220419645309448\n",
      "training: 35 batch 408 batch_loss: 0.17219990491867065\n",
      "training: 35 batch 409 batch_loss: 0.171023428440094\n",
      "training: 35 batch 410 batch_loss: 0.1753375232219696\n",
      "training: 35 batch 411 batch_loss: 0.1712443232536316\n",
      "training: 35 batch 412 batch_loss: 0.16989710927009583\n",
      "training: 35 batch 413 batch_loss: 0.17132258415222168\n",
      "training: 35 batch 414 batch_loss: 0.17592844367027283\n",
      "training: 35 batch 415 batch_loss: 0.17359405755996704\n",
      "training: 35 batch 416 batch_loss: 0.17342588305473328\n",
      "training: 35 batch 417 batch_loss: 0.1708245873451233\n",
      "training: 35 batch 418 batch_loss: 0.17143326997756958\n",
      "training: 35 batch 419 batch_loss: 0.171085387468338\n",
      "training: 35 batch 420 batch_loss: 0.17154502868652344\n",
      "training: 35 batch 421 batch_loss: 0.1683446764945984\n",
      "training: 35 batch 422 batch_loss: 0.17125219106674194\n",
      "training: 35 batch 423 batch_loss: 0.16848427057266235\n",
      "training: 35 batch 424 batch_loss: 0.17431217432022095\n",
      "training: 35 batch 425 batch_loss: 0.17281699180603027\n",
      "training: 35 batch 426 batch_loss: 0.17201024293899536\n",
      "training: 35 batch 427 batch_loss: 0.17344772815704346\n",
      "training: 35 batch 428 batch_loss: 0.17266115546226501\n",
      "training: 35 batch 429 batch_loss: 0.1747632920742035\n",
      "training: 35 batch 430 batch_loss: 0.17125600576400757\n",
      "training: 35 batch 431 batch_loss: 0.17279568314552307\n",
      "training: 35 batch 432 batch_loss: 0.171066015958786\n",
      "training: 35 batch 433 batch_loss: 0.17153960466384888\n",
      "training: 35 batch 434 batch_loss: 0.17230060696601868\n",
      "training: 35 batch 435 batch_loss: 0.17194536328315735\n",
      "training: 35 batch 436 batch_loss: 0.17222285270690918\n",
      "training: 35 batch 437 batch_loss: 0.17473319172859192\n",
      "training: 35 batch 438 batch_loss: 0.1746940016746521\n",
      "training: 35 batch 439 batch_loss: 0.17379337549209595\n",
      "training: 35 batch 440 batch_loss: 0.17497795820236206\n",
      "training: 35 batch 441 batch_loss: 0.17242568731307983\n",
      "training: 35 batch 442 batch_loss: 0.17255336046218872\n",
      "training: 35 batch 443 batch_loss: 0.17224550247192383\n",
      "training: 35 batch 444 batch_loss: 0.17196977138519287\n",
      "training: 35 batch 445 batch_loss: 0.1734216809272766\n",
      "training: 35 batch 446 batch_loss: 0.17056775093078613\n",
      "training: 35 batch 447 batch_loss: 0.16991591453552246\n",
      "training: 35 batch 448 batch_loss: 0.173031747341156\n",
      "training: 35 batch 449 batch_loss: 0.17421716451644897\n",
      "training: 35 batch 450 batch_loss: 0.1752631962299347\n",
      "training: 35 batch 451 batch_loss: 0.17450839281082153\n",
      "training: 35 batch 452 batch_loss: 0.16787013411521912\n",
      "training: 35 batch 453 batch_loss: 0.17141875624656677\n",
      "training: 35 batch 454 batch_loss: 0.17727577686309814\n",
      "training: 35 batch 455 batch_loss: 0.16989290714263916\n",
      "training: 35 batch 456 batch_loss: 0.17235177755355835\n",
      "training: 35 batch 457 batch_loss: 0.17014062404632568\n",
      "training: 35 batch 458 batch_loss: 0.17249345779418945\n",
      "training: 35 batch 459 batch_loss: 0.17129793763160706\n",
      "training: 35 batch 460 batch_loss: 0.16901841759681702\n",
      "training: 35 batch 461 batch_loss: 0.171156108379364\n",
      "training: 35 batch 462 batch_loss: 0.17144203186035156\n",
      "training: 35 batch 463 batch_loss: 0.16938677430152893\n",
      "training: 35 batch 464 batch_loss: 0.17297890782356262\n",
      "training: 35 batch 465 batch_loss: 0.17085498571395874\n",
      "training: 35 batch 466 batch_loss: 0.17348602414131165\n",
      "training: 35 batch 467 batch_loss: 0.17777976393699646\n",
      "training: 35 batch 468 batch_loss: 0.17486998438835144\n",
      "training: 35 batch 469 batch_loss: 0.1738937497138977\n",
      "training: 35 batch 470 batch_loss: 0.1723228096961975\n",
      "training: 35 batch 471 batch_loss: 0.17267334461212158\n",
      "training: 35 batch 472 batch_loss: 0.17353549599647522\n",
      "training: 35 batch 473 batch_loss: 0.17116901278495789\n",
      "training: 35 batch 474 batch_loss: 0.17091861367225647\n",
      "training: 35 batch 475 batch_loss: 0.17212173342704773\n",
      "training: 35 batch 476 batch_loss: 0.1715090274810791\n",
      "training: 35 batch 477 batch_loss: 0.1705411970615387\n",
      "training: 35 batch 478 batch_loss: 0.17482173442840576\n",
      "training: 35 batch 479 batch_loss: 0.17679178714752197\n",
      "training: 35 batch 480 batch_loss: 0.16863614320755005\n",
      "training: 35 batch 481 batch_loss: 0.16713804006576538\n",
      "training: 35 batch 482 batch_loss: 0.1758301854133606\n",
      "training: 35 batch 483 batch_loss: 0.17080283164978027\n",
      "training: 35 batch 484 batch_loss: 0.16945278644561768\n",
      "training: 35 batch 485 batch_loss: 0.17323952913284302\n",
      "training: 35 batch 486 batch_loss: 0.16797366738319397\n",
      "training: 35 batch 487 batch_loss: 0.17229041457176208\n",
      "training: 35 batch 488 batch_loss: 0.17382743954658508\n",
      "training: 35 batch 489 batch_loss: 0.16902422904968262\n",
      "training: 35 batch 490 batch_loss: 0.17558574676513672\n",
      "training: 35 batch 491 batch_loss: 0.17011988162994385\n",
      "training: 35 batch 492 batch_loss: 0.17464286088943481\n",
      "training: 35 batch 493 batch_loss: 0.17094522714614868\n",
      "training: 35 batch 494 batch_loss: 0.17071720957756042\n",
      "training: 35 batch 495 batch_loss: 0.1738586127758026\n",
      "training: 35 batch 496 batch_loss: 0.17185160517692566\n",
      "training: 35 batch 497 batch_loss: 0.1749495565891266\n",
      "training: 35 batch 498 batch_loss: 0.1751410961151123\n",
      "training: 35 batch 499 batch_loss: 0.1731799840927124\n",
      "training: 35 batch 500 batch_loss: 0.16835489869117737\n",
      "training: 35 batch 501 batch_loss: 0.16912579536437988\n",
      "training: 35 batch 502 batch_loss: 0.17425203323364258\n",
      "training: 35 batch 503 batch_loss: 0.17216262221336365\n",
      "training: 35 batch 504 batch_loss: 0.17473432421684265\n",
      "training: 35 batch 505 batch_loss: 0.17394578456878662\n",
      "training: 35 batch 506 batch_loss: 0.17170512676239014\n",
      "training: 35 batch 507 batch_loss: 0.17187127470970154\n",
      "training: 35 batch 508 batch_loss: 0.1712503433227539\n",
      "training: 35 batch 509 batch_loss: 0.17358839511871338\n",
      "training: 35 batch 510 batch_loss: 0.17170783877372742\n",
      "training: 35 batch 511 batch_loss: 0.17437517642974854\n",
      "training: 35 batch 512 batch_loss: 0.17292165756225586\n",
      "training: 35 batch 513 batch_loss: 0.17287510633468628\n",
      "training: 35 batch 514 batch_loss: 0.17392590641975403\n",
      "training: 35 batch 515 batch_loss: 0.1738603413105011\n",
      "training: 35 batch 516 batch_loss: 0.1712358593940735\n",
      "training: 35 batch 517 batch_loss: 0.17351171374320984\n",
      "training: 35 batch 518 batch_loss: 0.17539063096046448\n",
      "training: 35 batch 519 batch_loss: 0.17563685774803162\n",
      "training: 35 batch 520 batch_loss: 0.17182466387748718\n",
      "training: 35 batch 521 batch_loss: 0.17399951815605164\n",
      "training: 35 batch 522 batch_loss: 0.17302170395851135\n",
      "training: 35 batch 523 batch_loss: 0.17017725110054016\n",
      "training: 35 batch 524 batch_loss: 0.1699834167957306\n",
      "training: 35 batch 525 batch_loss: 0.16859027743339539\n",
      "training: 35 batch 526 batch_loss: 0.17235207557678223\n",
      "training: 35 batch 527 batch_loss: 0.16901367902755737\n",
      "training: 35 batch 528 batch_loss: 0.1710127890110016\n",
      "training: 35 batch 529 batch_loss: 0.17154717445373535\n",
      "training: 35 batch 530 batch_loss: 0.17382264137268066\n",
      "training: 35 batch 531 batch_loss: 0.17449334263801575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 35 batch 532 batch_loss: 0.17356842756271362\n",
      "training: 35 batch 533 batch_loss: 0.17326247692108154\n",
      "training: 35 batch 534 batch_loss: 0.16979721188545227\n",
      "training: 35 batch 535 batch_loss: 0.16951221227645874\n",
      "training: 35 batch 536 batch_loss: 0.1738455891609192\n",
      "training: 35 batch 537 batch_loss: 0.17193546891212463\n",
      "training: 35 batch 538 batch_loss: 0.17326930165290833\n",
      "training: 35 batch 539 batch_loss: 0.16701829433441162\n",
      "training: 35 batch 540 batch_loss: 0.17550653219223022\n",
      "training: 35 batch 541 batch_loss: 0.17316704988479614\n",
      "training: 35 batch 542 batch_loss: 0.17259791493415833\n",
      "training: 35 batch 543 batch_loss: 0.1727352738380432\n",
      "training: 35 batch 544 batch_loss: 0.1725481152534485\n",
      "training: 35 batch 545 batch_loss: 0.16874176263809204\n",
      "training: 35 batch 546 batch_loss: 0.17483127117156982\n",
      "training: 35 batch 547 batch_loss: 0.1722002625465393\n",
      "training: 35 batch 548 batch_loss: 0.1748875081539154\n",
      "training: 35 batch 549 batch_loss: 0.1700940728187561\n",
      "training: 35 batch 550 batch_loss: 0.17210152745246887\n",
      "training: 35 batch 551 batch_loss: 0.1774289608001709\n",
      "training: 35 batch 552 batch_loss: 0.17389419674873352\n",
      "training: 35 batch 553 batch_loss: 0.1730092167854309\n",
      "training: 35 batch 554 batch_loss: 0.17277133464813232\n",
      "training: 35 batch 555 batch_loss: 0.17187052965164185\n",
      "training: 35 batch 556 batch_loss: 0.1688755750656128\n",
      "training: 35 batch 557 batch_loss: 0.17722371220588684\n",
      "training: 35 batch 558 batch_loss: 0.17205053567886353\n",
      "training: 35 batch 559 batch_loss: 0.17253345251083374\n",
      "training: 35 batch 560 batch_loss: 0.1710861325263977\n",
      "training: 35 batch 561 batch_loss: 0.1729639172554016\n",
      "training: 35 batch 562 batch_loss: 0.17481479048728943\n",
      "training: 35 batch 563 batch_loss: 0.1756514310836792\n",
      "training: 35 batch 564 batch_loss: 0.17327824234962463\n",
      "training: 35 batch 565 batch_loss: 0.17272904515266418\n",
      "training: 35 batch 566 batch_loss: 0.1738414168357849\n",
      "training: 35 batch 567 batch_loss: 0.17206794023513794\n",
      "training: 35 batch 568 batch_loss: 0.17713913321495056\n",
      "training: 35 batch 569 batch_loss: 0.17344751954078674\n",
      "training: 35 batch 570 batch_loss: 0.1723366379737854\n",
      "training: 35 batch 571 batch_loss: 0.17403370141983032\n",
      "training: 35 batch 572 batch_loss: 0.17126354575157166\n",
      "training: 35 batch 573 batch_loss: 0.17186403274536133\n",
      "training: 35 batch 574 batch_loss: 0.17097976803779602\n",
      "training: 35 batch 575 batch_loss: 0.17307764291763306\n",
      "training: 35 batch 576 batch_loss: 0.1738673448562622\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 35, Hit Ratio:0.030973407108093792 | Precision:0.0456994003735378 | Recall:0.06120792632680579 | NDCG:0.058809171150581464\n",
      "*Best Performance* \n",
      "Epoch: 35, Hit Ratio:0.030973407108093792 | Precision:0.0456994003735378 | Recall:0.06120792632680579 | MDCG:0.058809171150581464\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 36 batch 0 batch_loss: 0.17303785681724548\n",
      "training: 36 batch 1 batch_loss: 0.16922226548194885\n",
      "training: 36 batch 2 batch_loss: 0.17083105444908142\n",
      "training: 36 batch 3 batch_loss: 0.17095720767974854\n",
      "training: 36 batch 4 batch_loss: 0.1684940755367279\n",
      "training: 36 batch 5 batch_loss: 0.17372190952301025\n",
      "training: 36 batch 6 batch_loss: 0.16670730710029602\n",
      "training: 36 batch 7 batch_loss: 0.17227596044540405\n",
      "training: 36 batch 8 batch_loss: 0.16795238852500916\n",
      "training: 36 batch 9 batch_loss: 0.16895797848701477\n",
      "training: 36 batch 10 batch_loss: 0.1724909245967865\n",
      "training: 36 batch 11 batch_loss: 0.17013081908226013\n",
      "training: 36 batch 12 batch_loss: 0.17431408166885376\n",
      "training: 36 batch 13 batch_loss: 0.17243242263793945\n",
      "training: 36 batch 14 batch_loss: 0.17191961407661438\n",
      "training: 36 batch 15 batch_loss: 0.16972070932388306\n",
      "training: 36 batch 16 batch_loss: 0.16665184497833252\n",
      "training: 36 batch 17 batch_loss: 0.17032134532928467\n",
      "training: 36 batch 18 batch_loss: 0.1712823510169983\n",
      "training: 36 batch 19 batch_loss: 0.17550408840179443\n",
      "training: 36 batch 20 batch_loss: 0.16807502508163452\n",
      "training: 36 batch 21 batch_loss: 0.1723341941833496\n",
      "training: 36 batch 22 batch_loss: 0.16875022649765015\n",
      "training: 36 batch 23 batch_loss: 0.1707162857055664\n",
      "training: 36 batch 24 batch_loss: 0.17088115215301514\n",
      "training: 36 batch 25 batch_loss: 0.17364639043807983\n",
      "training: 36 batch 26 batch_loss: 0.17160385847091675\n",
      "training: 36 batch 27 batch_loss: 0.17293459177017212\n",
      "training: 36 batch 28 batch_loss: 0.17709368467330933\n",
      "training: 36 batch 29 batch_loss: 0.16989529132843018\n",
      "training: 36 batch 30 batch_loss: 0.17072352766990662\n",
      "training: 36 batch 31 batch_loss: 0.17218360304832458\n",
      "training: 36 batch 32 batch_loss: 0.17178001999855042\n",
      "training: 36 batch 33 batch_loss: 0.17141568660736084\n",
      "training: 36 batch 34 batch_loss: 0.17109692096710205\n",
      "training: 36 batch 35 batch_loss: 0.17037785053253174\n",
      "training: 36 batch 36 batch_loss: 0.16799849271774292\n",
      "training: 36 batch 37 batch_loss: 0.17277002334594727\n",
      "training: 36 batch 38 batch_loss: 0.17039528489112854\n",
      "training: 36 batch 39 batch_loss: 0.16914665699005127\n",
      "training: 36 batch 40 batch_loss: 0.17160725593566895\n",
      "training: 36 batch 41 batch_loss: 0.17084509134292603\n",
      "training: 36 batch 42 batch_loss: 0.173126220703125\n",
      "training: 36 batch 43 batch_loss: 0.17172393202781677\n",
      "training: 36 batch 44 batch_loss: 0.17370310425758362\n",
      "training: 36 batch 45 batch_loss: 0.17424792051315308\n",
      "training: 36 batch 46 batch_loss: 0.17165452241897583\n",
      "training: 36 batch 47 batch_loss: 0.17346513271331787\n",
      "training: 36 batch 48 batch_loss: 0.1719667911529541\n",
      "training: 36 batch 49 batch_loss: 0.17218023538589478\n",
      "training: 36 batch 50 batch_loss: 0.16907894611358643\n",
      "training: 36 batch 51 batch_loss: 0.1684010624885559\n",
      "training: 36 batch 52 batch_loss: 0.17140904068946838\n",
      "training: 36 batch 53 batch_loss: 0.1715954840183258\n",
      "training: 36 batch 54 batch_loss: 0.16793394088745117\n",
      "training: 36 batch 55 batch_loss: 0.17306843400001526\n",
      "training: 36 batch 56 batch_loss: 0.17029258608818054\n",
      "training: 36 batch 57 batch_loss: 0.172420471906662\n",
      "training: 36 batch 58 batch_loss: 0.17131727933883667\n",
      "training: 36 batch 59 batch_loss: 0.17092695832252502\n",
      "training: 36 batch 60 batch_loss: 0.17378205060958862\n",
      "training: 36 batch 61 batch_loss: 0.17314428091049194\n",
      "training: 36 batch 62 batch_loss: 0.17091429233551025\n",
      "training: 36 batch 63 batch_loss: 0.1721743941307068\n",
      "training: 36 batch 64 batch_loss: 0.1666058897972107\n",
      "training: 36 batch 65 batch_loss: 0.17587804794311523\n",
      "training: 36 batch 66 batch_loss: 0.17229190468788147\n",
      "training: 36 batch 67 batch_loss: 0.17609375715255737\n",
      "training: 36 batch 68 batch_loss: 0.17337286472320557\n",
      "training: 36 batch 69 batch_loss: 0.17260605096817017\n",
      "training: 36 batch 70 batch_loss: 0.1697562336921692\n",
      "training: 36 batch 71 batch_loss: 0.16745054721832275\n",
      "training: 36 batch 72 batch_loss: 0.17091259360313416\n",
      "training: 36 batch 73 batch_loss: 0.17281681299209595\n",
      "training: 36 batch 74 batch_loss: 0.17313510179519653\n",
      "training: 36 batch 75 batch_loss: 0.17339399456977844\n",
      "training: 36 batch 76 batch_loss: 0.17146259546279907\n",
      "training: 36 batch 77 batch_loss: 0.17114770412445068\n",
      "training: 36 batch 78 batch_loss: 0.17350450158119202\n",
      "training: 36 batch 79 batch_loss: 0.17358967661857605\n",
      "training: 36 batch 80 batch_loss: 0.169880211353302\n",
      "training: 36 batch 81 batch_loss: 0.1739806830883026\n",
      "training: 36 batch 82 batch_loss: 0.17239075899124146\n",
      "training: 36 batch 83 batch_loss: 0.1726570725440979\n",
      "training: 36 batch 84 batch_loss: 0.17379340529441833\n",
      "training: 36 batch 85 batch_loss: 0.16798976063728333\n",
      "training: 36 batch 86 batch_loss: 0.173791766166687\n",
      "training: 36 batch 87 batch_loss: 0.1739673614501953\n",
      "training: 36 batch 88 batch_loss: 0.1737825870513916\n",
      "training: 36 batch 89 batch_loss: 0.1742577850818634\n",
      "training: 36 batch 90 batch_loss: 0.16796547174453735\n",
      "training: 36 batch 91 batch_loss: 0.1729303002357483\n",
      "training: 36 batch 92 batch_loss: 0.17184564471244812\n",
      "training: 36 batch 93 batch_loss: 0.1684306561946869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 batch 94 batch_loss: 0.17338049411773682\n",
      "training: 36 batch 95 batch_loss: 0.16978579759597778\n",
      "training: 36 batch 96 batch_loss: 0.17412814497947693\n",
      "training: 36 batch 97 batch_loss: 0.17196452617645264\n",
      "training: 36 batch 98 batch_loss: 0.17277291417121887\n",
      "training: 36 batch 99 batch_loss: 0.17145466804504395\n",
      "training: 36 batch 100 batch_loss: 0.1738269329071045\n",
      "training: 36 batch 101 batch_loss: 0.17253318428993225\n",
      "training: 36 batch 102 batch_loss: 0.17501598596572876\n",
      "training: 36 batch 103 batch_loss: 0.1730937659740448\n",
      "training: 36 batch 104 batch_loss: 0.17200839519500732\n",
      "training: 36 batch 105 batch_loss: 0.1752018928527832\n",
      "training: 36 batch 106 batch_loss: 0.17056450247764587\n",
      "training: 36 batch 107 batch_loss: 0.17196229100227356\n",
      "training: 36 batch 108 batch_loss: 0.17113935947418213\n",
      "training: 36 batch 109 batch_loss: 0.174099862575531\n",
      "training: 36 batch 110 batch_loss: 0.17335304617881775\n",
      "training: 36 batch 111 batch_loss: 0.17125338315963745\n",
      "training: 36 batch 112 batch_loss: 0.1733771562576294\n",
      "training: 36 batch 113 batch_loss: 0.17194366455078125\n",
      "training: 36 batch 114 batch_loss: 0.17675244808197021\n",
      "training: 36 batch 115 batch_loss: 0.17109227180480957\n",
      "training: 36 batch 116 batch_loss: 0.17130053043365479\n",
      "training: 36 batch 117 batch_loss: 0.17004474997520447\n",
      "training: 36 batch 118 batch_loss: 0.17004534602165222\n",
      "training: 36 batch 119 batch_loss: 0.17509186267852783\n",
      "training: 36 batch 120 batch_loss: 0.17299488186836243\n",
      "training: 36 batch 121 batch_loss: 0.17445045709609985\n",
      "training: 36 batch 122 batch_loss: 0.1703614592552185\n",
      "training: 36 batch 123 batch_loss: 0.17276251316070557\n",
      "training: 36 batch 124 batch_loss: 0.17309680581092834\n",
      "training: 36 batch 125 batch_loss: 0.16990965604782104\n",
      "training: 36 batch 126 batch_loss: 0.16937854886054993\n",
      "training: 36 batch 127 batch_loss: 0.17396172881126404\n",
      "training: 36 batch 128 batch_loss: 0.17421665787696838\n",
      "training: 36 batch 129 batch_loss: 0.1752704381942749\n",
      "training: 36 batch 130 batch_loss: 0.17321902513504028\n",
      "training: 36 batch 131 batch_loss: 0.17046454548835754\n",
      "training: 36 batch 132 batch_loss: 0.17079228162765503\n",
      "training: 36 batch 133 batch_loss: 0.17398247122764587\n",
      "training: 36 batch 134 batch_loss: 0.1715707778930664\n",
      "training: 36 batch 135 batch_loss: 0.174925297498703\n",
      "training: 36 batch 136 batch_loss: 0.1718910038471222\n",
      "training: 36 batch 137 batch_loss: 0.17484569549560547\n",
      "training: 36 batch 138 batch_loss: 0.17411094903945923\n",
      "training: 36 batch 139 batch_loss: 0.17115157842636108\n",
      "training: 36 batch 140 batch_loss: 0.1715974509716034\n",
      "training: 36 batch 141 batch_loss: 0.17426469922065735\n",
      "training: 36 batch 142 batch_loss: 0.17033368349075317\n",
      "training: 36 batch 143 batch_loss: 0.17232796549797058\n",
      "training: 36 batch 144 batch_loss: 0.16702789068222046\n",
      "training: 36 batch 145 batch_loss: 0.1711762547492981\n",
      "training: 36 batch 146 batch_loss: 0.1748809814453125\n",
      "training: 36 batch 147 batch_loss: 0.17041686177253723\n",
      "training: 36 batch 148 batch_loss: 0.16986370086669922\n",
      "training: 36 batch 149 batch_loss: 0.17336970567703247\n",
      "training: 36 batch 150 batch_loss: 0.17019301652908325\n",
      "training: 36 batch 151 batch_loss: 0.1718071699142456\n",
      "training: 36 batch 152 batch_loss: 0.17589527368545532\n",
      "training: 36 batch 153 batch_loss: 0.17065370082855225\n",
      "training: 36 batch 154 batch_loss: 0.17123645544052124\n",
      "training: 36 batch 155 batch_loss: 0.16633495688438416\n",
      "training: 36 batch 156 batch_loss: 0.17230263352394104\n",
      "training: 36 batch 157 batch_loss: 0.17169541120529175\n",
      "training: 36 batch 158 batch_loss: 0.1758362054824829\n",
      "training: 36 batch 159 batch_loss: 0.17252901196479797\n",
      "training: 36 batch 160 batch_loss: 0.17307138442993164\n",
      "training: 36 batch 161 batch_loss: 0.17175525426864624\n",
      "training: 36 batch 162 batch_loss: 0.17317309975624084\n",
      "training: 36 batch 163 batch_loss: 0.17280226945877075\n",
      "training: 36 batch 164 batch_loss: 0.1707514524459839\n",
      "training: 36 batch 165 batch_loss: 0.1739671230316162\n",
      "training: 36 batch 166 batch_loss: 0.17528319358825684\n",
      "training: 36 batch 167 batch_loss: 0.1737366020679474\n",
      "training: 36 batch 168 batch_loss: 0.17343470454216003\n",
      "training: 36 batch 169 batch_loss: 0.17251944541931152\n",
      "training: 36 batch 170 batch_loss: 0.17448192834854126\n",
      "training: 36 batch 171 batch_loss: 0.1721736490726471\n",
      "training: 36 batch 172 batch_loss: 0.17023375630378723\n",
      "training: 36 batch 173 batch_loss: 0.1756708025932312\n",
      "training: 36 batch 174 batch_loss: 0.17123043537139893\n",
      "training: 36 batch 175 batch_loss: 0.1705826222896576\n",
      "training: 36 batch 176 batch_loss: 0.17126840353012085\n",
      "training: 36 batch 177 batch_loss: 0.17334377765655518\n",
      "training: 36 batch 178 batch_loss: 0.17447197437286377\n",
      "training: 36 batch 179 batch_loss: 0.17130672931671143\n",
      "training: 36 batch 180 batch_loss: 0.16964352130889893\n",
      "training: 36 batch 181 batch_loss: 0.17297855019569397\n",
      "training: 36 batch 182 batch_loss: 0.1713225245475769\n",
      "training: 36 batch 183 batch_loss: 0.17035239934921265\n",
      "training: 36 batch 184 batch_loss: 0.16973870992660522\n",
      "training: 36 batch 185 batch_loss: 0.1689791977405548\n",
      "training: 36 batch 186 batch_loss: 0.17279472947120667\n",
      "training: 36 batch 187 batch_loss: 0.17303794622421265\n",
      "training: 36 batch 188 batch_loss: 0.16979193687438965\n",
      "training: 36 batch 189 batch_loss: 0.17192557454109192\n",
      "training: 36 batch 190 batch_loss: 0.17581191658973694\n",
      "training: 36 batch 191 batch_loss: 0.17147928476333618\n",
      "training: 36 batch 192 batch_loss: 0.17406970262527466\n",
      "training: 36 batch 193 batch_loss: 0.17067408561706543\n",
      "training: 36 batch 194 batch_loss: 0.171528160572052\n",
      "training: 36 batch 195 batch_loss: 0.1700330674648285\n",
      "training: 36 batch 196 batch_loss: 0.16953080892562866\n",
      "training: 36 batch 197 batch_loss: 0.17204630374908447\n",
      "training: 36 batch 198 batch_loss: 0.172176331281662\n",
      "training: 36 batch 199 batch_loss: 0.17134875059127808\n",
      "training: 36 batch 200 batch_loss: 0.17255941033363342\n",
      "training: 36 batch 201 batch_loss: 0.17106357216835022\n",
      "training: 36 batch 202 batch_loss: 0.16926530003547668\n",
      "training: 36 batch 203 batch_loss: 0.17326849699020386\n",
      "training: 36 batch 204 batch_loss: 0.1772306263446808\n",
      "training: 36 batch 205 batch_loss: 0.17697784304618835\n",
      "training: 36 batch 206 batch_loss: 0.17284151911735535\n",
      "training: 36 batch 207 batch_loss: 0.17109590768814087\n",
      "training: 36 batch 208 batch_loss: 0.17057424783706665\n",
      "training: 36 batch 209 batch_loss: 0.1759970784187317\n",
      "training: 36 batch 210 batch_loss: 0.17450976371765137\n",
      "training: 36 batch 211 batch_loss: 0.17720234394073486\n",
      "training: 36 batch 212 batch_loss: 0.16828924417495728\n",
      "training: 36 batch 213 batch_loss: 0.17142492532730103\n",
      "training: 36 batch 214 batch_loss: 0.17468151450157166\n",
      "training: 36 batch 215 batch_loss: 0.17540711164474487\n",
      "training: 36 batch 216 batch_loss: 0.17334085702896118\n",
      "training: 36 batch 217 batch_loss: 0.17231044173240662\n",
      "training: 36 batch 218 batch_loss: 0.17668503522872925\n",
      "training: 36 batch 219 batch_loss: 0.17028650641441345\n",
      "training: 36 batch 220 batch_loss: 0.17372912168502808\n",
      "training: 36 batch 221 batch_loss: 0.17341554164886475\n",
      "training: 36 batch 222 batch_loss: 0.17403057217597961\n",
      "training: 36 batch 223 batch_loss: 0.17354103922843933\n",
      "training: 36 batch 224 batch_loss: 0.17164915800094604\n",
      "training: 36 batch 225 batch_loss: 0.17532879114151\n",
      "training: 36 batch 226 batch_loss: 0.17157164216041565\n",
      "training: 36 batch 227 batch_loss: 0.17237168550491333\n",
      "training: 36 batch 228 batch_loss: 0.17146950960159302\n",
      "training: 36 batch 229 batch_loss: 0.17363613843917847\n",
      "training: 36 batch 230 batch_loss: 0.17493367195129395\n",
      "training: 36 batch 231 batch_loss: 0.17007464170455933\n",
      "training: 36 batch 232 batch_loss: 0.17247188091278076\n",
      "training: 36 batch 233 batch_loss: 0.16969043016433716\n",
      "training: 36 batch 234 batch_loss: 0.1696140468120575\n",
      "training: 36 batch 235 batch_loss: 0.17474761605262756\n",
      "training: 36 batch 236 batch_loss: 0.17321163415908813\n",
      "training: 36 batch 237 batch_loss: 0.17370697855949402\n",
      "training: 36 batch 238 batch_loss: 0.17290902137756348\n",
      "training: 36 batch 239 batch_loss: 0.17077061533927917\n",
      "training: 36 batch 240 batch_loss: 0.1712433397769928\n",
      "training: 36 batch 241 batch_loss: 0.17335817217826843\n",
      "training: 36 batch 242 batch_loss: 0.17222920060157776\n",
      "training: 36 batch 243 batch_loss: 0.17315346002578735\n",
      "training: 36 batch 244 batch_loss: 0.17123490571975708\n",
      "training: 36 batch 245 batch_loss: 0.16932028532028198\n",
      "training: 36 batch 246 batch_loss: 0.17349058389663696\n",
      "training: 36 batch 247 batch_loss: 0.173542320728302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 batch 248 batch_loss: 0.17333081364631653\n",
      "training: 36 batch 249 batch_loss: 0.17236709594726562\n",
      "training: 36 batch 250 batch_loss: 0.1685994267463684\n",
      "training: 36 batch 251 batch_loss: 0.17464983463287354\n",
      "training: 36 batch 252 batch_loss: 0.17486023902893066\n",
      "training: 36 batch 253 batch_loss: 0.16969138383865356\n",
      "training: 36 batch 254 batch_loss: 0.17796415090560913\n",
      "training: 36 batch 255 batch_loss: 0.17339366674423218\n",
      "training: 36 batch 256 batch_loss: 0.1737540364265442\n",
      "training: 36 batch 257 batch_loss: 0.17325127124786377\n",
      "training: 36 batch 258 batch_loss: 0.17101821303367615\n",
      "training: 36 batch 259 batch_loss: 0.17525413632392883\n",
      "training: 36 batch 260 batch_loss: 0.17342358827590942\n",
      "training: 36 batch 261 batch_loss: 0.16867998242378235\n",
      "training: 36 batch 262 batch_loss: 0.1764484941959381\n",
      "training: 36 batch 263 batch_loss: 0.17069575190544128\n",
      "training: 36 batch 264 batch_loss: 0.17662855982780457\n",
      "training: 36 batch 265 batch_loss: 0.1797468662261963\n",
      "training: 36 batch 266 batch_loss: 0.17128589749336243\n",
      "training: 36 batch 267 batch_loss: 0.1742914617061615\n",
      "training: 36 batch 268 batch_loss: 0.17148926854133606\n",
      "training: 36 batch 269 batch_loss: 0.17261353135108948\n",
      "training: 36 batch 270 batch_loss: 0.17265236377716064\n",
      "training: 36 batch 271 batch_loss: 0.1719588041305542\n",
      "training: 36 batch 272 batch_loss: 0.17403090000152588\n",
      "training: 36 batch 273 batch_loss: 0.17002803087234497\n",
      "training: 36 batch 274 batch_loss: 0.17381948232650757\n",
      "training: 36 batch 275 batch_loss: 0.17764616012573242\n",
      "training: 36 batch 276 batch_loss: 0.16838154196739197\n",
      "training: 36 batch 277 batch_loss: 0.17727938294410706\n",
      "training: 36 batch 278 batch_loss: 0.17584246397018433\n",
      "training: 36 batch 279 batch_loss: 0.17617973685264587\n",
      "training: 36 batch 280 batch_loss: 0.17437779903411865\n",
      "training: 36 batch 281 batch_loss: 0.1773105263710022\n",
      "training: 36 batch 282 batch_loss: 0.1732388734817505\n",
      "training: 36 batch 283 batch_loss: 0.17155903577804565\n",
      "training: 36 batch 284 batch_loss: 0.17801323533058167\n",
      "training: 36 batch 285 batch_loss: 0.1744333803653717\n",
      "training: 36 batch 286 batch_loss: 0.17150893807411194\n",
      "training: 36 batch 287 batch_loss: 0.16830259561538696\n",
      "training: 36 batch 288 batch_loss: 0.17287617921829224\n",
      "training: 36 batch 289 batch_loss: 0.17537131905555725\n",
      "training: 36 batch 290 batch_loss: 0.17427760362625122\n",
      "training: 36 batch 291 batch_loss: 0.16848784685134888\n",
      "training: 36 batch 292 batch_loss: 0.1755463182926178\n",
      "training: 36 batch 293 batch_loss: 0.17309021949768066\n",
      "training: 36 batch 294 batch_loss: 0.172715425491333\n",
      "training: 36 batch 295 batch_loss: 0.1700322926044464\n",
      "training: 36 batch 296 batch_loss: 0.17739629745483398\n",
      "training: 36 batch 297 batch_loss: 0.1694132685661316\n",
      "training: 36 batch 298 batch_loss: 0.17413607239723206\n",
      "training: 36 batch 299 batch_loss: 0.1698610484600067\n",
      "training: 36 batch 300 batch_loss: 0.17281261086463928\n",
      "training: 36 batch 301 batch_loss: 0.1731320321559906\n",
      "training: 36 batch 302 batch_loss: 0.17712807655334473\n",
      "training: 36 batch 303 batch_loss: 0.17186781764030457\n",
      "training: 36 batch 304 batch_loss: 0.1747509241104126\n",
      "training: 36 batch 305 batch_loss: 0.17231228947639465\n",
      "training: 36 batch 306 batch_loss: 0.17208030819892883\n",
      "training: 36 batch 307 batch_loss: 0.16907334327697754\n",
      "training: 36 batch 308 batch_loss: 0.17357796430587769\n",
      "training: 36 batch 309 batch_loss: 0.17423078417778015\n",
      "training: 36 batch 310 batch_loss: 0.170612633228302\n",
      "training: 36 batch 311 batch_loss: 0.16995146870613098\n",
      "training: 36 batch 312 batch_loss: 0.17482390999794006\n",
      "training: 36 batch 313 batch_loss: 0.17104437947273254\n",
      "training: 36 batch 314 batch_loss: 0.1789829134941101\n",
      "training: 36 batch 315 batch_loss: 0.17541640996932983\n",
      "training: 36 batch 316 batch_loss: 0.17048773169517517\n",
      "training: 36 batch 317 batch_loss: 0.17558056116104126\n",
      "training: 36 batch 318 batch_loss: 0.175190269947052\n",
      "training: 36 batch 319 batch_loss: 0.17284661531448364\n",
      "training: 36 batch 320 batch_loss: 0.1764734983444214\n",
      "training: 36 batch 321 batch_loss: 0.17249795794487\n",
      "training: 36 batch 322 batch_loss: 0.17174836993217468\n",
      "training: 36 batch 323 batch_loss: 0.17252320051193237\n",
      "training: 36 batch 324 batch_loss: 0.17613250017166138\n",
      "training: 36 batch 325 batch_loss: 0.17078548669815063\n",
      "training: 36 batch 326 batch_loss: 0.1729901134967804\n",
      "training: 36 batch 327 batch_loss: 0.17126944661140442\n",
      "training: 36 batch 328 batch_loss: 0.17517322301864624\n",
      "training: 36 batch 329 batch_loss: 0.17027220129966736\n",
      "training: 36 batch 330 batch_loss: 0.17476576566696167\n",
      "training: 36 batch 331 batch_loss: 0.17374977469444275\n",
      "training: 36 batch 332 batch_loss: 0.17965802550315857\n",
      "training: 36 batch 333 batch_loss: 0.17407777905464172\n",
      "training: 36 batch 334 batch_loss: 0.17349877953529358\n",
      "training: 36 batch 335 batch_loss: 0.17352622747421265\n",
      "training: 36 batch 336 batch_loss: 0.17242270708084106\n",
      "training: 36 batch 337 batch_loss: 0.17654994130134583\n",
      "training: 36 batch 338 batch_loss: 0.17403775453567505\n",
      "training: 36 batch 339 batch_loss: 0.1733972728252411\n",
      "training: 36 batch 340 batch_loss: 0.1717858910560608\n",
      "training: 36 batch 341 batch_loss: 0.1744237244129181\n",
      "training: 36 batch 342 batch_loss: 0.1717841625213623\n",
      "training: 36 batch 343 batch_loss: 0.17137256264686584\n",
      "training: 36 batch 344 batch_loss: 0.17442944645881653\n",
      "training: 36 batch 345 batch_loss: 0.17566430568695068\n",
      "training: 36 batch 346 batch_loss: 0.17280322313308716\n",
      "training: 36 batch 347 batch_loss: 0.1691475808620453\n",
      "training: 36 batch 348 batch_loss: 0.17155301570892334\n",
      "training: 36 batch 349 batch_loss: 0.1695459485054016\n",
      "training: 36 batch 350 batch_loss: 0.1740320324897766\n",
      "training: 36 batch 351 batch_loss: 0.1768946647644043\n",
      "training: 36 batch 352 batch_loss: 0.17135292291641235\n",
      "training: 36 batch 353 batch_loss: 0.17412632703781128\n",
      "training: 36 batch 354 batch_loss: 0.17488440871238708\n",
      "training: 36 batch 355 batch_loss: 0.1709524691104889\n",
      "training: 36 batch 356 batch_loss: 0.17418479919433594\n",
      "training: 36 batch 357 batch_loss: 0.17222771048545837\n",
      "training: 36 batch 358 batch_loss: 0.1732890009880066\n",
      "training: 36 batch 359 batch_loss: 0.1717403531074524\n",
      "training: 36 batch 360 batch_loss: 0.17332640290260315\n",
      "training: 36 batch 361 batch_loss: 0.17525193095207214\n",
      "training: 36 batch 362 batch_loss: 0.1767054796218872\n",
      "training: 36 batch 363 batch_loss: 0.1695711314678192\n",
      "training: 36 batch 364 batch_loss: 0.17149916291236877\n",
      "training: 36 batch 365 batch_loss: 0.17407968640327454\n",
      "training: 36 batch 366 batch_loss: 0.1750030219554901\n",
      "training: 36 batch 367 batch_loss: 0.17311596870422363\n",
      "training: 36 batch 368 batch_loss: 0.17626309394836426\n",
      "training: 36 batch 369 batch_loss: 0.17780059576034546\n",
      "training: 36 batch 370 batch_loss: 0.17486780881881714\n",
      "training: 36 batch 371 batch_loss: 0.17350518703460693\n",
      "training: 36 batch 372 batch_loss: 0.1744268536567688\n",
      "training: 36 batch 373 batch_loss: 0.17208126187324524\n",
      "training: 36 batch 374 batch_loss: 0.17535758018493652\n",
      "training: 36 batch 375 batch_loss: 0.1743530035018921\n",
      "training: 36 batch 376 batch_loss: 0.17292898893356323\n",
      "training: 36 batch 377 batch_loss: 0.17067429423332214\n",
      "training: 36 batch 378 batch_loss: 0.1707209050655365\n",
      "training: 36 batch 379 batch_loss: 0.17282834649085999\n",
      "training: 36 batch 380 batch_loss: 0.1740756630897522\n",
      "training: 36 batch 381 batch_loss: 0.17217311263084412\n",
      "training: 36 batch 382 batch_loss: 0.1748906373977661\n",
      "training: 36 batch 383 batch_loss: 0.17362841963768005\n",
      "training: 36 batch 384 batch_loss: 0.1699545979499817\n",
      "training: 36 batch 385 batch_loss: 0.1738739013671875\n",
      "training: 36 batch 386 batch_loss: 0.17468613386154175\n",
      "training: 36 batch 387 batch_loss: 0.17286568880081177\n",
      "training: 36 batch 388 batch_loss: 0.1730046272277832\n",
      "training: 36 batch 389 batch_loss: 0.17338153719902039\n",
      "training: 36 batch 390 batch_loss: 0.17463022470474243\n",
      "training: 36 batch 391 batch_loss: 0.17548149824142456\n",
      "training: 36 batch 392 batch_loss: 0.172775000333786\n",
      "training: 36 batch 393 batch_loss: 0.17382049560546875\n",
      "training: 36 batch 394 batch_loss: 0.17331629991531372\n",
      "training: 36 batch 395 batch_loss: 0.1738475263118744\n",
      "training: 36 batch 396 batch_loss: 0.17397785186767578\n",
      "training: 36 batch 397 batch_loss: 0.1772817075252533\n",
      "training: 36 batch 398 batch_loss: 0.17422020435333252\n",
      "training: 36 batch 399 batch_loss: 0.17616063356399536\n",
      "training: 36 batch 400 batch_loss: 0.17261961102485657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 batch 401 batch_loss: 0.17574918270111084\n",
      "training: 36 batch 402 batch_loss: 0.17626118659973145\n",
      "training: 36 batch 403 batch_loss: 0.17558404803276062\n",
      "training: 36 batch 404 batch_loss: 0.17324894666671753\n",
      "training: 36 batch 405 batch_loss: 0.17014646530151367\n",
      "training: 36 batch 406 batch_loss: 0.17219212651252747\n",
      "training: 36 batch 407 batch_loss: 0.17605406045913696\n",
      "training: 36 batch 408 batch_loss: 0.17076775431632996\n",
      "training: 36 batch 409 batch_loss: 0.174422025680542\n",
      "training: 36 batch 410 batch_loss: 0.17424902319908142\n",
      "training: 36 batch 411 batch_loss: 0.17300346493721008\n",
      "training: 36 batch 412 batch_loss: 0.1750045120716095\n",
      "training: 36 batch 413 batch_loss: 0.17673006653785706\n",
      "training: 36 batch 414 batch_loss: 0.17354658246040344\n",
      "training: 36 batch 415 batch_loss: 0.1767139434814453\n",
      "training: 36 batch 416 batch_loss: 0.17814844846725464\n",
      "training: 36 batch 417 batch_loss: 0.1747647523880005\n",
      "training: 36 batch 418 batch_loss: 0.1765175759792328\n",
      "training: 36 batch 419 batch_loss: 0.17358526587486267\n",
      "training: 36 batch 420 batch_loss: 0.17176073789596558\n",
      "training: 36 batch 421 batch_loss: 0.17574328184127808\n",
      "training: 36 batch 422 batch_loss: 0.1732272207736969\n",
      "training: 36 batch 423 batch_loss: 0.177954763174057\n",
      "training: 36 batch 424 batch_loss: 0.17244717478752136\n",
      "training: 36 batch 425 batch_loss: 0.17777559161186218\n",
      "training: 36 batch 426 batch_loss: 0.1760827898979187\n",
      "training: 36 batch 427 batch_loss: 0.17151662707328796\n",
      "training: 36 batch 428 batch_loss: 0.17357072234153748\n",
      "training: 36 batch 429 batch_loss: 0.17307370901107788\n",
      "training: 36 batch 430 batch_loss: 0.17406439781188965\n",
      "training: 36 batch 431 batch_loss: 0.17912998795509338\n",
      "training: 36 batch 432 batch_loss: 0.1769847869873047\n",
      "training: 36 batch 433 batch_loss: 0.17354273796081543\n",
      "training: 36 batch 434 batch_loss: 0.17103427648544312\n",
      "training: 36 batch 435 batch_loss: 0.1732024848461151\n",
      "training: 36 batch 436 batch_loss: 0.17555278539657593\n",
      "training: 36 batch 437 batch_loss: 0.17624393105506897\n",
      "training: 36 batch 438 batch_loss: 0.1730298399925232\n",
      "training: 36 batch 439 batch_loss: 0.17390963435173035\n",
      "training: 36 batch 440 batch_loss: 0.17628246545791626\n",
      "training: 36 batch 441 batch_loss: 0.17505109310150146\n",
      "training: 36 batch 442 batch_loss: 0.17397019267082214\n",
      "training: 36 batch 443 batch_loss: 0.17334192991256714\n",
      "training: 36 batch 444 batch_loss: 0.1724550426006317\n",
      "training: 36 batch 445 batch_loss: 0.17243415117263794\n",
      "training: 36 batch 446 batch_loss: 0.17526856064796448\n",
      "training: 36 batch 447 batch_loss: 0.17292091250419617\n",
      "training: 36 batch 448 batch_loss: 0.17301127314567566\n",
      "training: 36 batch 449 batch_loss: 0.1717793345451355\n",
      "training: 36 batch 450 batch_loss: 0.1725790798664093\n",
      "training: 36 batch 451 batch_loss: 0.17346644401550293\n",
      "training: 36 batch 452 batch_loss: 0.17519211769104004\n",
      "training: 36 batch 453 batch_loss: 0.1786966621875763\n",
      "training: 36 batch 454 batch_loss: 0.17300578951835632\n",
      "training: 36 batch 455 batch_loss: 0.17032313346862793\n",
      "training: 36 batch 456 batch_loss: 0.175914466381073\n",
      "training: 36 batch 457 batch_loss: 0.17039638757705688\n",
      "training: 36 batch 458 batch_loss: 0.17034244537353516\n",
      "training: 36 batch 459 batch_loss: 0.1766079068183899\n",
      "training: 36 batch 460 batch_loss: 0.1677270233631134\n",
      "training: 36 batch 461 batch_loss: 0.17192131280899048\n",
      "training: 36 batch 462 batch_loss: 0.17406883835792542\n",
      "training: 36 batch 463 batch_loss: 0.17496326565742493\n",
      "training: 36 batch 464 batch_loss: 0.177808940410614\n",
      "training: 36 batch 465 batch_loss: 0.17043164372444153\n",
      "training: 36 batch 466 batch_loss: 0.17456978559494019\n",
      "training: 36 batch 467 batch_loss: 0.17082494497299194\n",
      "training: 36 batch 468 batch_loss: 0.17792290449142456\n",
      "training: 36 batch 469 batch_loss: 0.17245903611183167\n",
      "training: 36 batch 470 batch_loss: 0.1751280426979065\n",
      "training: 36 batch 471 batch_loss: 0.17375785112380981\n",
      "training: 36 batch 472 batch_loss: 0.1734732687473297\n",
      "training: 36 batch 473 batch_loss: 0.17581579089164734\n",
      "training: 36 batch 474 batch_loss: 0.1741102635860443\n",
      "training: 36 batch 475 batch_loss: 0.17459642887115479\n",
      "training: 36 batch 476 batch_loss: 0.1750626266002655\n",
      "training: 36 batch 477 batch_loss: 0.17662128806114197\n",
      "training: 36 batch 478 batch_loss: 0.1731116771697998\n",
      "training: 36 batch 479 batch_loss: 0.17517554759979248\n",
      "training: 36 batch 480 batch_loss: 0.1723296046257019\n",
      "training: 36 batch 481 batch_loss: 0.17315080761909485\n",
      "training: 36 batch 482 batch_loss: 0.17168962955474854\n",
      "training: 36 batch 483 batch_loss: 0.1735263168811798\n",
      "training: 36 batch 484 batch_loss: 0.17400893568992615\n",
      "training: 36 batch 485 batch_loss: 0.1748417615890503\n",
      "training: 36 batch 486 batch_loss: 0.1767907440662384\n",
      "training: 36 batch 487 batch_loss: 0.17020869255065918\n",
      "training: 36 batch 488 batch_loss: 0.17247727513313293\n",
      "training: 36 batch 489 batch_loss: 0.17826709151268005\n",
      "training: 36 batch 490 batch_loss: 0.17183226346969604\n",
      "training: 36 batch 491 batch_loss: 0.17041128873825073\n",
      "training: 36 batch 492 batch_loss: 0.1729108989238739\n",
      "training: 36 batch 493 batch_loss: 0.1740550696849823\n",
      "training: 36 batch 494 batch_loss: 0.16900193691253662\n",
      "training: 36 batch 495 batch_loss: 0.1710498332977295\n",
      "training: 36 batch 496 batch_loss: 0.17264610528945923\n",
      "training: 36 batch 497 batch_loss: 0.17643752694129944\n",
      "training: 36 batch 498 batch_loss: 0.175603985786438\n",
      "training: 36 batch 499 batch_loss: 0.1807076632976532\n",
      "training: 36 batch 500 batch_loss: 0.17620429396629333\n",
      "training: 36 batch 501 batch_loss: 0.17607581615447998\n",
      "training: 36 batch 502 batch_loss: 0.17178812623023987\n",
      "training: 36 batch 503 batch_loss: 0.17283397912979126\n",
      "training: 36 batch 504 batch_loss: 0.17125433683395386\n",
      "training: 36 batch 505 batch_loss: 0.17361435294151306\n",
      "training: 36 batch 506 batch_loss: 0.177584707736969\n",
      "training: 36 batch 507 batch_loss: 0.17666539549827576\n",
      "training: 36 batch 508 batch_loss: 0.1728678047657013\n",
      "training: 36 batch 509 batch_loss: 0.1737501621246338\n",
      "training: 36 batch 510 batch_loss: 0.1731371283531189\n",
      "training: 36 batch 511 batch_loss: 0.16995707154273987\n",
      "training: 36 batch 512 batch_loss: 0.17026478052139282\n",
      "training: 36 batch 513 batch_loss: 0.177577942609787\n",
      "training: 36 batch 514 batch_loss: 0.1765846312046051\n",
      "training: 36 batch 515 batch_loss: 0.17625895142555237\n",
      "training: 36 batch 516 batch_loss: 0.1739618480205536\n",
      "training: 36 batch 517 batch_loss: 0.1738036870956421\n",
      "training: 36 batch 518 batch_loss: 0.17611292004585266\n",
      "training: 36 batch 519 batch_loss: 0.17417728900909424\n",
      "training: 36 batch 520 batch_loss: 0.17242595553398132\n",
      "training: 36 batch 521 batch_loss: 0.17689335346221924\n",
      "training: 36 batch 522 batch_loss: 0.17372551560401917\n",
      "training: 36 batch 523 batch_loss: 0.17624419927597046\n",
      "training: 36 batch 524 batch_loss: 0.17657142877578735\n",
      "training: 36 batch 525 batch_loss: 0.1777246594429016\n",
      "training: 36 batch 526 batch_loss: 0.1747189164161682\n",
      "training: 36 batch 527 batch_loss: 0.17481684684753418\n",
      "training: 36 batch 528 batch_loss: 0.1741984486579895\n",
      "training: 36 batch 529 batch_loss: 0.17599278688430786\n",
      "training: 36 batch 530 batch_loss: 0.17701825499534607\n",
      "training: 36 batch 531 batch_loss: 0.17577779293060303\n",
      "training: 36 batch 532 batch_loss: 0.17481645941734314\n",
      "training: 36 batch 533 batch_loss: 0.17343223094940186\n",
      "training: 36 batch 534 batch_loss: 0.17629724740982056\n",
      "training: 36 batch 535 batch_loss: 0.17386138439178467\n",
      "training: 36 batch 536 batch_loss: 0.17127984762191772\n",
      "training: 36 batch 537 batch_loss: 0.1732918620109558\n",
      "training: 36 batch 538 batch_loss: 0.16869759559631348\n",
      "training: 36 batch 539 batch_loss: 0.1740463376045227\n",
      "training: 36 batch 540 batch_loss: 0.17178082466125488\n",
      "training: 36 batch 541 batch_loss: 0.17459434270858765\n",
      "training: 36 batch 542 batch_loss: 0.17329618334770203\n",
      "training: 36 batch 543 batch_loss: 0.17668867111206055\n",
      "training: 36 batch 544 batch_loss: 0.17422649264335632\n",
      "training: 36 batch 545 batch_loss: 0.17566567659378052\n",
      "training: 36 batch 546 batch_loss: 0.1723405420780182\n",
      "training: 36 batch 547 batch_loss: 0.1774718165397644\n",
      "training: 36 batch 548 batch_loss: 0.17109352350234985\n",
      "training: 36 batch 549 batch_loss: 0.1716850996017456\n",
      "training: 36 batch 550 batch_loss: 0.17691129446029663\n",
      "training: 36 batch 551 batch_loss: 0.17291772365570068\n",
      "training: 36 batch 552 batch_loss: 0.17395955324172974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 36 batch 553 batch_loss: 0.1742476224899292\n",
      "training: 36 batch 554 batch_loss: 0.17450189590454102\n",
      "training: 36 batch 555 batch_loss: 0.17018240690231323\n",
      "training: 36 batch 556 batch_loss: 0.17498034238815308\n",
      "training: 36 batch 557 batch_loss: 0.17365610599517822\n",
      "training: 36 batch 558 batch_loss: 0.17417865991592407\n",
      "training: 36 batch 559 batch_loss: 0.1761760413646698\n",
      "training: 36 batch 560 batch_loss: 0.17415302991867065\n",
      "training: 36 batch 561 batch_loss: 0.17666175961494446\n",
      "training: 36 batch 562 batch_loss: 0.17104732990264893\n",
      "training: 36 batch 563 batch_loss: 0.17144185304641724\n",
      "training: 36 batch 564 batch_loss: 0.17687788605690002\n",
      "training: 36 batch 565 batch_loss: 0.17443209886550903\n",
      "training: 36 batch 566 batch_loss: 0.1721915602684021\n",
      "training: 36 batch 567 batch_loss: 0.17549851536750793\n",
      "training: 36 batch 568 batch_loss: 0.1779373586177826\n",
      "training: 36 batch 569 batch_loss: 0.17431879043579102\n",
      "training: 36 batch 570 batch_loss: 0.17193716764450073\n",
      "training: 36 batch 571 batch_loss: 0.17315274477005005\n",
      "training: 36 batch 572 batch_loss: 0.172468364238739\n",
      "training: 36 batch 573 batch_loss: 0.17510861158370972\n",
      "training: 36 batch 574 batch_loss: 0.17475712299346924\n",
      "training: 36 batch 575 batch_loss: 0.17752167582511902\n",
      "training: 36 batch 576 batch_loss: 0.17557063698768616\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 36, Hit Ratio:0.030943426395685442 | Precision:0.04565516563452276 | Recall:0.06095685609164386 | NDCG:0.058691830965950355\n",
      "*Best Performance* \n",
      "Epoch: 35, Hit Ratio:0.030973407108093792 | Precision:0.0456994003735378 | Recall:0.06120792632680579 | MDCG:0.058809171150581464\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 37 batch 0 batch_loss: 0.17640626430511475\n",
      "training: 37 batch 1 batch_loss: 0.1753256618976593\n",
      "training: 37 batch 2 batch_loss: 0.1741628348827362\n",
      "training: 37 batch 3 batch_loss: 0.16948556900024414\n",
      "training: 37 batch 4 batch_loss: 0.1700686812400818\n",
      "training: 37 batch 5 batch_loss: 0.1737341582775116\n",
      "training: 37 batch 6 batch_loss: 0.17340347170829773\n",
      "training: 37 batch 7 batch_loss: 0.17506182193756104\n",
      "training: 37 batch 8 batch_loss: 0.173651784658432\n",
      "training: 37 batch 9 batch_loss: 0.17191028594970703\n",
      "training: 37 batch 10 batch_loss: 0.17207059264183044\n",
      "training: 37 batch 11 batch_loss: 0.17353171110153198\n",
      "training: 37 batch 12 batch_loss: 0.1707967221736908\n",
      "training: 37 batch 13 batch_loss: 0.17325392365455627\n",
      "training: 37 batch 14 batch_loss: 0.1763143241405487\n",
      "training: 37 batch 15 batch_loss: 0.17531687021255493\n",
      "training: 37 batch 16 batch_loss: 0.17419052124023438\n",
      "training: 37 batch 17 batch_loss: 0.17137476801872253\n",
      "training: 37 batch 18 batch_loss: 0.17336636781692505\n",
      "training: 37 batch 19 batch_loss: 0.1711682379245758\n",
      "training: 37 batch 20 batch_loss: 0.1742408573627472\n",
      "training: 37 batch 21 batch_loss: 0.16980981826782227\n",
      "training: 37 batch 22 batch_loss: 0.17321103811264038\n",
      "training: 37 batch 23 batch_loss: 0.17031031847000122\n",
      "training: 37 batch 24 batch_loss: 0.17247700691223145\n",
      "training: 37 batch 25 batch_loss: 0.17467719316482544\n",
      "training: 37 batch 26 batch_loss: 0.17492282390594482\n",
      "training: 37 batch 27 batch_loss: 0.1762797236442566\n",
      "training: 37 batch 28 batch_loss: 0.170962393283844\n",
      "training: 37 batch 29 batch_loss: 0.1689966917037964\n",
      "training: 37 batch 30 batch_loss: 0.17472556233406067\n",
      "training: 37 batch 31 batch_loss: 0.16948890686035156\n",
      "training: 37 batch 32 batch_loss: 0.17004045844078064\n",
      "training: 37 batch 33 batch_loss: 0.17819556593894958\n",
      "training: 37 batch 34 batch_loss: 0.17306607961654663\n",
      "training: 37 batch 35 batch_loss: 0.1772472858428955\n",
      "training: 37 batch 36 batch_loss: 0.17028775811195374\n",
      "training: 37 batch 37 batch_loss: 0.17320188879966736\n",
      "training: 37 batch 38 batch_loss: 0.17340147495269775\n",
      "training: 37 batch 39 batch_loss: 0.17415529489517212\n",
      "training: 37 batch 40 batch_loss: 0.17583689093589783\n",
      "training: 37 batch 41 batch_loss: 0.17092633247375488\n",
      "training: 37 batch 42 batch_loss: 0.1731802225112915\n",
      "training: 37 batch 43 batch_loss: 0.1739424467086792\n",
      "training: 37 batch 44 batch_loss: 0.1757412552833557\n",
      "training: 37 batch 45 batch_loss: 0.17282703518867493\n",
      "training: 37 batch 46 batch_loss: 0.1760392189025879\n",
      "training: 37 batch 47 batch_loss: 0.1742267608642578\n",
      "training: 37 batch 48 batch_loss: 0.17159318923950195\n",
      "training: 37 batch 49 batch_loss: 0.1750771403312683\n",
      "training: 37 batch 50 batch_loss: 0.1713677942752838\n",
      "training: 37 batch 51 batch_loss: 0.17078334093093872\n",
      "training: 37 batch 52 batch_loss: 0.17364642024040222\n",
      "training: 37 batch 53 batch_loss: 0.17231643199920654\n",
      "training: 37 batch 54 batch_loss: 0.17876702547073364\n",
      "training: 37 batch 55 batch_loss: 0.17123857140541077\n",
      "training: 37 batch 56 batch_loss: 0.17023396492004395\n",
      "training: 37 batch 57 batch_loss: 0.17110061645507812\n",
      "training: 37 batch 58 batch_loss: 0.17265400290489197\n",
      "training: 37 batch 59 batch_loss: 0.17231744527816772\n",
      "training: 37 batch 60 batch_loss: 0.17297571897506714\n",
      "training: 37 batch 61 batch_loss: 0.1767592430114746\n",
      "training: 37 batch 62 batch_loss: 0.17202448844909668\n",
      "training: 37 batch 63 batch_loss: 0.16976633667945862\n",
      "training: 37 batch 64 batch_loss: 0.1701229214668274\n",
      "training: 37 batch 65 batch_loss: 0.1722792088985443\n",
      "training: 37 batch 66 batch_loss: 0.17651116847991943\n",
      "training: 37 batch 67 batch_loss: 0.17117708921432495\n",
      "training: 37 batch 68 batch_loss: 0.1705058515071869\n",
      "training: 37 batch 69 batch_loss: 0.173902690410614\n",
      "training: 37 batch 70 batch_loss: 0.17418017983436584\n",
      "training: 37 batch 71 batch_loss: 0.17493313550949097\n",
      "training: 37 batch 72 batch_loss: 0.1704912781715393\n",
      "training: 37 batch 73 batch_loss: 0.17155563831329346\n",
      "training: 37 batch 74 batch_loss: 0.17467159032821655\n",
      "training: 37 batch 75 batch_loss: 0.1753336489200592\n",
      "training: 37 batch 76 batch_loss: 0.17403975129127502\n",
      "training: 37 batch 77 batch_loss: 0.17332851886749268\n",
      "training: 37 batch 78 batch_loss: 0.17240604758262634\n",
      "training: 37 batch 79 batch_loss: 0.1670452058315277\n",
      "training: 37 batch 80 batch_loss: 0.1731867790222168\n",
      "training: 37 batch 81 batch_loss: 0.17420804500579834\n",
      "training: 37 batch 82 batch_loss: 0.17053264379501343\n",
      "training: 37 batch 83 batch_loss: 0.17386549711227417\n",
      "training: 37 batch 84 batch_loss: 0.17476314306259155\n",
      "training: 37 batch 85 batch_loss: 0.1722722053527832\n",
      "training: 37 batch 86 batch_loss: 0.17364704608917236\n",
      "training: 37 batch 87 batch_loss: 0.17240381240844727\n",
      "training: 37 batch 88 batch_loss: 0.1710580587387085\n",
      "training: 37 batch 89 batch_loss: 0.17380452156066895\n",
      "training: 37 batch 90 batch_loss: 0.17270851135253906\n",
      "training: 37 batch 91 batch_loss: 0.1740192472934723\n",
      "training: 37 batch 92 batch_loss: 0.17260509729385376\n",
      "training: 37 batch 93 batch_loss: 0.17385047674179077\n",
      "training: 37 batch 94 batch_loss: 0.1782642900943756\n",
      "training: 37 batch 95 batch_loss: 0.17373734712600708\n",
      "training: 37 batch 96 batch_loss: 0.16590428352355957\n",
      "training: 37 batch 97 batch_loss: 0.17315363883972168\n",
      "training: 37 batch 98 batch_loss: 0.17552819848060608\n",
      "training: 37 batch 99 batch_loss: 0.17251113057136536\n",
      "training: 37 batch 100 batch_loss: 0.17426255345344543\n",
      "training: 37 batch 101 batch_loss: 0.17320182919502258\n",
      "training: 37 batch 102 batch_loss: 0.17136108875274658\n",
      "training: 37 batch 103 batch_loss: 0.17108747363090515\n",
      "training: 37 batch 104 batch_loss: 0.17433437705039978\n",
      "training: 37 batch 105 batch_loss: 0.1742914915084839\n",
      "training: 37 batch 106 batch_loss: 0.17343223094940186\n",
      "training: 37 batch 107 batch_loss: 0.17498630285263062\n",
      "training: 37 batch 108 batch_loss: 0.17578646540641785\n",
      "training: 37 batch 109 batch_loss: 0.168992817401886\n",
      "training: 37 batch 110 batch_loss: 0.1745164394378662\n",
      "training: 37 batch 111 batch_loss: 0.1753881871700287\n",
      "training: 37 batch 112 batch_loss: 0.17457586526870728\n",
      "training: 37 batch 113 batch_loss: 0.17654088139533997\n",
      "training: 37 batch 114 batch_loss: 0.17315369844436646\n",
      "training: 37 batch 115 batch_loss: 0.17317363619804382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 37 batch 116 batch_loss: 0.17635869979858398\n",
      "training: 37 batch 117 batch_loss: 0.17220795154571533\n",
      "training: 37 batch 118 batch_loss: 0.17146912217140198\n",
      "training: 37 batch 119 batch_loss: 0.17184504866600037\n",
      "training: 37 batch 120 batch_loss: 0.17082849144935608\n",
      "training: 37 batch 121 batch_loss: 0.17580527067184448\n",
      "training: 37 batch 122 batch_loss: 0.17406901717185974\n",
      "training: 37 batch 123 batch_loss: 0.17170020937919617\n",
      "training: 37 batch 124 batch_loss: 0.17174285650253296\n",
      "training: 37 batch 125 batch_loss: 0.17445337772369385\n",
      "training: 37 batch 126 batch_loss: 0.17442530393600464\n",
      "training: 37 batch 127 batch_loss: 0.17394617199897766\n",
      "training: 37 batch 128 batch_loss: 0.1776452362537384\n",
      "training: 37 batch 129 batch_loss: 0.1723935902118683\n",
      "training: 37 batch 130 batch_loss: 0.17149358987808228\n",
      "training: 37 batch 131 batch_loss: 0.1711237132549286\n",
      "training: 37 batch 132 batch_loss: 0.1754811406135559\n",
      "training: 37 batch 133 batch_loss: 0.17080852389335632\n",
      "training: 37 batch 134 batch_loss: 0.1753942370414734\n",
      "training: 37 batch 135 batch_loss: 0.1709342896938324\n",
      "training: 37 batch 136 batch_loss: 0.174829363822937\n",
      "training: 37 batch 137 batch_loss: 0.17213669419288635\n",
      "training: 37 batch 138 batch_loss: 0.1725219190120697\n",
      "training: 37 batch 139 batch_loss: 0.17238646745681763\n",
      "training: 37 batch 140 batch_loss: 0.17172354459762573\n",
      "training: 37 batch 141 batch_loss: 0.17524439096450806\n",
      "training: 37 batch 142 batch_loss: 0.17324817180633545\n",
      "training: 37 batch 143 batch_loss: 0.1737295687198639\n",
      "training: 37 batch 144 batch_loss: 0.17255887389183044\n",
      "training: 37 batch 145 batch_loss: 0.1735595464706421\n",
      "training: 37 batch 146 batch_loss: 0.17172583937644958\n",
      "training: 37 batch 147 batch_loss: 0.1782243847846985\n",
      "training: 37 batch 148 batch_loss: 0.17461317777633667\n",
      "training: 37 batch 149 batch_loss: 0.17407363653182983\n",
      "training: 37 batch 150 batch_loss: 0.17385733127593994\n",
      "training: 37 batch 151 batch_loss: 0.17736023664474487\n",
      "training: 37 batch 152 batch_loss: 0.17343860864639282\n",
      "training: 37 batch 153 batch_loss: 0.17885228991508484\n",
      "training: 37 batch 154 batch_loss: 0.17475584149360657\n",
      "training: 37 batch 155 batch_loss: 0.17406487464904785\n",
      "training: 37 batch 156 batch_loss: 0.16973409056663513\n",
      "training: 37 batch 157 batch_loss: 0.17460504174232483\n",
      "training: 37 batch 158 batch_loss: 0.17283478379249573\n",
      "training: 37 batch 159 batch_loss: 0.17416158318519592\n",
      "training: 37 batch 160 batch_loss: 0.17830759286880493\n",
      "training: 37 batch 161 batch_loss: 0.17085754871368408\n",
      "training: 37 batch 162 batch_loss: 0.17108899354934692\n",
      "training: 37 batch 163 batch_loss: 0.17780166864395142\n",
      "training: 37 batch 164 batch_loss: 0.1748541295528412\n",
      "training: 37 batch 165 batch_loss: 0.17410096526145935\n",
      "training: 37 batch 166 batch_loss: 0.17327097058296204\n",
      "training: 37 batch 167 batch_loss: 0.17511770129203796\n",
      "training: 37 batch 168 batch_loss: 0.1741064488887787\n",
      "training: 37 batch 169 batch_loss: 0.17474687099456787\n",
      "training: 37 batch 170 batch_loss: 0.1736489236354828\n",
      "training: 37 batch 171 batch_loss: 0.1749923825263977\n",
      "training: 37 batch 172 batch_loss: 0.1731170117855072\n",
      "training: 37 batch 173 batch_loss: 0.17575779557228088\n",
      "training: 37 batch 174 batch_loss: 0.1720227599143982\n",
      "training: 37 batch 175 batch_loss: 0.17406535148620605\n",
      "training: 37 batch 176 batch_loss: 0.17414972186088562\n",
      "training: 37 batch 177 batch_loss: 0.17096269130706787\n",
      "training: 37 batch 178 batch_loss: 0.17500579357147217\n",
      "training: 37 batch 179 batch_loss: 0.17374512553215027\n",
      "training: 37 batch 180 batch_loss: 0.16945430636405945\n",
      "training: 37 batch 181 batch_loss: 0.17647740244865417\n",
      "training: 37 batch 182 batch_loss: 0.17375895380973816\n",
      "training: 37 batch 183 batch_loss: 0.1738744080066681\n",
      "training: 37 batch 184 batch_loss: 0.17170673608779907\n",
      "training: 37 batch 185 batch_loss: 0.17449435591697693\n",
      "training: 37 batch 186 batch_loss: 0.17234653234481812\n",
      "training: 37 batch 187 batch_loss: 0.1741575002670288\n",
      "training: 37 batch 188 batch_loss: 0.169525146484375\n",
      "training: 37 batch 189 batch_loss: 0.17350545525550842\n",
      "training: 37 batch 190 batch_loss: 0.17258644104003906\n",
      "training: 37 batch 191 batch_loss: 0.17276135087013245\n",
      "training: 37 batch 192 batch_loss: 0.17506438493728638\n",
      "training: 37 batch 193 batch_loss: 0.17435017228126526\n",
      "training: 37 batch 194 batch_loss: 0.17603743076324463\n",
      "training: 37 batch 195 batch_loss: 0.17265313863754272\n",
      "training: 37 batch 196 batch_loss: 0.17684191465377808\n",
      "training: 37 batch 197 batch_loss: 0.17363202571868896\n",
      "training: 37 batch 198 batch_loss: 0.1721140444278717\n",
      "training: 37 batch 199 batch_loss: 0.1716885268688202\n",
      "training: 37 batch 200 batch_loss: 0.17556554079055786\n",
      "training: 37 batch 201 batch_loss: 0.17039704322814941\n",
      "training: 37 batch 202 batch_loss: 0.17388325929641724\n",
      "training: 37 batch 203 batch_loss: 0.17229068279266357\n",
      "training: 37 batch 204 batch_loss: 0.17546549439430237\n",
      "training: 37 batch 205 batch_loss: 0.1726093292236328\n",
      "training: 37 batch 206 batch_loss: 0.17219528555870056\n",
      "training: 37 batch 207 batch_loss: 0.17531466484069824\n",
      "training: 37 batch 208 batch_loss: 0.17232966423034668\n",
      "training: 37 batch 209 batch_loss: 0.1752728521823883\n",
      "training: 37 batch 210 batch_loss: 0.17386871576309204\n",
      "training: 37 batch 211 batch_loss: 0.17816364765167236\n",
      "training: 37 batch 212 batch_loss: 0.17786356806755066\n",
      "training: 37 batch 213 batch_loss: 0.1741943061351776\n",
      "training: 37 batch 214 batch_loss: 0.1726384460926056\n",
      "training: 37 batch 215 batch_loss: 0.1715247929096222\n",
      "training: 37 batch 216 batch_loss: 0.17299050092697144\n",
      "training: 37 batch 217 batch_loss: 0.17660966515541077\n",
      "training: 37 batch 218 batch_loss: 0.17668062448501587\n",
      "training: 37 batch 219 batch_loss: 0.1733643114566803\n",
      "training: 37 batch 220 batch_loss: 0.1736309826374054\n",
      "training: 37 batch 221 batch_loss: 0.1727573573589325\n",
      "training: 37 batch 222 batch_loss: 0.17825791239738464\n",
      "training: 37 batch 223 batch_loss: 0.17490625381469727\n",
      "training: 37 batch 224 batch_loss: 0.17285782098770142\n",
      "training: 37 batch 225 batch_loss: 0.17672720551490784\n",
      "training: 37 batch 226 batch_loss: 0.17410922050476074\n",
      "training: 37 batch 227 batch_loss: 0.17566868662834167\n",
      "training: 37 batch 228 batch_loss: 0.17600521445274353\n",
      "training: 37 batch 229 batch_loss: 0.1782110333442688\n",
      "training: 37 batch 230 batch_loss: 0.1739148497581482\n",
      "training: 37 batch 231 batch_loss: 0.17331191897392273\n",
      "training: 37 batch 232 batch_loss: 0.17603358626365662\n",
      "training: 37 batch 233 batch_loss: 0.17117184400558472\n",
      "training: 37 batch 234 batch_loss: 0.1760096549987793\n",
      "training: 37 batch 235 batch_loss: 0.17506647109985352\n",
      "training: 37 batch 236 batch_loss: 0.17325294017791748\n",
      "training: 37 batch 237 batch_loss: 0.17364898324012756\n",
      "training: 37 batch 238 batch_loss: 0.1739305555820465\n",
      "training: 37 batch 239 batch_loss: 0.17097151279449463\n",
      "training: 37 batch 240 batch_loss: 0.17085951566696167\n",
      "training: 37 batch 241 batch_loss: 0.17507371306419373\n",
      "training: 37 batch 242 batch_loss: 0.1739361584186554\n",
      "training: 37 batch 243 batch_loss: 0.1770416498184204\n",
      "training: 37 batch 244 batch_loss: 0.17565101385116577\n",
      "training: 37 batch 245 batch_loss: 0.17985206842422485\n",
      "training: 37 batch 246 batch_loss: 0.17553412914276123\n",
      "training: 37 batch 247 batch_loss: 0.17788615822792053\n",
      "training: 37 batch 248 batch_loss: 0.17353981733322144\n",
      "training: 37 batch 249 batch_loss: 0.17630517482757568\n",
      "training: 37 batch 250 batch_loss: 0.1767282485961914\n",
      "training: 37 batch 251 batch_loss: 0.1737273931503296\n",
      "training: 37 batch 252 batch_loss: 0.17630180716514587\n",
      "training: 37 batch 253 batch_loss: 0.1758575141429901\n",
      "training: 37 batch 254 batch_loss: 0.17273789644241333\n",
      "training: 37 batch 255 batch_loss: 0.17457258701324463\n",
      "training: 37 batch 256 batch_loss: 0.17333602905273438\n",
      "training: 37 batch 257 batch_loss: 0.17843610048294067\n",
      "training: 37 batch 258 batch_loss: 0.17214703559875488\n",
      "training: 37 batch 259 batch_loss: 0.1763460338115692\n",
      "training: 37 batch 260 batch_loss: 0.171253502368927\n",
      "training: 37 batch 261 batch_loss: 0.17819270491600037\n",
      "training: 37 batch 262 batch_loss: 0.17225021123886108\n",
      "training: 37 batch 263 batch_loss: 0.1747112274169922\n",
      "training: 37 batch 264 batch_loss: 0.17493250966072083\n",
      "training: 37 batch 265 batch_loss: 0.17425057291984558\n",
      "training: 37 batch 266 batch_loss: 0.17670944333076477\n",
      "training: 37 batch 267 batch_loss: 0.1737639605998993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 37 batch 268 batch_loss: 0.17316511273384094\n",
      "training: 37 batch 269 batch_loss: 0.1723969578742981\n",
      "training: 37 batch 270 batch_loss: 0.17159658670425415\n",
      "training: 37 batch 271 batch_loss: 0.174818754196167\n",
      "training: 37 batch 272 batch_loss: 0.17923149466514587\n",
      "training: 37 batch 273 batch_loss: 0.17395690083503723\n",
      "training: 37 batch 274 batch_loss: 0.1737944781780243\n",
      "training: 37 batch 275 batch_loss: 0.1724400818347931\n",
      "training: 37 batch 276 batch_loss: 0.1723138988018036\n",
      "training: 37 batch 277 batch_loss: 0.17100530862808228\n",
      "training: 37 batch 278 batch_loss: 0.17857718467712402\n",
      "training: 37 batch 279 batch_loss: 0.17571648955345154\n",
      "training: 37 batch 280 batch_loss: 0.17701637744903564\n",
      "training: 37 batch 281 batch_loss: 0.17438223958015442\n",
      "training: 37 batch 282 batch_loss: 0.17785495519638062\n",
      "training: 37 batch 283 batch_loss: 0.1732805371284485\n",
      "training: 37 batch 284 batch_loss: 0.17371991276741028\n",
      "training: 37 batch 285 batch_loss: 0.17740681767463684\n",
      "training: 37 batch 286 batch_loss: 0.17342326045036316\n",
      "training: 37 batch 287 batch_loss: 0.1758708953857422\n",
      "training: 37 batch 288 batch_loss: 0.17742687463760376\n",
      "training: 37 batch 289 batch_loss: 0.17616045475006104\n",
      "training: 37 batch 290 batch_loss: 0.17400696873664856\n",
      "training: 37 batch 291 batch_loss: 0.1746593713760376\n",
      "training: 37 batch 292 batch_loss: 0.17111286520957947\n",
      "training: 37 batch 293 batch_loss: 0.17548829317092896\n",
      "training: 37 batch 294 batch_loss: 0.17723479866981506\n",
      "training: 37 batch 295 batch_loss: 0.17311683297157288\n",
      "training: 37 batch 296 batch_loss: 0.1709485650062561\n",
      "training: 37 batch 297 batch_loss: 0.17133274674415588\n",
      "training: 37 batch 298 batch_loss: 0.17710170149803162\n",
      "training: 37 batch 299 batch_loss: 0.1806791126728058\n",
      "training: 37 batch 300 batch_loss: 0.17742538452148438\n",
      "training: 37 batch 301 batch_loss: 0.17197775840759277\n",
      "training: 37 batch 302 batch_loss: 0.1748250126838684\n",
      "training: 37 batch 303 batch_loss: 0.175302654504776\n",
      "training: 37 batch 304 batch_loss: 0.1736832857131958\n",
      "training: 37 batch 305 batch_loss: 0.17532563209533691\n",
      "training: 37 batch 306 batch_loss: 0.17478513717651367\n",
      "training: 37 batch 307 batch_loss: 0.1715129315853119\n",
      "training: 37 batch 308 batch_loss: 0.17285513877868652\n",
      "training: 37 batch 309 batch_loss: 0.1750510334968567\n",
      "training: 37 batch 310 batch_loss: 0.17758551239967346\n",
      "training: 37 batch 311 batch_loss: 0.17647480964660645\n",
      "training: 37 batch 312 batch_loss: 0.1744731366634369\n",
      "training: 37 batch 313 batch_loss: 0.17453140020370483\n",
      "training: 37 batch 314 batch_loss: 0.17084276676177979\n",
      "training: 37 batch 315 batch_loss: 0.17100974917411804\n",
      "training: 37 batch 316 batch_loss: 0.17404645681381226\n",
      "training: 37 batch 317 batch_loss: 0.1781664490699768\n",
      "training: 37 batch 318 batch_loss: 0.17655688524246216\n",
      "training: 37 batch 319 batch_loss: 0.17750057578086853\n",
      "training: 37 batch 320 batch_loss: 0.1730343997478485\n",
      "training: 37 batch 321 batch_loss: 0.1738075613975525\n",
      "training: 37 batch 322 batch_loss: 0.17464706301689148\n",
      "training: 37 batch 323 batch_loss: 0.17702537775039673\n",
      "training: 37 batch 324 batch_loss: 0.17499428987503052\n",
      "training: 37 batch 325 batch_loss: 0.17191648483276367\n",
      "training: 37 batch 326 batch_loss: 0.17778873443603516\n",
      "training: 37 batch 327 batch_loss: 0.17663103342056274\n",
      "training: 37 batch 328 batch_loss: 0.1738569736480713\n",
      "training: 37 batch 329 batch_loss: 0.17551422119140625\n",
      "training: 37 batch 330 batch_loss: 0.17465171217918396\n",
      "training: 37 batch 331 batch_loss: 0.17191454768180847\n",
      "training: 37 batch 332 batch_loss: 0.17382892966270447\n",
      "training: 37 batch 333 batch_loss: 0.1766602098941803\n",
      "training: 37 batch 334 batch_loss: 0.17580801248550415\n",
      "training: 37 batch 335 batch_loss: 0.17448750138282776\n",
      "training: 37 batch 336 batch_loss: 0.17676997184753418\n",
      "training: 37 batch 337 batch_loss: 0.17427930235862732\n",
      "training: 37 batch 338 batch_loss: 0.1741926074028015\n",
      "training: 37 batch 339 batch_loss: 0.17390531301498413\n",
      "training: 37 batch 340 batch_loss: 0.17700886726379395\n",
      "training: 37 batch 341 batch_loss: 0.17876991629600525\n",
      "training: 37 batch 342 batch_loss: 0.17988735437393188\n",
      "training: 37 batch 343 batch_loss: 0.175495445728302\n",
      "training: 37 batch 344 batch_loss: 0.17364859580993652\n",
      "training: 37 batch 345 batch_loss: 0.17458945512771606\n",
      "training: 37 batch 346 batch_loss: 0.17597532272338867\n",
      "training: 37 batch 347 batch_loss: 0.17638638615608215\n",
      "training: 37 batch 348 batch_loss: 0.17488062381744385\n",
      "training: 37 batch 349 batch_loss: 0.17375454306602478\n",
      "training: 37 batch 350 batch_loss: 0.17224356532096863\n",
      "training: 37 batch 351 batch_loss: 0.17314046621322632\n",
      "training: 37 batch 352 batch_loss: 0.17186647653579712\n",
      "training: 37 batch 353 batch_loss: 0.17653188109397888\n",
      "training: 37 batch 354 batch_loss: 0.1773872971534729\n",
      "training: 37 batch 355 batch_loss: 0.17698171734809875\n",
      "training: 37 batch 356 batch_loss: 0.17637377977371216\n",
      "training: 37 batch 357 batch_loss: 0.1750669777393341\n",
      "training: 37 batch 358 batch_loss: 0.1735801100730896\n",
      "training: 37 batch 359 batch_loss: 0.1753057837486267\n",
      "training: 37 batch 360 batch_loss: 0.17487338185310364\n",
      "training: 37 batch 361 batch_loss: 0.17079401016235352\n",
      "training: 37 batch 362 batch_loss: 0.1749533712863922\n",
      "training: 37 batch 363 batch_loss: 0.17442607879638672\n",
      "training: 37 batch 364 batch_loss: 0.17641228437423706\n",
      "training: 37 batch 365 batch_loss: 0.1759198009967804\n",
      "training: 37 batch 366 batch_loss: 0.17632129788398743\n",
      "training: 37 batch 367 batch_loss: 0.17300039529800415\n",
      "training: 37 batch 368 batch_loss: 0.17363432049751282\n",
      "training: 37 batch 369 batch_loss: 0.172562837600708\n",
      "training: 37 batch 370 batch_loss: 0.1738772988319397\n",
      "training: 37 batch 371 batch_loss: 0.17811158299446106\n",
      "training: 37 batch 372 batch_loss: 0.17301049828529358\n",
      "training: 37 batch 373 batch_loss: 0.17471593618392944\n",
      "training: 37 batch 374 batch_loss: 0.1731921136379242\n",
      "training: 37 batch 375 batch_loss: 0.17877623438835144\n",
      "training: 37 batch 376 batch_loss: 0.17079299688339233\n",
      "training: 37 batch 377 batch_loss: 0.17163723707199097\n",
      "training: 37 batch 378 batch_loss: 0.1754242479801178\n",
      "training: 37 batch 379 batch_loss: 0.1762319803237915\n",
      "training: 37 batch 380 batch_loss: 0.17555713653564453\n",
      "training: 37 batch 381 batch_loss: 0.17395693063735962\n",
      "training: 37 batch 382 batch_loss: 0.17358195781707764\n",
      "training: 37 batch 383 batch_loss: 0.17407867312431335\n",
      "training: 37 batch 384 batch_loss: 0.1722370684146881\n",
      "training: 37 batch 385 batch_loss: 0.17378684878349304\n",
      "training: 37 batch 386 batch_loss: 0.17718952894210815\n",
      "training: 37 batch 387 batch_loss: 0.1753520965576172\n",
      "training: 37 batch 388 batch_loss: 0.17399132251739502\n",
      "training: 37 batch 389 batch_loss: 0.17613396048545837\n",
      "training: 37 batch 390 batch_loss: 0.17549842596054077\n",
      "training: 37 batch 391 batch_loss: 0.17162662744522095\n",
      "training: 37 batch 392 batch_loss: 0.17605099081993103\n",
      "training: 37 batch 393 batch_loss: 0.17308735847473145\n",
      "training: 37 batch 394 batch_loss: 0.17946264147758484\n",
      "training: 37 batch 395 batch_loss: 0.17223283648490906\n",
      "training: 37 batch 396 batch_loss: 0.17350375652313232\n",
      "training: 37 batch 397 batch_loss: 0.17601123452186584\n",
      "training: 37 batch 398 batch_loss: 0.1775064468383789\n",
      "training: 37 batch 399 batch_loss: 0.17726069688796997\n",
      "training: 37 batch 400 batch_loss: 0.17572030425071716\n",
      "training: 37 batch 401 batch_loss: 0.17704784870147705\n",
      "training: 37 batch 402 batch_loss: 0.175287663936615\n",
      "training: 37 batch 403 batch_loss: 0.17695292830467224\n",
      "training: 37 batch 404 batch_loss: 0.17901447415351868\n",
      "training: 37 batch 405 batch_loss: 0.17242440581321716\n",
      "training: 37 batch 406 batch_loss: 0.17951732873916626\n",
      "training: 37 batch 407 batch_loss: 0.1708860695362091\n",
      "training: 37 batch 408 batch_loss: 0.17890608310699463\n",
      "training: 37 batch 409 batch_loss: 0.1735610067844391\n",
      "training: 37 batch 410 batch_loss: 0.179082453250885\n",
      "training: 37 batch 411 batch_loss: 0.17436325550079346\n",
      "training: 37 batch 412 batch_loss: 0.18035149574279785\n",
      "training: 37 batch 413 batch_loss: 0.17450401186943054\n",
      "training: 37 batch 414 batch_loss: 0.1717732846736908\n",
      "training: 37 batch 415 batch_loss: 0.17882391810417175\n",
      "training: 37 batch 416 batch_loss: 0.17669618129730225\n",
      "training: 37 batch 417 batch_loss: 0.17473414540290833\n",
      "training: 37 batch 418 batch_loss: 0.17538252472877502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 37 batch 419 batch_loss: 0.17915582656860352\n",
      "training: 37 batch 420 batch_loss: 0.17381107807159424\n",
      "training: 37 batch 421 batch_loss: 0.17434942722320557\n",
      "training: 37 batch 422 batch_loss: 0.17291095852851868\n",
      "training: 37 batch 423 batch_loss: 0.17833954095840454\n",
      "training: 37 batch 424 batch_loss: 0.17770373821258545\n",
      "training: 37 batch 425 batch_loss: 0.1760915219783783\n",
      "training: 37 batch 426 batch_loss: 0.17367121577262878\n",
      "training: 37 batch 427 batch_loss: 0.17379888892173767\n",
      "training: 37 batch 428 batch_loss: 0.1769564151763916\n",
      "training: 37 batch 429 batch_loss: 0.17690247297286987\n",
      "training: 37 batch 430 batch_loss: 0.17393416166305542\n",
      "training: 37 batch 431 batch_loss: 0.17299461364746094\n",
      "training: 37 batch 432 batch_loss: 0.17400220036506653\n",
      "training: 37 batch 433 batch_loss: 0.17419284582138062\n",
      "training: 37 batch 434 batch_loss: 0.17581573128700256\n",
      "training: 37 batch 435 batch_loss: 0.17713302373886108\n",
      "training: 37 batch 436 batch_loss: 0.17404666543006897\n",
      "training: 37 batch 437 batch_loss: 0.17439842224121094\n",
      "training: 37 batch 438 batch_loss: 0.17165803909301758\n",
      "training: 37 batch 439 batch_loss: 0.1758517026901245\n",
      "training: 37 batch 440 batch_loss: 0.174276202917099\n",
      "training: 37 batch 441 batch_loss: 0.17939943075180054\n",
      "training: 37 batch 442 batch_loss: 0.1791936457157135\n",
      "training: 37 batch 443 batch_loss: 0.17369049787521362\n",
      "training: 37 batch 444 batch_loss: 0.17530882358551025\n",
      "training: 37 batch 445 batch_loss: 0.17631080746650696\n",
      "training: 37 batch 446 batch_loss: 0.18037444353103638\n",
      "training: 37 batch 447 batch_loss: 0.17440873384475708\n",
      "training: 37 batch 448 batch_loss: 0.17349857091903687\n",
      "training: 37 batch 449 batch_loss: 0.175072580575943\n",
      "training: 37 batch 450 batch_loss: 0.1745254397392273\n",
      "training: 37 batch 451 batch_loss: 0.174961119890213\n",
      "training: 37 batch 452 batch_loss: 0.17900007963180542\n",
      "training: 37 batch 453 batch_loss: 0.17394793033599854\n",
      "training: 37 batch 454 batch_loss: 0.17565467953681946\n",
      "training: 37 batch 455 batch_loss: 0.17842832207679749\n",
      "training: 37 batch 456 batch_loss: 0.17502659559249878\n",
      "training: 37 batch 457 batch_loss: 0.17876002192497253\n",
      "training: 37 batch 458 batch_loss: 0.17675122618675232\n",
      "training: 37 batch 459 batch_loss: 0.17368629574775696\n",
      "training: 37 batch 460 batch_loss: 0.1740332841873169\n",
      "training: 37 batch 461 batch_loss: 0.17657530307769775\n",
      "training: 37 batch 462 batch_loss: 0.17449432611465454\n",
      "training: 37 batch 463 batch_loss: 0.1737293303012848\n",
      "training: 37 batch 464 batch_loss: 0.17755389213562012\n",
      "training: 37 batch 465 batch_loss: 0.17353853583335876\n",
      "training: 37 batch 466 batch_loss: 0.174679696559906\n",
      "training: 37 batch 467 batch_loss: 0.176782488822937\n",
      "training: 37 batch 468 batch_loss: 0.17959338426589966\n",
      "training: 37 batch 469 batch_loss: 0.17639365792274475\n",
      "training: 37 batch 470 batch_loss: 0.17371830344200134\n",
      "training: 37 batch 471 batch_loss: 0.17246848344802856\n",
      "training: 37 batch 472 batch_loss: 0.1751047968864441\n",
      "training: 37 batch 473 batch_loss: 0.173059344291687\n",
      "training: 37 batch 474 batch_loss: 0.17439287900924683\n",
      "training: 37 batch 475 batch_loss: 0.17769384384155273\n",
      "training: 37 batch 476 batch_loss: 0.17533180117607117\n",
      "training: 37 batch 477 batch_loss: 0.17660793662071228\n",
      "training: 37 batch 478 batch_loss: 0.1738627552986145\n",
      "training: 37 batch 479 batch_loss: 0.17571404576301575\n",
      "training: 37 batch 480 batch_loss: 0.1770784556865692\n",
      "training: 37 batch 481 batch_loss: 0.17793112993240356\n",
      "training: 37 batch 482 batch_loss: 0.17385393381118774\n",
      "training: 37 batch 483 batch_loss: 0.17576727271080017\n",
      "training: 37 batch 484 batch_loss: 0.17530179023742676\n",
      "training: 37 batch 485 batch_loss: 0.17394688725471497\n",
      "training: 37 batch 486 batch_loss: 0.17575162649154663\n",
      "training: 37 batch 487 batch_loss: 0.1794450879096985\n",
      "training: 37 batch 488 batch_loss: 0.17832759022712708\n",
      "training: 37 batch 489 batch_loss: 0.17727699875831604\n",
      "training: 37 batch 490 batch_loss: 0.17852401733398438\n",
      "training: 37 batch 491 batch_loss: 0.1768331527709961\n",
      "training: 37 batch 492 batch_loss: 0.17559579014778137\n",
      "training: 37 batch 493 batch_loss: 0.17124176025390625\n",
      "training: 37 batch 494 batch_loss: 0.17382347583770752\n",
      "training: 37 batch 495 batch_loss: 0.17661002278327942\n",
      "training: 37 batch 496 batch_loss: 0.17515134811401367\n",
      "training: 37 batch 497 batch_loss: 0.1796460747718811\n",
      "training: 37 batch 498 batch_loss: 0.17424747347831726\n",
      "training: 37 batch 499 batch_loss: 0.17535972595214844\n",
      "training: 37 batch 500 batch_loss: 0.17381691932678223\n",
      "training: 37 batch 501 batch_loss: 0.17835953831672668\n",
      "training: 37 batch 502 batch_loss: 0.17875537276268005\n",
      "training: 37 batch 503 batch_loss: 0.17519071698188782\n",
      "training: 37 batch 504 batch_loss: 0.178021639585495\n",
      "training: 37 batch 505 batch_loss: 0.174370676279068\n",
      "training: 37 batch 506 batch_loss: 0.17555716633796692\n",
      "training: 37 batch 507 batch_loss: 0.173993319272995\n",
      "training: 37 batch 508 batch_loss: 0.18105831742286682\n",
      "training: 37 batch 509 batch_loss: 0.17599204182624817\n",
      "training: 37 batch 510 batch_loss: 0.1778290569782257\n",
      "training: 37 batch 511 batch_loss: 0.1790252923965454\n",
      "training: 37 batch 512 batch_loss: 0.17638087272644043\n",
      "training: 37 batch 513 batch_loss: 0.17980971932411194\n",
      "training: 37 batch 514 batch_loss: 0.17600709199905396\n",
      "training: 37 batch 515 batch_loss: 0.175674706697464\n",
      "training: 37 batch 516 batch_loss: 0.17710712552070618\n",
      "training: 37 batch 517 batch_loss: 0.17427140474319458\n",
      "training: 37 batch 518 batch_loss: 0.17503926157951355\n",
      "training: 37 batch 519 batch_loss: 0.1755436658859253\n",
      "training: 37 batch 520 batch_loss: 0.17473289370536804\n",
      "training: 37 batch 521 batch_loss: 0.1783774197101593\n",
      "training: 37 batch 522 batch_loss: 0.17478585243225098\n",
      "training: 37 batch 523 batch_loss: 0.1679825782775879\n",
      "training: 37 batch 524 batch_loss: 0.17601236701011658\n",
      "training: 37 batch 525 batch_loss: 0.17759323120117188\n",
      "training: 37 batch 526 batch_loss: 0.1729782223701477\n",
      "training: 37 batch 527 batch_loss: 0.17715641856193542\n",
      "training: 37 batch 528 batch_loss: 0.17689314484596252\n",
      "training: 37 batch 529 batch_loss: 0.17775192856788635\n",
      "training: 37 batch 530 batch_loss: 0.17748236656188965\n",
      "training: 37 batch 531 batch_loss: 0.17845159769058228\n",
      "training: 37 batch 532 batch_loss: 0.17079228162765503\n",
      "training: 37 batch 533 batch_loss: 0.17796409130096436\n",
      "training: 37 batch 534 batch_loss: 0.1752866804599762\n",
      "training: 37 batch 535 batch_loss: 0.1746593415737152\n",
      "training: 37 batch 536 batch_loss: 0.17939120531082153\n",
      "training: 37 batch 537 batch_loss: 0.17668798565864563\n",
      "training: 37 batch 538 batch_loss: 0.1769547164440155\n",
      "training: 37 batch 539 batch_loss: 0.17708748579025269\n",
      "training: 37 batch 540 batch_loss: 0.17917099595069885\n",
      "training: 37 batch 541 batch_loss: 0.17657235264778137\n",
      "training: 37 batch 542 batch_loss: 0.17858105897903442\n",
      "training: 37 batch 543 batch_loss: 0.1764518916606903\n",
      "training: 37 batch 544 batch_loss: 0.17349684238433838\n",
      "training: 37 batch 545 batch_loss: 0.1735178530216217\n",
      "training: 37 batch 546 batch_loss: 0.17716166377067566\n",
      "training: 37 batch 547 batch_loss: 0.17514148354530334\n",
      "training: 37 batch 548 batch_loss: 0.17829754948616028\n",
      "training: 37 batch 549 batch_loss: 0.17681080102920532\n",
      "training: 37 batch 550 batch_loss: 0.1721133589744568\n",
      "training: 37 batch 551 batch_loss: 0.17631196975708008\n",
      "training: 37 batch 552 batch_loss: 0.17611905932426453\n",
      "training: 37 batch 553 batch_loss: 0.17637988924980164\n",
      "training: 37 batch 554 batch_loss: 0.1745104193687439\n",
      "training: 37 batch 555 batch_loss: 0.17571938037872314\n",
      "training: 37 batch 556 batch_loss: 0.17531818151474\n",
      "training: 37 batch 557 batch_loss: 0.17138227820396423\n",
      "training: 37 batch 558 batch_loss: 0.18100541830062866\n",
      "training: 37 batch 559 batch_loss: 0.17519819736480713\n",
      "training: 37 batch 560 batch_loss: 0.1791313886642456\n",
      "training: 37 batch 561 batch_loss: 0.17917078733444214\n",
      "training: 37 batch 562 batch_loss: 0.17259499430656433\n",
      "training: 37 batch 563 batch_loss: 0.17734304070472717\n",
      "training: 37 batch 564 batch_loss: 0.18093442916870117\n",
      "training: 37 batch 565 batch_loss: 0.17799043655395508\n",
      "training: 37 batch 566 batch_loss: 0.17500993609428406\n",
      "training: 37 batch 567 batch_loss: 0.17652243375778198\n",
      "training: 37 batch 568 batch_loss: 0.17860963940620422\n",
      "training: 37 batch 569 batch_loss: 0.17711782455444336\n",
      "training: 37 batch 570 batch_loss: 0.1768682599067688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 37 batch 571 batch_loss: 0.17503812909126282\n",
      "training: 37 batch 572 batch_loss: 0.1757262945175171\n",
      "training: 37 batch 573 batch_loss: 0.17747455835342407\n",
      "training: 37 batch 574 batch_loss: 0.17747408151626587\n",
      "training: 37 batch 575 batch_loss: 0.17877429723739624\n",
      "training: 37 batch 576 batch_loss: 0.17844223976135254\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 37, Hit Ratio:0.030940095205417847 | Precision:0.04565025066352109 | Recall:0.06095152000599274 | NDCG:0.05848814673796846\n",
      "*Best Performance* \n",
      "Epoch: 35, Hit Ratio:0.030973407108093792 | Precision:0.0456994003735378 | Recall:0.06120792632680579 | MDCG:0.058809171150581464\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 38 batch 0 batch_loss: 0.1752200722694397\n",
      "training: 38 batch 1 batch_loss: 0.17096209526062012\n",
      "training: 38 batch 2 batch_loss: 0.17207789421081543\n",
      "training: 38 batch 3 batch_loss: 0.1753007471561432\n",
      "training: 38 batch 4 batch_loss: 0.17422494292259216\n",
      "training: 38 batch 5 batch_loss: 0.1756201982498169\n",
      "training: 38 batch 6 batch_loss: 0.17564591765403748\n",
      "training: 38 batch 7 batch_loss: 0.17258235812187195\n",
      "training: 38 batch 8 batch_loss: 0.17284634709358215\n",
      "training: 38 batch 9 batch_loss: 0.17307263612747192\n",
      "training: 38 batch 10 batch_loss: 0.17669826745986938\n",
      "training: 38 batch 11 batch_loss: 0.17563998699188232\n",
      "training: 38 batch 12 batch_loss: 0.17626777291297913\n",
      "training: 38 batch 13 batch_loss: 0.1724528670310974\n",
      "training: 38 batch 14 batch_loss: 0.17545121908187866\n",
      "training: 38 batch 15 batch_loss: 0.1752588152885437\n",
      "training: 38 batch 16 batch_loss: 0.17737790942192078\n",
      "training: 38 batch 17 batch_loss: 0.1728939414024353\n",
      "training: 38 batch 18 batch_loss: 0.17515170574188232\n",
      "training: 38 batch 19 batch_loss: 0.17512768507003784\n",
      "training: 38 batch 20 batch_loss: 0.17480891942977905\n",
      "training: 38 batch 21 batch_loss: 0.1745387315750122\n",
      "training: 38 batch 22 batch_loss: 0.17483434081077576\n",
      "training: 38 batch 23 batch_loss: 0.174218088388443\n",
      "training: 38 batch 24 batch_loss: 0.1707126796245575\n",
      "training: 38 batch 25 batch_loss: 0.17379382252693176\n",
      "training: 38 batch 26 batch_loss: 0.1727195680141449\n",
      "training: 38 batch 27 batch_loss: 0.17497137188911438\n",
      "training: 38 batch 28 batch_loss: 0.17372959852218628\n",
      "training: 38 batch 29 batch_loss: 0.17745822668075562\n",
      "training: 38 batch 30 batch_loss: 0.1740570068359375\n",
      "training: 38 batch 31 batch_loss: 0.17424562573432922\n",
      "training: 38 batch 32 batch_loss: 0.17659664154052734\n",
      "training: 38 batch 33 batch_loss: 0.17403137683868408\n",
      "training: 38 batch 34 batch_loss: 0.1745777726173401\n",
      "training: 38 batch 35 batch_loss: 0.1750086545944214\n",
      "training: 38 batch 36 batch_loss: 0.17496076226234436\n",
      "training: 38 batch 37 batch_loss: 0.17141050100326538\n",
      "training: 38 batch 38 batch_loss: 0.17542660236358643\n",
      "training: 38 batch 39 batch_loss: 0.17514446377754211\n",
      "training: 38 batch 40 batch_loss: 0.17545568943023682\n",
      "training: 38 batch 41 batch_loss: 0.17358464002609253\n",
      "training: 38 batch 42 batch_loss: 0.1734083890914917\n",
      "training: 38 batch 43 batch_loss: 0.17224931716918945\n",
      "training: 38 batch 44 batch_loss: 0.17383375763893127\n",
      "training: 38 batch 45 batch_loss: 0.17520594596862793\n",
      "training: 38 batch 46 batch_loss: 0.17782196402549744\n",
      "training: 38 batch 47 batch_loss: 0.17183610796928406\n",
      "training: 38 batch 48 batch_loss: 0.17947009205818176\n",
      "training: 38 batch 49 batch_loss: 0.1736990213394165\n",
      "training: 38 batch 50 batch_loss: 0.17482024431228638\n",
      "training: 38 batch 51 batch_loss: 0.17485123872756958\n",
      "training: 38 batch 52 batch_loss: 0.17649027705192566\n",
      "training: 38 batch 53 batch_loss: 0.1757371425628662\n",
      "training: 38 batch 54 batch_loss: 0.17213532328605652\n",
      "training: 38 batch 55 batch_loss: 0.1763612926006317\n",
      "training: 38 batch 56 batch_loss: 0.17594370245933533\n",
      "training: 38 batch 57 batch_loss: 0.17131772637367249\n",
      "training: 38 batch 58 batch_loss: 0.1727382242679596\n",
      "training: 38 batch 59 batch_loss: 0.17797952890396118\n",
      "training: 38 batch 60 batch_loss: 0.1731410026550293\n",
      "training: 38 batch 61 batch_loss: 0.17174234986305237\n",
      "training: 38 batch 62 batch_loss: 0.17614129185676575\n",
      "training: 38 batch 63 batch_loss: 0.17127102613449097\n",
      "training: 38 batch 64 batch_loss: 0.17974495887756348\n",
      "training: 38 batch 65 batch_loss: 0.17443978786468506\n",
      "training: 38 batch 66 batch_loss: 0.17649272084236145\n",
      "training: 38 batch 67 batch_loss: 0.17182019352912903\n",
      "training: 38 batch 68 batch_loss: 0.17507025599479675\n",
      "training: 38 batch 69 batch_loss: 0.17418119311332703\n",
      "training: 38 batch 70 batch_loss: 0.1781018078327179\n",
      "training: 38 batch 71 batch_loss: 0.17260929942131042\n",
      "training: 38 batch 72 batch_loss: 0.17681780457496643\n",
      "training: 38 batch 73 batch_loss: 0.17336434125900269\n",
      "training: 38 batch 74 batch_loss: 0.17531633377075195\n",
      "training: 38 batch 75 batch_loss: 0.17105981707572937\n",
      "training: 38 batch 76 batch_loss: 0.173750638961792\n",
      "training: 38 batch 77 batch_loss: 0.1764644980430603\n",
      "training: 38 batch 78 batch_loss: 0.17843160033226013\n",
      "training: 38 batch 79 batch_loss: 0.17488157749176025\n",
      "training: 38 batch 80 batch_loss: 0.17177516222000122\n",
      "training: 38 batch 81 batch_loss: 0.17847701907157898\n",
      "training: 38 batch 82 batch_loss: 0.17397117614746094\n",
      "training: 38 batch 83 batch_loss: 0.1757316291332245\n",
      "training: 38 batch 84 batch_loss: 0.17403075098991394\n",
      "training: 38 batch 85 batch_loss: 0.17510497570037842\n",
      "training: 38 batch 86 batch_loss: 0.176447331905365\n",
      "training: 38 batch 87 batch_loss: 0.17368930578231812\n",
      "training: 38 batch 88 batch_loss: 0.17189082503318787\n",
      "training: 38 batch 89 batch_loss: 0.17368730902671814\n",
      "training: 38 batch 90 batch_loss: 0.17564800381660461\n",
      "training: 38 batch 91 batch_loss: 0.17833194136619568\n",
      "training: 38 batch 92 batch_loss: 0.17679136991500854\n",
      "training: 38 batch 93 batch_loss: 0.17621397972106934\n",
      "training: 38 batch 94 batch_loss: 0.17357438802719116\n",
      "training: 38 batch 95 batch_loss: 0.17223051190376282\n",
      "training: 38 batch 96 batch_loss: 0.17121079564094543\n",
      "training: 38 batch 97 batch_loss: 0.1732628047466278\n",
      "training: 38 batch 98 batch_loss: 0.17473816871643066\n",
      "training: 38 batch 99 batch_loss: 0.1764841377735138\n",
      "training: 38 batch 100 batch_loss: 0.1718456745147705\n",
      "training: 38 batch 101 batch_loss: 0.17527040839195251\n",
      "training: 38 batch 102 batch_loss: 0.17646721005439758\n",
      "training: 38 batch 103 batch_loss: 0.1752665638923645\n",
      "training: 38 batch 104 batch_loss: 0.176052987575531\n",
      "training: 38 batch 105 batch_loss: 0.17461511492729187\n",
      "training: 38 batch 106 batch_loss: 0.17421409487724304\n",
      "training: 38 batch 107 batch_loss: 0.17747217416763306\n",
      "training: 38 batch 108 batch_loss: 0.17619311809539795\n",
      "training: 38 batch 109 batch_loss: 0.17167967557907104\n",
      "training: 38 batch 110 batch_loss: 0.1751447319984436\n",
      "training: 38 batch 111 batch_loss: 0.17451804876327515\n",
      "training: 38 batch 112 batch_loss: 0.17541569471359253\n",
      "training: 38 batch 113 batch_loss: 0.1789243519306183\n",
      "training: 38 batch 114 batch_loss: 0.173852801322937\n",
      "training: 38 batch 115 batch_loss: 0.1733960509300232\n",
      "training: 38 batch 116 batch_loss: 0.17371267080307007\n",
      "training: 38 batch 117 batch_loss: 0.17697155475616455\n",
      "training: 38 batch 118 batch_loss: 0.1758873462677002\n",
      "training: 38 batch 119 batch_loss: 0.17554092407226562\n",
      "training: 38 batch 120 batch_loss: 0.18034791946411133\n",
      "training: 38 batch 121 batch_loss: 0.17706039547920227\n",
      "training: 38 batch 122 batch_loss: 0.1782415807247162\n",
      "training: 38 batch 123 batch_loss: 0.17383384704589844\n",
      "training: 38 batch 124 batch_loss: 0.17440852522850037\n",
      "training: 38 batch 125 batch_loss: 0.1784743368625641\n",
      "training: 38 batch 126 batch_loss: 0.1735856533050537\n",
      "training: 38 batch 127 batch_loss: 0.17367613315582275\n",
      "training: 38 batch 128 batch_loss: 0.17579799890518188\n",
      "training: 38 batch 129 batch_loss: 0.1735522449016571\n",
      "training: 38 batch 130 batch_loss: 0.1755334436893463\n",
      "training: 38 batch 131 batch_loss: 0.1781206727027893\n",
      "training: 38 batch 132 batch_loss: 0.17544344067573547\n",
      "training: 38 batch 133 batch_loss: 0.17393016815185547\n",
      "training: 38 batch 134 batch_loss: 0.17538323998451233\n",
      "training: 38 batch 135 batch_loss: 0.17312735319137573\n",
      "training: 38 batch 136 batch_loss: 0.1774231195449829\n",
      "training: 38 batch 137 batch_loss: 0.1794258952140808\n",
      "training: 38 batch 138 batch_loss: 0.17471149563789368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 38 batch 139 batch_loss: 0.1711072325706482\n",
      "training: 38 batch 140 batch_loss: 0.17669224739074707\n",
      "training: 38 batch 141 batch_loss: 0.17654049396514893\n",
      "training: 38 batch 142 batch_loss: 0.17293205857276917\n",
      "training: 38 batch 143 batch_loss: 0.17737844586372375\n",
      "training: 38 batch 144 batch_loss: 0.17237737774848938\n",
      "training: 38 batch 145 batch_loss: 0.1755533218383789\n",
      "training: 38 batch 146 batch_loss: 0.17703944444656372\n",
      "training: 38 batch 147 batch_loss: 0.1733698844909668\n",
      "training: 38 batch 148 batch_loss: 0.17602327466011047\n",
      "training: 38 batch 149 batch_loss: 0.1734674572944641\n",
      "training: 38 batch 150 batch_loss: 0.17451444268226624\n",
      "training: 38 batch 151 batch_loss: 0.17459136247634888\n",
      "training: 38 batch 152 batch_loss: 0.17472895979881287\n",
      "training: 38 batch 153 batch_loss: 0.1770218014717102\n",
      "training: 38 batch 154 batch_loss: 0.17881500720977783\n",
      "training: 38 batch 155 batch_loss: 0.17590799927711487\n",
      "training: 38 batch 156 batch_loss: 0.17961949110031128\n",
      "training: 38 batch 157 batch_loss: 0.17349359393119812\n",
      "training: 38 batch 158 batch_loss: 0.1754932999610901\n",
      "training: 38 batch 159 batch_loss: 0.17362651228904724\n",
      "training: 38 batch 160 batch_loss: 0.177381694316864\n",
      "training: 38 batch 161 batch_loss: 0.17626246809959412\n",
      "training: 38 batch 162 batch_loss: 0.17483708262443542\n",
      "training: 38 batch 163 batch_loss: 0.17820805311203003\n",
      "training: 38 batch 164 batch_loss: 0.17690342664718628\n",
      "training: 38 batch 165 batch_loss: 0.17408064007759094\n",
      "training: 38 batch 166 batch_loss: 0.17257919907569885\n",
      "training: 38 batch 167 batch_loss: 0.1773531138896942\n",
      "training: 38 batch 168 batch_loss: 0.17514532804489136\n",
      "training: 38 batch 169 batch_loss: 0.17849647998809814\n",
      "training: 38 batch 170 batch_loss: 0.1751897931098938\n",
      "training: 38 batch 171 batch_loss: 0.17316555976867676\n",
      "training: 38 batch 172 batch_loss: 0.1739056408405304\n",
      "training: 38 batch 173 batch_loss: 0.1741715967655182\n",
      "training: 38 batch 174 batch_loss: 0.1748826801776886\n",
      "training: 38 batch 175 batch_loss: 0.17864015698432922\n",
      "training: 38 batch 176 batch_loss: 0.17795443534851074\n",
      "training: 38 batch 177 batch_loss: 0.1732112467288971\n",
      "training: 38 batch 178 batch_loss: 0.1761612892150879\n",
      "training: 38 batch 179 batch_loss: 0.17785754799842834\n",
      "training: 38 batch 180 batch_loss: 0.17719268798828125\n",
      "training: 38 batch 181 batch_loss: 0.17615607380867004\n",
      "training: 38 batch 182 batch_loss: 0.17646288871765137\n",
      "training: 38 batch 183 batch_loss: 0.18077552318572998\n",
      "training: 38 batch 184 batch_loss: 0.1771654486656189\n",
      "training: 38 batch 185 batch_loss: 0.17275011539459229\n",
      "training: 38 batch 186 batch_loss: 0.17246410250663757\n",
      "training: 38 batch 187 batch_loss: 0.17420828342437744\n",
      "training: 38 batch 188 batch_loss: 0.17596590518951416\n",
      "training: 38 batch 189 batch_loss: 0.1763019561767578\n",
      "training: 38 batch 190 batch_loss: 0.17398333549499512\n",
      "training: 38 batch 191 batch_loss: 0.1758520007133484\n",
      "training: 38 batch 192 batch_loss: 0.1768168807029724\n",
      "training: 38 batch 193 batch_loss: 0.17475157976150513\n",
      "training: 38 batch 194 batch_loss: 0.17555320262908936\n",
      "training: 38 batch 195 batch_loss: 0.1752505898475647\n",
      "training: 38 batch 196 batch_loss: 0.17621976137161255\n",
      "training: 38 batch 197 batch_loss: 0.17323878407478333\n",
      "training: 38 batch 198 batch_loss: 0.17378246784210205\n",
      "training: 38 batch 199 batch_loss: 0.17624050378799438\n",
      "training: 38 batch 200 batch_loss: 0.1756356954574585\n",
      "training: 38 batch 201 batch_loss: 0.1776866614818573\n",
      "training: 38 batch 202 batch_loss: 0.17664489150047302\n",
      "training: 38 batch 203 batch_loss: 0.17710301280021667\n",
      "training: 38 batch 204 batch_loss: 0.17786365747451782\n",
      "training: 38 batch 205 batch_loss: 0.175340473651886\n",
      "training: 38 batch 206 batch_loss: 0.1757040023803711\n",
      "training: 38 batch 207 batch_loss: 0.17487674951553345\n",
      "training: 38 batch 208 batch_loss: 0.1761970818042755\n",
      "training: 38 batch 209 batch_loss: 0.17701482772827148\n",
      "training: 38 batch 210 batch_loss: 0.17441871762275696\n",
      "training: 38 batch 211 batch_loss: 0.17685037851333618\n",
      "training: 38 batch 212 batch_loss: 0.17384397983551025\n",
      "training: 38 batch 213 batch_loss: 0.17263975739479065\n",
      "training: 38 batch 214 batch_loss: 0.18019241094589233\n",
      "training: 38 batch 215 batch_loss: 0.17725801467895508\n",
      "training: 38 batch 216 batch_loss: 0.18101054430007935\n",
      "training: 38 batch 217 batch_loss: 0.180913507938385\n",
      "training: 38 batch 218 batch_loss: 0.17606189846992493\n",
      "training: 38 batch 219 batch_loss: 0.17444980144500732\n",
      "training: 38 batch 220 batch_loss: 0.17811459302902222\n",
      "training: 38 batch 221 batch_loss: 0.1768530309200287\n",
      "training: 38 batch 222 batch_loss: 0.17462751269340515\n",
      "training: 38 batch 223 batch_loss: 0.17727047204971313\n",
      "training: 38 batch 224 batch_loss: 0.17573106288909912\n",
      "training: 38 batch 225 batch_loss: 0.1728743314743042\n",
      "training: 38 batch 226 batch_loss: 0.17426535487174988\n",
      "training: 38 batch 227 batch_loss: 0.17526113986968994\n",
      "training: 38 batch 228 batch_loss: 0.17394766211509705\n",
      "training: 38 batch 229 batch_loss: 0.1786147952079773\n",
      "training: 38 batch 230 batch_loss: 0.17845571041107178\n",
      "training: 38 batch 231 batch_loss: 0.17537766695022583\n",
      "training: 38 batch 232 batch_loss: 0.17722710967063904\n",
      "training: 38 batch 233 batch_loss: 0.17969056963920593\n",
      "training: 38 batch 234 batch_loss: 0.17769378423690796\n",
      "training: 38 batch 235 batch_loss: 0.17727094888687134\n",
      "training: 38 batch 236 batch_loss: 0.17321917414665222\n",
      "training: 38 batch 237 batch_loss: 0.17775961756706238\n",
      "training: 38 batch 238 batch_loss: 0.17416465282440186\n",
      "training: 38 batch 239 batch_loss: 0.17677569389343262\n",
      "training: 38 batch 240 batch_loss: 0.1703842282295227\n",
      "training: 38 batch 241 batch_loss: 0.17664387822151184\n",
      "training: 38 batch 242 batch_loss: 0.1753542423248291\n",
      "training: 38 batch 243 batch_loss: 0.17882567644119263\n",
      "training: 38 batch 244 batch_loss: 0.17761805653572083\n",
      "training: 38 batch 245 batch_loss: 0.17968055605888367\n",
      "training: 38 batch 246 batch_loss: 0.178544819355011\n",
      "training: 38 batch 247 batch_loss: 0.17528530955314636\n",
      "training: 38 batch 248 batch_loss: 0.17381355166435242\n",
      "training: 38 batch 249 batch_loss: 0.17985078692436218\n",
      "training: 38 batch 250 batch_loss: 0.1749267280101776\n",
      "training: 38 batch 251 batch_loss: 0.17783308029174805\n",
      "training: 38 batch 252 batch_loss: 0.17434486746788025\n",
      "training: 38 batch 253 batch_loss: 0.17402192950248718\n",
      "training: 38 batch 254 batch_loss: 0.17340555787086487\n",
      "training: 38 batch 255 batch_loss: 0.17540359497070312\n",
      "training: 38 batch 256 batch_loss: 0.17515724897384644\n",
      "training: 38 batch 257 batch_loss: 0.17622876167297363\n",
      "training: 38 batch 258 batch_loss: 0.17571571469306946\n",
      "training: 38 batch 259 batch_loss: 0.17163071036338806\n",
      "training: 38 batch 260 batch_loss: 0.17350587248802185\n",
      "training: 38 batch 261 batch_loss: 0.17510119080543518\n",
      "training: 38 batch 262 batch_loss: 0.17735573649406433\n",
      "training: 38 batch 263 batch_loss: 0.18280813097953796\n",
      "training: 38 batch 264 batch_loss: 0.17896056175231934\n",
      "training: 38 batch 265 batch_loss: 0.1730588674545288\n",
      "training: 38 batch 266 batch_loss: 0.17372986674308777\n",
      "training: 38 batch 267 batch_loss: 0.1728001832962036\n",
      "training: 38 batch 268 batch_loss: 0.17665204405784607\n",
      "training: 38 batch 269 batch_loss: 0.17514780163764954\n",
      "training: 38 batch 270 batch_loss: 0.17308825254440308\n",
      "training: 38 batch 271 batch_loss: 0.17815819382667542\n",
      "training: 38 batch 272 batch_loss: 0.176794171333313\n",
      "training: 38 batch 273 batch_loss: 0.17819440364837646\n",
      "training: 38 batch 274 batch_loss: 0.17751547694206238\n",
      "training: 38 batch 275 batch_loss: 0.17492738366127014\n",
      "training: 38 batch 276 batch_loss: 0.17567509412765503\n",
      "training: 38 batch 277 batch_loss: 0.1733238697052002\n",
      "training: 38 batch 278 batch_loss: 0.17722785472869873\n",
      "training: 38 batch 279 batch_loss: 0.1772485375404358\n",
      "training: 38 batch 280 batch_loss: 0.17450407147407532\n",
      "training: 38 batch 281 batch_loss: 0.17692193388938904\n",
      "training: 38 batch 282 batch_loss: 0.17801696062088013\n",
      "training: 38 batch 283 batch_loss: 0.1752019226551056\n",
      "training: 38 batch 284 batch_loss: 0.17738330364227295\n",
      "training: 38 batch 285 batch_loss: 0.1767778992652893\n",
      "training: 38 batch 286 batch_loss: 0.17515426874160767\n",
      "training: 38 batch 287 batch_loss: 0.17614662647247314\n",
      "training: 38 batch 288 batch_loss: 0.1745712161064148\n",
      "training: 38 batch 289 batch_loss: 0.17381048202514648\n",
      "training: 38 batch 290 batch_loss: 0.1757667064666748\n",
      "training: 38 batch 291 batch_loss: 0.1804177463054657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 38 batch 292 batch_loss: 0.17947402596473694\n",
      "training: 38 batch 293 batch_loss: 0.18092143535614014\n",
      "training: 38 batch 294 batch_loss: 0.17368018627166748\n",
      "training: 38 batch 295 batch_loss: 0.17770403623580933\n",
      "training: 38 batch 296 batch_loss: 0.17714935541152954\n",
      "training: 38 batch 297 batch_loss: 0.1761322021484375\n",
      "training: 38 batch 298 batch_loss: 0.17545562982559204\n",
      "training: 38 batch 299 batch_loss: 0.17366883158683777\n",
      "training: 38 batch 300 batch_loss: 0.1738552451133728\n",
      "training: 38 batch 301 batch_loss: 0.1746240258216858\n",
      "training: 38 batch 302 batch_loss: 0.17711371183395386\n",
      "training: 38 batch 303 batch_loss: 0.17542433738708496\n",
      "training: 38 batch 304 batch_loss: 0.17671650648117065\n",
      "training: 38 batch 305 batch_loss: 0.17642143368721008\n",
      "training: 38 batch 306 batch_loss: 0.1761276125907898\n",
      "training: 38 batch 307 batch_loss: 0.1786622703075409\n",
      "training: 38 batch 308 batch_loss: 0.17232567071914673\n",
      "training: 38 batch 309 batch_loss: 0.17939037084579468\n",
      "training: 38 batch 310 batch_loss: 0.1769218146800995\n",
      "training: 38 batch 311 batch_loss: 0.1771869957447052\n",
      "training: 38 batch 312 batch_loss: 0.1795341968536377\n",
      "training: 38 batch 313 batch_loss: 0.17667964100837708\n",
      "training: 38 batch 314 batch_loss: 0.18063649535179138\n",
      "training: 38 batch 315 batch_loss: 0.17181840538978577\n",
      "training: 38 batch 316 batch_loss: 0.1752692461013794\n",
      "training: 38 batch 317 batch_loss: 0.17430686950683594\n",
      "training: 38 batch 318 batch_loss: 0.17522692680358887\n",
      "training: 38 batch 319 batch_loss: 0.1777876615524292\n",
      "training: 38 batch 320 batch_loss: 0.1777110993862152\n",
      "training: 38 batch 321 batch_loss: 0.17605188488960266\n",
      "training: 38 batch 322 batch_loss: 0.17502164840698242\n",
      "training: 38 batch 323 batch_loss: 0.17689192295074463\n",
      "training: 38 batch 324 batch_loss: 0.1748938262462616\n",
      "training: 38 batch 325 batch_loss: 0.17369920015335083\n",
      "training: 38 batch 326 batch_loss: 0.17635861039161682\n",
      "training: 38 batch 327 batch_loss: 0.1791175901889801\n",
      "training: 38 batch 328 batch_loss: 0.17910653352737427\n",
      "training: 38 batch 329 batch_loss: 0.1802586317062378\n",
      "training: 38 batch 330 batch_loss: 0.17773160338401794\n",
      "training: 38 batch 331 batch_loss: 0.1763043999671936\n",
      "training: 38 batch 332 batch_loss: 0.17895960807800293\n",
      "training: 38 batch 333 batch_loss: 0.17595961689949036\n",
      "training: 38 batch 334 batch_loss: 0.17636540532112122\n",
      "training: 38 batch 335 batch_loss: 0.1776551902294159\n",
      "training: 38 batch 336 batch_loss: 0.18022093176841736\n",
      "training: 38 batch 337 batch_loss: 0.17823249101638794\n",
      "training: 38 batch 338 batch_loss: 0.17691129446029663\n",
      "training: 38 batch 339 batch_loss: 0.17973768711090088\n",
      "training: 38 batch 340 batch_loss: 0.17677199840545654\n",
      "training: 38 batch 341 batch_loss: 0.1745092272758484\n",
      "training: 38 batch 342 batch_loss: 0.1771780252456665\n",
      "training: 38 batch 343 batch_loss: 0.17734721302986145\n",
      "training: 38 batch 344 batch_loss: 0.17370936274528503\n",
      "training: 38 batch 345 batch_loss: 0.17813828587532043\n",
      "training: 38 batch 346 batch_loss: 0.1746317744255066\n",
      "training: 38 batch 347 batch_loss: 0.17611044645309448\n",
      "training: 38 batch 348 batch_loss: 0.17328780889511108\n",
      "training: 38 batch 349 batch_loss: 0.1819194257259369\n",
      "training: 38 batch 350 batch_loss: 0.17324650287628174\n",
      "training: 38 batch 351 batch_loss: 0.1779271960258484\n",
      "training: 38 batch 352 batch_loss: 0.17441704869270325\n",
      "training: 38 batch 353 batch_loss: 0.17768201231956482\n",
      "training: 38 batch 354 batch_loss: 0.17741504311561584\n",
      "training: 38 batch 355 batch_loss: 0.1756305694580078\n",
      "training: 38 batch 356 batch_loss: 0.1746043562889099\n",
      "training: 38 batch 357 batch_loss: 0.17445242404937744\n",
      "training: 38 batch 358 batch_loss: 0.179618239402771\n",
      "training: 38 batch 359 batch_loss: 0.1738918125629425\n",
      "training: 38 batch 360 batch_loss: 0.1773124635219574\n",
      "training: 38 batch 361 batch_loss: 0.1771448850631714\n",
      "training: 38 batch 362 batch_loss: 0.17654195427894592\n",
      "training: 38 batch 363 batch_loss: 0.17168167233467102\n",
      "training: 38 batch 364 batch_loss: 0.18099427223205566\n",
      "training: 38 batch 365 batch_loss: 0.17805027961730957\n",
      "training: 38 batch 366 batch_loss: 0.17316657304763794\n",
      "training: 38 batch 367 batch_loss: 0.17809131741523743\n",
      "training: 38 batch 368 batch_loss: 0.17625898122787476\n",
      "training: 38 batch 369 batch_loss: 0.1749928593635559\n",
      "training: 38 batch 370 batch_loss: 0.1722370982170105\n",
      "training: 38 batch 371 batch_loss: 0.17604899406433105\n",
      "training: 38 batch 372 batch_loss: 0.1744961142539978\n",
      "training: 38 batch 373 batch_loss: 0.175482839345932\n",
      "training: 38 batch 374 batch_loss: 0.17566585540771484\n",
      "training: 38 batch 375 batch_loss: 0.17436951398849487\n",
      "training: 38 batch 376 batch_loss: 0.17828989028930664\n",
      "training: 38 batch 377 batch_loss: 0.1749298870563507\n",
      "training: 38 batch 378 batch_loss: 0.1796509325504303\n",
      "training: 38 batch 379 batch_loss: 0.1786167323589325\n",
      "training: 38 batch 380 batch_loss: 0.17563790082931519\n",
      "training: 38 batch 381 batch_loss: 0.17777246236801147\n",
      "training: 38 batch 382 batch_loss: 0.17539793252944946\n",
      "training: 38 batch 383 batch_loss: 0.17708131670951843\n",
      "training: 38 batch 384 batch_loss: 0.17714828252792358\n",
      "training: 38 batch 385 batch_loss: 0.1772008240222931\n",
      "training: 38 batch 386 batch_loss: 0.1735565960407257\n",
      "training: 38 batch 387 batch_loss: 0.17719882726669312\n",
      "training: 38 batch 388 batch_loss: 0.17614218592643738\n",
      "training: 38 batch 389 batch_loss: 0.17673680186271667\n",
      "training: 38 batch 390 batch_loss: 0.1763102412223816\n",
      "training: 38 batch 391 batch_loss: 0.1756259799003601\n",
      "training: 38 batch 392 batch_loss: 0.18026593327522278\n",
      "training: 38 batch 393 batch_loss: 0.17528828978538513\n",
      "training: 38 batch 394 batch_loss: 0.1778661608695984\n",
      "training: 38 batch 395 batch_loss: 0.175886332988739\n",
      "training: 38 batch 396 batch_loss: 0.17674827575683594\n",
      "training: 38 batch 397 batch_loss: 0.179193913936615\n",
      "training: 38 batch 398 batch_loss: 0.17385879158973694\n",
      "training: 38 batch 399 batch_loss: 0.17736056447029114\n",
      "training: 38 batch 400 batch_loss: 0.17751950025558472\n",
      "training: 38 batch 401 batch_loss: 0.18004560470581055\n",
      "training: 38 batch 402 batch_loss: 0.17778509855270386\n",
      "training: 38 batch 403 batch_loss: 0.17913749814033508\n",
      "training: 38 batch 404 batch_loss: 0.17824608087539673\n",
      "training: 38 batch 405 batch_loss: 0.17719626426696777\n",
      "training: 38 batch 406 batch_loss: 0.17732387781143188\n",
      "training: 38 batch 407 batch_loss: 0.17944970726966858\n",
      "training: 38 batch 408 batch_loss: 0.1781865954399109\n",
      "training: 38 batch 409 batch_loss: 0.1775214672088623\n",
      "training: 38 batch 410 batch_loss: 0.1753033995628357\n",
      "training: 38 batch 411 batch_loss: 0.17685776948928833\n",
      "training: 38 batch 412 batch_loss: 0.17759236693382263\n",
      "training: 38 batch 413 batch_loss: 0.18140941858291626\n",
      "training: 38 batch 414 batch_loss: 0.17682266235351562\n",
      "training: 38 batch 415 batch_loss: 0.17440718412399292\n",
      "training: 38 batch 416 batch_loss: 0.17745935916900635\n",
      "training: 38 batch 417 batch_loss: 0.1766158640384674\n",
      "training: 38 batch 418 batch_loss: 0.1759195327758789\n",
      "training: 38 batch 419 batch_loss: 0.17833179235458374\n",
      "training: 38 batch 420 batch_loss: 0.17720934748649597\n",
      "training: 38 batch 421 batch_loss: 0.18182885646820068\n",
      "training: 38 batch 422 batch_loss: 0.17389851808547974\n",
      "training: 38 batch 423 batch_loss: 0.1793685257434845\n",
      "training: 38 batch 424 batch_loss: 0.17472293972969055\n",
      "training: 38 batch 425 batch_loss: 0.17611989378929138\n",
      "training: 38 batch 426 batch_loss: 0.1757197380065918\n",
      "training: 38 batch 427 batch_loss: 0.17967015504837036\n",
      "training: 38 batch 428 batch_loss: 0.17763382196426392\n",
      "training: 38 batch 429 batch_loss: 0.17709392309188843\n",
      "training: 38 batch 430 batch_loss: 0.17541834712028503\n",
      "training: 38 batch 431 batch_loss: 0.17627307772636414\n",
      "training: 38 batch 432 batch_loss: 0.17660576105117798\n",
      "training: 38 batch 433 batch_loss: 0.17427507042884827\n",
      "training: 38 batch 434 batch_loss: 0.17722657322883606\n",
      "training: 38 batch 435 batch_loss: 0.17862343788146973\n",
      "training: 38 batch 436 batch_loss: 0.17424196004867554\n",
      "training: 38 batch 437 batch_loss: 0.17605984210968018\n",
      "training: 38 batch 438 batch_loss: 0.1775801181793213\n",
      "training: 38 batch 439 batch_loss: 0.17631012201309204\n",
      "training: 38 batch 440 batch_loss: 0.1757652461528778\n",
      "training: 38 batch 441 batch_loss: 0.17766240239143372\n",
      "training: 38 batch 442 batch_loss: 0.1786119043827057\n",
      "training: 38 batch 443 batch_loss: 0.17319637537002563\n",
      "training: 38 batch 444 batch_loss: 0.17544984817504883\n",
      "training: 38 batch 445 batch_loss: 0.1792272925376892\n",
      "training: 38 batch 446 batch_loss: 0.17942702770233154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 38 batch 447 batch_loss: 0.17619550228118896\n",
      "training: 38 batch 448 batch_loss: 0.17320066690444946\n",
      "training: 38 batch 449 batch_loss: 0.1757734715938568\n",
      "training: 38 batch 450 batch_loss: 0.1784745156764984\n",
      "training: 38 batch 451 batch_loss: 0.17674490809440613\n",
      "training: 38 batch 452 batch_loss: 0.17451149225234985\n",
      "training: 38 batch 453 batch_loss: 0.17683905363082886\n",
      "training: 38 batch 454 batch_loss: 0.17700281739234924\n",
      "training: 38 batch 455 batch_loss: 0.17751213908195496\n",
      "training: 38 batch 456 batch_loss: 0.17983373999595642\n",
      "training: 38 batch 457 batch_loss: 0.17756733298301697\n",
      "training: 38 batch 458 batch_loss: 0.17624035477638245\n",
      "training: 38 batch 459 batch_loss: 0.17538964748382568\n",
      "training: 38 batch 460 batch_loss: 0.17752692103385925\n",
      "training: 38 batch 461 batch_loss: 0.18189510703086853\n",
      "training: 38 batch 462 batch_loss: 0.1789524257183075\n",
      "training: 38 batch 463 batch_loss: 0.17808544635772705\n",
      "training: 38 batch 464 batch_loss: 0.1796264946460724\n",
      "training: 38 batch 465 batch_loss: 0.17412960529327393\n",
      "training: 38 batch 466 batch_loss: 0.1735314130783081\n",
      "training: 38 batch 467 batch_loss: 0.1763424277305603\n",
      "training: 38 batch 468 batch_loss: 0.17723220586776733\n",
      "training: 38 batch 469 batch_loss: 0.17666640877723694\n",
      "training: 38 batch 470 batch_loss: 0.17717301845550537\n",
      "training: 38 batch 471 batch_loss: 0.17634999752044678\n",
      "training: 38 batch 472 batch_loss: 0.17911729216575623\n",
      "training: 38 batch 473 batch_loss: 0.17911303043365479\n",
      "training: 38 batch 474 batch_loss: 0.17726674675941467\n",
      "training: 38 batch 475 batch_loss: 0.17989188432693481\n",
      "training: 38 batch 476 batch_loss: 0.18018540740013123\n",
      "training: 38 batch 477 batch_loss: 0.17505696415901184\n",
      "training: 38 batch 478 batch_loss: 0.17660221457481384\n",
      "training: 38 batch 479 batch_loss: 0.17768415808677673\n",
      "training: 38 batch 480 batch_loss: 0.17717838287353516\n",
      "training: 38 batch 481 batch_loss: 0.17668312788009644\n",
      "training: 38 batch 482 batch_loss: 0.1790875792503357\n",
      "training: 38 batch 483 batch_loss: 0.17723432183265686\n",
      "training: 38 batch 484 batch_loss: 0.17501777410507202\n",
      "training: 38 batch 485 batch_loss: 0.1740405261516571\n",
      "training: 38 batch 486 batch_loss: 0.1783011555671692\n",
      "training: 38 batch 487 batch_loss: 0.17794936895370483\n",
      "training: 38 batch 488 batch_loss: 0.17956528067588806\n",
      "training: 38 batch 489 batch_loss: 0.18025913834571838\n",
      "training: 38 batch 490 batch_loss: 0.17674553394317627\n",
      "training: 38 batch 491 batch_loss: 0.17747539281845093\n",
      "training: 38 batch 492 batch_loss: 0.17861658334732056\n",
      "training: 38 batch 493 batch_loss: 0.1751917600631714\n",
      "training: 38 batch 494 batch_loss: 0.17696672677993774\n",
      "training: 38 batch 495 batch_loss: 0.17875632643699646\n",
      "training: 38 batch 496 batch_loss: 0.17852914333343506\n",
      "training: 38 batch 497 batch_loss: 0.17492103576660156\n",
      "training: 38 batch 498 batch_loss: 0.1791183352470398\n",
      "training: 38 batch 499 batch_loss: 0.1745075285434723\n",
      "training: 38 batch 500 batch_loss: 0.18049830198287964\n",
      "training: 38 batch 501 batch_loss: 0.17682227492332458\n",
      "training: 38 batch 502 batch_loss: 0.1754927933216095\n",
      "training: 38 batch 503 batch_loss: 0.17787954211235046\n",
      "training: 38 batch 504 batch_loss: 0.18009254336357117\n",
      "training: 38 batch 505 batch_loss: 0.17886415123939514\n",
      "training: 38 batch 506 batch_loss: 0.17555755376815796\n",
      "training: 38 batch 507 batch_loss: 0.1804959774017334\n",
      "training: 38 batch 508 batch_loss: 0.18000102043151855\n",
      "training: 38 batch 509 batch_loss: 0.17621591687202454\n",
      "training: 38 batch 510 batch_loss: 0.17925754189491272\n",
      "training: 38 batch 511 batch_loss: 0.18033018708229065\n",
      "training: 38 batch 512 batch_loss: 0.17824217677116394\n",
      "training: 38 batch 513 batch_loss: 0.17640581727027893\n",
      "training: 38 batch 514 batch_loss: 0.1786593198776245\n",
      "training: 38 batch 515 batch_loss: 0.17994806170463562\n",
      "training: 38 batch 516 batch_loss: 0.17639103531837463\n",
      "training: 38 batch 517 batch_loss: 0.17903095483779907\n",
      "training: 38 batch 518 batch_loss: 0.17698407173156738\n",
      "training: 38 batch 519 batch_loss: 0.1754170060157776\n",
      "training: 38 batch 520 batch_loss: 0.180404394865036\n",
      "training: 38 batch 521 batch_loss: 0.175895094871521\n",
      "training: 38 batch 522 batch_loss: 0.1762027144432068\n",
      "training: 38 batch 523 batch_loss: 0.17790192365646362\n",
      "training: 38 batch 524 batch_loss: 0.17798247933387756\n",
      "training: 38 batch 525 batch_loss: 0.17552641034126282\n",
      "training: 38 batch 526 batch_loss: 0.17936161160469055\n",
      "training: 38 batch 527 batch_loss: 0.18122410774230957\n",
      "training: 38 batch 528 batch_loss: 0.17797601222991943\n",
      "training: 38 batch 529 batch_loss: 0.17449486255645752\n",
      "training: 38 batch 530 batch_loss: 0.1752629578113556\n",
      "training: 38 batch 531 batch_loss: 0.17677247524261475\n",
      "training: 38 batch 532 batch_loss: 0.17826330661773682\n",
      "training: 38 batch 533 batch_loss: 0.17459958791732788\n",
      "training: 38 batch 534 batch_loss: 0.1789037585258484\n",
      "training: 38 batch 535 batch_loss: 0.17727282643318176\n",
      "training: 38 batch 536 batch_loss: 0.17968544363975525\n",
      "training: 38 batch 537 batch_loss: 0.17643430829048157\n",
      "training: 38 batch 538 batch_loss: 0.17688798904418945\n",
      "training: 38 batch 539 batch_loss: 0.17647457122802734\n",
      "training: 38 batch 540 batch_loss: 0.17754343152046204\n",
      "training: 38 batch 541 batch_loss: 0.17653268575668335\n",
      "training: 38 batch 542 batch_loss: 0.17696285247802734\n",
      "training: 38 batch 543 batch_loss: 0.17734038829803467\n",
      "training: 38 batch 544 batch_loss: 0.1757713258266449\n",
      "training: 38 batch 545 batch_loss: 0.17601770162582397\n",
      "training: 38 batch 546 batch_loss: 0.17671719193458557\n",
      "training: 38 batch 547 batch_loss: 0.17753562331199646\n",
      "training: 38 batch 548 batch_loss: 0.17543965578079224\n",
      "training: 38 batch 549 batch_loss: 0.1762724220752716\n",
      "training: 38 batch 550 batch_loss: 0.179212749004364\n",
      "training: 38 batch 551 batch_loss: 0.17922818660736084\n",
      "training: 38 batch 552 batch_loss: 0.18049567937850952\n",
      "training: 38 batch 553 batch_loss: 0.1808302402496338\n",
      "training: 38 batch 554 batch_loss: 0.17594683170318604\n",
      "training: 38 batch 555 batch_loss: 0.17805910110473633\n",
      "training: 38 batch 556 batch_loss: 0.17793861031532288\n",
      "training: 38 batch 557 batch_loss: 0.17610257863998413\n",
      "training: 38 batch 558 batch_loss: 0.17805200815200806\n",
      "training: 38 batch 559 batch_loss: 0.1766653060913086\n",
      "training: 38 batch 560 batch_loss: 0.17825859785079956\n",
      "training: 38 batch 561 batch_loss: 0.17601057887077332\n",
      "training: 38 batch 562 batch_loss: 0.17902520298957825\n",
      "training: 38 batch 563 batch_loss: 0.17576217651367188\n",
      "training: 38 batch 564 batch_loss: 0.17827433347702026\n",
      "training: 38 batch 565 batch_loss: 0.1782810389995575\n",
      "training: 38 batch 566 batch_loss: 0.1779496669769287\n",
      "training: 38 batch 567 batch_loss: 0.1784462034702301\n",
      "training: 38 batch 568 batch_loss: 0.17898613214492798\n",
      "training: 38 batch 569 batch_loss: 0.17722147703170776\n",
      "training: 38 batch 570 batch_loss: 0.17731353640556335\n",
      "training: 38 batch 571 batch_loss: 0.17986911535263062\n",
      "training: 38 batch 572 batch_loss: 0.17484569549560547\n",
      "training: 38 batch 573 batch_loss: 0.175834059715271\n",
      "training: 38 batch 574 batch_loss: 0.17840272188186646\n",
      "training: 38 batch 575 batch_loss: 0.17751044034957886\n",
      "training: 38 batch 576 batch_loss: 0.17558875679969788\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 38, Hit Ratio:0.03119992804629022 | Precision:0.04603361840165143 | Recall:0.06105512710452418 | NDCG:0.059088839669294624\n",
      "*Best Performance* \n",
      "Epoch: 38, Hit Ratio:0.03119992804629022 | Precision:0.04603361840165143 | Recall:0.06105512710452418 | MDCG:0.059088839669294624\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 39 batch 0 batch_loss: 0.1768554449081421\n",
      "training: 39 batch 1 batch_loss: 0.1736416220664978\n",
      "training: 39 batch 2 batch_loss: 0.1773032546043396\n",
      "training: 39 batch 3 batch_loss: 0.17018961906433105\n",
      "training: 39 batch 4 batch_loss: 0.17523440718650818\n",
      "training: 39 batch 5 batch_loss: 0.17468926310539246\n",
      "training: 39 batch 6 batch_loss: 0.17742714285850525\n",
      "training: 39 batch 7 batch_loss: 0.17637276649475098\n",
      "training: 39 batch 8 batch_loss: 0.17717218399047852\n",
      "training: 39 batch 9 batch_loss: 0.17740494012832642\n",
      "training: 39 batch 10 batch_loss: 0.17831867933273315\n",
      "training: 39 batch 11 batch_loss: 0.18066442012786865\n",
      "training: 39 batch 12 batch_loss: 0.17554950714111328\n",
      "training: 39 batch 13 batch_loss: 0.17384549975395203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 batch 14 batch_loss: 0.17245250940322876\n",
      "training: 39 batch 15 batch_loss: 0.178143709897995\n",
      "training: 39 batch 16 batch_loss: 0.17554602026939392\n",
      "training: 39 batch 17 batch_loss: 0.1765444576740265\n",
      "training: 39 batch 18 batch_loss: 0.18264862895011902\n",
      "training: 39 batch 19 batch_loss: 0.17403137683868408\n",
      "training: 39 batch 20 batch_loss: 0.175682932138443\n",
      "training: 39 batch 21 batch_loss: 0.1773928701877594\n",
      "training: 39 batch 22 batch_loss: 0.17283663153648376\n",
      "training: 39 batch 23 batch_loss: 0.1757553219795227\n",
      "training: 39 batch 24 batch_loss: 0.17593449354171753\n",
      "training: 39 batch 25 batch_loss: 0.1766280233860016\n",
      "training: 39 batch 26 batch_loss: 0.1788535714149475\n",
      "training: 39 batch 27 batch_loss: 0.1801483929157257\n",
      "training: 39 batch 28 batch_loss: 0.17613524198532104\n",
      "training: 39 batch 29 batch_loss: 0.17625221610069275\n",
      "training: 39 batch 30 batch_loss: 0.1745702028274536\n",
      "training: 39 batch 31 batch_loss: 0.1763302981853485\n",
      "training: 39 batch 32 batch_loss: 0.1758287250995636\n",
      "training: 39 batch 33 batch_loss: 0.17828524112701416\n",
      "training: 39 batch 34 batch_loss: 0.17916777729988098\n",
      "training: 39 batch 35 batch_loss: 0.17925265431404114\n",
      "training: 39 batch 36 batch_loss: 0.17551058530807495\n",
      "training: 39 batch 37 batch_loss: 0.1794823408126831\n",
      "training: 39 batch 38 batch_loss: 0.17615899443626404\n",
      "training: 39 batch 39 batch_loss: 0.17495682835578918\n",
      "training: 39 batch 40 batch_loss: 0.17773044109344482\n",
      "training: 39 batch 41 batch_loss: 0.18121278285980225\n",
      "training: 39 batch 42 batch_loss: 0.17527270317077637\n",
      "training: 39 batch 43 batch_loss: 0.1737024486064911\n",
      "training: 39 batch 44 batch_loss: 0.17396163940429688\n",
      "training: 39 batch 45 batch_loss: 0.1789751648902893\n",
      "training: 39 batch 46 batch_loss: 0.1765807569026947\n",
      "training: 39 batch 47 batch_loss: 0.1787954568862915\n",
      "training: 39 batch 48 batch_loss: 0.1769617795944214\n",
      "training: 39 batch 49 batch_loss: 0.17728349566459656\n",
      "training: 39 batch 50 batch_loss: 0.1726962924003601\n",
      "training: 39 batch 51 batch_loss: 0.17453110218048096\n",
      "training: 39 batch 52 batch_loss: 0.17917737364768982\n",
      "training: 39 batch 53 batch_loss: 0.17567527294158936\n",
      "training: 39 batch 54 batch_loss: 0.1771329641342163\n",
      "training: 39 batch 55 batch_loss: 0.17643412947654724\n",
      "training: 39 batch 56 batch_loss: 0.17433801293373108\n",
      "training: 39 batch 57 batch_loss: 0.17395243048667908\n",
      "training: 39 batch 58 batch_loss: 0.17488974332809448\n",
      "training: 39 batch 59 batch_loss: 0.17925024032592773\n",
      "training: 39 batch 60 batch_loss: 0.17660963535308838\n",
      "training: 39 batch 61 batch_loss: 0.1778402328491211\n",
      "training: 39 batch 62 batch_loss: 0.17431145906448364\n",
      "training: 39 batch 63 batch_loss: 0.1784905195236206\n",
      "training: 39 batch 64 batch_loss: 0.17489248514175415\n",
      "training: 39 batch 65 batch_loss: 0.17598223686218262\n",
      "training: 39 batch 66 batch_loss: 0.1784578263759613\n",
      "training: 39 batch 67 batch_loss: 0.17620205879211426\n",
      "training: 39 batch 68 batch_loss: 0.17710605263710022\n",
      "training: 39 batch 69 batch_loss: 0.1747274100780487\n",
      "training: 39 batch 70 batch_loss: 0.17473602294921875\n",
      "training: 39 batch 71 batch_loss: 0.17676717042922974\n",
      "training: 39 batch 72 batch_loss: 0.17289388179779053\n",
      "training: 39 batch 73 batch_loss: 0.17635777592658997\n",
      "training: 39 batch 74 batch_loss: 0.17676421999931335\n",
      "training: 39 batch 75 batch_loss: 0.17575669288635254\n",
      "training: 39 batch 76 batch_loss: 0.1756783127784729\n",
      "training: 39 batch 77 batch_loss: 0.17169639468193054\n",
      "training: 39 batch 78 batch_loss: 0.17542558908462524\n",
      "training: 39 batch 79 batch_loss: 0.1744329035282135\n",
      "training: 39 batch 80 batch_loss: 0.17476671934127808\n",
      "training: 39 batch 81 batch_loss: 0.17976200580596924\n",
      "training: 39 batch 82 batch_loss: 0.17457860708236694\n",
      "training: 39 batch 83 batch_loss: 0.1762726604938507\n",
      "training: 39 batch 84 batch_loss: 0.17571529746055603\n",
      "training: 39 batch 85 batch_loss: 0.1754978895187378\n",
      "training: 39 batch 86 batch_loss: 0.17520469427108765\n",
      "training: 39 batch 87 batch_loss: 0.17056608200073242\n",
      "training: 39 batch 88 batch_loss: 0.17550033330917358\n",
      "training: 39 batch 89 batch_loss: 0.1787278652191162\n",
      "training: 39 batch 90 batch_loss: 0.17668208479881287\n",
      "training: 39 batch 91 batch_loss: 0.176824688911438\n",
      "training: 39 batch 92 batch_loss: 0.17685973644256592\n",
      "training: 39 batch 93 batch_loss: 0.18168702721595764\n",
      "training: 39 batch 94 batch_loss: 0.1801549792289734\n",
      "training: 39 batch 95 batch_loss: 0.17872774600982666\n",
      "training: 39 batch 96 batch_loss: 0.1749286651611328\n",
      "training: 39 batch 97 batch_loss: 0.17921531200408936\n",
      "training: 39 batch 98 batch_loss: 0.17753368616104126\n",
      "training: 39 batch 99 batch_loss: 0.179082453250885\n",
      "training: 39 batch 100 batch_loss: 0.1771939992904663\n",
      "training: 39 batch 101 batch_loss: 0.177507221698761\n",
      "training: 39 batch 102 batch_loss: 0.17526677250862122\n",
      "training: 39 batch 103 batch_loss: 0.17840513586997986\n",
      "training: 39 batch 104 batch_loss: 0.17252200841903687\n",
      "training: 39 batch 105 batch_loss: 0.17880725860595703\n",
      "training: 39 batch 106 batch_loss: 0.1750989556312561\n",
      "training: 39 batch 107 batch_loss: 0.17693141102790833\n",
      "training: 39 batch 108 batch_loss: 0.17294836044311523\n",
      "training: 39 batch 109 batch_loss: 0.1766122281551361\n",
      "training: 39 batch 110 batch_loss: 0.1750919222831726\n",
      "training: 39 batch 111 batch_loss: 0.17503705620765686\n",
      "training: 39 batch 112 batch_loss: 0.17793715000152588\n",
      "training: 39 batch 113 batch_loss: 0.17522254586219788\n",
      "training: 39 batch 114 batch_loss: 0.17505010962486267\n",
      "training: 39 batch 115 batch_loss: 0.1772083044052124\n",
      "training: 39 batch 116 batch_loss: 0.17560884356498718\n",
      "training: 39 batch 117 batch_loss: 0.18077844381332397\n",
      "training: 39 batch 118 batch_loss: 0.17691203951835632\n",
      "training: 39 batch 119 batch_loss: 0.17765948176383972\n",
      "training: 39 batch 120 batch_loss: 0.17679473757743835\n",
      "training: 39 batch 121 batch_loss: 0.17724394798278809\n",
      "training: 39 batch 122 batch_loss: 0.17590883374214172\n",
      "training: 39 batch 123 batch_loss: 0.17419302463531494\n",
      "training: 39 batch 124 batch_loss: 0.1798677146434784\n",
      "training: 39 batch 125 batch_loss: 0.17969614267349243\n",
      "training: 39 batch 126 batch_loss: 0.1828719973564148\n",
      "training: 39 batch 127 batch_loss: 0.17681676149368286\n",
      "training: 39 batch 128 batch_loss: 0.1730530858039856\n",
      "training: 39 batch 129 batch_loss: 0.17877089977264404\n",
      "training: 39 batch 130 batch_loss: 0.17900490760803223\n",
      "training: 39 batch 131 batch_loss: 0.17631304264068604\n",
      "training: 39 batch 132 batch_loss: 0.17600885033607483\n",
      "training: 39 batch 133 batch_loss: 0.17644929885864258\n",
      "training: 39 batch 134 batch_loss: 0.17516860365867615\n",
      "training: 39 batch 135 batch_loss: 0.18036213517189026\n",
      "training: 39 batch 136 batch_loss: 0.17895156145095825\n",
      "training: 39 batch 137 batch_loss: 0.179461270570755\n",
      "training: 39 batch 138 batch_loss: 0.17827385663986206\n",
      "training: 39 batch 139 batch_loss: 0.17968285083770752\n",
      "training: 39 batch 140 batch_loss: 0.18069079518318176\n",
      "training: 39 batch 141 batch_loss: 0.1767711043357849\n",
      "training: 39 batch 142 batch_loss: 0.17565065622329712\n",
      "training: 39 batch 143 batch_loss: 0.17820385098457336\n",
      "training: 39 batch 144 batch_loss: 0.17751556634902954\n",
      "training: 39 batch 145 batch_loss: 0.17916274070739746\n",
      "training: 39 batch 146 batch_loss: 0.17598500847816467\n",
      "training: 39 batch 147 batch_loss: 0.1729346215724945\n",
      "training: 39 batch 148 batch_loss: 0.1775837540626526\n",
      "training: 39 batch 149 batch_loss: 0.17637649178504944\n",
      "training: 39 batch 150 batch_loss: 0.1792285144329071\n",
      "training: 39 batch 151 batch_loss: 0.17627623677253723\n",
      "training: 39 batch 152 batch_loss: 0.17668157815933228\n",
      "training: 39 batch 153 batch_loss: 0.1775132417678833\n",
      "training: 39 batch 154 batch_loss: 0.17682063579559326\n",
      "training: 39 batch 155 batch_loss: 0.17712008953094482\n",
      "training: 39 batch 156 batch_loss: 0.1776379942893982\n",
      "training: 39 batch 157 batch_loss: 0.1754557192325592\n",
      "training: 39 batch 158 batch_loss: 0.18001803755760193\n",
      "training: 39 batch 159 batch_loss: 0.17717525362968445\n",
      "training: 39 batch 160 batch_loss: 0.1764257252216339\n",
      "training: 39 batch 161 batch_loss: 0.1733168363571167\n",
      "training: 39 batch 162 batch_loss: 0.17601817846298218\n",
      "training: 39 batch 163 batch_loss: 0.17990809679031372\n",
      "training: 39 batch 164 batch_loss: 0.17699766159057617\n",
      "training: 39 batch 165 batch_loss: 0.17687809467315674\n",
      "training: 39 batch 166 batch_loss: 0.17888832092285156\n",
      "training: 39 batch 167 batch_loss: 0.17913323640823364\n",
      "training: 39 batch 168 batch_loss: 0.17645788192749023\n",
      "training: 39 batch 169 batch_loss: 0.17688298225402832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 batch 170 batch_loss: 0.17654019594192505\n",
      "training: 39 batch 171 batch_loss: 0.18165883421897888\n",
      "training: 39 batch 172 batch_loss: 0.17769786715507507\n",
      "training: 39 batch 173 batch_loss: 0.17577961087226868\n",
      "training: 39 batch 174 batch_loss: 0.1784769594669342\n",
      "training: 39 batch 175 batch_loss: 0.17381709814071655\n",
      "training: 39 batch 176 batch_loss: 0.18169337511062622\n",
      "training: 39 batch 177 batch_loss: 0.1757660210132599\n",
      "training: 39 batch 178 batch_loss: 0.17846640944480896\n",
      "training: 39 batch 179 batch_loss: 0.17536604404449463\n",
      "training: 39 batch 180 batch_loss: 0.17523348331451416\n",
      "training: 39 batch 181 batch_loss: 0.17332646250724792\n",
      "training: 39 batch 182 batch_loss: 0.17657190561294556\n",
      "training: 39 batch 183 batch_loss: 0.17647725343704224\n",
      "training: 39 batch 184 batch_loss: 0.17835798859596252\n",
      "training: 39 batch 185 batch_loss: 0.17836564779281616\n",
      "training: 39 batch 186 batch_loss: 0.17836636304855347\n",
      "training: 39 batch 187 batch_loss: 0.17332172393798828\n",
      "training: 39 batch 188 batch_loss: 0.17911523580551147\n",
      "training: 39 batch 189 batch_loss: 0.17862927913665771\n",
      "training: 39 batch 190 batch_loss: 0.17463228106498718\n",
      "training: 39 batch 191 batch_loss: 0.17610707879066467\n",
      "training: 39 batch 192 batch_loss: 0.18191921710968018\n",
      "training: 39 batch 193 batch_loss: 0.17623239755630493\n",
      "training: 39 batch 194 batch_loss: 0.17490693926811218\n",
      "training: 39 batch 195 batch_loss: 0.17845696210861206\n",
      "training: 39 batch 196 batch_loss: 0.1769065260887146\n",
      "training: 39 batch 197 batch_loss: 0.17576870322227478\n",
      "training: 39 batch 198 batch_loss: 0.17481005191802979\n",
      "training: 39 batch 199 batch_loss: 0.17880740761756897\n",
      "training: 39 batch 200 batch_loss: 0.1804307997226715\n",
      "training: 39 batch 201 batch_loss: 0.17602324485778809\n",
      "training: 39 batch 202 batch_loss: 0.17571744322776794\n",
      "training: 39 batch 203 batch_loss: 0.17635592818260193\n",
      "training: 39 batch 204 batch_loss: 0.17788183689117432\n",
      "training: 39 batch 205 batch_loss: 0.17912420630455017\n",
      "training: 39 batch 206 batch_loss: 0.1761091947555542\n",
      "training: 39 batch 207 batch_loss: 0.1783660352230072\n",
      "training: 39 batch 208 batch_loss: 0.17975032329559326\n",
      "training: 39 batch 209 batch_loss: 0.17675301432609558\n",
      "training: 39 batch 210 batch_loss: 0.17922908067703247\n",
      "training: 39 batch 211 batch_loss: 0.17992523312568665\n",
      "training: 39 batch 212 batch_loss: 0.17715036869049072\n",
      "training: 39 batch 213 batch_loss: 0.17918145656585693\n",
      "training: 39 batch 214 batch_loss: 0.1784430742263794\n",
      "training: 39 batch 215 batch_loss: 0.1789640486240387\n",
      "training: 39 batch 216 batch_loss: 0.1774112582206726\n",
      "training: 39 batch 217 batch_loss: 0.18073615431785583\n",
      "training: 39 batch 218 batch_loss: 0.17350924015045166\n",
      "training: 39 batch 219 batch_loss: 0.1747066080570221\n",
      "training: 39 batch 220 batch_loss: 0.17715877294540405\n",
      "training: 39 batch 221 batch_loss: 0.1773640215396881\n",
      "training: 39 batch 222 batch_loss: 0.1774888038635254\n",
      "training: 39 batch 223 batch_loss: 0.17400777339935303\n",
      "training: 39 batch 224 batch_loss: 0.18101340532302856\n",
      "training: 39 batch 225 batch_loss: 0.17894691228866577\n",
      "training: 39 batch 226 batch_loss: 0.17689377069473267\n",
      "training: 39 batch 227 batch_loss: 0.17737820744514465\n",
      "training: 39 batch 228 batch_loss: 0.17510521411895752\n",
      "training: 39 batch 229 batch_loss: 0.18361350893974304\n",
      "training: 39 batch 230 batch_loss: 0.1783125102519989\n",
      "training: 39 batch 231 batch_loss: 0.17525768280029297\n",
      "training: 39 batch 232 batch_loss: 0.17576032876968384\n",
      "training: 39 batch 233 batch_loss: 0.176272451877594\n",
      "training: 39 batch 234 batch_loss: 0.17747795581817627\n",
      "training: 39 batch 235 batch_loss: 0.17877519130706787\n",
      "training: 39 batch 236 batch_loss: 0.17533749341964722\n",
      "training: 39 batch 237 batch_loss: 0.17643150687217712\n",
      "training: 39 batch 238 batch_loss: 0.17797255516052246\n",
      "training: 39 batch 239 batch_loss: 0.18241888284683228\n",
      "training: 39 batch 240 batch_loss: 0.18019944429397583\n",
      "training: 39 batch 241 batch_loss: 0.17919450998306274\n",
      "training: 39 batch 242 batch_loss: 0.1768220067024231\n",
      "training: 39 batch 243 batch_loss: 0.1804160475730896\n",
      "training: 39 batch 244 batch_loss: 0.1763906478881836\n",
      "training: 39 batch 245 batch_loss: 0.17753219604492188\n",
      "training: 39 batch 246 batch_loss: 0.17421579360961914\n",
      "training: 39 batch 247 batch_loss: 0.1758367419242859\n",
      "training: 39 batch 248 batch_loss: 0.17778444290161133\n",
      "training: 39 batch 249 batch_loss: 0.18185016512870789\n",
      "training: 39 batch 250 batch_loss: 0.1782326102256775\n",
      "training: 39 batch 251 batch_loss: 0.17869028449058533\n",
      "training: 39 batch 252 batch_loss: 0.17638856172561646\n",
      "training: 39 batch 253 batch_loss: 0.17715919017791748\n",
      "training: 39 batch 254 batch_loss: 0.1790415644645691\n",
      "training: 39 batch 255 batch_loss: 0.17907783389091492\n",
      "training: 39 batch 256 batch_loss: 0.17930933833122253\n",
      "training: 39 batch 257 batch_loss: 0.17843663692474365\n",
      "training: 39 batch 258 batch_loss: 0.17757576704025269\n",
      "training: 39 batch 259 batch_loss: 0.17508846521377563\n",
      "training: 39 batch 260 batch_loss: 0.1766071915626526\n",
      "training: 39 batch 261 batch_loss: 0.1780289113521576\n",
      "training: 39 batch 262 batch_loss: 0.17840778827667236\n",
      "training: 39 batch 263 batch_loss: 0.1759617030620575\n",
      "training: 39 batch 264 batch_loss: 0.17905759811401367\n",
      "training: 39 batch 265 batch_loss: 0.17520523071289062\n",
      "training: 39 batch 266 batch_loss: 0.17777493596076965\n",
      "training: 39 batch 267 batch_loss: 0.17995405197143555\n",
      "training: 39 batch 268 batch_loss: 0.17622452974319458\n",
      "training: 39 batch 269 batch_loss: 0.17762327194213867\n",
      "training: 39 batch 270 batch_loss: 0.18093398213386536\n",
      "training: 39 batch 271 batch_loss: 0.1786162555217743\n",
      "training: 39 batch 272 batch_loss: 0.17938300967216492\n",
      "training: 39 batch 273 batch_loss: 0.18334552645683289\n",
      "training: 39 batch 274 batch_loss: 0.18127012252807617\n",
      "training: 39 batch 275 batch_loss: 0.17572784423828125\n",
      "training: 39 batch 276 batch_loss: 0.18069294095039368\n",
      "training: 39 batch 277 batch_loss: 0.17929980158805847\n",
      "training: 39 batch 278 batch_loss: 0.17610126733779907\n",
      "training: 39 batch 279 batch_loss: 0.17715507745742798\n",
      "training: 39 batch 280 batch_loss: 0.17573216557502747\n",
      "training: 39 batch 281 batch_loss: 0.18311291933059692\n",
      "training: 39 batch 282 batch_loss: 0.1752917468547821\n",
      "training: 39 batch 283 batch_loss: 0.17513954639434814\n",
      "training: 39 batch 284 batch_loss: 0.17840451002120972\n",
      "training: 39 batch 285 batch_loss: 0.17492839694023132\n",
      "training: 39 batch 286 batch_loss: 0.1795612871646881\n",
      "training: 39 batch 287 batch_loss: 0.1763460338115692\n",
      "training: 39 batch 288 batch_loss: 0.1760740876197815\n",
      "training: 39 batch 289 batch_loss: 0.1818690299987793\n",
      "training: 39 batch 290 batch_loss: 0.18004554510116577\n",
      "training: 39 batch 291 batch_loss: 0.1783837080001831\n",
      "training: 39 batch 292 batch_loss: 0.18156510591506958\n",
      "training: 39 batch 293 batch_loss: 0.17651230096817017\n",
      "training: 39 batch 294 batch_loss: 0.17649847269058228\n",
      "training: 39 batch 295 batch_loss: 0.17656707763671875\n",
      "training: 39 batch 296 batch_loss: 0.17994004487991333\n",
      "training: 39 batch 297 batch_loss: 0.1778707206249237\n",
      "training: 39 batch 298 batch_loss: 0.17769694328308105\n",
      "training: 39 batch 299 batch_loss: 0.17721635103225708\n",
      "training: 39 batch 300 batch_loss: 0.17552995681762695\n",
      "training: 39 batch 301 batch_loss: 0.17755529284477234\n",
      "training: 39 batch 302 batch_loss: 0.177554190158844\n",
      "training: 39 batch 303 batch_loss: 0.1782810389995575\n",
      "training: 39 batch 304 batch_loss: 0.17860689759254456\n",
      "training: 39 batch 305 batch_loss: 0.17719930410385132\n",
      "training: 39 batch 306 batch_loss: 0.17600494623184204\n",
      "training: 39 batch 307 batch_loss: 0.17671683430671692\n",
      "training: 39 batch 308 batch_loss: 0.1763629913330078\n",
      "training: 39 batch 309 batch_loss: 0.17820671200752258\n",
      "training: 39 batch 310 batch_loss: 0.1767689287662506\n",
      "training: 39 batch 311 batch_loss: 0.17560657858848572\n",
      "training: 39 batch 312 batch_loss: 0.17711201310157776\n",
      "training: 39 batch 313 batch_loss: 0.17905959486961365\n",
      "training: 39 batch 314 batch_loss: 0.17613855004310608\n",
      "training: 39 batch 315 batch_loss: 0.1750166118144989\n",
      "training: 39 batch 316 batch_loss: 0.17893505096435547\n",
      "training: 39 batch 317 batch_loss: 0.17796239256858826\n",
      "training: 39 batch 318 batch_loss: 0.1788853406906128\n",
      "training: 39 batch 319 batch_loss: 0.17930644750595093\n",
      "training: 39 batch 320 batch_loss: 0.17744237184524536\n",
      "training: 39 batch 321 batch_loss: 0.17657536268234253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 batch 322 batch_loss: 0.17861893773078918\n",
      "training: 39 batch 323 batch_loss: 0.1789134442806244\n",
      "training: 39 batch 324 batch_loss: 0.1793334186077118\n",
      "training: 39 batch 325 batch_loss: 0.17602264881134033\n",
      "training: 39 batch 326 batch_loss: 0.1763024926185608\n",
      "training: 39 batch 327 batch_loss: 0.1782892346382141\n",
      "training: 39 batch 328 batch_loss: 0.17979320883750916\n",
      "training: 39 batch 329 batch_loss: 0.17756232619285583\n",
      "training: 39 batch 330 batch_loss: 0.17744803428649902\n",
      "training: 39 batch 331 batch_loss: 0.17755591869354248\n",
      "training: 39 batch 332 batch_loss: 0.17971107363700867\n",
      "training: 39 batch 333 batch_loss: 0.17921966314315796\n",
      "training: 39 batch 334 batch_loss: 0.1773127019405365\n",
      "training: 39 batch 335 batch_loss: 0.17849195003509521\n",
      "training: 39 batch 336 batch_loss: 0.1780230700969696\n",
      "training: 39 batch 337 batch_loss: 0.1776958405971527\n",
      "training: 39 batch 338 batch_loss: 0.17547643184661865\n",
      "training: 39 batch 339 batch_loss: 0.1836426854133606\n",
      "training: 39 batch 340 batch_loss: 0.17533469200134277\n",
      "training: 39 batch 341 batch_loss: 0.17514964938163757\n",
      "training: 39 batch 342 batch_loss: 0.18096041679382324\n",
      "training: 39 batch 343 batch_loss: 0.17518877983093262\n",
      "training: 39 batch 344 batch_loss: 0.17984259128570557\n",
      "training: 39 batch 345 batch_loss: 0.17746949195861816\n",
      "training: 39 batch 346 batch_loss: 0.17807626724243164\n",
      "training: 39 batch 347 batch_loss: 0.17572718858718872\n",
      "training: 39 batch 348 batch_loss: 0.17665013670921326\n",
      "training: 39 batch 349 batch_loss: 0.17674517631530762\n",
      "training: 39 batch 350 batch_loss: 0.17663201689720154\n",
      "training: 39 batch 351 batch_loss: 0.1785215437412262\n",
      "training: 39 batch 352 batch_loss: 0.17564606666564941\n",
      "training: 39 batch 353 batch_loss: 0.1785968542098999\n",
      "training: 39 batch 354 batch_loss: 0.18047454953193665\n",
      "training: 39 batch 355 batch_loss: 0.18446698784828186\n",
      "training: 39 batch 356 batch_loss: 0.17676687240600586\n",
      "training: 39 batch 357 batch_loss: 0.18072310090065002\n",
      "training: 39 batch 358 batch_loss: 0.17970138788223267\n",
      "training: 39 batch 359 batch_loss: 0.18033361434936523\n",
      "training: 39 batch 360 batch_loss: 0.17734447121620178\n",
      "training: 39 batch 361 batch_loss: 0.17972251772880554\n",
      "training: 39 batch 362 batch_loss: 0.17775657773017883\n",
      "training: 39 batch 363 batch_loss: 0.17972338199615479\n",
      "training: 39 batch 364 batch_loss: 0.17672038078308105\n",
      "training: 39 batch 365 batch_loss: 0.17813703417778015\n",
      "training: 39 batch 366 batch_loss: 0.1813746988773346\n",
      "training: 39 batch 367 batch_loss: 0.17655038833618164\n",
      "training: 39 batch 368 batch_loss: 0.17884579300880432\n",
      "training: 39 batch 369 batch_loss: 0.1810513436794281\n",
      "training: 39 batch 370 batch_loss: 0.1769421398639679\n",
      "training: 39 batch 371 batch_loss: 0.18020477890968323\n",
      "training: 39 batch 372 batch_loss: 0.17991799116134644\n",
      "training: 39 batch 373 batch_loss: 0.1757577359676361\n",
      "training: 39 batch 374 batch_loss: 0.1725751757621765\n",
      "training: 39 batch 375 batch_loss: 0.17826485633850098\n",
      "training: 39 batch 376 batch_loss: 0.1745864748954773\n",
      "training: 39 batch 377 batch_loss: 0.18129348754882812\n",
      "training: 39 batch 378 batch_loss: 0.1762244701385498\n",
      "training: 39 batch 379 batch_loss: 0.17726415395736694\n",
      "training: 39 batch 380 batch_loss: 0.18065959215164185\n",
      "training: 39 batch 381 batch_loss: 0.1776767373085022\n",
      "training: 39 batch 382 batch_loss: 0.177265465259552\n",
      "training: 39 batch 383 batch_loss: 0.1796831488609314\n",
      "training: 39 batch 384 batch_loss: 0.1794833540916443\n",
      "training: 39 batch 385 batch_loss: 0.17865639925003052\n",
      "training: 39 batch 386 batch_loss: 0.17669358849525452\n",
      "training: 39 batch 387 batch_loss: 0.17285066843032837\n",
      "training: 39 batch 388 batch_loss: 0.1757603883743286\n",
      "training: 39 batch 389 batch_loss: 0.1770254373550415\n",
      "training: 39 batch 390 batch_loss: 0.17912563681602478\n",
      "training: 39 batch 391 batch_loss: 0.17883235216140747\n",
      "training: 39 batch 392 batch_loss: 0.17579835653305054\n",
      "training: 39 batch 393 batch_loss: 0.1795244812965393\n",
      "training: 39 batch 394 batch_loss: 0.17925900220870972\n",
      "training: 39 batch 395 batch_loss: 0.1785600483417511\n",
      "training: 39 batch 396 batch_loss: 0.1792466640472412\n",
      "training: 39 batch 397 batch_loss: 0.17955291271209717\n",
      "training: 39 batch 398 batch_loss: 0.17816543579101562\n",
      "training: 39 batch 399 batch_loss: 0.17705830931663513\n",
      "training: 39 batch 400 batch_loss: 0.1784428060054779\n",
      "training: 39 batch 401 batch_loss: 0.17754852771759033\n",
      "training: 39 batch 402 batch_loss: 0.1780570149421692\n",
      "training: 39 batch 403 batch_loss: 0.18142437934875488\n",
      "training: 39 batch 404 batch_loss: 0.17930588126182556\n",
      "training: 39 batch 405 batch_loss: 0.176223486661911\n",
      "training: 39 batch 406 batch_loss: 0.17495229840278625\n",
      "training: 39 batch 407 batch_loss: 0.18040227890014648\n",
      "training: 39 batch 408 batch_loss: 0.17691272497177124\n",
      "training: 39 batch 409 batch_loss: 0.1775970757007599\n",
      "training: 39 batch 410 batch_loss: 0.17551419138908386\n",
      "training: 39 batch 411 batch_loss: 0.17767715454101562\n",
      "training: 39 batch 412 batch_loss: 0.17747530341148376\n",
      "training: 39 batch 413 batch_loss: 0.17602810263633728\n",
      "training: 39 batch 414 batch_loss: 0.17494559288024902\n",
      "training: 39 batch 415 batch_loss: 0.18090322613716125\n",
      "training: 39 batch 416 batch_loss: 0.1768646240234375\n",
      "training: 39 batch 417 batch_loss: 0.17865097522735596\n",
      "training: 39 batch 418 batch_loss: 0.1744002103805542\n",
      "training: 39 batch 419 batch_loss: 0.17488929629325867\n",
      "training: 39 batch 420 batch_loss: 0.1781192421913147\n",
      "training: 39 batch 421 batch_loss: 0.17823949456214905\n",
      "training: 39 batch 422 batch_loss: 0.1786579191684723\n",
      "training: 39 batch 423 batch_loss: 0.17738017439842224\n",
      "training: 39 batch 424 batch_loss: 0.17820101976394653\n",
      "training: 39 batch 425 batch_loss: 0.1751146912574768\n",
      "training: 39 batch 426 batch_loss: 0.17805665731430054\n",
      "training: 39 batch 427 batch_loss: 0.17588943243026733\n",
      "training: 39 batch 428 batch_loss: 0.1801566183567047\n",
      "training: 39 batch 429 batch_loss: 0.1799321174621582\n",
      "training: 39 batch 430 batch_loss: 0.18146902322769165\n",
      "training: 39 batch 431 batch_loss: 0.17914029955863953\n",
      "training: 39 batch 432 batch_loss: 0.17767715454101562\n",
      "training: 39 batch 433 batch_loss: 0.1757926642894745\n",
      "training: 39 batch 434 batch_loss: 0.18030664324760437\n",
      "training: 39 batch 435 batch_loss: 0.1801188886165619\n",
      "training: 39 batch 436 batch_loss: 0.17928922176361084\n",
      "training: 39 batch 437 batch_loss: 0.1722448170185089\n",
      "training: 39 batch 438 batch_loss: 0.17919057607650757\n",
      "training: 39 batch 439 batch_loss: 0.17956089973449707\n",
      "training: 39 batch 440 batch_loss: 0.18528977036476135\n",
      "training: 39 batch 441 batch_loss: 0.17864298820495605\n",
      "training: 39 batch 442 batch_loss: 0.18021485209465027\n",
      "training: 39 batch 443 batch_loss: 0.17753177881240845\n",
      "training: 39 batch 444 batch_loss: 0.17964670062065125\n",
      "training: 39 batch 445 batch_loss: 0.17821869254112244\n",
      "training: 39 batch 446 batch_loss: 0.17536917328834534\n",
      "training: 39 batch 447 batch_loss: 0.1762949526309967\n",
      "training: 39 batch 448 batch_loss: 0.1792597770690918\n",
      "training: 39 batch 449 batch_loss: 0.17807206511497498\n",
      "training: 39 batch 450 batch_loss: 0.17873278260231018\n",
      "training: 39 batch 451 batch_loss: 0.1789523959159851\n",
      "training: 39 batch 452 batch_loss: 0.1808311641216278\n",
      "training: 39 batch 453 batch_loss: 0.17757809162139893\n",
      "training: 39 batch 454 batch_loss: 0.17793616652488708\n",
      "training: 39 batch 455 batch_loss: 0.1784505546092987\n",
      "training: 39 batch 456 batch_loss: 0.17997083067893982\n",
      "training: 39 batch 457 batch_loss: 0.1786213517189026\n",
      "training: 39 batch 458 batch_loss: 0.17825248837471008\n",
      "training: 39 batch 459 batch_loss: 0.17641094326972961\n",
      "training: 39 batch 460 batch_loss: 0.17985853552818298\n",
      "training: 39 batch 461 batch_loss: 0.1768750548362732\n",
      "training: 39 batch 462 batch_loss: 0.1775846779346466\n",
      "training: 39 batch 463 batch_loss: 0.17960748076438904\n",
      "training: 39 batch 464 batch_loss: 0.17444327473640442\n",
      "training: 39 batch 465 batch_loss: 0.18037950992584229\n",
      "training: 39 batch 466 batch_loss: 0.1813359558582306\n",
      "training: 39 batch 467 batch_loss: 0.17795249819755554\n",
      "training: 39 batch 468 batch_loss: 0.17643675208091736\n",
      "training: 39 batch 469 batch_loss: 0.18217158317565918\n",
      "training: 39 batch 470 batch_loss: 0.18048441410064697\n",
      "training: 39 batch 471 batch_loss: 0.1818820834159851\n",
      "training: 39 batch 472 batch_loss: 0.18034327030181885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 39 batch 473 batch_loss: 0.17951351404190063\n",
      "training: 39 batch 474 batch_loss: 0.17842525243759155\n",
      "training: 39 batch 475 batch_loss: 0.1791251003742218\n",
      "training: 39 batch 476 batch_loss: 0.17750027775764465\n",
      "training: 39 batch 477 batch_loss: 0.17924228310585022\n",
      "training: 39 batch 478 batch_loss: 0.1778182089328766\n",
      "training: 39 batch 479 batch_loss: 0.178178071975708\n",
      "training: 39 batch 480 batch_loss: 0.18245387077331543\n",
      "training: 39 batch 481 batch_loss: 0.17899763584136963\n",
      "training: 39 batch 482 batch_loss: 0.17990484833717346\n",
      "training: 39 batch 483 batch_loss: 0.18176567554473877\n",
      "training: 39 batch 484 batch_loss: 0.1743740439414978\n",
      "training: 39 batch 485 batch_loss: 0.18378794193267822\n",
      "training: 39 batch 486 batch_loss: 0.1816776692867279\n",
      "training: 39 batch 487 batch_loss: 0.17835867404937744\n",
      "training: 39 batch 488 batch_loss: 0.17835596203804016\n",
      "training: 39 batch 489 batch_loss: 0.18134942650794983\n",
      "training: 39 batch 490 batch_loss: 0.175254225730896\n",
      "training: 39 batch 491 batch_loss: 0.1796332597732544\n",
      "training: 39 batch 492 batch_loss: 0.17950326204299927\n",
      "training: 39 batch 493 batch_loss: 0.18455776572227478\n",
      "training: 39 batch 494 batch_loss: 0.17882072925567627\n",
      "training: 39 batch 495 batch_loss: 0.18005293607711792\n",
      "training: 39 batch 496 batch_loss: 0.18606427311897278\n",
      "training: 39 batch 497 batch_loss: 0.17753902077674866\n",
      "training: 39 batch 498 batch_loss: 0.18053632974624634\n",
      "training: 39 batch 499 batch_loss: 0.1759198009967804\n",
      "training: 39 batch 500 batch_loss: 0.1824573576450348\n",
      "training: 39 batch 501 batch_loss: 0.17626264691352844\n",
      "training: 39 batch 502 batch_loss: 0.17625579237937927\n",
      "training: 39 batch 503 batch_loss: 0.1806163787841797\n",
      "training: 39 batch 504 batch_loss: 0.18032494187355042\n",
      "training: 39 batch 505 batch_loss: 0.1798267364501953\n",
      "training: 39 batch 506 batch_loss: 0.1758190393447876\n",
      "training: 39 batch 507 batch_loss: 0.1811266541481018\n",
      "training: 39 batch 508 batch_loss: 0.1789315938949585\n",
      "training: 39 batch 509 batch_loss: 0.17471161484718323\n",
      "training: 39 batch 510 batch_loss: 0.17826133966445923\n",
      "training: 39 batch 511 batch_loss: 0.18026486039161682\n",
      "training: 39 batch 512 batch_loss: 0.18022951483726501\n",
      "training: 39 batch 513 batch_loss: 0.18375402688980103\n",
      "training: 39 batch 514 batch_loss: 0.17621690034866333\n",
      "training: 39 batch 515 batch_loss: 0.18088719248771667\n",
      "training: 39 batch 516 batch_loss: 0.17492932081222534\n",
      "training: 39 batch 517 batch_loss: 0.18007242679595947\n",
      "training: 39 batch 518 batch_loss: 0.17849165201187134\n",
      "training: 39 batch 519 batch_loss: 0.17961516976356506\n",
      "training: 39 batch 520 batch_loss: 0.18164286017417908\n",
      "training: 39 batch 521 batch_loss: 0.18076801300048828\n",
      "training: 39 batch 522 batch_loss: 0.18077343702316284\n",
      "training: 39 batch 523 batch_loss: 0.180158793926239\n",
      "training: 39 batch 524 batch_loss: 0.1850423812866211\n",
      "training: 39 batch 525 batch_loss: 0.17572933435440063\n",
      "training: 39 batch 526 batch_loss: 0.1750766932964325\n",
      "training: 39 batch 527 batch_loss: 0.1794748604297638\n",
      "training: 39 batch 528 batch_loss: 0.18171212077140808\n",
      "training: 39 batch 529 batch_loss: 0.17676419019699097\n",
      "training: 39 batch 530 batch_loss: 0.18148258328437805\n",
      "training: 39 batch 531 batch_loss: 0.1778905987739563\n",
      "training: 39 batch 532 batch_loss: 0.17360210418701172\n",
      "training: 39 batch 533 batch_loss: 0.17779457569122314\n",
      "training: 39 batch 534 batch_loss: 0.18436062335968018\n",
      "training: 39 batch 535 batch_loss: 0.18066266179084778\n",
      "training: 39 batch 536 batch_loss: 0.17997050285339355\n",
      "training: 39 batch 537 batch_loss: 0.18139836192131042\n",
      "training: 39 batch 538 batch_loss: 0.1763998568058014\n",
      "training: 39 batch 539 batch_loss: 0.18024855852127075\n",
      "training: 39 batch 540 batch_loss: 0.17540544271469116\n",
      "training: 39 batch 541 batch_loss: 0.18128454685211182\n",
      "training: 39 batch 542 batch_loss: 0.17904657125473022\n",
      "training: 39 batch 543 batch_loss: 0.17900162935256958\n",
      "training: 39 batch 544 batch_loss: 0.17890626192092896\n",
      "training: 39 batch 545 batch_loss: 0.17902588844299316\n",
      "training: 39 batch 546 batch_loss: 0.18202561140060425\n",
      "training: 39 batch 547 batch_loss: 0.18019703030586243\n",
      "training: 39 batch 548 batch_loss: 0.17522859573364258\n",
      "training: 39 batch 549 batch_loss: 0.17908206582069397\n",
      "training: 39 batch 550 batch_loss: 0.17643219232559204\n",
      "training: 39 batch 551 batch_loss: 0.1799512803554535\n",
      "training: 39 batch 552 batch_loss: 0.18114355206489563\n",
      "training: 39 batch 553 batch_loss: 0.18135136365890503\n",
      "training: 39 batch 554 batch_loss: 0.182003915309906\n",
      "training: 39 batch 555 batch_loss: 0.18072324991226196\n",
      "training: 39 batch 556 batch_loss: 0.1808135211467743\n",
      "training: 39 batch 557 batch_loss: 0.18179604411125183\n",
      "training: 39 batch 558 batch_loss: 0.1786964237689972\n",
      "training: 39 batch 559 batch_loss: 0.1797512173652649\n",
      "training: 39 batch 560 batch_loss: 0.17989838123321533\n",
      "training: 39 batch 561 batch_loss: 0.17751693725585938\n",
      "training: 39 batch 562 batch_loss: 0.18235379457473755\n",
      "training: 39 batch 563 batch_loss: 0.18096667528152466\n",
      "training: 39 batch 564 batch_loss: 0.1783546507358551\n",
      "training: 39 batch 565 batch_loss: 0.18038564920425415\n",
      "training: 39 batch 566 batch_loss: 0.17802289128303528\n",
      "training: 39 batch 567 batch_loss: 0.17813321948051453\n",
      "training: 39 batch 568 batch_loss: 0.17925593256950378\n",
      "training: 39 batch 569 batch_loss: 0.17628279328346252\n",
      "training: 39 batch 570 batch_loss: 0.17890989780426025\n",
      "training: 39 batch 571 batch_loss: 0.17991620302200317\n",
      "training: 39 batch 572 batch_loss: 0.17827317118644714\n",
      "training: 39 batch 573 batch_loss: 0.17710646986961365\n",
      "training: 39 batch 574 batch_loss: 0.1803067922592163\n",
      "training: 39 batch 575 batch_loss: 0.17533740401268005\n",
      "training: 39 batch 576 batch_loss: 0.17541924118995667\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 39, Hit Ratio:0.03095008877622063 | Precision:0.0456649955765261 | Recall:0.06142474946143685 | NDCG:0.058761998527664464\n",
      "*Best Performance* \n",
      "Epoch: 38, Hit Ratio:0.03119992804629022 | Precision:0.04603361840165143 | Recall:0.06105512710452418 | MDCG:0.059088839669294624\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 40 batch 0 batch_loss: 0.1785990595817566\n",
      "training: 40 batch 1 batch_loss: 0.17801201343536377\n",
      "training: 40 batch 2 batch_loss: 0.17839288711547852\n",
      "training: 40 batch 3 batch_loss: 0.17968818545341492\n",
      "training: 40 batch 4 batch_loss: 0.176760733127594\n",
      "training: 40 batch 5 batch_loss: 0.17762145400047302\n",
      "training: 40 batch 6 batch_loss: 0.18424761295318604\n",
      "training: 40 batch 7 batch_loss: 0.17960986495018005\n",
      "training: 40 batch 8 batch_loss: 0.1785660982131958\n",
      "training: 40 batch 9 batch_loss: 0.17715758085250854\n",
      "training: 40 batch 10 batch_loss: 0.1773850917816162\n",
      "training: 40 batch 11 batch_loss: 0.1789536476135254\n",
      "training: 40 batch 12 batch_loss: 0.1767284870147705\n",
      "training: 40 batch 13 batch_loss: 0.17568176984786987\n",
      "training: 40 batch 14 batch_loss: 0.17827531695365906\n",
      "training: 40 batch 15 batch_loss: 0.17587122321128845\n",
      "training: 40 batch 16 batch_loss: 0.17789536714553833\n",
      "training: 40 batch 17 batch_loss: 0.17681637406349182\n",
      "training: 40 batch 18 batch_loss: 0.18080005049705505\n",
      "training: 40 batch 19 batch_loss: 0.17352327704429626\n",
      "training: 40 batch 20 batch_loss: 0.17430517077445984\n",
      "training: 40 batch 21 batch_loss: 0.17845898866653442\n",
      "training: 40 batch 22 batch_loss: 0.18140560388565063\n",
      "training: 40 batch 23 batch_loss: 0.1761760711669922\n",
      "training: 40 batch 24 batch_loss: 0.18015515804290771\n",
      "training: 40 batch 25 batch_loss: 0.17531132698059082\n",
      "training: 40 batch 26 batch_loss: 0.173776775598526\n",
      "training: 40 batch 27 batch_loss: 0.1773356795310974\n",
      "training: 40 batch 28 batch_loss: 0.17382502555847168\n",
      "training: 40 batch 29 batch_loss: 0.1813654899597168\n",
      "training: 40 batch 30 batch_loss: 0.17831462621688843\n",
      "training: 40 batch 31 batch_loss: 0.1765129268169403\n",
      "training: 40 batch 32 batch_loss: 0.17784911394119263\n",
      "training: 40 batch 33 batch_loss: 0.17856842279434204\n",
      "training: 40 batch 34 batch_loss: 0.17723563313484192\n",
      "training: 40 batch 35 batch_loss: 0.1770680844783783\n",
      "training: 40 batch 36 batch_loss: 0.1772683560848236\n",
      "training: 40 batch 37 batch_loss: 0.17612582445144653\n",
      "training: 40 batch 38 batch_loss: 0.17639011144638062\n",
      "training: 40 batch 39 batch_loss: 0.1752920150756836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 40 batch 40 batch_loss: 0.17728447914123535\n",
      "training: 40 batch 41 batch_loss: 0.17558422684669495\n",
      "training: 40 batch 42 batch_loss: 0.1809302270412445\n",
      "training: 40 batch 43 batch_loss: 0.1769810914993286\n",
      "training: 40 batch 44 batch_loss: 0.1765047311782837\n",
      "training: 40 batch 45 batch_loss: 0.1787329912185669\n",
      "training: 40 batch 46 batch_loss: 0.17726126313209534\n",
      "training: 40 batch 47 batch_loss: 0.17308968305587769\n",
      "training: 40 batch 48 batch_loss: 0.1763022243976593\n",
      "training: 40 batch 49 batch_loss: 0.1807183027267456\n",
      "training: 40 batch 50 batch_loss: 0.17494475841522217\n",
      "training: 40 batch 51 batch_loss: 0.1766124665737152\n",
      "training: 40 batch 52 batch_loss: 0.17718106508255005\n",
      "training: 40 batch 53 batch_loss: 0.1773853898048401\n",
      "training: 40 batch 54 batch_loss: 0.17917588353157043\n",
      "training: 40 batch 55 batch_loss: 0.17810192704200745\n",
      "training: 40 batch 56 batch_loss: 0.1783146858215332\n",
      "training: 40 batch 57 batch_loss: 0.1783989667892456\n",
      "training: 40 batch 58 batch_loss: 0.17795518040657043\n",
      "training: 40 batch 59 batch_loss: 0.17843839526176453\n",
      "training: 40 batch 60 batch_loss: 0.17672204971313477\n",
      "training: 40 batch 61 batch_loss: 0.1835196316242218\n",
      "training: 40 batch 62 batch_loss: 0.17799222469329834\n",
      "training: 40 batch 63 batch_loss: 0.1786697506904602\n",
      "training: 40 batch 64 batch_loss: 0.18581628799438477\n",
      "training: 40 batch 65 batch_loss: 0.17852124571800232\n",
      "training: 40 batch 66 batch_loss: 0.18191367387771606\n",
      "training: 40 batch 67 batch_loss: 0.17831319570541382\n",
      "training: 40 batch 68 batch_loss: 0.17448565363883972\n",
      "training: 40 batch 69 batch_loss: 0.18263131380081177\n",
      "training: 40 batch 70 batch_loss: 0.17900267243385315\n",
      "training: 40 batch 71 batch_loss: 0.17438405752182007\n",
      "training: 40 batch 72 batch_loss: 0.17849653959274292\n",
      "training: 40 batch 73 batch_loss: 0.17777380347251892\n",
      "training: 40 batch 74 batch_loss: 0.17611870169639587\n",
      "training: 40 batch 75 batch_loss: 0.17931914329528809\n",
      "training: 40 batch 76 batch_loss: 0.17620763182640076\n",
      "training: 40 batch 77 batch_loss: 0.1781107783317566\n",
      "training: 40 batch 78 batch_loss: 0.17824959754943848\n",
      "training: 40 batch 79 batch_loss: 0.17346817255020142\n",
      "training: 40 batch 80 batch_loss: 0.1758248209953308\n",
      "training: 40 batch 81 batch_loss: 0.17475363612174988\n",
      "training: 40 batch 82 batch_loss: 0.17660850286483765\n",
      "training: 40 batch 83 batch_loss: 0.1790328025817871\n",
      "training: 40 batch 84 batch_loss: 0.17640089988708496\n",
      "training: 40 batch 85 batch_loss: 0.17542269825935364\n",
      "training: 40 batch 86 batch_loss: 0.17669939994812012\n",
      "training: 40 batch 87 batch_loss: 0.18325379490852356\n",
      "training: 40 batch 88 batch_loss: 0.17705556750297546\n",
      "training: 40 batch 89 batch_loss: 0.17589625716209412\n",
      "training: 40 batch 90 batch_loss: 0.1795652210712433\n",
      "training: 40 batch 91 batch_loss: 0.17737358808517456\n",
      "training: 40 batch 92 batch_loss: 0.1792096495628357\n",
      "training: 40 batch 93 batch_loss: 0.18241050839424133\n",
      "training: 40 batch 94 batch_loss: 0.17943161725997925\n",
      "training: 40 batch 95 batch_loss: 0.17670828104019165\n",
      "training: 40 batch 96 batch_loss: 0.17842432856559753\n",
      "training: 40 batch 97 batch_loss: 0.17737320065498352\n",
      "training: 40 batch 98 batch_loss: 0.18434667587280273\n",
      "training: 40 batch 99 batch_loss: 0.17524448037147522\n",
      "training: 40 batch 100 batch_loss: 0.17976194620132446\n",
      "training: 40 batch 101 batch_loss: 0.17440947890281677\n",
      "training: 40 batch 102 batch_loss: 0.17390823364257812\n",
      "training: 40 batch 103 batch_loss: 0.1769433319568634\n",
      "training: 40 batch 104 batch_loss: 0.1774452030658722\n",
      "training: 40 batch 105 batch_loss: 0.17699342966079712\n",
      "training: 40 batch 106 batch_loss: 0.17823314666748047\n",
      "training: 40 batch 107 batch_loss: 0.18027210235595703\n",
      "training: 40 batch 108 batch_loss: 0.18092936277389526\n",
      "training: 40 batch 109 batch_loss: 0.17643660306930542\n",
      "training: 40 batch 110 batch_loss: 0.1828508973121643\n",
      "training: 40 batch 111 batch_loss: 0.1803940236568451\n",
      "training: 40 batch 112 batch_loss: 0.1761901080608368\n",
      "training: 40 batch 113 batch_loss: 0.1812554895877838\n",
      "training: 40 batch 114 batch_loss: 0.17760926485061646\n",
      "training: 40 batch 115 batch_loss: 0.17609870433807373\n",
      "training: 40 batch 116 batch_loss: 0.1780295968055725\n",
      "training: 40 batch 117 batch_loss: 0.18275555968284607\n",
      "training: 40 batch 118 batch_loss: 0.17953386902809143\n",
      "training: 40 batch 119 batch_loss: 0.18017518520355225\n",
      "training: 40 batch 120 batch_loss: 0.17760905623435974\n",
      "training: 40 batch 121 batch_loss: 0.17817002534866333\n",
      "training: 40 batch 122 batch_loss: 0.18242421746253967\n",
      "training: 40 batch 123 batch_loss: 0.18162286281585693\n",
      "training: 40 batch 124 batch_loss: 0.1751120686531067\n",
      "training: 40 batch 125 batch_loss: 0.1799164116382599\n",
      "training: 40 batch 126 batch_loss: 0.17890730500221252\n",
      "training: 40 batch 127 batch_loss: 0.1789039671421051\n",
      "training: 40 batch 128 batch_loss: 0.1790807843208313\n",
      "training: 40 batch 129 batch_loss: 0.18007707595825195\n",
      "training: 40 batch 130 batch_loss: 0.18298959732055664\n",
      "training: 40 batch 131 batch_loss: 0.175939679145813\n",
      "training: 40 batch 132 batch_loss: 0.17783117294311523\n",
      "training: 40 batch 133 batch_loss: 0.1795041263103485\n",
      "training: 40 batch 134 batch_loss: 0.17733606696128845\n",
      "training: 40 batch 135 batch_loss: 0.1818825602531433\n",
      "training: 40 batch 136 batch_loss: 0.17840662598609924\n",
      "training: 40 batch 137 batch_loss: 0.17724746465682983\n",
      "training: 40 batch 138 batch_loss: 0.17556139826774597\n",
      "training: 40 batch 139 batch_loss: 0.1794445812702179\n",
      "training: 40 batch 140 batch_loss: 0.17915111780166626\n",
      "training: 40 batch 141 batch_loss: 0.17669570446014404\n",
      "training: 40 batch 142 batch_loss: 0.18298235535621643\n",
      "training: 40 batch 143 batch_loss: 0.17361444234848022\n",
      "training: 40 batch 144 batch_loss: 0.18092656135559082\n",
      "training: 40 batch 145 batch_loss: 0.18089145421981812\n",
      "training: 40 batch 146 batch_loss: 0.17706376314163208\n",
      "training: 40 batch 147 batch_loss: 0.17601382732391357\n",
      "training: 40 batch 148 batch_loss: 0.17861276865005493\n",
      "training: 40 batch 149 batch_loss: 0.17641609907150269\n",
      "training: 40 batch 150 batch_loss: 0.1802675724029541\n",
      "training: 40 batch 151 batch_loss: 0.17653703689575195\n",
      "training: 40 batch 152 batch_loss: 0.17809149622917175\n",
      "training: 40 batch 153 batch_loss: 0.17684736847877502\n",
      "training: 40 batch 154 batch_loss: 0.17855527997016907\n",
      "training: 40 batch 155 batch_loss: 0.1750054955482483\n",
      "training: 40 batch 156 batch_loss: 0.18172582983970642\n",
      "training: 40 batch 157 batch_loss: 0.17845198512077332\n",
      "training: 40 batch 158 batch_loss: 0.17781555652618408\n",
      "training: 40 batch 159 batch_loss: 0.18006682395935059\n",
      "training: 40 batch 160 batch_loss: 0.1791950762271881\n",
      "training: 40 batch 161 batch_loss: 0.18029412627220154\n",
      "training: 40 batch 162 batch_loss: 0.17506206035614014\n",
      "training: 40 batch 163 batch_loss: 0.17847493290901184\n",
      "training: 40 batch 164 batch_loss: 0.17756754159927368\n",
      "training: 40 batch 165 batch_loss: 0.17972740530967712\n",
      "training: 40 batch 166 batch_loss: 0.17424538731575012\n",
      "training: 40 batch 167 batch_loss: 0.1757165491580963\n",
      "training: 40 batch 168 batch_loss: 0.17911195755004883\n",
      "training: 40 batch 169 batch_loss: 0.17867118120193481\n",
      "training: 40 batch 170 batch_loss: 0.17738240957260132\n",
      "training: 40 batch 171 batch_loss: 0.17763450741767883\n",
      "training: 40 batch 172 batch_loss: 0.1782224476337433\n",
      "training: 40 batch 173 batch_loss: 0.18064025044441223\n",
      "training: 40 batch 174 batch_loss: 0.17599472403526306\n",
      "training: 40 batch 175 batch_loss: 0.18215128779411316\n",
      "training: 40 batch 176 batch_loss: 0.18125790357589722\n",
      "training: 40 batch 177 batch_loss: 0.1826757788658142\n",
      "training: 40 batch 178 batch_loss: 0.17379087209701538\n",
      "training: 40 batch 179 batch_loss: 0.17879918217658997\n",
      "training: 40 batch 180 batch_loss: 0.17760396003723145\n",
      "training: 40 batch 181 batch_loss: 0.17501318454742432\n",
      "training: 40 batch 182 batch_loss: 0.1776770055294037\n",
      "training: 40 batch 183 batch_loss: 0.17787989974021912\n",
      "training: 40 batch 184 batch_loss: 0.18389129638671875\n",
      "training: 40 batch 185 batch_loss: 0.1785854697227478\n",
      "training: 40 batch 186 batch_loss: 0.1770731806755066\n",
      "training: 40 batch 187 batch_loss: 0.181745707988739\n",
      "training: 40 batch 188 batch_loss: 0.18145623803138733\n",
      "training: 40 batch 189 batch_loss: 0.17594271898269653\n",
      "training: 40 batch 190 batch_loss: 0.1807209849357605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 40 batch 191 batch_loss: 0.18028706312179565\n",
      "training: 40 batch 192 batch_loss: 0.1793254017829895\n",
      "training: 40 batch 193 batch_loss: 0.18064850568771362\n",
      "training: 40 batch 194 batch_loss: 0.17790409922599792\n",
      "training: 40 batch 195 batch_loss: 0.17546430230140686\n",
      "training: 40 batch 196 batch_loss: 0.1786130964756012\n",
      "training: 40 batch 197 batch_loss: 0.18296825885772705\n",
      "training: 40 batch 198 batch_loss: 0.17508387565612793\n",
      "training: 40 batch 199 batch_loss: 0.1789209544658661\n",
      "training: 40 batch 200 batch_loss: 0.17688044905662537\n",
      "training: 40 batch 201 batch_loss: 0.18009987473487854\n",
      "training: 40 batch 202 batch_loss: 0.17795926332473755\n",
      "training: 40 batch 203 batch_loss: 0.1810704469680786\n",
      "training: 40 batch 204 batch_loss: 0.179706871509552\n",
      "training: 40 batch 205 batch_loss: 0.17890092730522156\n",
      "training: 40 batch 206 batch_loss: 0.1809641718864441\n",
      "training: 40 batch 207 batch_loss: 0.1779516637325287\n",
      "training: 40 batch 208 batch_loss: 0.1790737509727478\n",
      "training: 40 batch 209 batch_loss: 0.17490169405937195\n",
      "training: 40 batch 210 batch_loss: 0.17795157432556152\n",
      "training: 40 batch 211 batch_loss: 0.18344652652740479\n",
      "training: 40 batch 212 batch_loss: 0.1808893382549286\n",
      "training: 40 batch 213 batch_loss: 0.17657548189163208\n",
      "training: 40 batch 214 batch_loss: 0.18269455432891846\n",
      "training: 40 batch 215 batch_loss: 0.17841964960098267\n",
      "training: 40 batch 216 batch_loss: 0.17772477865219116\n",
      "training: 40 batch 217 batch_loss: 0.17881852388381958\n",
      "training: 40 batch 218 batch_loss: 0.18178501725196838\n",
      "training: 40 batch 219 batch_loss: 0.17830565571784973\n",
      "training: 40 batch 220 batch_loss: 0.18071052432060242\n",
      "training: 40 batch 221 batch_loss: 0.18244555592536926\n",
      "training: 40 batch 222 batch_loss: 0.17843729257583618\n",
      "training: 40 batch 223 batch_loss: 0.17395803332328796\n",
      "training: 40 batch 224 batch_loss: 0.17870798707008362\n",
      "training: 40 batch 225 batch_loss: 0.18063777685165405\n",
      "training: 40 batch 226 batch_loss: 0.17929980158805847\n",
      "training: 40 batch 227 batch_loss: 0.18016836047172546\n",
      "training: 40 batch 228 batch_loss: 0.1789415180683136\n",
      "training: 40 batch 229 batch_loss: 0.18122082948684692\n",
      "training: 40 batch 230 batch_loss: 0.17552798986434937\n",
      "training: 40 batch 231 batch_loss: 0.17852169275283813\n",
      "training: 40 batch 232 batch_loss: 0.18023467063903809\n",
      "training: 40 batch 233 batch_loss: 0.17961138486862183\n",
      "training: 40 batch 234 batch_loss: 0.17715483903884888\n",
      "training: 40 batch 235 batch_loss: 0.1773758828639984\n",
      "training: 40 batch 236 batch_loss: 0.17474722862243652\n",
      "training: 40 batch 237 batch_loss: 0.17877089977264404\n",
      "training: 40 batch 238 batch_loss: 0.17972636222839355\n",
      "training: 40 batch 239 batch_loss: 0.17638900876045227\n",
      "training: 40 batch 240 batch_loss: 0.18257346749305725\n",
      "training: 40 batch 241 batch_loss: 0.17851611971855164\n",
      "training: 40 batch 242 batch_loss: 0.18252062797546387\n",
      "training: 40 batch 243 batch_loss: 0.18016409873962402\n",
      "training: 40 batch 244 batch_loss: 0.17865028977394104\n",
      "training: 40 batch 245 batch_loss: 0.17687472701072693\n",
      "training: 40 batch 246 batch_loss: 0.1787319779396057\n",
      "training: 40 batch 247 batch_loss: 0.1760014295578003\n",
      "training: 40 batch 248 batch_loss: 0.17926886677742004\n",
      "training: 40 batch 249 batch_loss: 0.18014490604400635\n",
      "training: 40 batch 250 batch_loss: 0.17850130796432495\n",
      "training: 40 batch 251 batch_loss: 0.17829659581184387\n",
      "training: 40 batch 252 batch_loss: 0.17865639925003052\n",
      "training: 40 batch 253 batch_loss: 0.17980358004570007\n",
      "training: 40 batch 254 batch_loss: 0.1745910942554474\n",
      "training: 40 batch 255 batch_loss: 0.18322688341140747\n",
      "training: 40 batch 256 batch_loss: 0.1821134388446808\n",
      "training: 40 batch 257 batch_loss: 0.18230122327804565\n",
      "training: 40 batch 258 batch_loss: 0.18130645155906677\n",
      "training: 40 batch 259 batch_loss: 0.1763792634010315\n",
      "training: 40 batch 260 batch_loss: 0.17837610840797424\n",
      "training: 40 batch 261 batch_loss: 0.1774601936340332\n",
      "training: 40 batch 262 batch_loss: 0.17987990379333496\n",
      "training: 40 batch 263 batch_loss: 0.1774979829788208\n",
      "training: 40 batch 264 batch_loss: 0.18215352296829224\n",
      "training: 40 batch 265 batch_loss: 0.18041983246803284\n",
      "training: 40 batch 266 batch_loss: 0.17859965562820435\n",
      "training: 40 batch 267 batch_loss: 0.17851543426513672\n",
      "training: 40 batch 268 batch_loss: 0.1802249550819397\n",
      "training: 40 batch 269 batch_loss: 0.18261495232582092\n",
      "training: 40 batch 270 batch_loss: 0.17858946323394775\n",
      "training: 40 batch 271 batch_loss: 0.18010678887367249\n",
      "training: 40 batch 272 batch_loss: 0.18101996183395386\n",
      "training: 40 batch 273 batch_loss: 0.17794018983840942\n",
      "training: 40 batch 274 batch_loss: 0.17686864733695984\n",
      "training: 40 batch 275 batch_loss: 0.18204185366630554\n",
      "training: 40 batch 276 batch_loss: 0.1813080906867981\n",
      "training: 40 batch 277 batch_loss: 0.17934578657150269\n",
      "training: 40 batch 278 batch_loss: 0.18296551704406738\n",
      "training: 40 batch 279 batch_loss: 0.17698794603347778\n",
      "training: 40 batch 280 batch_loss: 0.1807672381401062\n",
      "training: 40 batch 281 batch_loss: 0.1810603141784668\n",
      "training: 40 batch 282 batch_loss: 0.1785951554775238\n",
      "training: 40 batch 283 batch_loss: 0.18189144134521484\n",
      "training: 40 batch 284 batch_loss: 0.1805373728275299\n",
      "training: 40 batch 285 batch_loss: 0.1841895580291748\n",
      "training: 40 batch 286 batch_loss: 0.18015635013580322\n",
      "training: 40 batch 287 batch_loss: 0.17898526787757874\n",
      "training: 40 batch 288 batch_loss: 0.18001723289489746\n",
      "training: 40 batch 289 batch_loss: 0.1798962950706482\n",
      "training: 40 batch 290 batch_loss: 0.17902252078056335\n",
      "training: 40 batch 291 batch_loss: 0.17667025327682495\n",
      "training: 40 batch 292 batch_loss: 0.18027734756469727\n",
      "training: 40 batch 293 batch_loss: 0.17831304669380188\n",
      "training: 40 batch 294 batch_loss: 0.17979854345321655\n",
      "training: 40 batch 295 batch_loss: 0.17984431982040405\n",
      "training: 40 batch 296 batch_loss: 0.1768757700920105\n",
      "training: 40 batch 297 batch_loss: 0.18275755643844604\n",
      "training: 40 batch 298 batch_loss: 0.1781121790409088\n",
      "training: 40 batch 299 batch_loss: 0.17804092168807983\n",
      "training: 40 batch 300 batch_loss: 0.18092840909957886\n",
      "training: 40 batch 301 batch_loss: 0.18121641874313354\n",
      "training: 40 batch 302 batch_loss: 0.1816430687904358\n",
      "training: 40 batch 303 batch_loss: 0.18025875091552734\n",
      "training: 40 batch 304 batch_loss: 0.18174320459365845\n",
      "training: 40 batch 305 batch_loss: 0.17948049306869507\n",
      "training: 40 batch 306 batch_loss: 0.1785183846950531\n",
      "training: 40 batch 307 batch_loss: 0.1812306046485901\n",
      "training: 40 batch 308 batch_loss: 0.17528477311134338\n",
      "training: 40 batch 309 batch_loss: 0.182964026927948\n",
      "training: 40 batch 310 batch_loss: 0.17785018682479858\n",
      "training: 40 batch 311 batch_loss: 0.17892560362815857\n",
      "training: 40 batch 312 batch_loss: 0.18291306495666504\n",
      "training: 40 batch 313 batch_loss: 0.17868295311927795\n",
      "training: 40 batch 314 batch_loss: 0.1793050467967987\n",
      "training: 40 batch 315 batch_loss: 0.18006861209869385\n",
      "training: 40 batch 316 batch_loss: 0.1790083646774292\n",
      "training: 40 batch 317 batch_loss: 0.17740419507026672\n",
      "training: 40 batch 318 batch_loss: 0.1781625747680664\n",
      "training: 40 batch 319 batch_loss: 0.17769300937652588\n",
      "training: 40 batch 320 batch_loss: 0.18147745728492737\n",
      "training: 40 batch 321 batch_loss: 0.1810913383960724\n",
      "training: 40 batch 322 batch_loss: 0.17711329460144043\n",
      "training: 40 batch 323 batch_loss: 0.18007659912109375\n",
      "training: 40 batch 324 batch_loss: 0.17741110920906067\n",
      "training: 40 batch 325 batch_loss: 0.18173837661743164\n",
      "training: 40 batch 326 batch_loss: 0.177321195602417\n",
      "training: 40 batch 327 batch_loss: 0.18306061625480652\n",
      "training: 40 batch 328 batch_loss: 0.18355423212051392\n",
      "training: 40 batch 329 batch_loss: 0.1769859492778778\n",
      "training: 40 batch 330 batch_loss: 0.17918437719345093\n",
      "training: 40 batch 331 batch_loss: 0.18346244096755981\n",
      "training: 40 batch 332 batch_loss: 0.18184226751327515\n",
      "training: 40 batch 333 batch_loss: 0.1808566451072693\n",
      "training: 40 batch 334 batch_loss: 0.1787821352481842\n",
      "training: 40 batch 335 batch_loss: 0.1791803538799286\n",
      "training: 40 batch 336 batch_loss: 0.17866289615631104\n",
      "training: 40 batch 337 batch_loss: 0.17328622937202454\n",
      "training: 40 batch 338 batch_loss: 0.17807593941688538\n",
      "training: 40 batch 339 batch_loss: 0.17978569865226746\n",
      "training: 40 batch 340 batch_loss: 0.18120843172073364\n",
      "training: 40 batch 341 batch_loss: 0.18048381805419922\n",
      "training: 40 batch 342 batch_loss: 0.17937961220741272\n",
      "training: 40 batch 343 batch_loss: 0.17402443289756775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 40 batch 344 batch_loss: 0.1866896152496338\n",
      "training: 40 batch 345 batch_loss: 0.17968091368675232\n",
      "training: 40 batch 346 batch_loss: 0.18189170956611633\n",
      "training: 40 batch 347 batch_loss: 0.18096628785133362\n",
      "training: 40 batch 348 batch_loss: 0.1787211298942566\n",
      "training: 40 batch 349 batch_loss: 0.17303341627120972\n",
      "training: 40 batch 350 batch_loss: 0.18579202890396118\n",
      "training: 40 batch 351 batch_loss: 0.18593716621398926\n",
      "training: 40 batch 352 batch_loss: 0.17951175570487976\n",
      "training: 40 batch 353 batch_loss: 0.17771396040916443\n",
      "training: 40 batch 354 batch_loss: 0.17858004570007324\n",
      "training: 40 batch 355 batch_loss: 0.17824649810791016\n",
      "training: 40 batch 356 batch_loss: 0.18357938528060913\n",
      "training: 40 batch 357 batch_loss: 0.1773381233215332\n",
      "training: 40 batch 358 batch_loss: 0.1827961504459381\n",
      "training: 40 batch 359 batch_loss: 0.1808956265449524\n",
      "training: 40 batch 360 batch_loss: 0.18078100681304932\n",
      "training: 40 batch 361 batch_loss: 0.1788809895515442\n",
      "training: 40 batch 362 batch_loss: 0.1819971799850464\n",
      "training: 40 batch 363 batch_loss: 0.1813409924507141\n",
      "training: 40 batch 364 batch_loss: 0.18123692274093628\n",
      "training: 40 batch 365 batch_loss: 0.17737075686454773\n",
      "training: 40 batch 366 batch_loss: 0.18200671672821045\n",
      "training: 40 batch 367 batch_loss: 0.17816230654716492\n",
      "training: 40 batch 368 batch_loss: 0.17915019392967224\n",
      "training: 40 batch 369 batch_loss: 0.18245017528533936\n",
      "training: 40 batch 370 batch_loss: 0.18005943298339844\n",
      "training: 40 batch 371 batch_loss: 0.18017002940177917\n",
      "training: 40 batch 372 batch_loss: 0.17828509211540222\n",
      "training: 40 batch 373 batch_loss: 0.17745941877365112\n",
      "training: 40 batch 374 batch_loss: 0.17481637001037598\n",
      "training: 40 batch 375 batch_loss: 0.17786455154418945\n",
      "training: 40 batch 376 batch_loss: 0.18332529067993164\n",
      "training: 40 batch 377 batch_loss: 0.17722561955451965\n",
      "training: 40 batch 378 batch_loss: 0.17976748943328857\n",
      "training: 40 batch 379 batch_loss: 0.175837904214859\n",
      "training: 40 batch 380 batch_loss: 0.17752262949943542\n",
      "training: 40 batch 381 batch_loss: 0.17877885699272156\n",
      "training: 40 batch 382 batch_loss: 0.18390804529190063\n",
      "training: 40 batch 383 batch_loss: 0.18352758884429932\n",
      "training: 40 batch 384 batch_loss: 0.18277508020401\n",
      "training: 40 batch 385 batch_loss: 0.18040525913238525\n",
      "training: 40 batch 386 batch_loss: 0.18587100505828857\n",
      "training: 40 batch 387 batch_loss: 0.18082094192504883\n",
      "training: 40 batch 388 batch_loss: 0.18279770016670227\n",
      "training: 40 batch 389 batch_loss: 0.18158119916915894\n",
      "training: 40 batch 390 batch_loss: 0.18068909645080566\n",
      "training: 40 batch 391 batch_loss: 0.1780586540699005\n",
      "training: 40 batch 392 batch_loss: 0.17661350965499878\n",
      "training: 40 batch 393 batch_loss: 0.18235692381858826\n",
      "training: 40 batch 394 batch_loss: 0.1788492202758789\n",
      "training: 40 batch 395 batch_loss: 0.1824561059474945\n",
      "training: 40 batch 396 batch_loss: 0.18190205097198486\n",
      "training: 40 batch 397 batch_loss: 0.1786465346813202\n",
      "training: 40 batch 398 batch_loss: 0.1757207214832306\n",
      "training: 40 batch 399 batch_loss: 0.17991280555725098\n",
      "training: 40 batch 400 batch_loss: 0.17624187469482422\n",
      "training: 40 batch 401 batch_loss: 0.1802005171775818\n",
      "training: 40 batch 402 batch_loss: 0.1813647747039795\n",
      "training: 40 batch 403 batch_loss: 0.17971497774124146\n",
      "training: 40 batch 404 batch_loss: 0.1772386133670807\n",
      "training: 40 batch 405 batch_loss: 0.179110586643219\n",
      "training: 40 batch 406 batch_loss: 0.18207111954689026\n",
      "training: 40 batch 407 batch_loss: 0.17829227447509766\n",
      "training: 40 batch 408 batch_loss: 0.17796534299850464\n",
      "training: 40 batch 409 batch_loss: 0.1793535351753235\n",
      "training: 40 batch 410 batch_loss: 0.18107888102531433\n",
      "training: 40 batch 411 batch_loss: 0.18099316954612732\n",
      "training: 40 batch 412 batch_loss: 0.17816996574401855\n",
      "training: 40 batch 413 batch_loss: 0.1804789900779724\n",
      "training: 40 batch 414 batch_loss: 0.17507022619247437\n",
      "training: 40 batch 415 batch_loss: 0.17645013332366943\n",
      "training: 40 batch 416 batch_loss: 0.1825179159641266\n",
      "training: 40 batch 417 batch_loss: 0.17699721455574036\n",
      "training: 40 batch 418 batch_loss: 0.1767815351486206\n",
      "training: 40 batch 419 batch_loss: 0.18025535345077515\n",
      "training: 40 batch 420 batch_loss: 0.17741143703460693\n",
      "training: 40 batch 421 batch_loss: 0.1813698410987854\n",
      "training: 40 batch 422 batch_loss: 0.1821521818637848\n",
      "training: 40 batch 423 batch_loss: 0.1806541085243225\n",
      "training: 40 batch 424 batch_loss: 0.17909133434295654\n",
      "training: 40 batch 425 batch_loss: 0.17986619472503662\n",
      "training: 40 batch 426 batch_loss: 0.17755243182182312\n",
      "training: 40 batch 427 batch_loss: 0.1820237636566162\n",
      "training: 40 batch 428 batch_loss: 0.1808266043663025\n",
      "training: 40 batch 429 batch_loss: 0.18205639719963074\n",
      "training: 40 batch 430 batch_loss: 0.178523987531662\n",
      "training: 40 batch 431 batch_loss: 0.1808229684829712\n",
      "training: 40 batch 432 batch_loss: 0.18255215883255005\n",
      "training: 40 batch 433 batch_loss: 0.18116462230682373\n",
      "training: 40 batch 434 batch_loss: 0.1786409318447113\n",
      "training: 40 batch 435 batch_loss: 0.1798872947692871\n",
      "training: 40 batch 436 batch_loss: 0.17602545022964478\n",
      "training: 40 batch 437 batch_loss: 0.17902973294258118\n",
      "training: 40 batch 438 batch_loss: 0.17974820733070374\n",
      "training: 40 batch 439 batch_loss: 0.18365478515625\n",
      "training: 40 batch 440 batch_loss: 0.18390214443206787\n",
      "training: 40 batch 441 batch_loss: 0.17872506380081177\n",
      "training: 40 batch 442 batch_loss: 0.17741826176643372\n",
      "training: 40 batch 443 batch_loss: 0.17997854948043823\n",
      "training: 40 batch 444 batch_loss: 0.18255376815795898\n",
      "training: 40 batch 445 batch_loss: 0.18859416246414185\n",
      "training: 40 batch 446 batch_loss: 0.18268615007400513\n",
      "training: 40 batch 447 batch_loss: 0.1829175353050232\n",
      "training: 40 batch 448 batch_loss: 0.18057578802108765\n",
      "training: 40 batch 449 batch_loss: 0.17963337898254395\n",
      "training: 40 batch 450 batch_loss: 0.183109849691391\n",
      "training: 40 batch 451 batch_loss: 0.18122327327728271\n",
      "training: 40 batch 452 batch_loss: 0.1798766553401947\n",
      "training: 40 batch 453 batch_loss: 0.17930001020431519\n",
      "training: 40 batch 454 batch_loss: 0.17615434527397156\n",
      "training: 40 batch 455 batch_loss: 0.18190112709999084\n",
      "training: 40 batch 456 batch_loss: 0.18003436923027039\n",
      "training: 40 batch 457 batch_loss: 0.17993402481079102\n",
      "training: 40 batch 458 batch_loss: 0.17890304327011108\n",
      "training: 40 batch 459 batch_loss: 0.1815982162952423\n",
      "training: 40 batch 460 batch_loss: 0.17609307169914246\n",
      "training: 40 batch 461 batch_loss: 0.18207868933677673\n",
      "training: 40 batch 462 batch_loss: 0.17980849742889404\n",
      "training: 40 batch 463 batch_loss: 0.17986255884170532\n",
      "training: 40 batch 464 batch_loss: 0.18308544158935547\n",
      "training: 40 batch 465 batch_loss: 0.18198716640472412\n",
      "training: 40 batch 466 batch_loss: 0.1791929006576538\n",
      "training: 40 batch 467 batch_loss: 0.17727753520011902\n",
      "training: 40 batch 468 batch_loss: 0.17521625757217407\n",
      "training: 40 batch 469 batch_loss: 0.1809726059436798\n",
      "training: 40 batch 470 batch_loss: 0.18003955483436584\n",
      "training: 40 batch 471 batch_loss: 0.1793302297592163\n",
      "training: 40 batch 472 batch_loss: 0.17922645807266235\n",
      "training: 40 batch 473 batch_loss: 0.18419674038887024\n",
      "training: 40 batch 474 batch_loss: 0.1770607829093933\n",
      "training: 40 batch 475 batch_loss: 0.17833450436592102\n",
      "training: 40 batch 476 batch_loss: 0.17952364683151245\n",
      "training: 40 batch 477 batch_loss: 0.17720019817352295\n",
      "training: 40 batch 478 batch_loss: 0.17938822507858276\n",
      "training: 40 batch 479 batch_loss: 0.18320471048355103\n",
      "training: 40 batch 480 batch_loss: 0.18095040321350098\n",
      "training: 40 batch 481 batch_loss: 0.1786729097366333\n",
      "training: 40 batch 482 batch_loss: 0.18169456720352173\n",
      "training: 40 batch 483 batch_loss: 0.1784055531024933\n",
      "training: 40 batch 484 batch_loss: 0.1811637580394745\n",
      "training: 40 batch 485 batch_loss: 0.17984366416931152\n",
      "training: 40 batch 486 batch_loss: 0.18308544158935547\n",
      "training: 40 batch 487 batch_loss: 0.17865949869155884\n",
      "training: 40 batch 488 batch_loss: 0.18024897575378418\n",
      "training: 40 batch 489 batch_loss: 0.17992717027664185\n",
      "training: 40 batch 490 batch_loss: 0.18179693818092346\n",
      "training: 40 batch 491 batch_loss: 0.17991849780082703\n",
      "training: 40 batch 492 batch_loss: 0.18108204007148743\n",
      "training: 40 batch 493 batch_loss: 0.17725250124931335\n",
      "training: 40 batch 494 batch_loss: 0.17828398942947388\n",
      "training: 40 batch 495 batch_loss: 0.1787874698638916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 40 batch 496 batch_loss: 0.17563265562057495\n",
      "training: 40 batch 497 batch_loss: 0.1822938323020935\n",
      "training: 40 batch 498 batch_loss: 0.18028080463409424\n",
      "training: 40 batch 499 batch_loss: 0.18022966384887695\n",
      "training: 40 batch 500 batch_loss: 0.18003398180007935\n",
      "training: 40 batch 501 batch_loss: 0.180962473154068\n",
      "training: 40 batch 502 batch_loss: 0.18056142330169678\n",
      "training: 40 batch 503 batch_loss: 0.1790405511856079\n",
      "training: 40 batch 504 batch_loss: 0.1841452419757843\n",
      "training: 40 batch 505 batch_loss: 0.18288969993591309\n",
      "training: 40 batch 506 batch_loss: 0.1812869906425476\n",
      "training: 40 batch 507 batch_loss: 0.17990922927856445\n",
      "training: 40 batch 508 batch_loss: 0.18308648467063904\n",
      "training: 40 batch 509 batch_loss: 0.1807732880115509\n",
      "training: 40 batch 510 batch_loss: 0.1835896372795105\n",
      "training: 40 batch 511 batch_loss: 0.1835334300994873\n",
      "training: 40 batch 512 batch_loss: 0.18125727772712708\n",
      "training: 40 batch 513 batch_loss: 0.17648354172706604\n",
      "training: 40 batch 514 batch_loss: 0.1858806610107422\n",
      "training: 40 batch 515 batch_loss: 0.17897170782089233\n",
      "training: 40 batch 516 batch_loss: 0.18321174383163452\n",
      "training: 40 batch 517 batch_loss: 0.18122264742851257\n",
      "training: 40 batch 518 batch_loss: 0.18103647232055664\n",
      "training: 40 batch 519 batch_loss: 0.18007519841194153\n",
      "training: 40 batch 520 batch_loss: 0.17855054140090942\n",
      "training: 40 batch 521 batch_loss: 0.17997336387634277\n",
      "training: 40 batch 522 batch_loss: 0.17939138412475586\n",
      "training: 40 batch 523 batch_loss: 0.18407779932022095\n",
      "training: 40 batch 524 batch_loss: 0.17939361929893494\n",
      "training: 40 batch 525 batch_loss: 0.1784641146659851\n",
      "training: 40 batch 526 batch_loss: 0.18097606301307678\n",
      "training: 40 batch 527 batch_loss: 0.17921555042266846\n",
      "training: 40 batch 528 batch_loss: 0.1797180473804474\n",
      "training: 40 batch 529 batch_loss: 0.18162962794303894\n",
      "training: 40 batch 530 batch_loss: 0.17722946405410767\n",
      "training: 40 batch 531 batch_loss: 0.18069234490394592\n",
      "training: 40 batch 532 batch_loss: 0.18377801775932312\n",
      "training: 40 batch 533 batch_loss: 0.1857985258102417\n",
      "training: 40 batch 534 batch_loss: 0.1811811625957489\n",
      "training: 40 batch 535 batch_loss: 0.18157225847244263\n",
      "training: 40 batch 536 batch_loss: 0.1812780499458313\n",
      "training: 40 batch 537 batch_loss: 0.18135935068130493\n",
      "training: 40 batch 538 batch_loss: 0.18221184611320496\n",
      "training: 40 batch 539 batch_loss: 0.17761051654815674\n",
      "training: 40 batch 540 batch_loss: 0.179345965385437\n",
      "training: 40 batch 541 batch_loss: 0.17900651693344116\n",
      "training: 40 batch 542 batch_loss: 0.18723341822624207\n",
      "training: 40 batch 543 batch_loss: 0.18465137481689453\n",
      "training: 40 batch 544 batch_loss: 0.17898458242416382\n",
      "training: 40 batch 545 batch_loss: 0.18256977200508118\n",
      "training: 40 batch 546 batch_loss: 0.17700546979904175\n",
      "training: 40 batch 547 batch_loss: 0.1826609969139099\n",
      "training: 40 batch 548 batch_loss: 0.17799264192581177\n",
      "training: 40 batch 549 batch_loss: 0.18356120586395264\n",
      "training: 40 batch 550 batch_loss: 0.1851758062839508\n",
      "training: 40 batch 551 batch_loss: 0.1797485053539276\n",
      "training: 40 batch 552 batch_loss: 0.1788785457611084\n",
      "training: 40 batch 553 batch_loss: 0.17758893966674805\n",
      "training: 40 batch 554 batch_loss: 0.17801302671432495\n",
      "training: 40 batch 555 batch_loss: 0.18102720379829407\n",
      "training: 40 batch 556 batch_loss: 0.1785333752632141\n",
      "training: 40 batch 557 batch_loss: 0.18272057175636292\n",
      "training: 40 batch 558 batch_loss: 0.1781831979751587\n",
      "training: 40 batch 559 batch_loss: 0.180855393409729\n",
      "training: 40 batch 560 batch_loss: 0.18047136068344116\n",
      "training: 40 batch 561 batch_loss: 0.18119841814041138\n",
      "training: 40 batch 562 batch_loss: 0.18027228116989136\n",
      "training: 40 batch 563 batch_loss: 0.18104323744773865\n",
      "training: 40 batch 564 batch_loss: 0.17937961220741272\n",
      "training: 40 batch 565 batch_loss: 0.1803341507911682\n",
      "training: 40 batch 566 batch_loss: 0.18032008409500122\n",
      "training: 40 batch 567 batch_loss: 0.18331485986709595\n",
      "training: 40 batch 568 batch_loss: 0.18035224080085754\n",
      "training: 40 batch 569 batch_loss: 0.1823309063911438\n",
      "training: 40 batch 570 batch_loss: 0.18228846788406372\n",
      "training: 40 batch 571 batch_loss: 0.17887356877326965\n",
      "training: 40 batch 572 batch_loss: 0.1839424967765808\n",
      "training: 40 batch 573 batch_loss: 0.18292850255966187\n",
      "training: 40 batch 574 batch_loss: 0.18108779191970825\n",
      "training: 40 batch 575 batch_loss: 0.18416211009025574\n",
      "training: 40 batch 576 batch_loss: 0.18212246894836426\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 40, Hit Ratio:0.03089012735140393 | Precision:0.045576526098496016 | Recall:0.061099160966174765 | NDCG:0.0588569393593108\n",
      "*Best Performance* \n",
      "Epoch: 38, Hit Ratio:0.03119992804629022 | Precision:0.04603361840165143 | Recall:0.06105512710452418 | MDCG:0.059088839669294624\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 41 batch 0 batch_loss: 0.17208591103553772\n",
      "training: 41 batch 1 batch_loss: 0.18179261684417725\n",
      "training: 41 batch 2 batch_loss: 0.17815688252449036\n",
      "training: 41 batch 3 batch_loss: 0.1798558235168457\n",
      "training: 41 batch 4 batch_loss: 0.17693787813186646\n",
      "training: 41 batch 5 batch_loss: 0.18194124102592468\n",
      "training: 41 batch 6 batch_loss: 0.17690730094909668\n",
      "training: 41 batch 7 batch_loss: 0.17668920755386353\n",
      "training: 41 batch 8 batch_loss: 0.1783149242401123\n",
      "training: 41 batch 9 batch_loss: 0.1775924563407898\n",
      "training: 41 batch 10 batch_loss: 0.17945992946624756\n",
      "training: 41 batch 11 batch_loss: 0.17620187997817993\n",
      "training: 41 batch 12 batch_loss: 0.18014323711395264\n",
      "training: 41 batch 13 batch_loss: 0.1760541796684265\n",
      "training: 41 batch 14 batch_loss: 0.1815190315246582\n",
      "training: 41 batch 15 batch_loss: 0.18007388710975647\n",
      "training: 41 batch 16 batch_loss: 0.1795331835746765\n",
      "training: 41 batch 17 batch_loss: 0.17886653542518616\n",
      "training: 41 batch 18 batch_loss: 0.17898041009902954\n",
      "training: 41 batch 19 batch_loss: 0.18125900626182556\n",
      "training: 41 batch 20 batch_loss: 0.17954832315444946\n",
      "training: 41 batch 21 batch_loss: 0.17916953563690186\n",
      "training: 41 batch 22 batch_loss: 0.18239173293113708\n",
      "training: 41 batch 23 batch_loss: 0.17736753821372986\n",
      "training: 41 batch 24 batch_loss: 0.17937666177749634\n",
      "training: 41 batch 25 batch_loss: 0.17711111903190613\n",
      "training: 41 batch 26 batch_loss: 0.1815231740474701\n",
      "training: 41 batch 27 batch_loss: 0.18075114488601685\n",
      "training: 41 batch 28 batch_loss: 0.18001890182495117\n",
      "training: 41 batch 29 batch_loss: 0.18150195479393005\n",
      "training: 41 batch 30 batch_loss: 0.18065643310546875\n",
      "training: 41 batch 31 batch_loss: 0.1823732852935791\n",
      "training: 41 batch 32 batch_loss: 0.18112725019454956\n",
      "training: 41 batch 33 batch_loss: 0.1782163381576538\n",
      "training: 41 batch 34 batch_loss: 0.1764209270477295\n",
      "training: 41 batch 35 batch_loss: 0.18247169256210327\n",
      "training: 41 batch 36 batch_loss: 0.17510941624641418\n",
      "training: 41 batch 37 batch_loss: 0.17772042751312256\n",
      "training: 41 batch 38 batch_loss: 0.17908966541290283\n",
      "training: 41 batch 39 batch_loss: 0.17911005020141602\n",
      "training: 41 batch 40 batch_loss: 0.1783958077430725\n",
      "training: 41 batch 41 batch_loss: 0.1767497956752777\n",
      "training: 41 batch 42 batch_loss: 0.1766349971294403\n",
      "training: 41 batch 43 batch_loss: 0.17845502495765686\n",
      "training: 41 batch 44 batch_loss: 0.18021345138549805\n",
      "training: 41 batch 45 batch_loss: 0.1785990297794342\n",
      "training: 41 batch 46 batch_loss: 0.17937028408050537\n",
      "training: 41 batch 47 batch_loss: 0.17900040745735168\n",
      "training: 41 batch 48 batch_loss: 0.17923074960708618\n",
      "training: 41 batch 49 batch_loss: 0.17775684595108032\n",
      "training: 41 batch 50 batch_loss: 0.1835661232471466\n",
      "training: 41 batch 51 batch_loss: 0.1794368326663971\n",
      "training: 41 batch 52 batch_loss: 0.17585712671279907\n",
      "training: 41 batch 53 batch_loss: 0.1771327555179596\n",
      "training: 41 batch 54 batch_loss: 0.18383550643920898\n",
      "training: 41 batch 55 batch_loss: 0.18142017722129822\n",
      "training: 41 batch 56 batch_loss: 0.1770210564136505\n",
      "training: 41 batch 57 batch_loss: 0.18015828728675842\n",
      "training: 41 batch 58 batch_loss: 0.1817600429058075\n",
      "training: 41 batch 59 batch_loss: 0.1812874972820282\n",
      "training: 41 batch 60 batch_loss: 0.1773081123828888\n",
      "training: 41 batch 61 batch_loss: 0.18106448650360107\n",
      "training: 41 batch 62 batch_loss: 0.1812833845615387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 41 batch 63 batch_loss: 0.18216508626937866\n",
      "training: 41 batch 64 batch_loss: 0.17889773845672607\n",
      "training: 41 batch 65 batch_loss: 0.1815592646598816\n",
      "training: 41 batch 66 batch_loss: 0.18002140522003174\n",
      "training: 41 batch 67 batch_loss: 0.1779494285583496\n",
      "training: 41 batch 68 batch_loss: 0.18055862188339233\n",
      "training: 41 batch 69 batch_loss: 0.18312156200408936\n",
      "training: 41 batch 70 batch_loss: 0.17661431431770325\n",
      "training: 41 batch 71 batch_loss: 0.1791096329689026\n",
      "training: 41 batch 72 batch_loss: 0.17968609929084778\n",
      "training: 41 batch 73 batch_loss: 0.17921775579452515\n",
      "training: 41 batch 74 batch_loss: 0.1822473406791687\n",
      "training: 41 batch 75 batch_loss: 0.17987796664237976\n",
      "training: 41 batch 76 batch_loss: 0.1799907684326172\n",
      "training: 41 batch 77 batch_loss: 0.18105149269104004\n",
      "training: 41 batch 78 batch_loss: 0.18347680568695068\n",
      "training: 41 batch 79 batch_loss: 0.1800556182861328\n",
      "training: 41 batch 80 batch_loss: 0.17814865708351135\n",
      "training: 41 batch 81 batch_loss: 0.17993003129959106\n",
      "training: 41 batch 82 batch_loss: 0.18240141868591309\n",
      "training: 41 batch 83 batch_loss: 0.18173938989639282\n",
      "training: 41 batch 84 batch_loss: 0.1807784140110016\n",
      "training: 41 batch 85 batch_loss: 0.177077054977417\n",
      "training: 41 batch 86 batch_loss: 0.17947372794151306\n",
      "training: 41 batch 87 batch_loss: 0.17905697226524353\n",
      "training: 41 batch 88 batch_loss: 0.1790095567703247\n",
      "training: 41 batch 89 batch_loss: 0.17663872241973877\n",
      "training: 41 batch 90 batch_loss: 0.18140289187431335\n",
      "training: 41 batch 91 batch_loss: 0.17931309342384338\n",
      "training: 41 batch 92 batch_loss: 0.1801503300666809\n",
      "training: 41 batch 93 batch_loss: 0.18087026476860046\n",
      "training: 41 batch 94 batch_loss: 0.18278264999389648\n",
      "training: 41 batch 95 batch_loss: 0.17901349067687988\n",
      "training: 41 batch 96 batch_loss: 0.17873185873031616\n",
      "training: 41 batch 97 batch_loss: 0.1805809736251831\n",
      "training: 41 batch 98 batch_loss: 0.17910867929458618\n",
      "training: 41 batch 99 batch_loss: 0.17524397373199463\n",
      "training: 41 batch 100 batch_loss: 0.18159252405166626\n",
      "training: 41 batch 101 batch_loss: 0.17833548784255981\n",
      "training: 41 batch 102 batch_loss: 0.18161574006080627\n",
      "training: 41 batch 103 batch_loss: 0.1783469319343567\n",
      "training: 41 batch 104 batch_loss: 0.18153005838394165\n",
      "training: 41 batch 105 batch_loss: 0.17893612384796143\n",
      "training: 41 batch 106 batch_loss: 0.1793729066848755\n",
      "training: 41 batch 107 batch_loss: 0.17653262615203857\n",
      "training: 41 batch 108 batch_loss: 0.17675462365150452\n",
      "training: 41 batch 109 batch_loss: 0.17798787355422974\n",
      "training: 41 batch 110 batch_loss: 0.18032076954841614\n",
      "training: 41 batch 111 batch_loss: 0.1806032359600067\n",
      "training: 41 batch 112 batch_loss: 0.17911291122436523\n",
      "training: 41 batch 113 batch_loss: 0.17799493670463562\n",
      "training: 41 batch 114 batch_loss: 0.17849934101104736\n",
      "training: 41 batch 115 batch_loss: 0.1804124414920807\n",
      "training: 41 batch 116 batch_loss: 0.18329793214797974\n",
      "training: 41 batch 117 batch_loss: 0.18197017908096313\n",
      "training: 41 batch 118 batch_loss: 0.1818130910396576\n",
      "training: 41 batch 119 batch_loss: 0.17851868271827698\n",
      "training: 41 batch 120 batch_loss: 0.18043404817581177\n",
      "training: 41 batch 121 batch_loss: 0.18336066603660583\n",
      "training: 41 batch 122 batch_loss: 0.17985647916793823\n",
      "training: 41 batch 123 batch_loss: 0.18014642596244812\n",
      "training: 41 batch 124 batch_loss: 0.1790083348751068\n",
      "training: 41 batch 125 batch_loss: 0.1801445186138153\n",
      "training: 41 batch 126 batch_loss: 0.17765724658966064\n",
      "training: 41 batch 127 batch_loss: 0.1777190864086151\n",
      "training: 41 batch 128 batch_loss: 0.18185675144195557\n",
      "training: 41 batch 129 batch_loss: 0.1770983338356018\n",
      "training: 41 batch 130 batch_loss: 0.17978724837303162\n",
      "training: 41 batch 131 batch_loss: 0.18180245161056519\n",
      "training: 41 batch 132 batch_loss: 0.18342724442481995\n",
      "training: 41 batch 133 batch_loss: 0.17858779430389404\n",
      "training: 41 batch 134 batch_loss: 0.17779892683029175\n",
      "training: 41 batch 135 batch_loss: 0.17733600735664368\n",
      "training: 41 batch 136 batch_loss: 0.17811191082000732\n",
      "training: 41 batch 137 batch_loss: 0.1847134232521057\n",
      "training: 41 batch 138 batch_loss: 0.18051883578300476\n",
      "training: 41 batch 139 batch_loss: 0.17996692657470703\n",
      "training: 41 batch 140 batch_loss: 0.17812472581863403\n",
      "training: 41 batch 141 batch_loss: 0.1831243932247162\n",
      "training: 41 batch 142 batch_loss: 0.18245947360992432\n",
      "training: 41 batch 143 batch_loss: 0.1777665615081787\n",
      "training: 41 batch 144 batch_loss: 0.17764747142791748\n",
      "training: 41 batch 145 batch_loss: 0.18077325820922852\n",
      "training: 41 batch 146 batch_loss: 0.17985865473747253\n",
      "training: 41 batch 147 batch_loss: 0.1793268322944641\n",
      "training: 41 batch 148 batch_loss: 0.17742428183555603\n",
      "training: 41 batch 149 batch_loss: 0.17588448524475098\n",
      "training: 41 batch 150 batch_loss: 0.18413370847702026\n",
      "training: 41 batch 151 batch_loss: 0.17891964316368103\n",
      "training: 41 batch 152 batch_loss: 0.1799095869064331\n",
      "training: 41 batch 153 batch_loss: 0.17910230159759521\n",
      "training: 41 batch 154 batch_loss: 0.18053099513053894\n",
      "training: 41 batch 155 batch_loss: 0.17736300826072693\n",
      "training: 41 batch 156 batch_loss: 0.17759886384010315\n",
      "training: 41 batch 157 batch_loss: 0.17829006910324097\n",
      "training: 41 batch 158 batch_loss: 0.18166577816009521\n",
      "training: 41 batch 159 batch_loss: 0.18120813369750977\n",
      "training: 41 batch 160 batch_loss: 0.18169742822647095\n",
      "training: 41 batch 161 batch_loss: 0.17957034707069397\n",
      "training: 41 batch 162 batch_loss: 0.18143245577812195\n",
      "training: 41 batch 163 batch_loss: 0.18012011051177979\n",
      "training: 41 batch 164 batch_loss: 0.17964529991149902\n",
      "training: 41 batch 165 batch_loss: 0.1778567135334015\n",
      "training: 41 batch 166 batch_loss: 0.17933830618858337\n",
      "training: 41 batch 167 batch_loss: 0.18013545870780945\n",
      "training: 41 batch 168 batch_loss: 0.18001937866210938\n",
      "training: 41 batch 169 batch_loss: 0.18007534742355347\n",
      "training: 41 batch 170 batch_loss: 0.18001267313957214\n",
      "training: 41 batch 171 batch_loss: 0.17715194821357727\n",
      "training: 41 batch 172 batch_loss: 0.18303149938583374\n",
      "training: 41 batch 173 batch_loss: 0.17732033133506775\n",
      "training: 41 batch 174 batch_loss: 0.17917147278785706\n",
      "training: 41 batch 175 batch_loss: 0.18134480714797974\n",
      "training: 41 batch 176 batch_loss: 0.17895007133483887\n",
      "training: 41 batch 177 batch_loss: 0.17662620544433594\n",
      "training: 41 batch 178 batch_loss: 0.18086549639701843\n",
      "training: 41 batch 179 batch_loss: 0.18148016929626465\n",
      "training: 41 batch 180 batch_loss: 0.1789534091949463\n",
      "training: 41 batch 181 batch_loss: 0.18384581804275513\n",
      "training: 41 batch 182 batch_loss: 0.1790771484375\n",
      "training: 41 batch 183 batch_loss: 0.18176090717315674\n",
      "training: 41 batch 184 batch_loss: 0.18320277333259583\n",
      "training: 41 batch 185 batch_loss: 0.18461918830871582\n",
      "training: 41 batch 186 batch_loss: 0.18056783080101013\n",
      "training: 41 batch 187 batch_loss: 0.17986232042312622\n",
      "training: 41 batch 188 batch_loss: 0.1822388768196106\n",
      "training: 41 batch 189 batch_loss: 0.1812814474105835\n",
      "training: 41 batch 190 batch_loss: 0.17846211791038513\n",
      "training: 41 batch 191 batch_loss: 0.18098896741867065\n",
      "training: 41 batch 192 batch_loss: 0.18087610602378845\n",
      "training: 41 batch 193 batch_loss: 0.17649200558662415\n",
      "training: 41 batch 194 batch_loss: 0.18495556712150574\n",
      "training: 41 batch 195 batch_loss: 0.1760430634021759\n",
      "training: 41 batch 196 batch_loss: 0.1800539791584015\n",
      "training: 41 batch 197 batch_loss: 0.1813029646873474\n",
      "training: 41 batch 198 batch_loss: 0.17945194244384766\n",
      "training: 41 batch 199 batch_loss: 0.17973697185516357\n",
      "training: 41 batch 200 batch_loss: 0.18115198612213135\n",
      "training: 41 batch 201 batch_loss: 0.17788493633270264\n",
      "training: 41 batch 202 batch_loss: 0.1804909110069275\n",
      "training: 41 batch 203 batch_loss: 0.18142682313919067\n",
      "training: 41 batch 204 batch_loss: 0.18072015047073364\n",
      "training: 41 batch 205 batch_loss: 0.1827726662158966\n",
      "training: 41 batch 206 batch_loss: 0.18089410662651062\n",
      "training: 41 batch 207 batch_loss: 0.1781041920185089\n",
      "training: 41 batch 208 batch_loss: 0.17839857935905457\n",
      "training: 41 batch 209 batch_loss: 0.1815190315246582\n",
      "training: 41 batch 210 batch_loss: 0.17906072735786438\n",
      "training: 41 batch 211 batch_loss: 0.1814672350883484\n",
      "training: 41 batch 212 batch_loss: 0.18220841884613037\n",
      "training: 41 batch 213 batch_loss: 0.1821867823600769\n",
      "training: 41 batch 214 batch_loss: 0.17870819568634033\n",
      "training: 41 batch 215 batch_loss: 0.1791665256023407\n",
      "training: 41 batch 216 batch_loss: 0.18057891726493835\n",
      "training: 41 batch 217 batch_loss: 0.18260157108306885\n",
      "training: 41 batch 218 batch_loss: 0.17857754230499268\n",
      "training: 41 batch 219 batch_loss: 0.18094468116760254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 41 batch 220 batch_loss: 0.1812703013420105\n",
      "training: 41 batch 221 batch_loss: 0.18024224042892456\n",
      "training: 41 batch 222 batch_loss: 0.18096691370010376\n",
      "training: 41 batch 223 batch_loss: 0.17886608839035034\n",
      "training: 41 batch 224 batch_loss: 0.18140077590942383\n",
      "training: 41 batch 225 batch_loss: 0.17792969942092896\n",
      "training: 41 batch 226 batch_loss: 0.18385818600654602\n",
      "training: 41 batch 227 batch_loss: 0.1784537434577942\n",
      "training: 41 batch 228 batch_loss: 0.18223997950553894\n",
      "training: 41 batch 229 batch_loss: 0.18214964866638184\n",
      "training: 41 batch 230 batch_loss: 0.18320566415786743\n",
      "training: 41 batch 231 batch_loss: 0.18277019262313843\n",
      "training: 41 batch 232 batch_loss: 0.1792643964290619\n",
      "training: 41 batch 233 batch_loss: 0.17874199151992798\n",
      "training: 41 batch 234 batch_loss: 0.18171393871307373\n",
      "training: 41 batch 235 batch_loss: 0.1839025914669037\n",
      "training: 41 batch 236 batch_loss: 0.1770714521408081\n",
      "training: 41 batch 237 batch_loss: 0.17858964204788208\n",
      "training: 41 batch 238 batch_loss: 0.1822664737701416\n",
      "training: 41 batch 239 batch_loss: 0.1852545142173767\n",
      "training: 41 batch 240 batch_loss: 0.17958000302314758\n",
      "training: 41 batch 241 batch_loss: 0.18431898951530457\n",
      "training: 41 batch 242 batch_loss: 0.18363147974014282\n",
      "training: 41 batch 243 batch_loss: 0.17768308520317078\n",
      "training: 41 batch 244 batch_loss: 0.1822560727596283\n",
      "training: 41 batch 245 batch_loss: 0.17837458848953247\n",
      "training: 41 batch 246 batch_loss: 0.17982876300811768\n",
      "training: 41 batch 247 batch_loss: 0.17830565571784973\n",
      "training: 41 batch 248 batch_loss: 0.18033894896507263\n",
      "training: 41 batch 249 batch_loss: 0.17956888675689697\n",
      "training: 41 batch 250 batch_loss: 0.1802184283733368\n",
      "training: 41 batch 251 batch_loss: 0.17776912450790405\n",
      "training: 41 batch 252 batch_loss: 0.18047678470611572\n",
      "training: 41 batch 253 batch_loss: 0.17927253246307373\n",
      "training: 41 batch 254 batch_loss: 0.18317300081253052\n",
      "training: 41 batch 255 batch_loss: 0.18028312921524048\n",
      "training: 41 batch 256 batch_loss: 0.1803869605064392\n",
      "training: 41 batch 257 batch_loss: 0.1868920922279358\n",
      "training: 41 batch 258 batch_loss: 0.18348175287246704\n",
      "training: 41 batch 259 batch_loss: 0.18263232707977295\n",
      "training: 41 batch 260 batch_loss: 0.17984843254089355\n",
      "training: 41 batch 261 batch_loss: 0.175814688205719\n",
      "training: 41 batch 262 batch_loss: 0.18061691522598267\n",
      "training: 41 batch 263 batch_loss: 0.18420231342315674\n",
      "training: 41 batch 264 batch_loss: 0.17794033885002136\n",
      "training: 41 batch 265 batch_loss: 0.18223837018013\n",
      "training: 41 batch 266 batch_loss: 0.17850232124328613\n",
      "training: 41 batch 267 batch_loss: 0.18150541186332703\n",
      "training: 41 batch 268 batch_loss: 0.18126937747001648\n",
      "training: 41 batch 269 batch_loss: 0.1789175271987915\n",
      "training: 41 batch 270 batch_loss: 0.18202903866767883\n",
      "training: 41 batch 271 batch_loss: 0.17993107438087463\n",
      "training: 41 batch 272 batch_loss: 0.18100467324256897\n",
      "training: 41 batch 273 batch_loss: 0.17832359671592712\n",
      "training: 41 batch 274 batch_loss: 0.1830168068408966\n",
      "training: 41 batch 275 batch_loss: 0.18341153860092163\n",
      "training: 41 batch 276 batch_loss: 0.18323153257369995\n",
      "training: 41 batch 277 batch_loss: 0.1776852309703827\n",
      "training: 41 batch 278 batch_loss: 0.17877882719039917\n",
      "training: 41 batch 279 batch_loss: 0.17916029691696167\n",
      "training: 41 batch 280 batch_loss: 0.17901167273521423\n",
      "training: 41 batch 281 batch_loss: 0.1809307038784027\n",
      "training: 41 batch 282 batch_loss: 0.17920684814453125\n",
      "training: 41 batch 283 batch_loss: 0.1795632541179657\n",
      "training: 41 batch 284 batch_loss: 0.1785333752632141\n",
      "training: 41 batch 285 batch_loss: 0.1788647174835205\n",
      "training: 41 batch 286 batch_loss: 0.17895832657814026\n",
      "training: 41 batch 287 batch_loss: 0.18204498291015625\n",
      "training: 41 batch 288 batch_loss: 0.18260309100151062\n",
      "training: 41 batch 289 batch_loss: 0.18050342798233032\n",
      "training: 41 batch 290 batch_loss: 0.17976301908493042\n",
      "training: 41 batch 291 batch_loss: 0.18503212928771973\n",
      "training: 41 batch 292 batch_loss: 0.17771396040916443\n",
      "training: 41 batch 293 batch_loss: 0.18054798245429993\n",
      "training: 41 batch 294 batch_loss: 0.18506231904029846\n",
      "training: 41 batch 295 batch_loss: 0.18205946683883667\n",
      "training: 41 batch 296 batch_loss: 0.18525442481040955\n",
      "training: 41 batch 297 batch_loss: 0.17957338690757751\n",
      "training: 41 batch 298 batch_loss: 0.18378007411956787\n",
      "training: 41 batch 299 batch_loss: 0.18395498394966125\n",
      "training: 41 batch 300 batch_loss: 0.17822805047035217\n",
      "training: 41 batch 301 batch_loss: 0.17721125483512878\n",
      "training: 41 batch 302 batch_loss: 0.17985600233078003\n",
      "training: 41 batch 303 batch_loss: 0.1816081702709198\n",
      "training: 41 batch 304 batch_loss: 0.1799478828907013\n",
      "training: 41 batch 305 batch_loss: 0.18032371997833252\n",
      "training: 41 batch 306 batch_loss: 0.18201595544815063\n",
      "training: 41 batch 307 batch_loss: 0.1812760829925537\n",
      "training: 41 batch 308 batch_loss: 0.18033906817436218\n",
      "training: 41 batch 309 batch_loss: 0.18035027384757996\n",
      "training: 41 batch 310 batch_loss: 0.18141257762908936\n",
      "training: 41 batch 311 batch_loss: 0.17884281277656555\n",
      "training: 41 batch 312 batch_loss: 0.18365079164505005\n",
      "training: 41 batch 313 batch_loss: 0.1860608458518982\n",
      "training: 41 batch 314 batch_loss: 0.1815139651298523\n",
      "training: 41 batch 315 batch_loss: 0.18635714054107666\n",
      "training: 41 batch 316 batch_loss: 0.17794471979141235\n",
      "training: 41 batch 317 batch_loss: 0.18341389298439026\n",
      "training: 41 batch 318 batch_loss: 0.18036454916000366\n",
      "training: 41 batch 319 batch_loss: 0.1808050274848938\n",
      "training: 41 batch 320 batch_loss: 0.1834719181060791\n",
      "training: 41 batch 321 batch_loss: 0.1872287392616272\n",
      "training: 41 batch 322 batch_loss: 0.18215608596801758\n",
      "training: 41 batch 323 batch_loss: 0.1775924563407898\n",
      "training: 41 batch 324 batch_loss: 0.17999202013015747\n",
      "training: 41 batch 325 batch_loss: 0.17855864763259888\n",
      "training: 41 batch 326 batch_loss: 0.18090581893920898\n",
      "training: 41 batch 327 batch_loss: 0.18172687292099\n",
      "training: 41 batch 328 batch_loss: 0.18520697951316833\n",
      "training: 41 batch 329 batch_loss: 0.18108105659484863\n",
      "training: 41 batch 330 batch_loss: 0.1771724820137024\n",
      "training: 41 batch 331 batch_loss: 0.1803421676158905\n",
      "training: 41 batch 332 batch_loss: 0.1825261414051056\n",
      "training: 41 batch 333 batch_loss: 0.18343719840049744\n",
      "training: 41 batch 334 batch_loss: 0.1826118528842926\n",
      "training: 41 batch 335 batch_loss: 0.1809816062450409\n",
      "training: 41 batch 336 batch_loss: 0.18057548999786377\n",
      "training: 41 batch 337 batch_loss: 0.18033450841903687\n",
      "training: 41 batch 338 batch_loss: 0.18288129568099976\n",
      "training: 41 batch 339 batch_loss: 0.17842096090316772\n",
      "training: 41 batch 340 batch_loss: 0.18356037139892578\n",
      "training: 41 batch 341 batch_loss: 0.18182384967803955\n",
      "training: 41 batch 342 batch_loss: 0.18105575442314148\n",
      "training: 41 batch 343 batch_loss: 0.17763829231262207\n",
      "training: 41 batch 344 batch_loss: 0.18193817138671875\n",
      "training: 41 batch 345 batch_loss: 0.1842333972454071\n",
      "training: 41 batch 346 batch_loss: 0.18224036693572998\n",
      "training: 41 batch 347 batch_loss: 0.18094560503959656\n",
      "training: 41 batch 348 batch_loss: 0.18355077505111694\n",
      "training: 41 batch 349 batch_loss: 0.1788475513458252\n",
      "training: 41 batch 350 batch_loss: 0.17978110909461975\n",
      "training: 41 batch 351 batch_loss: 0.17800894379615784\n",
      "training: 41 batch 352 batch_loss: 0.18302488327026367\n",
      "training: 41 batch 353 batch_loss: 0.18265146017074585\n",
      "training: 41 batch 354 batch_loss: 0.17761754989624023\n",
      "training: 41 batch 355 batch_loss: 0.18241971731185913\n",
      "training: 41 batch 356 batch_loss: 0.18239089846611023\n",
      "training: 41 batch 357 batch_loss: 0.18024736642837524\n",
      "training: 41 batch 358 batch_loss: 0.18115875124931335\n",
      "training: 41 batch 359 batch_loss: 0.18253237009048462\n",
      "training: 41 batch 360 batch_loss: 0.17940333485603333\n",
      "training: 41 batch 361 batch_loss: 0.18053659796714783\n",
      "training: 41 batch 362 batch_loss: 0.18408054113388062\n",
      "training: 41 batch 363 batch_loss: 0.17378979921340942\n",
      "training: 41 batch 364 batch_loss: 0.18260809779167175\n",
      "training: 41 batch 365 batch_loss: 0.18038952350616455\n",
      "training: 41 batch 366 batch_loss: 0.18444016575813293\n",
      "training: 41 batch 367 batch_loss: 0.17551535367965698\n",
      "training: 41 batch 368 batch_loss: 0.17678233981132507\n",
      "training: 41 batch 369 batch_loss: 0.18639472126960754\n",
      "training: 41 batch 370 batch_loss: 0.1777525246143341\n",
      "training: 41 batch 371 batch_loss: 0.18256688117980957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 41 batch 372 batch_loss: 0.1769956350326538\n",
      "training: 41 batch 373 batch_loss: 0.18064215779304504\n",
      "training: 41 batch 374 batch_loss: 0.17705732583999634\n",
      "training: 41 batch 375 batch_loss: 0.18108955025672913\n",
      "training: 41 batch 376 batch_loss: 0.18320760130882263\n",
      "training: 41 batch 377 batch_loss: 0.17646527290344238\n",
      "training: 41 batch 378 batch_loss: 0.18345841765403748\n",
      "training: 41 batch 379 batch_loss: 0.1870788037776947\n",
      "training: 41 batch 380 batch_loss: 0.17936649918556213\n",
      "training: 41 batch 381 batch_loss: 0.18290799856185913\n",
      "training: 41 batch 382 batch_loss: 0.1821911334991455\n",
      "training: 41 batch 383 batch_loss: 0.1830470860004425\n",
      "training: 41 batch 384 batch_loss: 0.18512409925460815\n",
      "training: 41 batch 385 batch_loss: 0.18010643124580383\n",
      "training: 41 batch 386 batch_loss: 0.17894569039344788\n",
      "training: 41 batch 387 batch_loss: 0.18121469020843506\n",
      "training: 41 batch 388 batch_loss: 0.18134236335754395\n",
      "training: 41 batch 389 batch_loss: 0.18441292643547058\n",
      "training: 41 batch 390 batch_loss: 0.18123197555541992\n",
      "training: 41 batch 391 batch_loss: 0.180122971534729\n",
      "training: 41 batch 392 batch_loss: 0.1818082332611084\n",
      "training: 41 batch 393 batch_loss: 0.18260180950164795\n",
      "training: 41 batch 394 batch_loss: 0.18334519863128662\n",
      "training: 41 batch 395 batch_loss: 0.177994966506958\n",
      "training: 41 batch 396 batch_loss: 0.1800467073917389\n",
      "training: 41 batch 397 batch_loss: 0.1861611008644104\n",
      "training: 41 batch 398 batch_loss: 0.17947152256965637\n",
      "training: 41 batch 399 batch_loss: 0.18190166354179382\n",
      "training: 41 batch 400 batch_loss: 0.18532446026802063\n",
      "training: 41 batch 401 batch_loss: 0.18025603890419006\n",
      "training: 41 batch 402 batch_loss: 0.18115058541297913\n",
      "training: 41 batch 403 batch_loss: 0.17931202054023743\n",
      "training: 41 batch 404 batch_loss: 0.1806832253932953\n",
      "training: 41 batch 405 batch_loss: 0.17859584093093872\n",
      "training: 41 batch 406 batch_loss: 0.1842688024044037\n",
      "training: 41 batch 407 batch_loss: 0.1802193820476532\n",
      "training: 41 batch 408 batch_loss: 0.17840725183486938\n",
      "training: 41 batch 409 batch_loss: 0.186524897813797\n",
      "training: 41 batch 410 batch_loss: 0.18182075023651123\n",
      "training: 41 batch 411 batch_loss: 0.18106207251548767\n",
      "training: 41 batch 412 batch_loss: 0.18208110332489014\n",
      "training: 41 batch 413 batch_loss: 0.18568798899650574\n",
      "training: 41 batch 414 batch_loss: 0.18320924043655396\n",
      "training: 41 batch 415 batch_loss: 0.18457266688346863\n",
      "training: 41 batch 416 batch_loss: 0.18354010581970215\n",
      "training: 41 batch 417 batch_loss: 0.1796257197856903\n",
      "training: 41 batch 418 batch_loss: 0.17898237705230713\n",
      "training: 41 batch 419 batch_loss: 0.1813340187072754\n",
      "training: 41 batch 420 batch_loss: 0.18059107661247253\n",
      "training: 41 batch 421 batch_loss: 0.18062317371368408\n",
      "training: 41 batch 422 batch_loss: 0.1815319061279297\n",
      "training: 41 batch 423 batch_loss: 0.17923757433891296\n",
      "training: 41 batch 424 batch_loss: 0.181360125541687\n",
      "training: 41 batch 425 batch_loss: 0.17754411697387695\n",
      "training: 41 batch 426 batch_loss: 0.1809883713722229\n",
      "training: 41 batch 427 batch_loss: 0.18177884817123413\n",
      "training: 41 batch 428 batch_loss: 0.18576788902282715\n",
      "training: 41 batch 429 batch_loss: 0.17810401320457458\n",
      "training: 41 batch 430 batch_loss: 0.18086162209510803\n",
      "training: 41 batch 431 batch_loss: 0.18402248620986938\n",
      "training: 41 batch 432 batch_loss: 0.18225035071372986\n",
      "training: 41 batch 433 batch_loss: 0.1803225874900818\n",
      "training: 41 batch 434 batch_loss: 0.1841772198677063\n",
      "training: 41 batch 435 batch_loss: 0.1793592870235443\n",
      "training: 41 batch 436 batch_loss: 0.1830974817276001\n",
      "training: 41 batch 437 batch_loss: 0.1829054355621338\n",
      "training: 41 batch 438 batch_loss: 0.1835566759109497\n",
      "training: 41 batch 439 batch_loss: 0.18058836460113525\n",
      "training: 41 batch 440 batch_loss: 0.18550783395767212\n",
      "training: 41 batch 441 batch_loss: 0.1811537742614746\n",
      "training: 41 batch 442 batch_loss: 0.18102344870567322\n",
      "training: 41 batch 443 batch_loss: 0.17959260940551758\n",
      "training: 41 batch 444 batch_loss: 0.18420112133026123\n",
      "training: 41 batch 445 batch_loss: 0.1812293827533722\n",
      "training: 41 batch 446 batch_loss: 0.18153437972068787\n",
      "training: 41 batch 447 batch_loss: 0.18035709857940674\n",
      "training: 41 batch 448 batch_loss: 0.1846112608909607\n",
      "training: 41 batch 449 batch_loss: 0.1825810670852661\n",
      "training: 41 batch 450 batch_loss: 0.18083176016807556\n",
      "training: 41 batch 451 batch_loss: 0.18313226103782654\n",
      "training: 41 batch 452 batch_loss: 0.1813998818397522\n",
      "training: 41 batch 453 batch_loss: 0.18162581324577332\n",
      "training: 41 batch 454 batch_loss: 0.18423837423324585\n",
      "training: 41 batch 455 batch_loss: 0.18362608551979065\n",
      "training: 41 batch 456 batch_loss: 0.18094637989997864\n",
      "training: 41 batch 457 batch_loss: 0.1805657148361206\n",
      "training: 41 batch 458 batch_loss: 0.1803743541240692\n",
      "training: 41 batch 459 batch_loss: 0.18000733852386475\n",
      "training: 41 batch 460 batch_loss: 0.17997288703918457\n",
      "training: 41 batch 461 batch_loss: 0.18107497692108154\n",
      "training: 41 batch 462 batch_loss: 0.18097132444381714\n",
      "training: 41 batch 463 batch_loss: 0.18203258514404297\n",
      "training: 41 batch 464 batch_loss: 0.18210238218307495\n",
      "training: 41 batch 465 batch_loss: 0.17850860953330994\n",
      "training: 41 batch 466 batch_loss: 0.1855459213256836\n",
      "training: 41 batch 467 batch_loss: 0.1837402880191803\n",
      "training: 41 batch 468 batch_loss: 0.1822848916053772\n",
      "training: 41 batch 469 batch_loss: 0.18568938970565796\n",
      "training: 41 batch 470 batch_loss: 0.181851327419281\n",
      "training: 41 batch 471 batch_loss: 0.17934006452560425\n",
      "training: 41 batch 472 batch_loss: 0.18097558617591858\n",
      "training: 41 batch 473 batch_loss: 0.18442541360855103\n",
      "training: 41 batch 474 batch_loss: 0.18298110365867615\n",
      "training: 41 batch 475 batch_loss: 0.18021824955940247\n",
      "training: 41 batch 476 batch_loss: 0.18581563234329224\n",
      "training: 41 batch 477 batch_loss: 0.18085026741027832\n",
      "training: 41 batch 478 batch_loss: 0.18110093474388123\n",
      "training: 41 batch 479 batch_loss: 0.18344566226005554\n",
      "training: 41 batch 480 batch_loss: 0.18185827136039734\n",
      "training: 41 batch 481 batch_loss: 0.18070358037948608\n",
      "training: 41 batch 482 batch_loss: 0.18147391080856323\n",
      "training: 41 batch 483 batch_loss: 0.18173840641975403\n",
      "training: 41 batch 484 batch_loss: 0.17849212884902954\n",
      "training: 41 batch 485 batch_loss: 0.18079116940498352\n",
      "training: 41 batch 486 batch_loss: 0.1776568591594696\n",
      "training: 41 batch 487 batch_loss: 0.18207034468650818\n",
      "training: 41 batch 488 batch_loss: 0.18244051933288574\n",
      "training: 41 batch 489 batch_loss: 0.18517646193504333\n",
      "training: 41 batch 490 batch_loss: 0.18642207980155945\n",
      "training: 41 batch 491 batch_loss: 0.18131014704704285\n",
      "training: 41 batch 492 batch_loss: 0.18309572339057922\n",
      "training: 41 batch 493 batch_loss: 0.18277788162231445\n",
      "training: 41 batch 494 batch_loss: 0.1802220642566681\n",
      "training: 41 batch 495 batch_loss: 0.1834837794303894\n",
      "training: 41 batch 496 batch_loss: 0.1847350299358368\n",
      "training: 41 batch 497 batch_loss: 0.18396636843681335\n",
      "training: 41 batch 498 batch_loss: 0.18092268705368042\n",
      "training: 41 batch 499 batch_loss: 0.1838514804840088\n",
      "training: 41 batch 500 batch_loss: 0.18226581811904907\n",
      "training: 41 batch 501 batch_loss: 0.18021267652511597\n",
      "training: 41 batch 502 batch_loss: 0.18000397086143494\n",
      "training: 41 batch 503 batch_loss: 0.18353432416915894\n",
      "training: 41 batch 504 batch_loss: 0.18141889572143555\n",
      "training: 41 batch 505 batch_loss: 0.18221566081047058\n",
      "training: 41 batch 506 batch_loss: 0.18142634630203247\n",
      "training: 41 batch 507 batch_loss: 0.1802113950252533\n",
      "training: 41 batch 508 batch_loss: 0.17856475710868835\n",
      "training: 41 batch 509 batch_loss: 0.182092547416687\n",
      "training: 41 batch 510 batch_loss: 0.18537339568138123\n",
      "training: 41 batch 511 batch_loss: 0.18684428930282593\n",
      "training: 41 batch 512 batch_loss: 0.18258586525917053\n",
      "training: 41 batch 513 batch_loss: 0.17835170030593872\n",
      "training: 41 batch 514 batch_loss: 0.1861872673034668\n",
      "training: 41 batch 515 batch_loss: 0.1826077401638031\n",
      "training: 41 batch 516 batch_loss: 0.1810416281223297\n",
      "training: 41 batch 517 batch_loss: 0.18185582756996155\n",
      "training: 41 batch 518 batch_loss: 0.18245425820350647\n",
      "training: 41 batch 519 batch_loss: 0.18527507781982422\n",
      "training: 41 batch 520 batch_loss: 0.1816648244857788\n",
      "training: 41 batch 521 batch_loss: 0.18276852369308472\n",
      "training: 41 batch 522 batch_loss: 0.18033546209335327\n",
      "training: 41 batch 523 batch_loss: 0.18016797304153442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 41 batch 524 batch_loss: 0.18172809481620789\n",
      "training: 41 batch 525 batch_loss: 0.18227344751358032\n",
      "training: 41 batch 526 batch_loss: 0.18146714568138123\n",
      "training: 41 batch 527 batch_loss: 0.18155214190483093\n",
      "training: 41 batch 528 batch_loss: 0.1826341152191162\n",
      "training: 41 batch 529 batch_loss: 0.1793592870235443\n",
      "training: 41 batch 530 batch_loss: 0.1834316849708557\n",
      "training: 41 batch 531 batch_loss: 0.1848413646221161\n",
      "training: 41 batch 532 batch_loss: 0.18319377303123474\n",
      "training: 41 batch 533 batch_loss: 0.18045908212661743\n",
      "training: 41 batch 534 batch_loss: 0.18082523345947266\n",
      "training: 41 batch 535 batch_loss: 0.1805250644683838\n",
      "training: 41 batch 536 batch_loss: 0.17995911836624146\n",
      "training: 41 batch 537 batch_loss: 0.18305379152297974\n",
      "training: 41 batch 538 batch_loss: 0.18239283561706543\n",
      "training: 41 batch 539 batch_loss: 0.1813446581363678\n",
      "training: 41 batch 540 batch_loss: 0.17980659008026123\n",
      "training: 41 batch 541 batch_loss: 0.1832047998905182\n",
      "training: 41 batch 542 batch_loss: 0.18087252974510193\n",
      "training: 41 batch 543 batch_loss: 0.18152213096618652\n",
      "training: 41 batch 544 batch_loss: 0.1796705722808838\n",
      "training: 41 batch 545 batch_loss: 0.17992079257965088\n",
      "training: 41 batch 546 batch_loss: 0.182442307472229\n",
      "training: 41 batch 547 batch_loss: 0.18107813596725464\n",
      "training: 41 batch 548 batch_loss: 0.1826220452785492\n",
      "training: 41 batch 549 batch_loss: 0.17799890041351318\n",
      "training: 41 batch 550 batch_loss: 0.18292224407196045\n",
      "training: 41 batch 551 batch_loss: 0.1824207603931427\n",
      "training: 41 batch 552 batch_loss: 0.18301421403884888\n",
      "training: 41 batch 553 batch_loss: 0.176835834980011\n",
      "training: 41 batch 554 batch_loss: 0.18121641874313354\n",
      "training: 41 batch 555 batch_loss: 0.18396630883216858\n",
      "training: 41 batch 556 batch_loss: 0.18193531036376953\n",
      "training: 41 batch 557 batch_loss: 0.18522456288337708\n",
      "training: 41 batch 558 batch_loss: 0.18244698643684387\n",
      "training: 41 batch 559 batch_loss: 0.18290546536445618\n",
      "training: 41 batch 560 batch_loss: 0.18280768394470215\n",
      "training: 41 batch 561 batch_loss: 0.17908450961112976\n",
      "training: 41 batch 562 batch_loss: 0.18070223927497864\n",
      "training: 41 batch 563 batch_loss: 0.1770416796207428\n",
      "training: 41 batch 564 batch_loss: 0.1831425130367279\n",
      "training: 41 batch 565 batch_loss: 0.180128276348114\n",
      "training: 41 batch 566 batch_loss: 0.17917466163635254\n",
      "training: 41 batch 567 batch_loss: 0.18278849124908447\n",
      "training: 41 batch 568 batch_loss: 0.1852964460849762\n",
      "training: 41 batch 569 batch_loss: 0.18776151537895203\n",
      "training: 41 batch 570 batch_loss: 0.183001309633255\n",
      "training: 41 batch 571 batch_loss: 0.18262922763824463\n",
      "training: 41 batch 572 batch_loss: 0.17976781725883484\n",
      "training: 41 batch 573 batch_loss: 0.18667995929718018\n",
      "training: 41 batch 574 batch_loss: 0.1853574514389038\n",
      "training: 41 batch 575 batch_loss: 0.17989152669906616\n",
      "training: 41 batch 576 batch_loss: 0.17791220545768738\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 41, Hit Ratio:0.031056686864783654 | Precision:0.045822274648579575 | Recall:0.06125216371310793 | NDCG:0.05922230839856843\n",
      "*Best Performance* \n",
      "Epoch: 38, Hit Ratio:0.03119992804629022 | Precision:0.04603361840165143 | Recall:0.06105512710452418 | MDCG:0.059088839669294624\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 42 batch 0 batch_loss: 0.1798010766506195\n",
      "training: 42 batch 1 batch_loss: 0.17873099446296692\n",
      "training: 42 batch 2 batch_loss: 0.1819070279598236\n",
      "training: 42 batch 3 batch_loss: 0.17992040514945984\n",
      "training: 42 batch 4 batch_loss: 0.1847151517868042\n",
      "training: 42 batch 5 batch_loss: 0.17719584703445435\n",
      "training: 42 batch 6 batch_loss: 0.18272742629051208\n",
      "training: 42 batch 7 batch_loss: 0.17781895399093628\n",
      "training: 42 batch 8 batch_loss: 0.18176740407943726\n",
      "training: 42 batch 9 batch_loss: 0.1792295277118683\n",
      "training: 42 batch 10 batch_loss: 0.18022096157073975\n",
      "training: 42 batch 11 batch_loss: 0.18261626362800598\n",
      "training: 42 batch 12 batch_loss: 0.18151357769966125\n",
      "training: 42 batch 13 batch_loss: 0.17961272597312927\n",
      "training: 42 batch 14 batch_loss: 0.17814725637435913\n",
      "training: 42 batch 15 batch_loss: 0.18360668420791626\n",
      "training: 42 batch 16 batch_loss: 0.17901551723480225\n",
      "training: 42 batch 17 batch_loss: 0.18405738472938538\n",
      "training: 42 batch 18 batch_loss: 0.18008697032928467\n",
      "training: 42 batch 19 batch_loss: 0.18017107248306274\n",
      "training: 42 batch 20 batch_loss: 0.1829497516155243\n",
      "training: 42 batch 21 batch_loss: 0.18420761823654175\n",
      "training: 42 batch 22 batch_loss: 0.1799774169921875\n",
      "training: 42 batch 23 batch_loss: 0.17863059043884277\n",
      "training: 42 batch 24 batch_loss: 0.18160071969032288\n",
      "training: 42 batch 25 batch_loss: 0.1795126497745514\n",
      "training: 42 batch 26 batch_loss: 0.1842564344406128\n",
      "training: 42 batch 27 batch_loss: 0.17642265558242798\n",
      "training: 42 batch 28 batch_loss: 0.1813516914844513\n",
      "training: 42 batch 29 batch_loss: 0.18092361092567444\n",
      "training: 42 batch 30 batch_loss: 0.18072056770324707\n",
      "training: 42 batch 31 batch_loss: 0.17951586842536926\n",
      "training: 42 batch 32 batch_loss: 0.1773172914981842\n",
      "training: 42 batch 33 batch_loss: 0.18421918153762817\n",
      "training: 42 batch 34 batch_loss: 0.1774013638496399\n",
      "training: 42 batch 35 batch_loss: 0.17723211646080017\n",
      "training: 42 batch 36 batch_loss: 0.1796606183052063\n",
      "training: 42 batch 37 batch_loss: 0.18250304460525513\n",
      "training: 42 batch 38 batch_loss: 0.18248897790908813\n",
      "training: 42 batch 39 batch_loss: 0.18358856439590454\n",
      "training: 42 batch 40 batch_loss: 0.17990851402282715\n",
      "training: 42 batch 41 batch_loss: 0.17928007245063782\n",
      "training: 42 batch 42 batch_loss: 0.18017560243606567\n",
      "training: 42 batch 43 batch_loss: 0.1834457516670227\n",
      "training: 42 batch 44 batch_loss: 0.17992857098579407\n",
      "training: 42 batch 45 batch_loss: 0.17820316553115845\n",
      "training: 42 batch 46 batch_loss: 0.18402481079101562\n",
      "training: 42 batch 47 batch_loss: 0.1825782060623169\n",
      "training: 42 batch 48 batch_loss: 0.18295255303382874\n",
      "training: 42 batch 49 batch_loss: 0.18104586005210876\n",
      "training: 42 batch 50 batch_loss: 0.1806909739971161\n",
      "training: 42 batch 51 batch_loss: 0.18067198991775513\n",
      "training: 42 batch 52 batch_loss: 0.1831527054309845\n",
      "training: 42 batch 53 batch_loss: 0.18587088584899902\n",
      "training: 42 batch 54 batch_loss: 0.18172752857208252\n",
      "training: 42 batch 55 batch_loss: 0.17810720205307007\n",
      "training: 42 batch 56 batch_loss: 0.1775343120098114\n",
      "training: 42 batch 57 batch_loss: 0.17851945757865906\n",
      "training: 42 batch 58 batch_loss: 0.17966917157173157\n",
      "training: 42 batch 59 batch_loss: 0.18307191133499146\n",
      "training: 42 batch 60 batch_loss: 0.18305158615112305\n",
      "training: 42 batch 61 batch_loss: 0.18086957931518555\n",
      "training: 42 batch 62 batch_loss: 0.18091726303100586\n",
      "training: 42 batch 63 batch_loss: 0.18380099534988403\n",
      "training: 42 batch 64 batch_loss: 0.18305769562721252\n",
      "training: 42 batch 65 batch_loss: 0.18373441696166992\n",
      "training: 42 batch 66 batch_loss: 0.17899519205093384\n",
      "training: 42 batch 67 batch_loss: 0.18018460273742676\n",
      "training: 42 batch 68 batch_loss: 0.181300550699234\n",
      "training: 42 batch 69 batch_loss: 0.18162652850151062\n",
      "training: 42 batch 70 batch_loss: 0.1789468228816986\n",
      "training: 42 batch 71 batch_loss: 0.18304497003555298\n",
      "training: 42 batch 72 batch_loss: 0.18089646100997925\n",
      "training: 42 batch 73 batch_loss: 0.18336308002471924\n",
      "training: 42 batch 74 batch_loss: 0.1775164008140564\n",
      "training: 42 batch 75 batch_loss: 0.18480759859085083\n",
      "training: 42 batch 76 batch_loss: 0.1828024983406067\n",
      "training: 42 batch 77 batch_loss: 0.1808469295501709\n",
      "training: 42 batch 78 batch_loss: 0.18003559112548828\n",
      "training: 42 batch 79 batch_loss: 0.178124338388443\n",
      "training: 42 batch 80 batch_loss: 0.17978203296661377\n",
      "training: 42 batch 81 batch_loss: 0.18037846684455872\n",
      "training: 42 batch 82 batch_loss: 0.17905142903327942\n",
      "training: 42 batch 83 batch_loss: 0.18353012204170227\n",
      "training: 42 batch 84 batch_loss: 0.18163594603538513\n",
      "training: 42 batch 85 batch_loss: 0.18036645650863647\n",
      "training: 42 batch 86 batch_loss: 0.1821327805519104\n",
      "training: 42 batch 87 batch_loss: 0.18428772687911987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 42 batch 88 batch_loss: 0.18712204694747925\n",
      "training: 42 batch 89 batch_loss: 0.18172794580459595\n",
      "training: 42 batch 90 batch_loss: 0.1823863387107849\n",
      "training: 42 batch 91 batch_loss: 0.18002036213874817\n",
      "training: 42 batch 92 batch_loss: 0.17766356468200684\n",
      "training: 42 batch 93 batch_loss: 0.18349260091781616\n",
      "training: 42 batch 94 batch_loss: 0.18239173293113708\n",
      "training: 42 batch 95 batch_loss: 0.1811785101890564\n",
      "training: 42 batch 96 batch_loss: 0.18445801734924316\n",
      "training: 42 batch 97 batch_loss: 0.1844768524169922\n",
      "training: 42 batch 98 batch_loss: 0.18038064241409302\n",
      "training: 42 batch 99 batch_loss: 0.1829807162284851\n",
      "training: 42 batch 100 batch_loss: 0.18310850858688354\n",
      "training: 42 batch 101 batch_loss: 0.18096253275871277\n",
      "training: 42 batch 102 batch_loss: 0.17988771200180054\n",
      "training: 42 batch 103 batch_loss: 0.1809409260749817\n",
      "training: 42 batch 104 batch_loss: 0.17694824934005737\n",
      "training: 42 batch 105 batch_loss: 0.18342986702919006\n",
      "training: 42 batch 106 batch_loss: 0.18063780665397644\n",
      "training: 42 batch 107 batch_loss: 0.1771838366985321\n",
      "training: 42 batch 108 batch_loss: 0.18170234560966492\n",
      "training: 42 batch 109 batch_loss: 0.1867300271987915\n",
      "training: 42 batch 110 batch_loss: 0.1811736822128296\n",
      "training: 42 batch 111 batch_loss: 0.17897331714630127\n",
      "training: 42 batch 112 batch_loss: 0.18540123105049133\n",
      "training: 42 batch 113 batch_loss: 0.18157464265823364\n",
      "training: 42 batch 114 batch_loss: 0.18230798840522766\n",
      "training: 42 batch 115 batch_loss: 0.18155831098556519\n",
      "training: 42 batch 116 batch_loss: 0.17948433756828308\n",
      "training: 42 batch 117 batch_loss: 0.18403685092926025\n",
      "training: 42 batch 118 batch_loss: 0.1804884672164917\n",
      "training: 42 batch 119 batch_loss: 0.1823311746120453\n",
      "training: 42 batch 120 batch_loss: 0.18193945288658142\n",
      "training: 42 batch 121 batch_loss: 0.1777077317237854\n",
      "training: 42 batch 122 batch_loss: 0.1790129542350769\n",
      "training: 42 batch 123 batch_loss: 0.18049675226211548\n",
      "training: 42 batch 124 batch_loss: 0.18104562163352966\n",
      "training: 42 batch 125 batch_loss: 0.18361780047416687\n",
      "training: 42 batch 126 batch_loss: 0.1805107593536377\n",
      "training: 42 batch 127 batch_loss: 0.18536704778671265\n",
      "training: 42 batch 128 batch_loss: 0.18317103385925293\n",
      "training: 42 batch 129 batch_loss: 0.18280303478240967\n",
      "training: 42 batch 130 batch_loss: 0.1815035045146942\n",
      "training: 42 batch 131 batch_loss: 0.18040698766708374\n",
      "training: 42 batch 132 batch_loss: 0.18221116065979004\n",
      "training: 42 batch 133 batch_loss: 0.18454623222351074\n",
      "training: 42 batch 134 batch_loss: 0.17981868982315063\n",
      "training: 42 batch 135 batch_loss: 0.1824723780155182\n",
      "training: 42 batch 136 batch_loss: 0.1826387345790863\n",
      "training: 42 batch 137 batch_loss: 0.18087580800056458\n",
      "training: 42 batch 138 batch_loss: 0.17634651064872742\n",
      "training: 42 batch 139 batch_loss: 0.1799021065235138\n",
      "training: 42 batch 140 batch_loss: 0.1824449896812439\n",
      "training: 42 batch 141 batch_loss: 0.18057003617286682\n",
      "training: 42 batch 142 batch_loss: 0.182695209980011\n",
      "training: 42 batch 143 batch_loss: 0.1798914670944214\n",
      "training: 42 batch 144 batch_loss: 0.18411260843276978\n",
      "training: 42 batch 145 batch_loss: 0.17827960848808289\n",
      "training: 42 batch 146 batch_loss: 0.18417423963546753\n",
      "training: 42 batch 147 batch_loss: 0.1788596510887146\n",
      "training: 42 batch 148 batch_loss: 0.18081331253051758\n",
      "training: 42 batch 149 batch_loss: 0.18416568636894226\n",
      "training: 42 batch 150 batch_loss: 0.1803293228149414\n",
      "training: 42 batch 151 batch_loss: 0.1813834309577942\n",
      "training: 42 batch 152 batch_loss: 0.18086212873458862\n",
      "training: 42 batch 153 batch_loss: 0.18095442652702332\n",
      "training: 42 batch 154 batch_loss: 0.18072566390037537\n",
      "training: 42 batch 155 batch_loss: 0.1839059591293335\n",
      "training: 42 batch 156 batch_loss: 0.18245357275009155\n",
      "training: 42 batch 157 batch_loss: 0.18381810188293457\n",
      "training: 42 batch 158 batch_loss: 0.18012937903404236\n",
      "training: 42 batch 159 batch_loss: 0.17465731501579285\n",
      "training: 42 batch 160 batch_loss: 0.18145698308944702\n",
      "training: 42 batch 161 batch_loss: 0.1844671666622162\n",
      "training: 42 batch 162 batch_loss: 0.18344002962112427\n",
      "training: 42 batch 163 batch_loss: 0.1796284317970276\n",
      "training: 42 batch 164 batch_loss: 0.18295741081237793\n",
      "training: 42 batch 165 batch_loss: 0.1848350167274475\n",
      "training: 42 batch 166 batch_loss: 0.1823958456516266\n",
      "training: 42 batch 167 batch_loss: 0.18271180987358093\n",
      "training: 42 batch 168 batch_loss: 0.1821780800819397\n",
      "training: 42 batch 169 batch_loss: 0.18173593282699585\n",
      "training: 42 batch 170 batch_loss: 0.1821196973323822\n",
      "training: 42 batch 171 batch_loss: 0.17585960030555725\n",
      "training: 42 batch 172 batch_loss: 0.18156248331069946\n",
      "training: 42 batch 173 batch_loss: 0.1799805760383606\n",
      "training: 42 batch 174 batch_loss: 0.18074968457221985\n",
      "training: 42 batch 175 batch_loss: 0.18225741386413574\n",
      "training: 42 batch 176 batch_loss: 0.18001216650009155\n",
      "training: 42 batch 177 batch_loss: 0.17809543013572693\n",
      "training: 42 batch 178 batch_loss: 0.18158823251724243\n",
      "training: 42 batch 179 batch_loss: 0.18358337879180908\n",
      "training: 42 batch 180 batch_loss: 0.18106982111930847\n",
      "training: 42 batch 181 batch_loss: 0.18118715286254883\n",
      "training: 42 batch 182 batch_loss: 0.18137726187705994\n",
      "training: 42 batch 183 batch_loss: 0.18206658959388733\n",
      "training: 42 batch 184 batch_loss: 0.1834026277065277\n",
      "training: 42 batch 185 batch_loss: 0.1764155924320221\n",
      "training: 42 batch 186 batch_loss: 0.17904776334762573\n",
      "training: 42 batch 187 batch_loss: 0.18182340264320374\n",
      "training: 42 batch 188 batch_loss: 0.18005186319351196\n",
      "training: 42 batch 189 batch_loss: 0.18358349800109863\n",
      "training: 42 batch 190 batch_loss: 0.18158897757530212\n",
      "training: 42 batch 191 batch_loss: 0.1825476586818695\n",
      "training: 42 batch 192 batch_loss: 0.18139031529426575\n",
      "training: 42 batch 193 batch_loss: 0.18251264095306396\n",
      "training: 42 batch 194 batch_loss: 0.1793341040611267\n",
      "training: 42 batch 195 batch_loss: 0.18423151969909668\n",
      "training: 42 batch 196 batch_loss: 0.18467587232589722\n",
      "training: 42 batch 197 batch_loss: 0.18236371874809265\n",
      "training: 42 batch 198 batch_loss: 0.18100059032440186\n",
      "training: 42 batch 199 batch_loss: 0.1806471347808838\n",
      "training: 42 batch 200 batch_loss: 0.1815689206123352\n",
      "training: 42 batch 201 batch_loss: 0.18669819831848145\n",
      "training: 42 batch 202 batch_loss: 0.1809602975845337\n",
      "training: 42 batch 203 batch_loss: 0.18395555019378662\n",
      "training: 42 batch 204 batch_loss: 0.18117916584014893\n",
      "training: 42 batch 205 batch_loss: 0.1774919331073761\n",
      "training: 42 batch 206 batch_loss: 0.1763814091682434\n",
      "training: 42 batch 207 batch_loss: 0.185641348361969\n",
      "training: 42 batch 208 batch_loss: 0.182001531124115\n",
      "training: 42 batch 209 batch_loss: 0.1835947334766388\n",
      "training: 42 batch 210 batch_loss: 0.17559713125228882\n",
      "training: 42 batch 211 batch_loss: 0.1816316843032837\n",
      "training: 42 batch 212 batch_loss: 0.17931219935417175\n",
      "training: 42 batch 213 batch_loss: 0.17952996492385864\n",
      "training: 42 batch 214 batch_loss: 0.17788541316986084\n",
      "training: 42 batch 215 batch_loss: 0.17835944890975952\n",
      "training: 42 batch 216 batch_loss: 0.18515005707740784\n",
      "training: 42 batch 217 batch_loss: 0.1855623424053192\n",
      "training: 42 batch 218 batch_loss: 0.18113496899604797\n",
      "training: 42 batch 219 batch_loss: 0.1756347119808197\n",
      "training: 42 batch 220 batch_loss: 0.1841689944267273\n",
      "training: 42 batch 221 batch_loss: 0.18193039298057556\n",
      "training: 42 batch 222 batch_loss: 0.18486371636390686\n",
      "training: 42 batch 223 batch_loss: 0.18232563138008118\n",
      "training: 42 batch 224 batch_loss: 0.18297314643859863\n",
      "training: 42 batch 225 batch_loss: 0.1800495982170105\n",
      "training: 42 batch 226 batch_loss: 0.17627421021461487\n",
      "training: 42 batch 227 batch_loss: 0.1810150444507599\n",
      "training: 42 batch 228 batch_loss: 0.18103203177452087\n",
      "training: 42 batch 229 batch_loss: 0.18232598900794983\n",
      "training: 42 batch 230 batch_loss: 0.18415507674217224\n",
      "training: 42 batch 231 batch_loss: 0.18346929550170898\n",
      "training: 42 batch 232 batch_loss: 0.17990219593048096\n",
      "training: 42 batch 233 batch_loss: 0.17998307943344116\n",
      "training: 42 batch 234 batch_loss: 0.18127888441085815\n",
      "training: 42 batch 235 batch_loss: 0.1866045892238617\n",
      "training: 42 batch 236 batch_loss: 0.18643933534622192\n",
      "training: 42 batch 237 batch_loss: 0.18378937244415283\n",
      "training: 42 batch 238 batch_loss: 0.18532517552375793\n",
      "training: 42 batch 239 batch_loss: 0.18491986393928528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 42 batch 240 batch_loss: 0.17747557163238525\n",
      "training: 42 batch 241 batch_loss: 0.18086853623390198\n",
      "training: 42 batch 242 batch_loss: 0.182157963514328\n",
      "training: 42 batch 243 batch_loss: 0.17887961864471436\n",
      "training: 42 batch 244 batch_loss: 0.17951110005378723\n",
      "training: 42 batch 245 batch_loss: 0.17839616537094116\n",
      "training: 42 batch 246 batch_loss: 0.1805955171585083\n",
      "training: 42 batch 247 batch_loss: 0.18148699402809143\n",
      "training: 42 batch 248 batch_loss: 0.18062424659729004\n",
      "training: 42 batch 249 batch_loss: 0.18254181742668152\n",
      "training: 42 batch 250 batch_loss: 0.17923209071159363\n",
      "training: 42 batch 251 batch_loss: 0.1809036135673523\n",
      "training: 42 batch 252 batch_loss: 0.1819164752960205\n",
      "training: 42 batch 253 batch_loss: 0.18310129642486572\n",
      "training: 42 batch 254 batch_loss: 0.18477240204811096\n",
      "training: 42 batch 255 batch_loss: 0.17890694737434387\n",
      "training: 42 batch 256 batch_loss: 0.18253320455551147\n",
      "training: 42 batch 257 batch_loss: 0.17629846930503845\n",
      "training: 42 batch 258 batch_loss: 0.17977017164230347\n",
      "training: 42 batch 259 batch_loss: 0.18332397937774658\n",
      "training: 42 batch 260 batch_loss: 0.18018943071365356\n",
      "training: 42 batch 261 batch_loss: 0.18249526619911194\n",
      "training: 42 batch 262 batch_loss: 0.18476524949073792\n",
      "training: 42 batch 263 batch_loss: 0.179521381855011\n",
      "training: 42 batch 264 batch_loss: 0.18105864524841309\n",
      "training: 42 batch 265 batch_loss: 0.18619844317436218\n",
      "training: 42 batch 266 batch_loss: 0.18248838186264038\n",
      "training: 42 batch 267 batch_loss: 0.18355780839920044\n",
      "training: 42 batch 268 batch_loss: 0.18153530359268188\n",
      "training: 42 batch 269 batch_loss: 0.18376898765563965\n",
      "training: 42 batch 270 batch_loss: 0.18294858932495117\n",
      "training: 42 batch 271 batch_loss: 0.18333476781845093\n",
      "training: 42 batch 272 batch_loss: 0.17879390716552734\n",
      "training: 42 batch 273 batch_loss: 0.18540805578231812\n",
      "training: 42 batch 274 batch_loss: 0.18017134070396423\n",
      "training: 42 batch 275 batch_loss: 0.17885062098503113\n",
      "training: 42 batch 276 batch_loss: 0.18226397037506104\n",
      "training: 42 batch 277 batch_loss: 0.18123498558998108\n",
      "training: 42 batch 278 batch_loss: 0.1816522479057312\n",
      "training: 42 batch 279 batch_loss: 0.18541285395622253\n",
      "training: 42 batch 280 batch_loss: 0.18154820799827576\n",
      "training: 42 batch 281 batch_loss: 0.1841583549976349\n",
      "training: 42 batch 282 batch_loss: 0.17593681812286377\n",
      "training: 42 batch 283 batch_loss: 0.18121355772018433\n",
      "training: 42 batch 284 batch_loss: 0.18056190013885498\n",
      "training: 42 batch 285 batch_loss: 0.18231931328773499\n",
      "training: 42 batch 286 batch_loss: 0.18091928958892822\n",
      "training: 42 batch 287 batch_loss: 0.18668237328529358\n",
      "training: 42 batch 288 batch_loss: 0.1824135184288025\n",
      "training: 42 batch 289 batch_loss: 0.1836615800857544\n",
      "training: 42 batch 290 batch_loss: 0.1819385588169098\n",
      "training: 42 batch 291 batch_loss: 0.18052247166633606\n",
      "training: 42 batch 292 batch_loss: 0.18185529112815857\n",
      "training: 42 batch 293 batch_loss: 0.18114310503005981\n",
      "training: 42 batch 294 batch_loss: 0.18404269218444824\n",
      "training: 42 batch 295 batch_loss: 0.18507176637649536\n",
      "training: 42 batch 296 batch_loss: 0.180306077003479\n",
      "training: 42 batch 297 batch_loss: 0.18513959646224976\n",
      "training: 42 batch 298 batch_loss: 0.1861553192138672\n",
      "training: 42 batch 299 batch_loss: 0.1868278682231903\n",
      "training: 42 batch 300 batch_loss: 0.18028423190116882\n",
      "training: 42 batch 301 batch_loss: 0.18318814039230347\n",
      "training: 42 batch 302 batch_loss: 0.18160825967788696\n",
      "training: 42 batch 303 batch_loss: 0.17830169200897217\n",
      "training: 42 batch 304 batch_loss: 0.17422205209732056\n",
      "training: 42 batch 305 batch_loss: 0.1832287609577179\n",
      "training: 42 batch 306 batch_loss: 0.18234464526176453\n",
      "training: 42 batch 307 batch_loss: 0.1789412498474121\n",
      "training: 42 batch 308 batch_loss: 0.1821383237838745\n",
      "training: 42 batch 309 batch_loss: 0.18398192524909973\n",
      "training: 42 batch 310 batch_loss: 0.17868119478225708\n",
      "training: 42 batch 311 batch_loss: 0.17987322807312012\n",
      "training: 42 batch 312 batch_loss: 0.18031933903694153\n",
      "training: 42 batch 313 batch_loss: 0.18037474155426025\n",
      "training: 42 batch 314 batch_loss: 0.18113350868225098\n",
      "training: 42 batch 315 batch_loss: 0.179032564163208\n",
      "training: 42 batch 316 batch_loss: 0.17931467294692993\n",
      "training: 42 batch 317 batch_loss: 0.18227910995483398\n",
      "training: 42 batch 318 batch_loss: 0.18449127674102783\n",
      "training: 42 batch 319 batch_loss: 0.1859123706817627\n",
      "training: 42 batch 320 batch_loss: 0.1828346848487854\n",
      "training: 42 batch 321 batch_loss: 0.1790866255760193\n",
      "training: 42 batch 322 batch_loss: 0.18213000893592834\n",
      "training: 42 batch 323 batch_loss: 0.1833968162536621\n",
      "training: 42 batch 324 batch_loss: 0.1823665201663971\n",
      "training: 42 batch 325 batch_loss: 0.18322968482971191\n",
      "training: 42 batch 326 batch_loss: 0.18451282382011414\n",
      "training: 42 batch 327 batch_loss: 0.1810809075832367\n",
      "training: 42 batch 328 batch_loss: 0.18651306629180908\n",
      "training: 42 batch 329 batch_loss: 0.180019348859787\n",
      "training: 42 batch 330 batch_loss: 0.18662399053573608\n",
      "training: 42 batch 331 batch_loss: 0.18133625388145447\n",
      "training: 42 batch 332 batch_loss: 0.1827959418296814\n",
      "training: 42 batch 333 batch_loss: 0.1838359534740448\n",
      "training: 42 batch 334 batch_loss: 0.18303611874580383\n",
      "training: 42 batch 335 batch_loss: 0.17619407176971436\n",
      "training: 42 batch 336 batch_loss: 0.17981475591659546\n",
      "training: 42 batch 337 batch_loss: 0.18015363812446594\n",
      "training: 42 batch 338 batch_loss: 0.18204110860824585\n",
      "training: 42 batch 339 batch_loss: 0.1830022931098938\n",
      "training: 42 batch 340 batch_loss: 0.1806623637676239\n",
      "training: 42 batch 341 batch_loss: 0.1793668270111084\n",
      "training: 42 batch 342 batch_loss: 0.18182498216629028\n",
      "training: 42 batch 343 batch_loss: 0.1836949586868286\n",
      "training: 42 batch 344 batch_loss: 0.18545001745224\n",
      "training: 42 batch 345 batch_loss: 0.18174511194229126\n",
      "training: 42 batch 346 batch_loss: 0.18387925624847412\n",
      "training: 42 batch 347 batch_loss: 0.18077844381332397\n",
      "training: 42 batch 348 batch_loss: 0.18308043479919434\n",
      "training: 42 batch 349 batch_loss: 0.18564176559448242\n",
      "training: 42 batch 350 batch_loss: 0.18370643258094788\n",
      "training: 42 batch 351 batch_loss: 0.1795063018798828\n",
      "training: 42 batch 352 batch_loss: 0.18100017309188843\n",
      "training: 42 batch 353 batch_loss: 0.18071791529655457\n",
      "training: 42 batch 354 batch_loss: 0.18018728494644165\n",
      "training: 42 batch 355 batch_loss: 0.18188855051994324\n",
      "training: 42 batch 356 batch_loss: 0.1834697127342224\n",
      "training: 42 batch 357 batch_loss: 0.18419212102890015\n",
      "training: 42 batch 358 batch_loss: 0.18483227491378784\n",
      "training: 42 batch 359 batch_loss: 0.18316510319709778\n",
      "training: 42 batch 360 batch_loss: 0.1841854751110077\n",
      "training: 42 batch 361 batch_loss: 0.1819230020046234\n",
      "training: 42 batch 362 batch_loss: 0.18242940306663513\n",
      "training: 42 batch 363 batch_loss: 0.1851658821105957\n",
      "training: 42 batch 364 batch_loss: 0.18390575051307678\n",
      "training: 42 batch 365 batch_loss: 0.1835240125656128\n",
      "training: 42 batch 366 batch_loss: 0.18175002932548523\n",
      "training: 42 batch 367 batch_loss: 0.18438762426376343\n",
      "training: 42 batch 368 batch_loss: 0.18509826064109802\n",
      "training: 42 batch 369 batch_loss: 0.1843704879283905\n",
      "training: 42 batch 370 batch_loss: 0.183984637260437\n",
      "training: 42 batch 371 batch_loss: 0.17979827523231506\n",
      "training: 42 batch 372 batch_loss: 0.1816020905971527\n",
      "training: 42 batch 373 batch_loss: 0.18256822228431702\n",
      "training: 42 batch 374 batch_loss: 0.18376290798187256\n",
      "training: 42 batch 375 batch_loss: 0.18350592255592346\n",
      "training: 42 batch 376 batch_loss: 0.1796090006828308\n",
      "training: 42 batch 377 batch_loss: 0.18327900767326355\n",
      "training: 42 batch 378 batch_loss: 0.18573668599128723\n",
      "training: 42 batch 379 batch_loss: 0.18137535452842712\n",
      "training: 42 batch 380 batch_loss: 0.1793566644191742\n",
      "training: 42 batch 381 batch_loss: 0.17927879095077515\n",
      "training: 42 batch 382 batch_loss: 0.1870524287223816\n",
      "training: 42 batch 383 batch_loss: 0.17832118272781372\n",
      "training: 42 batch 384 batch_loss: 0.18048515915870667\n",
      "training: 42 batch 385 batch_loss: 0.1758083999156952\n",
      "training: 42 batch 386 batch_loss: 0.18176224827766418\n",
      "training: 42 batch 387 batch_loss: 0.1843680739402771\n",
      "training: 42 batch 388 batch_loss: 0.18163806200027466\n",
      "training: 42 batch 389 batch_loss: 0.1787174940109253\n",
      "training: 42 batch 390 batch_loss: 0.17946922779083252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 42 batch 391 batch_loss: 0.18445199728012085\n",
      "training: 42 batch 392 batch_loss: 0.1866433322429657\n",
      "training: 42 batch 393 batch_loss: 0.18435701727867126\n",
      "training: 42 batch 394 batch_loss: 0.18114757537841797\n",
      "training: 42 batch 395 batch_loss: 0.1797301471233368\n",
      "training: 42 batch 396 batch_loss: 0.18309491872787476\n",
      "training: 42 batch 397 batch_loss: 0.18278688192367554\n",
      "training: 42 batch 398 batch_loss: 0.17904415726661682\n",
      "training: 42 batch 399 batch_loss: 0.1867450773715973\n",
      "training: 42 batch 400 batch_loss: 0.18120011687278748\n",
      "training: 42 batch 401 batch_loss: 0.18321165442466736\n",
      "training: 42 batch 402 batch_loss: 0.18784552812576294\n",
      "training: 42 batch 403 batch_loss: 0.18304866552352905\n",
      "training: 42 batch 404 batch_loss: 0.1850588321685791\n",
      "training: 42 batch 405 batch_loss: 0.18201375007629395\n",
      "training: 42 batch 406 batch_loss: 0.18387866020202637\n",
      "training: 42 batch 407 batch_loss: 0.1834273338317871\n",
      "training: 42 batch 408 batch_loss: 0.18212872743606567\n",
      "training: 42 batch 409 batch_loss: 0.18040740489959717\n",
      "training: 42 batch 410 batch_loss: 0.1874144971370697\n",
      "training: 42 batch 411 batch_loss: 0.18502408266067505\n",
      "training: 42 batch 412 batch_loss: 0.18077117204666138\n",
      "training: 42 batch 413 batch_loss: 0.18503940105438232\n",
      "training: 42 batch 414 batch_loss: 0.18105676770210266\n",
      "training: 42 batch 415 batch_loss: 0.18224990367889404\n",
      "training: 42 batch 416 batch_loss: 0.1847870647907257\n",
      "training: 42 batch 417 batch_loss: 0.18105226755142212\n",
      "training: 42 batch 418 batch_loss: 0.18122225999832153\n",
      "training: 42 batch 419 batch_loss: 0.1837673783302307\n",
      "training: 42 batch 420 batch_loss: 0.18337330222129822\n",
      "training: 42 batch 421 batch_loss: 0.18381184339523315\n",
      "training: 42 batch 422 batch_loss: 0.18736445903778076\n",
      "training: 42 batch 423 batch_loss: 0.184247225522995\n",
      "training: 42 batch 424 batch_loss: 0.18584659695625305\n",
      "training: 42 batch 425 batch_loss: 0.18212676048278809\n",
      "training: 42 batch 426 batch_loss: 0.18040481209754944\n",
      "training: 42 batch 427 batch_loss: 0.1838987171649933\n",
      "training: 42 batch 428 batch_loss: 0.18502697348594666\n",
      "training: 42 batch 429 batch_loss: 0.1837928593158722\n",
      "training: 42 batch 430 batch_loss: 0.18390727043151855\n",
      "training: 42 batch 431 batch_loss: 0.18626976013183594\n",
      "training: 42 batch 432 batch_loss: 0.1855696737766266\n",
      "training: 42 batch 433 batch_loss: 0.18317687511444092\n",
      "training: 42 batch 434 batch_loss: 0.18459057807922363\n",
      "training: 42 batch 435 batch_loss: 0.1834433674812317\n",
      "training: 42 batch 436 batch_loss: 0.18526878952980042\n",
      "training: 42 batch 437 batch_loss: 0.17880958318710327\n",
      "training: 42 batch 438 batch_loss: 0.18042367696762085\n",
      "training: 42 batch 439 batch_loss: 0.17964163422584534\n",
      "training: 42 batch 440 batch_loss: 0.18228000402450562\n",
      "training: 42 batch 441 batch_loss: 0.17976966500282288\n",
      "training: 42 batch 442 batch_loss: 0.18372541666030884\n",
      "training: 42 batch 443 batch_loss: 0.18296849727630615\n",
      "training: 42 batch 444 batch_loss: 0.1842726469039917\n",
      "training: 42 batch 445 batch_loss: 0.18138167262077332\n",
      "training: 42 batch 446 batch_loss: 0.18171554803848267\n",
      "training: 42 batch 447 batch_loss: 0.18483668565750122\n",
      "training: 42 batch 448 batch_loss: 0.18411099910736084\n",
      "training: 42 batch 449 batch_loss: 0.18471768498420715\n",
      "training: 42 batch 450 batch_loss: 0.18202659487724304\n",
      "training: 42 batch 451 batch_loss: 0.18515974283218384\n",
      "training: 42 batch 452 batch_loss: 0.1836404800415039\n",
      "training: 42 batch 453 batch_loss: 0.17976242303848267\n",
      "training: 42 batch 454 batch_loss: 0.18160122632980347\n",
      "training: 42 batch 455 batch_loss: 0.1849081814289093\n",
      "training: 42 batch 456 batch_loss: 0.18205711245536804\n",
      "training: 42 batch 457 batch_loss: 0.18249157071113586\n",
      "training: 42 batch 458 batch_loss: 0.1811412274837494\n",
      "training: 42 batch 459 batch_loss: 0.1849745213985443\n",
      "training: 42 batch 460 batch_loss: 0.18384084105491638\n",
      "training: 42 batch 461 batch_loss: 0.18940818309783936\n",
      "training: 42 batch 462 batch_loss: 0.18293774127960205\n",
      "training: 42 batch 463 batch_loss: 0.1832011342048645\n",
      "training: 42 batch 464 batch_loss: 0.1840282678604126\n",
      "training: 42 batch 465 batch_loss: 0.18372976779937744\n",
      "training: 42 batch 466 batch_loss: 0.18154484033584595\n",
      "training: 42 batch 467 batch_loss: 0.18378734588623047\n",
      "training: 42 batch 468 batch_loss: 0.1843019425868988\n",
      "training: 42 batch 469 batch_loss: 0.1818324327468872\n",
      "training: 42 batch 470 batch_loss: 0.18603050708770752\n",
      "training: 42 batch 471 batch_loss: 0.18272072076797485\n",
      "training: 42 batch 472 batch_loss: 0.18332311511039734\n",
      "training: 42 batch 473 batch_loss: 0.1798698604106903\n",
      "training: 42 batch 474 batch_loss: 0.18275785446166992\n",
      "training: 42 batch 475 batch_loss: 0.18193340301513672\n",
      "training: 42 batch 476 batch_loss: 0.18560126423835754\n",
      "training: 42 batch 477 batch_loss: 0.1868458390235901\n",
      "training: 42 batch 478 batch_loss: 0.17989039421081543\n",
      "training: 42 batch 479 batch_loss: 0.1826704740524292\n",
      "training: 42 batch 480 batch_loss: 0.1788349151611328\n",
      "training: 42 batch 481 batch_loss: 0.18546709418296814\n",
      "training: 42 batch 482 batch_loss: 0.18625256419181824\n",
      "training: 42 batch 483 batch_loss: 0.18212950229644775\n",
      "training: 42 batch 484 batch_loss: 0.18197333812713623\n",
      "training: 42 batch 485 batch_loss: 0.18866658210754395\n",
      "training: 42 batch 486 batch_loss: 0.18475192785263062\n",
      "training: 42 batch 487 batch_loss: 0.1875254511833191\n",
      "training: 42 batch 488 batch_loss: 0.18422147631645203\n",
      "training: 42 batch 489 batch_loss: 0.1822027862071991\n",
      "training: 42 batch 490 batch_loss: 0.1790153980255127\n",
      "training: 42 batch 491 batch_loss: 0.17938661575317383\n",
      "training: 42 batch 492 batch_loss: 0.18665242195129395\n",
      "training: 42 batch 493 batch_loss: 0.18136322498321533\n",
      "training: 42 batch 494 batch_loss: 0.18406987190246582\n",
      "training: 42 batch 495 batch_loss: 0.18581601977348328\n",
      "training: 42 batch 496 batch_loss: 0.18370604515075684\n",
      "training: 42 batch 497 batch_loss: 0.18459230661392212\n",
      "training: 42 batch 498 batch_loss: 0.18095484375953674\n",
      "training: 42 batch 499 batch_loss: 0.1836419701576233\n",
      "training: 42 batch 500 batch_loss: 0.18847763538360596\n",
      "training: 42 batch 501 batch_loss: 0.18302667140960693\n",
      "training: 42 batch 502 batch_loss: 0.1816478669643402\n",
      "training: 42 batch 503 batch_loss: 0.18354493379592896\n",
      "training: 42 batch 504 batch_loss: 0.1817314624786377\n",
      "training: 42 batch 505 batch_loss: 0.18175962567329407\n",
      "training: 42 batch 506 batch_loss: 0.18314939737319946\n",
      "training: 42 batch 507 batch_loss: 0.1792447865009308\n",
      "training: 42 batch 508 batch_loss: 0.1860680878162384\n",
      "training: 42 batch 509 batch_loss: 0.18362915515899658\n",
      "training: 42 batch 510 batch_loss: 0.18350696563720703\n",
      "training: 42 batch 511 batch_loss: 0.18689268827438354\n",
      "training: 42 batch 512 batch_loss: 0.18233337998390198\n",
      "training: 42 batch 513 batch_loss: 0.18464931845664978\n",
      "training: 42 batch 514 batch_loss: 0.1806538701057434\n",
      "training: 42 batch 515 batch_loss: 0.18457195162773132\n",
      "training: 42 batch 516 batch_loss: 0.1826176643371582\n",
      "training: 42 batch 517 batch_loss: 0.19072026014328003\n",
      "training: 42 batch 518 batch_loss: 0.18291276693344116\n",
      "training: 42 batch 519 batch_loss: 0.18487608432769775\n",
      "training: 42 batch 520 batch_loss: 0.18522262573242188\n",
      "training: 42 batch 521 batch_loss: 0.18755948543548584\n",
      "training: 42 batch 522 batch_loss: 0.18199223279953003\n",
      "training: 42 batch 523 batch_loss: 0.18328315019607544\n",
      "training: 42 batch 524 batch_loss: 0.1811903715133667\n",
      "training: 42 batch 525 batch_loss: 0.18503105640411377\n",
      "training: 42 batch 526 batch_loss: 0.1813182234764099\n",
      "training: 42 batch 527 batch_loss: 0.18109261989593506\n",
      "training: 42 batch 528 batch_loss: 0.18497848510742188\n",
      "training: 42 batch 529 batch_loss: 0.1836903691291809\n",
      "training: 42 batch 530 batch_loss: 0.1847270429134369\n",
      "training: 42 batch 531 batch_loss: 0.1817307472229004\n",
      "training: 42 batch 532 batch_loss: 0.18473923206329346\n",
      "training: 42 batch 533 batch_loss: 0.1821940541267395\n",
      "training: 42 batch 534 batch_loss: 0.18222397565841675\n",
      "training: 42 batch 535 batch_loss: 0.17942655086517334\n",
      "training: 42 batch 536 batch_loss: 0.18349438905715942\n",
      "training: 42 batch 537 batch_loss: 0.18382829427719116\n",
      "training: 42 batch 538 batch_loss: 0.1874805986881256\n",
      "training: 42 batch 539 batch_loss: 0.1840326189994812\n",
      "training: 42 batch 540 batch_loss: 0.1811811923980713\n",
      "training: 42 batch 541 batch_loss: 0.18044891953468323\n",
      "training: 42 batch 542 batch_loss: 0.18515890836715698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 42 batch 543 batch_loss: 0.18255043029785156\n",
      "training: 42 batch 544 batch_loss: 0.18197700381278992\n",
      "training: 42 batch 545 batch_loss: 0.18314233422279358\n",
      "training: 42 batch 546 batch_loss: 0.1822843849658966\n",
      "training: 42 batch 547 batch_loss: 0.1833888292312622\n",
      "training: 42 batch 548 batch_loss: 0.18616396188735962\n",
      "training: 42 batch 549 batch_loss: 0.18625864386558533\n",
      "training: 42 batch 550 batch_loss: 0.18442559242248535\n",
      "training: 42 batch 551 batch_loss: 0.18624353408813477\n",
      "training: 42 batch 552 batch_loss: 0.185960590839386\n",
      "training: 42 batch 553 batch_loss: 0.18631824851036072\n",
      "training: 42 batch 554 batch_loss: 0.18658679723739624\n",
      "training: 42 batch 555 batch_loss: 0.1816217601299286\n",
      "training: 42 batch 556 batch_loss: 0.17987167835235596\n",
      "training: 42 batch 557 batch_loss: 0.18427419662475586\n",
      "training: 42 batch 558 batch_loss: 0.1862100064754486\n",
      "training: 42 batch 559 batch_loss: 0.18142130970954895\n",
      "training: 42 batch 560 batch_loss: 0.1845744550228119\n",
      "training: 42 batch 561 batch_loss: 0.18539977073669434\n",
      "training: 42 batch 562 batch_loss: 0.18504345417022705\n",
      "training: 42 batch 563 batch_loss: 0.18357476592063904\n",
      "training: 42 batch 564 batch_loss: 0.18600550293922424\n",
      "training: 42 batch 565 batch_loss: 0.18298184871673584\n",
      "training: 42 batch 566 batch_loss: 0.17924243211746216\n",
      "training: 42 batch 567 batch_loss: 0.18078845739364624\n",
      "training: 42 batch 568 batch_loss: 0.18440783023834229\n",
      "training: 42 batch 569 batch_loss: 0.18490928411483765\n",
      "training: 42 batch 570 batch_loss: 0.1833229660987854\n",
      "training: 42 batch 571 batch_loss: 0.17973726987838745\n",
      "training: 42 batch 572 batch_loss: 0.18652617931365967\n",
      "training: 42 batch 573 batch_loss: 0.18780362606048584\n",
      "training: 42 batch 574 batch_loss: 0.18718302249908447\n",
      "training: 42 batch 575 batch_loss: 0.18222206830978394\n",
      "training: 42 batch 576 batch_loss: 0.17764469981193542\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 42, Hit Ratio:0.03140646184288108 | Precision:0.046338346603755035 | Recall:0.06190608394888862 | NDCG:0.05987162979216292\n",
      "*Best Performance* \n",
      "Epoch: 42, Hit Ratio:0.03140646184288108 | Precision:0.046338346603755035 | Recall:0.06190608394888862 | MDCG:0.05987162979216292\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 43 batch 0 batch_loss: 0.18226751685142517\n",
      "training: 43 batch 1 batch_loss: 0.18150490522384644\n",
      "training: 43 batch 2 batch_loss: 0.18148788809776306\n",
      "training: 43 batch 3 batch_loss: 0.18525314331054688\n",
      "training: 43 batch 4 batch_loss: 0.18123769760131836\n",
      "training: 43 batch 5 batch_loss: 0.1819426417350769\n",
      "training: 43 batch 6 batch_loss: 0.1811566948890686\n",
      "training: 43 batch 7 batch_loss: 0.18241748213768005\n",
      "training: 43 batch 8 batch_loss: 0.1790054440498352\n",
      "training: 43 batch 9 batch_loss: 0.18324676156044006\n",
      "training: 43 batch 10 batch_loss: 0.18178272247314453\n",
      "training: 43 batch 11 batch_loss: 0.18131482601165771\n",
      "training: 43 batch 12 batch_loss: 0.18182647228240967\n",
      "training: 43 batch 13 batch_loss: 0.18123045563697815\n",
      "training: 43 batch 14 batch_loss: 0.1827271580696106\n",
      "training: 43 batch 15 batch_loss: 0.18313682079315186\n",
      "training: 43 batch 16 batch_loss: 0.17976975440979004\n",
      "training: 43 batch 17 batch_loss: 0.18415886163711548\n",
      "training: 43 batch 18 batch_loss: 0.1817338764667511\n",
      "training: 43 batch 19 batch_loss: 0.18267425894737244\n",
      "training: 43 batch 20 batch_loss: 0.17726191878318787\n",
      "training: 43 batch 21 batch_loss: 0.18245810270309448\n",
      "training: 43 batch 22 batch_loss: 0.18048298358917236\n",
      "training: 43 batch 23 batch_loss: 0.18073523044586182\n",
      "training: 43 batch 24 batch_loss: 0.18343591690063477\n",
      "training: 43 batch 25 batch_loss: 0.18364650011062622\n",
      "training: 43 batch 26 batch_loss: 0.1781250536441803\n",
      "training: 43 batch 27 batch_loss: 0.18116596341133118\n",
      "training: 43 batch 28 batch_loss: 0.1784026026725769\n",
      "training: 43 batch 29 batch_loss: 0.17833095788955688\n",
      "training: 43 batch 30 batch_loss: 0.18362045288085938\n",
      "training: 43 batch 31 batch_loss: 0.1855170726776123\n",
      "training: 43 batch 32 batch_loss: 0.1818927526473999\n",
      "training: 43 batch 33 batch_loss: 0.18128210306167603\n",
      "training: 43 batch 34 batch_loss: 0.18087559938430786\n",
      "training: 43 batch 35 batch_loss: 0.18076884746551514\n",
      "training: 43 batch 36 batch_loss: 0.18146783113479614\n",
      "training: 43 batch 37 batch_loss: 0.18064773082733154\n",
      "training: 43 batch 38 batch_loss: 0.17860785126686096\n",
      "training: 43 batch 39 batch_loss: 0.17985475063323975\n",
      "training: 43 batch 40 batch_loss: 0.17639100551605225\n",
      "training: 43 batch 41 batch_loss: 0.181407630443573\n",
      "training: 43 batch 42 batch_loss: 0.18642881512641907\n",
      "training: 43 batch 43 batch_loss: 0.18036723136901855\n",
      "training: 43 batch 44 batch_loss: 0.17897304892539978\n",
      "training: 43 batch 45 batch_loss: 0.18465512990951538\n",
      "training: 43 batch 46 batch_loss: 0.18184158205986023\n",
      "training: 43 batch 47 batch_loss: 0.1845528483390808\n",
      "training: 43 batch 48 batch_loss: 0.18228429555892944\n",
      "training: 43 batch 49 batch_loss: 0.1852547526359558\n",
      "training: 43 batch 50 batch_loss: 0.18089866638183594\n",
      "training: 43 batch 51 batch_loss: 0.1826859712600708\n",
      "training: 43 batch 52 batch_loss: 0.18481379747390747\n",
      "training: 43 batch 53 batch_loss: 0.18138760328292847\n",
      "training: 43 batch 54 batch_loss: 0.1809816062450409\n",
      "training: 43 batch 55 batch_loss: 0.18282783031463623\n",
      "training: 43 batch 56 batch_loss: 0.1836060881614685\n",
      "training: 43 batch 57 batch_loss: 0.1827082633972168\n",
      "training: 43 batch 58 batch_loss: 0.183709979057312\n",
      "training: 43 batch 59 batch_loss: 0.1845000684261322\n",
      "training: 43 batch 60 batch_loss: 0.18189343810081482\n",
      "training: 43 batch 61 batch_loss: 0.18205881118774414\n",
      "training: 43 batch 62 batch_loss: 0.17970070242881775\n",
      "training: 43 batch 63 batch_loss: 0.18261396884918213\n",
      "training: 43 batch 64 batch_loss: 0.17975720763206482\n",
      "training: 43 batch 65 batch_loss: 0.18081983923912048\n",
      "training: 43 batch 66 batch_loss: 0.18225491046905518\n",
      "training: 43 batch 67 batch_loss: 0.18230366706848145\n",
      "training: 43 batch 68 batch_loss: 0.18125051259994507\n",
      "training: 43 batch 69 batch_loss: 0.18432000279426575\n",
      "training: 43 batch 70 batch_loss: 0.1886003315448761\n",
      "training: 43 batch 71 batch_loss: 0.1837383508682251\n",
      "training: 43 batch 72 batch_loss: 0.18704348802566528\n",
      "training: 43 batch 73 batch_loss: 0.17850348353385925\n",
      "training: 43 batch 74 batch_loss: 0.18108758330345154\n",
      "training: 43 batch 75 batch_loss: 0.18124550580978394\n",
      "training: 43 batch 76 batch_loss: 0.18273216485977173\n",
      "training: 43 batch 77 batch_loss: 0.1810702085494995\n",
      "training: 43 batch 78 batch_loss: 0.17955517768859863\n",
      "training: 43 batch 79 batch_loss: 0.18319940567016602\n",
      "training: 43 batch 80 batch_loss: 0.1820029616355896\n",
      "training: 43 batch 81 batch_loss: 0.18130534887313843\n",
      "training: 43 batch 82 batch_loss: 0.1795385777950287\n",
      "training: 43 batch 83 batch_loss: 0.18027210235595703\n",
      "training: 43 batch 84 batch_loss: 0.18339776992797852\n",
      "training: 43 batch 85 batch_loss: 0.181745707988739\n",
      "training: 43 batch 86 batch_loss: 0.18113389611244202\n",
      "training: 43 batch 87 batch_loss: 0.18099671602249146\n",
      "training: 43 batch 88 batch_loss: 0.18270042538642883\n",
      "training: 43 batch 89 batch_loss: 0.18615609407424927\n",
      "training: 43 batch 90 batch_loss: 0.1831933856010437\n",
      "training: 43 batch 91 batch_loss: 0.18339788913726807\n",
      "training: 43 batch 92 batch_loss: 0.18257567286491394\n",
      "training: 43 batch 93 batch_loss: 0.18005990982055664\n",
      "training: 43 batch 94 batch_loss: 0.18310922384262085\n",
      "training: 43 batch 95 batch_loss: 0.18311414122581482\n",
      "training: 43 batch 96 batch_loss: 0.18440157175064087\n",
      "training: 43 batch 97 batch_loss: 0.18652111291885376\n",
      "training: 43 batch 98 batch_loss: 0.18319225311279297\n",
      "training: 43 batch 99 batch_loss: 0.1799730658531189\n",
      "training: 43 batch 100 batch_loss: 0.18627005815505981\n",
      "training: 43 batch 101 batch_loss: 0.18147286772727966\n",
      "training: 43 batch 102 batch_loss: 0.17991888523101807\n",
      "training: 43 batch 103 batch_loss: 0.18341606855392456\n",
      "training: 43 batch 104 batch_loss: 0.18669450283050537\n",
      "training: 43 batch 105 batch_loss: 0.18340706825256348\n",
      "training: 43 batch 106 batch_loss: 0.17919549345970154\n",
      "training: 43 batch 107 batch_loss: 0.18427547812461853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 43 batch 108 batch_loss: 0.18433672189712524\n",
      "training: 43 batch 109 batch_loss: 0.18213987350463867\n",
      "training: 43 batch 110 batch_loss: 0.18412312865257263\n",
      "training: 43 batch 111 batch_loss: 0.18518903851509094\n",
      "training: 43 batch 112 batch_loss: 0.18549352884292603\n",
      "training: 43 batch 113 batch_loss: 0.18482449650764465\n",
      "training: 43 batch 114 batch_loss: 0.18328839540481567\n",
      "training: 43 batch 115 batch_loss: 0.18609648942947388\n",
      "training: 43 batch 116 batch_loss: 0.18390905857086182\n",
      "training: 43 batch 117 batch_loss: 0.18086588382720947\n",
      "training: 43 batch 118 batch_loss: 0.18288761377334595\n",
      "training: 43 batch 119 batch_loss: 0.18227508664131165\n",
      "training: 43 batch 120 batch_loss: 0.18403983116149902\n",
      "training: 43 batch 121 batch_loss: 0.18593460321426392\n",
      "training: 43 batch 122 batch_loss: 0.18556642532348633\n",
      "training: 43 batch 123 batch_loss: 0.18574517965316772\n",
      "training: 43 batch 124 batch_loss: 0.18219614028930664\n",
      "training: 43 batch 125 batch_loss: 0.18480393290519714\n",
      "training: 43 batch 126 batch_loss: 0.18443500995635986\n",
      "training: 43 batch 127 batch_loss: 0.1814403235912323\n",
      "training: 43 batch 128 batch_loss: 0.18213018774986267\n",
      "training: 43 batch 129 batch_loss: 0.18483099341392517\n",
      "training: 43 batch 130 batch_loss: 0.18772920966148376\n",
      "training: 43 batch 131 batch_loss: 0.18084385991096497\n",
      "training: 43 batch 132 batch_loss: 0.17978379130363464\n",
      "training: 43 batch 133 batch_loss: 0.1836959719657898\n",
      "training: 43 batch 134 batch_loss: 0.1831103265285492\n",
      "training: 43 batch 135 batch_loss: 0.18210825324058533\n",
      "training: 43 batch 136 batch_loss: 0.18777912855148315\n",
      "training: 43 batch 137 batch_loss: 0.184310644865036\n",
      "training: 43 batch 138 batch_loss: 0.18416127562522888\n",
      "training: 43 batch 139 batch_loss: 0.18453454971313477\n",
      "training: 43 batch 140 batch_loss: 0.18036305904388428\n",
      "training: 43 batch 141 batch_loss: 0.18069157004356384\n",
      "training: 43 batch 142 batch_loss: 0.17835071682929993\n",
      "training: 43 batch 143 batch_loss: 0.1829584836959839\n",
      "training: 43 batch 144 batch_loss: 0.18293723464012146\n",
      "training: 43 batch 145 batch_loss: 0.1815255880355835\n",
      "training: 43 batch 146 batch_loss: 0.18472018837928772\n",
      "training: 43 batch 147 batch_loss: 0.1819630265235901\n",
      "training: 43 batch 148 batch_loss: 0.18509727716445923\n",
      "training: 43 batch 149 batch_loss: 0.1838693916797638\n",
      "training: 43 batch 150 batch_loss: 0.18322107195854187\n",
      "training: 43 batch 151 batch_loss: 0.18047437071800232\n",
      "training: 43 batch 152 batch_loss: 0.1841900646686554\n",
      "training: 43 batch 153 batch_loss: 0.1837887465953827\n",
      "training: 43 batch 154 batch_loss: 0.1846407949924469\n",
      "training: 43 batch 155 batch_loss: 0.18139523267745972\n",
      "training: 43 batch 156 batch_loss: 0.18086367845535278\n",
      "training: 43 batch 157 batch_loss: 0.18351298570632935\n",
      "training: 43 batch 158 batch_loss: 0.1867976188659668\n",
      "training: 43 batch 159 batch_loss: 0.18624964356422424\n",
      "training: 43 batch 160 batch_loss: 0.18099737167358398\n",
      "training: 43 batch 161 batch_loss: 0.18167316913604736\n",
      "training: 43 batch 162 batch_loss: 0.18346533179283142\n",
      "training: 43 batch 163 batch_loss: 0.18459242582321167\n",
      "training: 43 batch 164 batch_loss: 0.18257975578308105\n",
      "training: 43 batch 165 batch_loss: 0.18264293670654297\n",
      "training: 43 batch 166 batch_loss: 0.18167543411254883\n",
      "training: 43 batch 167 batch_loss: 0.18309122323989868\n",
      "training: 43 batch 168 batch_loss: 0.18434348702430725\n",
      "training: 43 batch 169 batch_loss: 0.18113282322883606\n",
      "training: 43 batch 170 batch_loss: 0.18495985865592957\n",
      "training: 43 batch 171 batch_loss: 0.18264323472976685\n",
      "training: 43 batch 172 batch_loss: 0.18122130632400513\n",
      "training: 43 batch 173 batch_loss: 0.1819271445274353\n",
      "training: 43 batch 174 batch_loss: 0.18036049604415894\n",
      "training: 43 batch 175 batch_loss: 0.18146663904190063\n",
      "training: 43 batch 176 batch_loss: 0.18254393339157104\n",
      "training: 43 batch 177 batch_loss: 0.18247827887535095\n",
      "training: 43 batch 178 batch_loss: 0.1887272596359253\n",
      "training: 43 batch 179 batch_loss: 0.18435809016227722\n",
      "training: 43 batch 180 batch_loss: 0.18497127294540405\n",
      "training: 43 batch 181 batch_loss: 0.1821679174900055\n",
      "training: 43 batch 182 batch_loss: 0.17908310890197754\n",
      "training: 43 batch 183 batch_loss: 0.182784765958786\n",
      "training: 43 batch 184 batch_loss: 0.1833830177783966\n",
      "training: 43 batch 185 batch_loss: 0.1832892894744873\n",
      "training: 43 batch 186 batch_loss: 0.18113258481025696\n",
      "training: 43 batch 187 batch_loss: 0.18237394094467163\n",
      "training: 43 batch 188 batch_loss: 0.18439438939094543\n",
      "training: 43 batch 189 batch_loss: 0.18490946292877197\n",
      "training: 43 batch 190 batch_loss: 0.18076717853546143\n",
      "training: 43 batch 191 batch_loss: 0.1822003722190857\n",
      "training: 43 batch 192 batch_loss: 0.18146264553070068\n",
      "training: 43 batch 193 batch_loss: 0.18668949604034424\n",
      "training: 43 batch 194 batch_loss: 0.17867884039878845\n",
      "training: 43 batch 195 batch_loss: 0.18306434154510498\n",
      "training: 43 batch 196 batch_loss: 0.18200808763504028\n",
      "training: 43 batch 197 batch_loss: 0.18246373534202576\n",
      "training: 43 batch 198 batch_loss: 0.1809578537940979\n",
      "training: 43 batch 199 batch_loss: 0.182285338640213\n",
      "training: 43 batch 200 batch_loss: 0.18423756957054138\n",
      "training: 43 batch 201 batch_loss: 0.183051198720932\n",
      "training: 43 batch 202 batch_loss: 0.18364906311035156\n",
      "training: 43 batch 203 batch_loss: 0.18241584300994873\n",
      "training: 43 batch 204 batch_loss: 0.1812976896762848\n",
      "training: 43 batch 205 batch_loss: 0.18188709020614624\n",
      "training: 43 batch 206 batch_loss: 0.18410030007362366\n",
      "training: 43 batch 207 batch_loss: 0.18586641550064087\n",
      "training: 43 batch 208 batch_loss: 0.18051549792289734\n",
      "training: 43 batch 209 batch_loss: 0.18592995405197144\n",
      "training: 43 batch 210 batch_loss: 0.18048027157783508\n",
      "training: 43 batch 211 batch_loss: 0.1807008683681488\n",
      "training: 43 batch 212 batch_loss: 0.18411105871200562\n",
      "training: 43 batch 213 batch_loss: 0.1809232234954834\n",
      "training: 43 batch 214 batch_loss: 0.17917460203170776\n",
      "training: 43 batch 215 batch_loss: 0.1842508316040039\n",
      "training: 43 batch 216 batch_loss: 0.18400925397872925\n",
      "training: 43 batch 217 batch_loss: 0.18748989701271057\n",
      "training: 43 batch 218 batch_loss: 0.18593516945838928\n",
      "training: 43 batch 219 batch_loss: 0.1842385232448578\n",
      "training: 43 batch 220 batch_loss: 0.1828397512435913\n",
      "training: 43 batch 221 batch_loss: 0.18218296766281128\n",
      "training: 43 batch 222 batch_loss: 0.18196174502372742\n",
      "training: 43 batch 223 batch_loss: 0.18319609761238098\n",
      "training: 43 batch 224 batch_loss: 0.18439435958862305\n",
      "training: 43 batch 225 batch_loss: 0.18335369229316711\n",
      "training: 43 batch 226 batch_loss: 0.18639600276947021\n",
      "training: 43 batch 227 batch_loss: 0.18325385451316833\n",
      "training: 43 batch 228 batch_loss: 0.18260273337364197\n",
      "training: 43 batch 229 batch_loss: 0.1796385645866394\n",
      "training: 43 batch 230 batch_loss: 0.18439719080924988\n",
      "training: 43 batch 231 batch_loss: 0.186038076877594\n",
      "training: 43 batch 232 batch_loss: 0.1824636459350586\n",
      "training: 43 batch 233 batch_loss: 0.18314212560653687\n",
      "training: 43 batch 234 batch_loss: 0.18094274401664734\n",
      "training: 43 batch 235 batch_loss: 0.19346067309379578\n",
      "training: 43 batch 236 batch_loss: 0.18190953135490417\n",
      "training: 43 batch 237 batch_loss: 0.1840781271457672\n",
      "training: 43 batch 238 batch_loss: 0.18560245633125305\n",
      "training: 43 batch 239 batch_loss: 0.1841929852962494\n",
      "training: 43 batch 240 batch_loss: 0.18462389707565308\n",
      "training: 43 batch 241 batch_loss: 0.1821361482143402\n",
      "training: 43 batch 242 batch_loss: 0.18808555603027344\n",
      "training: 43 batch 243 batch_loss: 0.18412283062934875\n",
      "training: 43 batch 244 batch_loss: 0.18678689002990723\n",
      "training: 43 batch 245 batch_loss: 0.18088111281394958\n",
      "training: 43 batch 246 batch_loss: 0.18555137515068054\n",
      "training: 43 batch 247 batch_loss: 0.18336719274520874\n",
      "training: 43 batch 248 batch_loss: 0.18406394124031067\n",
      "training: 43 batch 249 batch_loss: 0.18160933256149292\n",
      "training: 43 batch 250 batch_loss: 0.1890259087085724\n",
      "training: 43 batch 251 batch_loss: 0.18453559279441833\n",
      "training: 43 batch 252 batch_loss: 0.1816786825656891\n",
      "training: 43 batch 253 batch_loss: 0.17880958318710327\n",
      "training: 43 batch 254 batch_loss: 0.18499326705932617\n",
      "training: 43 batch 255 batch_loss: 0.18044701218605042\n",
      "training: 43 batch 256 batch_loss: 0.1828373670578003\n",
      "training: 43 batch 257 batch_loss: 0.18588986992835999\n",
      "training: 43 batch 258 batch_loss: 0.18125760555267334\n",
      "training: 43 batch 259 batch_loss: 0.18048334121704102\n",
      "training: 43 batch 260 batch_loss: 0.1874070167541504\n",
      "training: 43 batch 261 batch_loss: 0.1822739839553833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 43 batch 262 batch_loss: 0.18492847681045532\n",
      "training: 43 batch 263 batch_loss: 0.18579316139221191\n",
      "training: 43 batch 264 batch_loss: 0.18145108222961426\n",
      "training: 43 batch 265 batch_loss: 0.18633410334587097\n",
      "training: 43 batch 266 batch_loss: 0.1760968565940857\n",
      "training: 43 batch 267 batch_loss: 0.1815408170223236\n",
      "training: 43 batch 268 batch_loss: 0.1814907193183899\n",
      "training: 43 batch 269 batch_loss: 0.18464544415473938\n",
      "training: 43 batch 270 batch_loss: 0.18210077285766602\n",
      "training: 43 batch 271 batch_loss: 0.1869300901889801\n",
      "training: 43 batch 272 batch_loss: 0.18256914615631104\n",
      "training: 43 batch 273 batch_loss: 0.18342003226280212\n",
      "training: 43 batch 274 batch_loss: 0.1861705183982849\n",
      "training: 43 batch 275 batch_loss: 0.1838163435459137\n",
      "training: 43 batch 276 batch_loss: 0.18306177854537964\n",
      "training: 43 batch 277 batch_loss: 0.18632709980010986\n",
      "training: 43 batch 278 batch_loss: 0.18588212132453918\n",
      "training: 43 batch 279 batch_loss: 0.18518280982971191\n",
      "training: 43 batch 280 batch_loss: 0.18247756361961365\n",
      "training: 43 batch 281 batch_loss: 0.18269914388656616\n",
      "training: 43 batch 282 batch_loss: 0.18085554242134094\n",
      "training: 43 batch 283 batch_loss: 0.18341100215911865\n",
      "training: 43 batch 284 batch_loss: 0.18406888842582703\n",
      "training: 43 batch 285 batch_loss: 0.18434908986091614\n",
      "training: 43 batch 286 batch_loss: 0.18452629446983337\n",
      "training: 43 batch 287 batch_loss: 0.18590831756591797\n",
      "training: 43 batch 288 batch_loss: 0.18352988362312317\n",
      "training: 43 batch 289 batch_loss: 0.1858132779598236\n",
      "training: 43 batch 290 batch_loss: 0.18892398476600647\n",
      "training: 43 batch 291 batch_loss: 0.18243291974067688\n",
      "training: 43 batch 292 batch_loss: 0.18042203783988953\n",
      "training: 43 batch 293 batch_loss: 0.18373501300811768\n",
      "training: 43 batch 294 batch_loss: 0.18317463994026184\n",
      "training: 43 batch 295 batch_loss: 0.18710511922836304\n",
      "training: 43 batch 296 batch_loss: 0.1851830780506134\n",
      "training: 43 batch 297 batch_loss: 0.1840992271900177\n",
      "training: 43 batch 298 batch_loss: 0.18259546160697937\n",
      "training: 43 batch 299 batch_loss: 0.1824435293674469\n",
      "training: 43 batch 300 batch_loss: 0.1867489516735077\n",
      "training: 43 batch 301 batch_loss: 0.18315401673316956\n",
      "training: 43 batch 302 batch_loss: 0.1839798092842102\n",
      "training: 43 batch 303 batch_loss: 0.18405413627624512\n",
      "training: 43 batch 304 batch_loss: 0.18213298916816711\n",
      "training: 43 batch 305 batch_loss: 0.18530380725860596\n",
      "training: 43 batch 306 batch_loss: 0.1838778257369995\n",
      "training: 43 batch 307 batch_loss: 0.18379950523376465\n",
      "training: 43 batch 308 batch_loss: 0.18154487013816833\n",
      "training: 43 batch 309 batch_loss: 0.18279826641082764\n",
      "training: 43 batch 310 batch_loss: 0.1848335862159729\n",
      "training: 43 batch 311 batch_loss: 0.1833903193473816\n",
      "training: 43 batch 312 batch_loss: 0.18685033917427063\n",
      "training: 43 batch 313 batch_loss: 0.18386343121528625\n",
      "training: 43 batch 314 batch_loss: 0.18197953701019287\n",
      "training: 43 batch 315 batch_loss: 0.18660598993301392\n",
      "training: 43 batch 316 batch_loss: 0.18425413966178894\n",
      "training: 43 batch 317 batch_loss: 0.18471303582191467\n",
      "training: 43 batch 318 batch_loss: 0.18494942784309387\n",
      "training: 43 batch 319 batch_loss: 0.18350565433502197\n",
      "training: 43 batch 320 batch_loss: 0.18418553471565247\n",
      "training: 43 batch 321 batch_loss: 0.18534043431282043\n",
      "training: 43 batch 322 batch_loss: 0.18329522013664246\n",
      "training: 43 batch 323 batch_loss: 0.18325766921043396\n",
      "training: 43 batch 324 batch_loss: 0.18284371495246887\n",
      "training: 43 batch 325 batch_loss: 0.18344616889953613\n",
      "training: 43 batch 326 batch_loss: 0.18225601315498352\n",
      "training: 43 batch 327 batch_loss: 0.1869083046913147\n",
      "training: 43 batch 328 batch_loss: 0.18478983640670776\n",
      "training: 43 batch 329 batch_loss: 0.18402090668678284\n",
      "training: 43 batch 330 batch_loss: 0.1802108883857727\n",
      "training: 43 batch 331 batch_loss: 0.18612635135650635\n",
      "training: 43 batch 332 batch_loss: 0.18996912240982056\n",
      "training: 43 batch 333 batch_loss: 0.18314173817634583\n",
      "training: 43 batch 334 batch_loss: 0.18580672144889832\n",
      "training: 43 batch 335 batch_loss: 0.18021413683891296\n",
      "training: 43 batch 336 batch_loss: 0.18464326858520508\n",
      "training: 43 batch 337 batch_loss: 0.18526369333267212\n",
      "training: 43 batch 338 batch_loss: 0.17758801579475403\n",
      "training: 43 batch 339 batch_loss: 0.18591207265853882\n",
      "training: 43 batch 340 batch_loss: 0.18558067083358765\n",
      "training: 43 batch 341 batch_loss: 0.18313372135162354\n",
      "training: 43 batch 342 batch_loss: 0.18519964814186096\n",
      "training: 43 batch 343 batch_loss: 0.17992359399795532\n",
      "training: 43 batch 344 batch_loss: 0.18366384506225586\n",
      "training: 43 batch 345 batch_loss: 0.18546977639198303\n",
      "training: 43 batch 346 batch_loss: 0.18240272998809814\n",
      "training: 43 batch 347 batch_loss: 0.18751287460327148\n",
      "training: 43 batch 348 batch_loss: 0.1816304624080658\n",
      "training: 43 batch 349 batch_loss: 0.18669745326042175\n",
      "training: 43 batch 350 batch_loss: 0.17988801002502441\n",
      "training: 43 batch 351 batch_loss: 0.18358129262924194\n",
      "training: 43 batch 352 batch_loss: 0.1860111653804779\n",
      "training: 43 batch 353 batch_loss: 0.18383264541625977\n",
      "training: 43 batch 354 batch_loss: 0.18402397632598877\n",
      "training: 43 batch 355 batch_loss: 0.18068483471870422\n",
      "training: 43 batch 356 batch_loss: 0.18642735481262207\n",
      "training: 43 batch 357 batch_loss: 0.17970257997512817\n",
      "training: 43 batch 358 batch_loss: 0.18344652652740479\n",
      "training: 43 batch 359 batch_loss: 0.17911794781684875\n",
      "training: 43 batch 360 batch_loss: 0.18069729208946228\n",
      "training: 43 batch 361 batch_loss: 0.18342620134353638\n",
      "training: 43 batch 362 batch_loss: 0.18666449189186096\n",
      "training: 43 batch 363 batch_loss: 0.18722477555274963\n",
      "training: 43 batch 364 batch_loss: 0.18663236498832703\n",
      "training: 43 batch 365 batch_loss: 0.18725314736366272\n",
      "training: 43 batch 366 batch_loss: 0.18492162227630615\n",
      "training: 43 batch 367 batch_loss: 0.18276336789131165\n",
      "training: 43 batch 368 batch_loss: 0.18621563911437988\n",
      "training: 43 batch 369 batch_loss: 0.18725550174713135\n",
      "training: 43 batch 370 batch_loss: 0.18420842289924622\n",
      "training: 43 batch 371 batch_loss: 0.18645638227462769\n",
      "training: 43 batch 372 batch_loss: 0.1847628951072693\n",
      "training: 43 batch 373 batch_loss: 0.18394765257835388\n",
      "training: 43 batch 374 batch_loss: 0.18349483609199524\n",
      "training: 43 batch 375 batch_loss: 0.17909055948257446\n",
      "training: 43 batch 376 batch_loss: 0.18679246306419373\n",
      "training: 43 batch 377 batch_loss: 0.1872493326663971\n",
      "training: 43 batch 378 batch_loss: 0.1847960650920868\n",
      "training: 43 batch 379 batch_loss: 0.1846158504486084\n",
      "training: 43 batch 380 batch_loss: 0.1848868727684021\n",
      "training: 43 batch 381 batch_loss: 0.18238508701324463\n",
      "training: 43 batch 382 batch_loss: 0.18431469798088074\n",
      "training: 43 batch 383 batch_loss: 0.18631142377853394\n",
      "training: 43 batch 384 batch_loss: 0.18181893229484558\n",
      "training: 43 batch 385 batch_loss: 0.18341147899627686\n",
      "training: 43 batch 386 batch_loss: 0.18638476729393005\n",
      "training: 43 batch 387 batch_loss: 0.1829105019569397\n",
      "training: 43 batch 388 batch_loss: 0.18496596813201904\n",
      "training: 43 batch 389 batch_loss: 0.18410468101501465\n",
      "training: 43 batch 390 batch_loss: 0.18866592645645142\n",
      "training: 43 batch 391 batch_loss: 0.1831514835357666\n",
      "training: 43 batch 392 batch_loss: 0.18577399849891663\n",
      "training: 43 batch 393 batch_loss: 0.18825024366378784\n",
      "training: 43 batch 394 batch_loss: 0.18312352895736694\n",
      "training: 43 batch 395 batch_loss: 0.18136009573936462\n",
      "training: 43 batch 396 batch_loss: 0.1850312054157257\n",
      "training: 43 batch 397 batch_loss: 0.18357545137405396\n",
      "training: 43 batch 398 batch_loss: 0.1901056170463562\n",
      "training: 43 batch 399 batch_loss: 0.18604761362075806\n",
      "training: 43 batch 400 batch_loss: 0.1823200285434723\n",
      "training: 43 batch 401 batch_loss: 0.18518558144569397\n",
      "training: 43 batch 402 batch_loss: 0.18611747026443481\n",
      "training: 43 batch 403 batch_loss: 0.18601727485656738\n",
      "training: 43 batch 404 batch_loss: 0.1850910186767578\n",
      "training: 43 batch 405 batch_loss: 0.18274670839309692\n",
      "training: 43 batch 406 batch_loss: 0.18662822246551514\n",
      "training: 43 batch 407 batch_loss: 0.1864030361175537\n",
      "training: 43 batch 408 batch_loss: 0.1861056387424469\n",
      "training: 43 batch 409 batch_loss: 0.1861450970172882\n",
      "training: 43 batch 410 batch_loss: 0.18336522579193115\n",
      "training: 43 batch 411 batch_loss: 0.18539279699325562\n",
      "training: 43 batch 412 batch_loss: 0.18495988845825195\n",
      "training: 43 batch 413 batch_loss: 0.18359050154685974\n",
      "training: 43 batch 414 batch_loss: 0.18466338515281677\n",
      "training: 43 batch 415 batch_loss: 0.18351781368255615\n",
      "training: 43 batch 416 batch_loss: 0.18220171332359314\n",
      "training: 43 batch 417 batch_loss: 0.18601563572883606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 43 batch 418 batch_loss: 0.1831974983215332\n",
      "training: 43 batch 419 batch_loss: 0.18436682224273682\n",
      "training: 43 batch 420 batch_loss: 0.18800309300422668\n",
      "training: 43 batch 421 batch_loss: 0.18667057156562805\n",
      "training: 43 batch 422 batch_loss: 0.18503352999687195\n",
      "training: 43 batch 423 batch_loss: 0.1849403977394104\n",
      "training: 43 batch 424 batch_loss: 0.18419736623764038\n",
      "training: 43 batch 425 batch_loss: 0.1867491602897644\n",
      "training: 43 batch 426 batch_loss: 0.18126675486564636\n",
      "training: 43 batch 427 batch_loss: 0.18802130222320557\n",
      "training: 43 batch 428 batch_loss: 0.18550634384155273\n",
      "training: 43 batch 429 batch_loss: 0.18480807542800903\n",
      "training: 43 batch 430 batch_loss: 0.18796613812446594\n",
      "training: 43 batch 431 batch_loss: 0.18475019931793213\n",
      "training: 43 batch 432 batch_loss: 0.18149706721305847\n",
      "training: 43 batch 433 batch_loss: 0.18241792917251587\n",
      "training: 43 batch 434 batch_loss: 0.18603238463401794\n",
      "training: 43 batch 435 batch_loss: 0.18437892198562622\n",
      "training: 43 batch 436 batch_loss: 0.18263405561447144\n",
      "training: 43 batch 437 batch_loss: 0.18074911832809448\n",
      "training: 43 batch 438 batch_loss: 0.18369221687316895\n",
      "training: 43 batch 439 batch_loss: 0.18689948320388794\n",
      "training: 43 batch 440 batch_loss: 0.18613213300704956\n",
      "training: 43 batch 441 batch_loss: 0.18406641483306885\n",
      "training: 43 batch 442 batch_loss: 0.18372881412506104\n",
      "training: 43 batch 443 batch_loss: 0.18474364280700684\n",
      "training: 43 batch 444 batch_loss: 0.1868809461593628\n",
      "training: 43 batch 445 batch_loss: 0.18429219722747803\n",
      "training: 43 batch 446 batch_loss: 0.18587207794189453\n",
      "training: 43 batch 447 batch_loss: 0.1866627037525177\n",
      "training: 43 batch 448 batch_loss: 0.18633168935775757\n",
      "training: 43 batch 449 batch_loss: 0.18613940477371216\n",
      "training: 43 batch 450 batch_loss: 0.18549734354019165\n",
      "training: 43 batch 451 batch_loss: 0.18252187967300415\n",
      "training: 43 batch 452 batch_loss: 0.18571597337722778\n",
      "training: 43 batch 453 batch_loss: 0.18613824248313904\n",
      "training: 43 batch 454 batch_loss: 0.18527641892433167\n",
      "training: 43 batch 455 batch_loss: 0.18372514843940735\n",
      "training: 43 batch 456 batch_loss: 0.18447768688201904\n",
      "training: 43 batch 457 batch_loss: 0.1898787021636963\n",
      "training: 43 batch 458 batch_loss: 0.18775027990341187\n",
      "training: 43 batch 459 batch_loss: 0.18276771903038025\n",
      "training: 43 batch 460 batch_loss: 0.18636038899421692\n",
      "training: 43 batch 461 batch_loss: 0.18309235572814941\n",
      "training: 43 batch 462 batch_loss: 0.18003594875335693\n",
      "training: 43 batch 463 batch_loss: 0.18742799758911133\n",
      "training: 43 batch 464 batch_loss: 0.18833377957344055\n",
      "training: 43 batch 465 batch_loss: 0.18684014678001404\n",
      "training: 43 batch 466 batch_loss: 0.1807430386543274\n",
      "training: 43 batch 467 batch_loss: 0.1839924454689026\n",
      "training: 43 batch 468 batch_loss: 0.1854325234889984\n",
      "training: 43 batch 469 batch_loss: 0.18004101514816284\n",
      "training: 43 batch 470 batch_loss: 0.18630319833755493\n",
      "training: 43 batch 471 batch_loss: 0.18274971842765808\n",
      "training: 43 batch 472 batch_loss: 0.18594148755073547\n",
      "training: 43 batch 473 batch_loss: 0.18379834294319153\n",
      "training: 43 batch 474 batch_loss: 0.18184953927993774\n",
      "training: 43 batch 475 batch_loss: 0.18323656916618347\n",
      "training: 43 batch 476 batch_loss: 0.18486079573631287\n",
      "training: 43 batch 477 batch_loss: 0.18729251623153687\n",
      "training: 43 batch 478 batch_loss: 0.1861627697944641\n",
      "training: 43 batch 479 batch_loss: 0.186344176530838\n",
      "training: 43 batch 480 batch_loss: 0.18150711059570312\n",
      "training: 43 batch 481 batch_loss: 0.18320974707603455\n",
      "training: 43 batch 482 batch_loss: 0.1850222945213318\n",
      "training: 43 batch 483 batch_loss: 0.1850515604019165\n",
      "training: 43 batch 484 batch_loss: 0.18434393405914307\n",
      "training: 43 batch 485 batch_loss: 0.18012601137161255\n",
      "training: 43 batch 486 batch_loss: 0.18482130765914917\n",
      "training: 43 batch 487 batch_loss: 0.18126091361045837\n",
      "training: 43 batch 488 batch_loss: 0.18616896867752075\n",
      "training: 43 batch 489 batch_loss: 0.18456315994262695\n",
      "training: 43 batch 490 batch_loss: 0.1888079047203064\n",
      "training: 43 batch 491 batch_loss: 0.18225330114364624\n",
      "training: 43 batch 492 batch_loss: 0.18361124396324158\n",
      "training: 43 batch 493 batch_loss: 0.18754348158836365\n",
      "training: 43 batch 494 batch_loss: 0.1853942573070526\n",
      "training: 43 batch 495 batch_loss: 0.18866410851478577\n",
      "training: 43 batch 496 batch_loss: 0.18404370546340942\n",
      "training: 43 batch 497 batch_loss: 0.18839916586875916\n",
      "training: 43 batch 498 batch_loss: 0.18586289882659912\n",
      "training: 43 batch 499 batch_loss: 0.18318480253219604\n",
      "training: 43 batch 500 batch_loss: 0.18443933129310608\n",
      "training: 43 batch 501 batch_loss: 0.18358579277992249\n",
      "training: 43 batch 502 batch_loss: 0.1844843626022339\n",
      "training: 43 batch 503 batch_loss: 0.18310660123825073\n",
      "training: 43 batch 504 batch_loss: 0.18501520156860352\n",
      "training: 43 batch 505 batch_loss: 0.18070325255393982\n",
      "training: 43 batch 506 batch_loss: 0.18388408422470093\n",
      "training: 43 batch 507 batch_loss: 0.18283766508102417\n",
      "training: 43 batch 508 batch_loss: 0.18534860014915466\n",
      "training: 43 batch 509 batch_loss: 0.18226146697998047\n",
      "training: 43 batch 510 batch_loss: 0.18168392777442932\n",
      "training: 43 batch 511 batch_loss: 0.18534168601036072\n",
      "training: 43 batch 512 batch_loss: 0.1842745542526245\n",
      "training: 43 batch 513 batch_loss: 0.18420881032943726\n",
      "training: 43 batch 514 batch_loss: 0.1816481649875641\n",
      "training: 43 batch 515 batch_loss: 0.1831463873386383\n",
      "training: 43 batch 516 batch_loss: 0.18750685453414917\n",
      "training: 43 batch 517 batch_loss: 0.18141037225723267\n",
      "training: 43 batch 518 batch_loss: 0.18547913432121277\n",
      "training: 43 batch 519 batch_loss: 0.18715882301330566\n",
      "training: 43 batch 520 batch_loss: 0.18378585577011108\n",
      "training: 43 batch 521 batch_loss: 0.18323683738708496\n",
      "training: 43 batch 522 batch_loss: 0.18725082278251648\n",
      "training: 43 batch 523 batch_loss: 0.18462204933166504\n",
      "training: 43 batch 524 batch_loss: 0.1852518618106842\n",
      "training: 43 batch 525 batch_loss: 0.18167749047279358\n",
      "training: 43 batch 526 batch_loss: 0.18348059058189392\n",
      "training: 43 batch 527 batch_loss: 0.18692520260810852\n",
      "training: 43 batch 528 batch_loss: 0.18523883819580078\n",
      "training: 43 batch 529 batch_loss: 0.1848057508468628\n",
      "training: 43 batch 530 batch_loss: 0.18748077750205994\n",
      "training: 43 batch 531 batch_loss: 0.186143696308136\n",
      "training: 43 batch 532 batch_loss: 0.18595623970031738\n",
      "training: 43 batch 533 batch_loss: 0.1844029724597931\n",
      "training: 43 batch 534 batch_loss: 0.18467441201210022\n",
      "training: 43 batch 535 batch_loss: 0.18303224444389343\n",
      "training: 43 batch 536 batch_loss: 0.18593564629554749\n",
      "training: 43 batch 537 batch_loss: 0.18057259917259216\n",
      "training: 43 batch 538 batch_loss: 0.18505874276161194\n",
      "training: 43 batch 539 batch_loss: 0.18182086944580078\n",
      "training: 43 batch 540 batch_loss: 0.18148189783096313\n",
      "training: 43 batch 541 batch_loss: 0.18547216057777405\n",
      "training: 43 batch 542 batch_loss: 0.1891728639602661\n",
      "training: 43 batch 543 batch_loss: 0.18240538239479065\n",
      "training: 43 batch 544 batch_loss: 0.18445393443107605\n",
      "training: 43 batch 545 batch_loss: 0.18287384510040283\n",
      "training: 43 batch 546 batch_loss: 0.18500414490699768\n",
      "training: 43 batch 547 batch_loss: 0.18640601634979248\n",
      "training: 43 batch 548 batch_loss: 0.18602266907691956\n",
      "training: 43 batch 549 batch_loss: 0.1852404773235321\n",
      "training: 43 batch 550 batch_loss: 0.18453088402748108\n",
      "training: 43 batch 551 batch_loss: 0.1818884015083313\n",
      "training: 43 batch 552 batch_loss: 0.18299835920333862\n",
      "training: 43 batch 553 batch_loss: 0.18505513668060303\n",
      "training: 43 batch 554 batch_loss: 0.1830165684223175\n",
      "training: 43 batch 555 batch_loss: 0.188071608543396\n",
      "training: 43 batch 556 batch_loss: 0.18157801032066345\n",
      "training: 43 batch 557 batch_loss: 0.18129509687423706\n",
      "training: 43 batch 558 batch_loss: 0.18494722247123718\n",
      "training: 43 batch 559 batch_loss: 0.18347153067588806\n",
      "training: 43 batch 560 batch_loss: 0.18677842617034912\n",
      "training: 43 batch 561 batch_loss: 0.183163583278656\n",
      "training: 43 batch 562 batch_loss: 0.18157678842544556\n",
      "training: 43 batch 563 batch_loss: 0.18515121936798096\n",
      "training: 43 batch 564 batch_loss: 0.18585678935050964\n",
      "training: 43 batch 565 batch_loss: 0.18251758813858032\n",
      "training: 43 batch 566 batch_loss: 0.18355226516723633\n",
      "training: 43 batch 567 batch_loss: 0.18611809611320496\n",
      "training: 43 batch 568 batch_loss: 0.18343421816825867\n",
      "training: 43 batch 569 batch_loss: 0.1841064691543579\n",
      "training: 43 batch 570 batch_loss: 0.1827847957611084\n",
      "training: 43 batch 571 batch_loss: 0.18365567922592163\n",
      "training: 43 batch 572 batch_loss: 0.1850447654724121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 43 batch 573 batch_loss: 0.1881607174873352\n",
      "training: 43 batch 574 batch_loss: 0.18235424160957336\n",
      "training: 43 batch 575 batch_loss: 0.1863882839679718\n",
      "training: 43 batch 576 batch_loss: 0.18616265058517456\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 43, Hit Ratio:0.03184951014847115 | Precision:0.04699203774697729 | Recall:0.06261647091061 | NDCG:0.0607415751955159\n",
      "*Best Performance* \n",
      "Epoch: 43, Hit Ratio:0.03184951014847115 | Precision:0.04699203774697729 | Recall:0.06261647091061 | MDCG:0.0607415751955159\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 44 batch 0 batch_loss: 0.17887994647026062\n",
      "training: 44 batch 1 batch_loss: 0.18608227372169495\n",
      "training: 44 batch 2 batch_loss: 0.18065527081489563\n",
      "training: 44 batch 3 batch_loss: 0.18399280309677124\n",
      "training: 44 batch 4 batch_loss: 0.18246781826019287\n",
      "training: 44 batch 5 batch_loss: 0.18145304918289185\n",
      "training: 44 batch 6 batch_loss: 0.17917156219482422\n",
      "training: 44 batch 7 batch_loss: 0.1808375120162964\n",
      "training: 44 batch 8 batch_loss: 0.18239721655845642\n",
      "training: 44 batch 9 batch_loss: 0.18382269144058228\n",
      "training: 44 batch 10 batch_loss: 0.18136709928512573\n",
      "training: 44 batch 11 batch_loss: 0.17966872453689575\n",
      "training: 44 batch 12 batch_loss: 0.18347015976905823\n",
      "training: 44 batch 13 batch_loss: 0.18113097548484802\n",
      "training: 44 batch 14 batch_loss: 0.18530917167663574\n",
      "training: 44 batch 15 batch_loss: 0.18385472893714905\n",
      "training: 44 batch 16 batch_loss: 0.18141263723373413\n",
      "training: 44 batch 17 batch_loss: 0.1827315390110016\n",
      "training: 44 batch 18 batch_loss: 0.18334180116653442\n",
      "training: 44 batch 19 batch_loss: 0.18436703085899353\n",
      "training: 44 batch 20 batch_loss: 0.1810530126094818\n",
      "training: 44 batch 21 batch_loss: 0.18254852294921875\n",
      "training: 44 batch 22 batch_loss: 0.18328848481178284\n",
      "training: 44 batch 23 batch_loss: 0.17949992418289185\n",
      "training: 44 batch 24 batch_loss: 0.18412703275680542\n",
      "training: 44 batch 25 batch_loss: 0.18254292011260986\n",
      "training: 44 batch 26 batch_loss: 0.1831926703453064\n",
      "training: 44 batch 27 batch_loss: 0.18556594848632812\n",
      "training: 44 batch 28 batch_loss: 0.18368911743164062\n",
      "training: 44 batch 29 batch_loss: 0.18313810229301453\n",
      "training: 44 batch 30 batch_loss: 0.18457213044166565\n",
      "training: 44 batch 31 batch_loss: 0.18554577231407166\n",
      "training: 44 batch 32 batch_loss: 0.1810455620288849\n",
      "training: 44 batch 33 batch_loss: 0.1807105541229248\n",
      "training: 44 batch 34 batch_loss: 0.18569618463516235\n",
      "training: 44 batch 35 batch_loss: 0.18542057275772095\n",
      "training: 44 batch 36 batch_loss: 0.1810140311717987\n",
      "training: 44 batch 37 batch_loss: 0.18399912118911743\n",
      "training: 44 batch 38 batch_loss: 0.18352258205413818\n",
      "training: 44 batch 39 batch_loss: 0.1858961284160614\n",
      "training: 44 batch 40 batch_loss: 0.18402045965194702\n",
      "training: 44 batch 41 batch_loss: 0.18631401658058167\n",
      "training: 44 batch 42 batch_loss: 0.1829671859741211\n",
      "training: 44 batch 43 batch_loss: 0.18368050456047058\n",
      "training: 44 batch 44 batch_loss: 0.18618008494377136\n",
      "training: 44 batch 45 batch_loss: 0.18235471844673157\n",
      "training: 44 batch 46 batch_loss: 0.18473702669143677\n",
      "training: 44 batch 47 batch_loss: 0.18475621938705444\n",
      "training: 44 batch 48 batch_loss: 0.18267405033111572\n",
      "training: 44 batch 49 batch_loss: 0.18372520804405212\n",
      "training: 44 batch 50 batch_loss: 0.1840146780014038\n",
      "training: 44 batch 51 batch_loss: 0.1869928538799286\n",
      "training: 44 batch 52 batch_loss: 0.18491095304489136\n",
      "training: 44 batch 53 batch_loss: 0.18425703048706055\n",
      "training: 44 batch 54 batch_loss: 0.18264463543891907\n",
      "training: 44 batch 55 batch_loss: 0.1866510808467865\n",
      "training: 44 batch 56 batch_loss: 0.1831004023551941\n",
      "training: 44 batch 57 batch_loss: 0.1833174228668213\n",
      "training: 44 batch 58 batch_loss: 0.18372124433517456\n",
      "training: 44 batch 59 batch_loss: 0.18403524160385132\n",
      "training: 44 batch 60 batch_loss: 0.18463900685310364\n",
      "training: 44 batch 61 batch_loss: 0.187098890542984\n",
      "training: 44 batch 62 batch_loss: 0.18080341815948486\n",
      "training: 44 batch 63 batch_loss: 0.18157267570495605\n",
      "training: 44 batch 64 batch_loss: 0.18433114886283875\n",
      "training: 44 batch 65 batch_loss: 0.18100476264953613\n",
      "training: 44 batch 66 batch_loss: 0.18466037511825562\n",
      "training: 44 batch 67 batch_loss: 0.18509253859519958\n",
      "training: 44 batch 68 batch_loss: 0.18486672639846802\n",
      "training: 44 batch 69 batch_loss: 0.18278303742408752\n",
      "training: 44 batch 70 batch_loss: 0.18336066603660583\n",
      "training: 44 batch 71 batch_loss: 0.1857573688030243\n",
      "training: 44 batch 72 batch_loss: 0.18256685137748718\n",
      "training: 44 batch 73 batch_loss: 0.1870766580104828\n",
      "training: 44 batch 74 batch_loss: 0.1886374056339264\n",
      "training: 44 batch 75 batch_loss: 0.1822502613067627\n",
      "training: 44 batch 76 batch_loss: 0.1879386305809021\n",
      "training: 44 batch 77 batch_loss: 0.18559348583221436\n",
      "training: 44 batch 78 batch_loss: 0.18427211046218872\n",
      "training: 44 batch 79 batch_loss: 0.18136292695999146\n",
      "training: 44 batch 80 batch_loss: 0.18578281998634338\n",
      "training: 44 batch 81 batch_loss: 0.18321162462234497\n",
      "training: 44 batch 82 batch_loss: 0.18486613035202026\n",
      "training: 44 batch 83 batch_loss: 0.1893078088760376\n",
      "training: 44 batch 84 batch_loss: 0.18220415711402893\n",
      "training: 44 batch 85 batch_loss: 0.18334585428237915\n",
      "training: 44 batch 86 batch_loss: 0.17959636449813843\n",
      "training: 44 batch 87 batch_loss: 0.18460813164710999\n",
      "training: 44 batch 88 batch_loss: 0.1844973862171173\n",
      "training: 44 batch 89 batch_loss: 0.18240109086036682\n",
      "training: 44 batch 90 batch_loss: 0.18546485900878906\n",
      "training: 44 batch 91 batch_loss: 0.18812283873558044\n",
      "training: 44 batch 92 batch_loss: 0.18074637651443481\n",
      "training: 44 batch 93 batch_loss: 0.18222755193710327\n",
      "training: 44 batch 94 batch_loss: 0.1859314739704132\n",
      "training: 44 batch 95 batch_loss: 0.18623286485671997\n",
      "training: 44 batch 96 batch_loss: 0.18659067153930664\n",
      "training: 44 batch 97 batch_loss: 0.18399176001548767\n",
      "training: 44 batch 98 batch_loss: 0.18166175484657288\n",
      "training: 44 batch 99 batch_loss: 0.18132120370864868\n",
      "training: 44 batch 100 batch_loss: 0.18382281064987183\n",
      "training: 44 batch 101 batch_loss: 0.1829204559326172\n",
      "training: 44 batch 102 batch_loss: 0.18666642904281616\n",
      "training: 44 batch 103 batch_loss: 0.18165835738182068\n",
      "training: 44 batch 104 batch_loss: 0.18401172757148743\n",
      "training: 44 batch 105 batch_loss: 0.18659353256225586\n",
      "training: 44 batch 106 batch_loss: 0.1844271421432495\n",
      "training: 44 batch 107 batch_loss: 0.18383145332336426\n",
      "training: 44 batch 108 batch_loss: 0.1842488944530487\n",
      "training: 44 batch 109 batch_loss: 0.18347734212875366\n",
      "training: 44 batch 110 batch_loss: 0.18284887075424194\n",
      "training: 44 batch 111 batch_loss: 0.18497484922409058\n",
      "training: 44 batch 112 batch_loss: 0.18332532048225403\n",
      "training: 44 batch 113 batch_loss: 0.18458691239356995\n",
      "training: 44 batch 114 batch_loss: 0.188543438911438\n",
      "training: 44 batch 115 batch_loss: 0.18647655844688416\n",
      "training: 44 batch 116 batch_loss: 0.18889546394348145\n",
      "training: 44 batch 117 batch_loss: 0.18313005566596985\n",
      "training: 44 batch 118 batch_loss: 0.1848546862602234\n",
      "training: 44 batch 119 batch_loss: 0.18264985084533691\n",
      "training: 44 batch 120 batch_loss: 0.18464291095733643\n",
      "training: 44 batch 121 batch_loss: 0.18413123488426208\n",
      "training: 44 batch 122 batch_loss: 0.18489974737167358\n",
      "training: 44 batch 123 batch_loss: 0.1851814091205597\n",
      "training: 44 batch 124 batch_loss: 0.18229910731315613\n",
      "training: 44 batch 125 batch_loss: 0.17981407046318054\n",
      "training: 44 batch 126 batch_loss: 0.17981034517288208\n",
      "training: 44 batch 127 batch_loss: 0.18526506423950195\n",
      "training: 44 batch 128 batch_loss: 0.18494874238967896\n",
      "training: 44 batch 129 batch_loss: 0.18407434225082397\n",
      "training: 44 batch 130 batch_loss: 0.18396759033203125\n",
      "training: 44 batch 131 batch_loss: 0.18329903483390808\n",
      "training: 44 batch 132 batch_loss: 0.1899796426296234\n",
      "training: 44 batch 133 batch_loss: 0.18221652507781982\n",
      "training: 44 batch 134 batch_loss: 0.18500858545303345\n",
      "training: 44 batch 135 batch_loss: 0.18571063876152039\n",
      "training: 44 batch 136 batch_loss: 0.18329626321792603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 44 batch 137 batch_loss: 0.18374326825141907\n",
      "training: 44 batch 138 batch_loss: 0.1860741674900055\n",
      "training: 44 batch 139 batch_loss: 0.18365049362182617\n",
      "training: 44 batch 140 batch_loss: 0.18628409504890442\n",
      "training: 44 batch 141 batch_loss: 0.18192914128303528\n",
      "training: 44 batch 142 batch_loss: 0.18225282430648804\n",
      "training: 44 batch 143 batch_loss: 0.18642958998680115\n",
      "training: 44 batch 144 batch_loss: 0.18512946367263794\n",
      "training: 44 batch 145 batch_loss: 0.1840030550956726\n",
      "training: 44 batch 146 batch_loss: 0.18296116590499878\n",
      "training: 44 batch 147 batch_loss: 0.18624255061149597\n",
      "training: 44 batch 148 batch_loss: 0.1811407506465912\n",
      "training: 44 batch 149 batch_loss: 0.183683842420578\n",
      "training: 44 batch 150 batch_loss: 0.1837635636329651\n",
      "training: 44 batch 151 batch_loss: 0.17939046025276184\n",
      "training: 44 batch 152 batch_loss: 0.1852216124534607\n",
      "training: 44 batch 153 batch_loss: 0.18572300672531128\n",
      "training: 44 batch 154 batch_loss: 0.18211078643798828\n",
      "training: 44 batch 155 batch_loss: 0.1814277172088623\n",
      "training: 44 batch 156 batch_loss: 0.18445312976837158\n",
      "training: 44 batch 157 batch_loss: 0.1812724769115448\n",
      "training: 44 batch 158 batch_loss: 0.1823955476284027\n",
      "training: 44 batch 159 batch_loss: 0.18765246868133545\n",
      "training: 44 batch 160 batch_loss: 0.18544167280197144\n",
      "training: 44 batch 161 batch_loss: 0.18709266185760498\n",
      "training: 44 batch 162 batch_loss: 0.1859125792980194\n",
      "training: 44 batch 163 batch_loss: 0.18364393711090088\n",
      "training: 44 batch 164 batch_loss: 0.18109315633773804\n",
      "training: 44 batch 165 batch_loss: 0.18488579988479614\n",
      "training: 44 batch 166 batch_loss: 0.18416064977645874\n",
      "training: 44 batch 167 batch_loss: 0.18706709146499634\n",
      "training: 44 batch 168 batch_loss: 0.1855018138885498\n",
      "training: 44 batch 169 batch_loss: 0.18397819995880127\n",
      "training: 44 batch 170 batch_loss: 0.18860170245170593\n",
      "training: 44 batch 171 batch_loss: 0.18765240907669067\n",
      "training: 44 batch 172 batch_loss: 0.18311893939971924\n",
      "training: 44 batch 173 batch_loss: 0.18313583731651306\n",
      "training: 44 batch 174 batch_loss: 0.180668443441391\n",
      "training: 44 batch 175 batch_loss: 0.18461856245994568\n",
      "training: 44 batch 176 batch_loss: 0.18334126472473145\n",
      "training: 44 batch 177 batch_loss: 0.1867886781692505\n",
      "training: 44 batch 178 batch_loss: 0.18674317002296448\n",
      "training: 44 batch 179 batch_loss: 0.18718406558036804\n",
      "training: 44 batch 180 batch_loss: 0.18532031774520874\n",
      "training: 44 batch 181 batch_loss: 0.18319398164749146\n",
      "training: 44 batch 182 batch_loss: 0.18031543493270874\n",
      "training: 44 batch 183 batch_loss: 0.1820061206817627\n",
      "training: 44 batch 184 batch_loss: 0.18099752068519592\n",
      "training: 44 batch 185 batch_loss: 0.1816132664680481\n",
      "training: 44 batch 186 batch_loss: 0.18464750051498413\n",
      "training: 44 batch 187 batch_loss: 0.18207091093063354\n",
      "training: 44 batch 188 batch_loss: 0.18255263566970825\n",
      "training: 44 batch 189 batch_loss: 0.1833038330078125\n",
      "training: 44 batch 190 batch_loss: 0.18227827548980713\n",
      "training: 44 batch 191 batch_loss: 0.1843940019607544\n",
      "training: 44 batch 192 batch_loss: 0.18275368213653564\n",
      "training: 44 batch 193 batch_loss: 0.18538233637809753\n",
      "training: 44 batch 194 batch_loss: 0.18645265698432922\n",
      "training: 44 batch 195 batch_loss: 0.18642038106918335\n",
      "training: 44 batch 196 batch_loss: 0.1826860010623932\n",
      "training: 44 batch 197 batch_loss: 0.18843704462051392\n",
      "training: 44 batch 198 batch_loss: 0.1860235631465912\n",
      "training: 44 batch 199 batch_loss: 0.18591806292533875\n",
      "training: 44 batch 200 batch_loss: 0.18500426411628723\n",
      "training: 44 batch 201 batch_loss: 0.18897122144699097\n",
      "training: 44 batch 202 batch_loss: 0.1852523684501648\n",
      "training: 44 batch 203 batch_loss: 0.18377330899238586\n",
      "training: 44 batch 204 batch_loss: 0.18270328640937805\n",
      "training: 44 batch 205 batch_loss: 0.18415987491607666\n",
      "training: 44 batch 206 batch_loss: 0.18223679065704346\n",
      "training: 44 batch 207 batch_loss: 0.18287980556488037\n",
      "training: 44 batch 208 batch_loss: 0.18282926082611084\n",
      "training: 44 batch 209 batch_loss: 0.18382498621940613\n",
      "training: 44 batch 210 batch_loss: 0.1850452721118927\n",
      "training: 44 batch 211 batch_loss: 0.18394529819488525\n",
      "training: 44 batch 212 batch_loss: 0.1841425895690918\n",
      "training: 44 batch 213 batch_loss: 0.18583610653877258\n",
      "training: 44 batch 214 batch_loss: 0.1865473985671997\n",
      "training: 44 batch 215 batch_loss: 0.18238425254821777\n",
      "training: 44 batch 216 batch_loss: 0.18632683157920837\n",
      "training: 44 batch 217 batch_loss: 0.1857975721359253\n",
      "training: 44 batch 218 batch_loss: 0.18335092067718506\n",
      "training: 44 batch 219 batch_loss: 0.18329310417175293\n",
      "training: 44 batch 220 batch_loss: 0.18538036942481995\n",
      "training: 44 batch 221 batch_loss: 0.18422871828079224\n",
      "training: 44 batch 222 batch_loss: 0.18106484413146973\n",
      "training: 44 batch 223 batch_loss: 0.18733564019203186\n",
      "training: 44 batch 224 batch_loss: 0.1853373646736145\n",
      "training: 44 batch 225 batch_loss: 0.1862177848815918\n",
      "training: 44 batch 226 batch_loss: 0.18362906575202942\n",
      "training: 44 batch 227 batch_loss: 0.18239861726760864\n",
      "training: 44 batch 228 batch_loss: 0.18472978472709656\n",
      "training: 44 batch 229 batch_loss: 0.1841585636138916\n",
      "training: 44 batch 230 batch_loss: 0.18543240427970886\n",
      "training: 44 batch 231 batch_loss: 0.18458452820777893\n",
      "training: 44 batch 232 batch_loss: 0.18351557850837708\n",
      "training: 44 batch 233 batch_loss: 0.1855263113975525\n",
      "training: 44 batch 234 batch_loss: 0.1822269856929779\n",
      "training: 44 batch 235 batch_loss: 0.18337848782539368\n",
      "training: 44 batch 236 batch_loss: 0.18759402632713318\n",
      "training: 44 batch 237 batch_loss: 0.1857379972934723\n",
      "training: 44 batch 238 batch_loss: 0.18491363525390625\n",
      "training: 44 batch 239 batch_loss: 0.18548798561096191\n",
      "training: 44 batch 240 batch_loss: 0.18903020024299622\n",
      "training: 44 batch 241 batch_loss: 0.18682187795639038\n",
      "training: 44 batch 242 batch_loss: 0.1824495792388916\n",
      "training: 44 batch 243 batch_loss: 0.18420785665512085\n",
      "training: 44 batch 244 batch_loss: 0.18585115671157837\n",
      "training: 44 batch 245 batch_loss: 0.18390387296676636\n",
      "training: 44 batch 246 batch_loss: 0.18474134802818298\n",
      "training: 44 batch 247 batch_loss: 0.1857883334159851\n",
      "training: 44 batch 248 batch_loss: 0.18468177318572998\n",
      "training: 44 batch 249 batch_loss: 0.1822625994682312\n",
      "training: 44 batch 250 batch_loss: 0.1843896210193634\n",
      "training: 44 batch 251 batch_loss: 0.1881001591682434\n",
      "training: 44 batch 252 batch_loss: 0.18833184242248535\n",
      "training: 44 batch 253 batch_loss: 0.18715965747833252\n",
      "training: 44 batch 254 batch_loss: 0.18289634585380554\n",
      "training: 44 batch 255 batch_loss: 0.19040942192077637\n",
      "training: 44 batch 256 batch_loss: 0.18316754698753357\n",
      "training: 44 batch 257 batch_loss: 0.18293660879135132\n",
      "training: 44 batch 258 batch_loss: 0.18014591932296753\n",
      "training: 44 batch 259 batch_loss: 0.18222349882125854\n",
      "training: 44 batch 260 batch_loss: 0.1820204257965088\n",
      "training: 44 batch 261 batch_loss: 0.18347889184951782\n",
      "training: 44 batch 262 batch_loss: 0.18567407131195068\n",
      "training: 44 batch 263 batch_loss: 0.18861842155456543\n",
      "training: 44 batch 264 batch_loss: 0.18420836329460144\n",
      "training: 44 batch 265 batch_loss: 0.18479487299919128\n",
      "training: 44 batch 266 batch_loss: 0.18457454442977905\n",
      "training: 44 batch 267 batch_loss: 0.18857774138450623\n",
      "training: 44 batch 268 batch_loss: 0.1851138174533844\n",
      "training: 44 batch 269 batch_loss: 0.1851484775543213\n",
      "training: 44 batch 270 batch_loss: 0.183173269033432\n",
      "training: 44 batch 271 batch_loss: 0.18481871485710144\n",
      "training: 44 batch 272 batch_loss: 0.18630394339561462\n",
      "training: 44 batch 273 batch_loss: 0.18695956468582153\n",
      "training: 44 batch 274 batch_loss: 0.1895989179611206\n",
      "training: 44 batch 275 batch_loss: 0.18626832962036133\n",
      "training: 44 batch 276 batch_loss: 0.18488016724586487\n",
      "training: 44 batch 277 batch_loss: 0.18449795246124268\n",
      "training: 44 batch 278 batch_loss: 0.18216747045516968\n",
      "training: 44 batch 279 batch_loss: 0.18782103061676025\n",
      "training: 44 batch 280 batch_loss: 0.1880854070186615\n",
      "training: 44 batch 281 batch_loss: 0.19166576862335205\n",
      "training: 44 batch 282 batch_loss: 0.18532267212867737\n",
      "training: 44 batch 283 batch_loss: 0.186894953250885\n",
      "training: 44 batch 284 batch_loss: 0.18548792600631714\n",
      "training: 44 batch 285 batch_loss: 0.18307852745056152\n",
      "training: 44 batch 286 batch_loss: 0.1878904402256012\n",
      "training: 44 batch 287 batch_loss: 0.1844286322593689\n",
      "training: 44 batch 288 batch_loss: 0.1830015778541565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 44 batch 289 batch_loss: 0.18441107869148254\n",
      "training: 44 batch 290 batch_loss: 0.18508657813072205\n",
      "training: 44 batch 291 batch_loss: 0.1852131187915802\n",
      "training: 44 batch 292 batch_loss: 0.1872294545173645\n",
      "training: 44 batch 293 batch_loss: 0.18343529105186462\n",
      "training: 44 batch 294 batch_loss: 0.18411308526992798\n",
      "training: 44 batch 295 batch_loss: 0.18231764435768127\n",
      "training: 44 batch 296 batch_loss: 0.18770918250083923\n",
      "training: 44 batch 297 batch_loss: 0.1870684027671814\n",
      "training: 44 batch 298 batch_loss: 0.18501082062721252\n",
      "training: 44 batch 299 batch_loss: 0.18485772609710693\n",
      "training: 44 batch 300 batch_loss: 0.1847742795944214\n",
      "training: 44 batch 301 batch_loss: 0.18418991565704346\n",
      "training: 44 batch 302 batch_loss: 0.1849607527256012\n",
      "training: 44 batch 303 batch_loss: 0.18550333380699158\n",
      "training: 44 batch 304 batch_loss: 0.18286138772964478\n",
      "training: 44 batch 305 batch_loss: 0.18661189079284668\n",
      "training: 44 batch 306 batch_loss: 0.18634873628616333\n",
      "training: 44 batch 307 batch_loss: 0.18497255444526672\n",
      "training: 44 batch 308 batch_loss: 0.18280291557312012\n",
      "training: 44 batch 309 batch_loss: 0.18323245644569397\n",
      "training: 44 batch 310 batch_loss: 0.18242394924163818\n",
      "training: 44 batch 311 batch_loss: 0.1804673671722412\n",
      "training: 44 batch 312 batch_loss: 0.1893986463546753\n",
      "training: 44 batch 313 batch_loss: 0.1846972107887268\n",
      "training: 44 batch 314 batch_loss: 0.18291586637496948\n",
      "training: 44 batch 315 batch_loss: 0.18586239218711853\n",
      "training: 44 batch 316 batch_loss: 0.18762075901031494\n",
      "training: 44 batch 317 batch_loss: 0.18971875309944153\n",
      "training: 44 batch 318 batch_loss: 0.1832866668701172\n",
      "training: 44 batch 319 batch_loss: 0.18736079335212708\n",
      "training: 44 batch 320 batch_loss: 0.1855699121952057\n",
      "training: 44 batch 321 batch_loss: 0.18627458810806274\n",
      "training: 44 batch 322 batch_loss: 0.18510663509368896\n",
      "training: 44 batch 323 batch_loss: 0.18827500939369202\n",
      "training: 44 batch 324 batch_loss: 0.18462902307510376\n",
      "training: 44 batch 325 batch_loss: 0.18646922707557678\n",
      "training: 44 batch 326 batch_loss: 0.18552076816558838\n",
      "training: 44 batch 327 batch_loss: 0.18431255221366882\n",
      "training: 44 batch 328 batch_loss: 0.1823003590106964\n",
      "training: 44 batch 329 batch_loss: 0.18321764469146729\n",
      "training: 44 batch 330 batch_loss: 0.1852400302886963\n",
      "training: 44 batch 331 batch_loss: 0.18504643440246582\n",
      "training: 44 batch 332 batch_loss: 0.18602502346038818\n",
      "training: 44 batch 333 batch_loss: 0.18759950995445251\n",
      "training: 44 batch 334 batch_loss: 0.18733012676239014\n",
      "training: 44 batch 335 batch_loss: 0.18563157320022583\n",
      "training: 44 batch 336 batch_loss: 0.18455377221107483\n",
      "training: 44 batch 337 batch_loss: 0.1874259114265442\n",
      "training: 44 batch 338 batch_loss: 0.18166622519493103\n",
      "training: 44 batch 339 batch_loss: 0.18713617324829102\n",
      "training: 44 batch 340 batch_loss: 0.18248432874679565\n",
      "training: 44 batch 341 batch_loss: 0.18702828884124756\n",
      "training: 44 batch 342 batch_loss: 0.18653762340545654\n",
      "training: 44 batch 343 batch_loss: 0.18558192253112793\n",
      "training: 44 batch 344 batch_loss: 0.18591678142547607\n",
      "training: 44 batch 345 batch_loss: 0.18483802676200867\n",
      "training: 44 batch 346 batch_loss: 0.18269804120063782\n",
      "training: 44 batch 347 batch_loss: 0.18273505568504333\n",
      "training: 44 batch 348 batch_loss: 0.18164563179016113\n",
      "training: 44 batch 349 batch_loss: 0.18640363216400146\n",
      "training: 44 batch 350 batch_loss: 0.18612056970596313\n",
      "training: 44 batch 351 batch_loss: 0.18818867206573486\n",
      "training: 44 batch 352 batch_loss: 0.18478786945343018\n",
      "training: 44 batch 353 batch_loss: 0.1806986927986145\n",
      "training: 44 batch 354 batch_loss: 0.17917969822883606\n",
      "training: 44 batch 355 batch_loss: 0.18715626001358032\n",
      "training: 44 batch 356 batch_loss: 0.18305709958076477\n",
      "training: 44 batch 357 batch_loss: 0.19077053666114807\n",
      "training: 44 batch 358 batch_loss: 0.18482831120491028\n",
      "training: 44 batch 359 batch_loss: 0.18364346027374268\n",
      "training: 44 batch 360 batch_loss: 0.18162795901298523\n",
      "training: 44 batch 361 batch_loss: 0.18519002199172974\n",
      "training: 44 batch 362 batch_loss: 0.18453091382980347\n",
      "training: 44 batch 363 batch_loss: 0.18854761123657227\n",
      "training: 44 batch 364 batch_loss: 0.18267041444778442\n",
      "training: 44 batch 365 batch_loss: 0.18223801255226135\n",
      "training: 44 batch 366 batch_loss: 0.1831224262714386\n",
      "training: 44 batch 367 batch_loss: 0.18662595748901367\n",
      "training: 44 batch 368 batch_loss: 0.18447339534759521\n",
      "training: 44 batch 369 batch_loss: 0.188581645488739\n",
      "training: 44 batch 370 batch_loss: 0.18582683801651\n",
      "training: 44 batch 371 batch_loss: 0.18808820843696594\n",
      "training: 44 batch 372 batch_loss: 0.18163996934890747\n",
      "training: 44 batch 373 batch_loss: 0.18971416354179382\n",
      "training: 44 batch 374 batch_loss: 0.18631938099861145\n",
      "training: 44 batch 375 batch_loss: 0.1810474395751953\n",
      "training: 44 batch 376 batch_loss: 0.18294933438301086\n",
      "training: 44 batch 377 batch_loss: 0.18561401963233948\n",
      "training: 44 batch 378 batch_loss: 0.1877419650554657\n",
      "training: 44 batch 379 batch_loss: 0.18491411209106445\n",
      "training: 44 batch 380 batch_loss: 0.18381920456886292\n",
      "training: 44 batch 381 batch_loss: 0.18162167072296143\n",
      "training: 44 batch 382 batch_loss: 0.1855868101119995\n",
      "training: 44 batch 383 batch_loss: 0.18401449918746948\n",
      "training: 44 batch 384 batch_loss: 0.18333002924919128\n",
      "training: 44 batch 385 batch_loss: 0.18602189421653748\n",
      "training: 44 batch 386 batch_loss: 0.18464720249176025\n",
      "training: 44 batch 387 batch_loss: 0.1846158802509308\n",
      "training: 44 batch 388 batch_loss: 0.18265163898468018\n",
      "training: 44 batch 389 batch_loss: 0.18501681089401245\n",
      "training: 44 batch 390 batch_loss: 0.18071579933166504\n",
      "training: 44 batch 391 batch_loss: 0.186791330575943\n",
      "training: 44 batch 392 batch_loss: 0.18664199113845825\n",
      "training: 44 batch 393 batch_loss: 0.18746858835220337\n",
      "training: 44 batch 394 batch_loss: 0.18825894594192505\n",
      "training: 44 batch 395 batch_loss: 0.18566030263900757\n",
      "training: 44 batch 396 batch_loss: 0.18895113468170166\n",
      "training: 44 batch 397 batch_loss: 0.1890459656715393\n",
      "training: 44 batch 398 batch_loss: 0.184012770652771\n",
      "training: 44 batch 399 batch_loss: 0.1862780749797821\n",
      "training: 44 batch 400 batch_loss: 0.18508610129356384\n",
      "training: 44 batch 401 batch_loss: 0.18294891715049744\n",
      "training: 44 batch 402 batch_loss: 0.18414321541786194\n",
      "training: 44 batch 403 batch_loss: 0.18583130836486816\n",
      "training: 44 batch 404 batch_loss: 0.18772023916244507\n",
      "training: 44 batch 405 batch_loss: 0.18486493825912476\n",
      "training: 44 batch 406 batch_loss: 0.1913202702999115\n",
      "training: 44 batch 407 batch_loss: 0.185747891664505\n",
      "training: 44 batch 408 batch_loss: 0.18909403681755066\n",
      "training: 44 batch 409 batch_loss: 0.18769007921218872\n",
      "training: 44 batch 410 batch_loss: 0.18715673685073853\n",
      "training: 44 batch 411 batch_loss: 0.1886804699897766\n",
      "training: 44 batch 412 batch_loss: 0.18459650874137878\n",
      "training: 44 batch 413 batch_loss: 0.18333140015602112\n",
      "training: 44 batch 414 batch_loss: 0.18864932656288147\n",
      "training: 44 batch 415 batch_loss: 0.18888479471206665\n",
      "training: 44 batch 416 batch_loss: 0.18567901849746704\n",
      "training: 44 batch 417 batch_loss: 0.18548160791397095\n",
      "training: 44 batch 418 batch_loss: 0.18949967622756958\n",
      "training: 44 batch 419 batch_loss: 0.18894046545028687\n",
      "training: 44 batch 420 batch_loss: 0.18640398979187012\n",
      "training: 44 batch 421 batch_loss: 0.183577299118042\n",
      "training: 44 batch 422 batch_loss: 0.18410691618919373\n",
      "training: 44 batch 423 batch_loss: 0.18773257732391357\n",
      "training: 44 batch 424 batch_loss: 0.18789267539978027\n",
      "training: 44 batch 425 batch_loss: 0.18793725967407227\n",
      "training: 44 batch 426 batch_loss: 0.18332496285438538\n",
      "training: 44 batch 427 batch_loss: 0.18679657578468323\n",
      "training: 44 batch 428 batch_loss: 0.18172067403793335\n",
      "training: 44 batch 429 batch_loss: 0.19013914465904236\n",
      "training: 44 batch 430 batch_loss: 0.18618786334991455\n",
      "training: 44 batch 431 batch_loss: 0.1872863471508026\n",
      "training: 44 batch 432 batch_loss: 0.18395644426345825\n",
      "training: 44 batch 433 batch_loss: 0.18613514304161072\n",
      "training: 44 batch 434 batch_loss: 0.18643268942832947\n",
      "training: 44 batch 435 batch_loss: 0.18593987822532654\n",
      "training: 44 batch 436 batch_loss: 0.18721118569374084\n",
      "training: 44 batch 437 batch_loss: 0.18606483936309814\n",
      "training: 44 batch 438 batch_loss: 0.18658259510993958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 44 batch 439 batch_loss: 0.18476912379264832\n",
      "training: 44 batch 440 batch_loss: 0.19022142887115479\n",
      "training: 44 batch 441 batch_loss: 0.1846584677696228\n",
      "training: 44 batch 442 batch_loss: 0.18598869442939758\n",
      "training: 44 batch 443 batch_loss: 0.1859723925590515\n",
      "training: 44 batch 444 batch_loss: 0.18431556224822998\n",
      "training: 44 batch 445 batch_loss: 0.18748393654823303\n",
      "training: 44 batch 446 batch_loss: 0.1834559440612793\n",
      "training: 44 batch 447 batch_loss: 0.1842459738254547\n",
      "training: 44 batch 448 batch_loss: 0.18608573079109192\n",
      "training: 44 batch 449 batch_loss: 0.18901050090789795\n",
      "training: 44 batch 450 batch_loss: 0.18728968501091003\n",
      "training: 44 batch 451 batch_loss: 0.18711471557617188\n",
      "training: 44 batch 452 batch_loss: 0.1868729591369629\n",
      "training: 44 batch 453 batch_loss: 0.18835529685020447\n",
      "training: 44 batch 454 batch_loss: 0.18846458196640015\n",
      "training: 44 batch 455 batch_loss: 0.18353807926177979\n",
      "training: 44 batch 456 batch_loss: 0.18589982390403748\n",
      "training: 44 batch 457 batch_loss: 0.1785827875137329\n",
      "training: 44 batch 458 batch_loss: 0.182570219039917\n",
      "training: 44 batch 459 batch_loss: 0.18597012758255005\n",
      "training: 44 batch 460 batch_loss: 0.18426954746246338\n",
      "training: 44 batch 461 batch_loss: 0.1826591193675995\n",
      "training: 44 batch 462 batch_loss: 0.1880362629890442\n",
      "training: 44 batch 463 batch_loss: 0.18368491530418396\n",
      "training: 44 batch 464 batch_loss: 0.18704938888549805\n",
      "training: 44 batch 465 batch_loss: 0.18283376097679138\n",
      "training: 44 batch 466 batch_loss: 0.1855315864086151\n",
      "training: 44 batch 467 batch_loss: 0.18395918607711792\n",
      "training: 44 batch 468 batch_loss: 0.18755048513412476\n",
      "training: 44 batch 469 batch_loss: 0.18824267387390137\n",
      "training: 44 batch 470 batch_loss: 0.18497660756111145\n",
      "training: 44 batch 471 batch_loss: 0.18641433119773865\n",
      "training: 44 batch 472 batch_loss: 0.18332847952842712\n",
      "training: 44 batch 473 batch_loss: 0.18845874071121216\n",
      "training: 44 batch 474 batch_loss: 0.18330109119415283\n",
      "training: 44 batch 475 batch_loss: 0.18971896171569824\n",
      "training: 44 batch 476 batch_loss: 0.1859239637851715\n",
      "training: 44 batch 477 batch_loss: 0.1826390027999878\n",
      "training: 44 batch 478 batch_loss: 0.18577995896339417\n",
      "training: 44 batch 479 batch_loss: 0.18638953566551208\n",
      "training: 44 batch 480 batch_loss: 0.1874224841594696\n",
      "training: 44 batch 481 batch_loss: 0.18964511156082153\n",
      "training: 44 batch 482 batch_loss: 0.18637299537658691\n",
      "training: 44 batch 483 batch_loss: 0.1832767128944397\n",
      "training: 44 batch 484 batch_loss: 0.18768498301506042\n",
      "training: 44 batch 485 batch_loss: 0.1906641721725464\n",
      "training: 44 batch 486 batch_loss: 0.18662163615226746\n",
      "training: 44 batch 487 batch_loss: 0.18798398971557617\n",
      "training: 44 batch 488 batch_loss: 0.18777784705162048\n",
      "training: 44 batch 489 batch_loss: 0.18628105521202087\n",
      "training: 44 batch 490 batch_loss: 0.1831822395324707\n",
      "training: 44 batch 491 batch_loss: 0.18439188599586487\n",
      "training: 44 batch 492 batch_loss: 0.18502190709114075\n",
      "training: 44 batch 493 batch_loss: 0.18367105722427368\n",
      "training: 44 batch 494 batch_loss: 0.18674391508102417\n",
      "training: 44 batch 495 batch_loss: 0.19004330039024353\n",
      "training: 44 batch 496 batch_loss: 0.1846485733985901\n",
      "training: 44 batch 497 batch_loss: 0.18348276615142822\n",
      "training: 44 batch 498 batch_loss: 0.18676403164863586\n",
      "training: 44 batch 499 batch_loss: 0.18845263123512268\n",
      "training: 44 batch 500 batch_loss: 0.18727537989616394\n",
      "training: 44 batch 501 batch_loss: 0.18688109517097473\n",
      "training: 44 batch 502 batch_loss: 0.1859053373336792\n",
      "training: 44 batch 503 batch_loss: 0.18416771292686462\n",
      "training: 44 batch 504 batch_loss: 0.18427658081054688\n",
      "training: 44 batch 505 batch_loss: 0.18507921695709229\n",
      "training: 44 batch 506 batch_loss: 0.18494302034378052\n",
      "training: 44 batch 507 batch_loss: 0.18870335817337036\n",
      "training: 44 batch 508 batch_loss: 0.1845981478691101\n",
      "training: 44 batch 509 batch_loss: 0.18694207072257996\n",
      "training: 44 batch 510 batch_loss: 0.18655094504356384\n",
      "training: 44 batch 511 batch_loss: 0.18751215934753418\n",
      "training: 44 batch 512 batch_loss: 0.1835566759109497\n",
      "training: 44 batch 513 batch_loss: 0.1863783597946167\n",
      "training: 44 batch 514 batch_loss: 0.18739855289459229\n",
      "training: 44 batch 515 batch_loss: 0.185989111661911\n",
      "training: 44 batch 516 batch_loss: 0.18813872337341309\n",
      "training: 44 batch 517 batch_loss: 0.1887616515159607\n",
      "training: 44 batch 518 batch_loss: 0.18386021256446838\n",
      "training: 44 batch 519 batch_loss: 0.18593421578407288\n",
      "training: 44 batch 520 batch_loss: 0.18255364894866943\n",
      "training: 44 batch 521 batch_loss: 0.1872372329235077\n",
      "training: 44 batch 522 batch_loss: 0.18780779838562012\n",
      "training: 44 batch 523 batch_loss: 0.18182000517845154\n",
      "training: 44 batch 524 batch_loss: 0.18604162335395813\n",
      "training: 44 batch 525 batch_loss: 0.1832635998725891\n",
      "training: 44 batch 526 batch_loss: 0.18730586767196655\n",
      "training: 44 batch 527 batch_loss: 0.18664047122001648\n",
      "training: 44 batch 528 batch_loss: 0.1894456148147583\n",
      "training: 44 batch 529 batch_loss: 0.18697506189346313\n",
      "training: 44 batch 530 batch_loss: 0.18567422032356262\n",
      "training: 44 batch 531 batch_loss: 0.18870985507965088\n",
      "training: 44 batch 532 batch_loss: 0.18471425771713257\n",
      "training: 44 batch 533 batch_loss: 0.18906551599502563\n",
      "training: 44 batch 534 batch_loss: 0.18424808979034424\n",
      "training: 44 batch 535 batch_loss: 0.1848132312297821\n",
      "training: 44 batch 536 batch_loss: 0.18845012784004211\n",
      "training: 44 batch 537 batch_loss: 0.1898348331451416\n",
      "training: 44 batch 538 batch_loss: 0.18428084254264832\n",
      "training: 44 batch 539 batch_loss: 0.18443837761878967\n",
      "training: 44 batch 540 batch_loss: 0.18670248985290527\n",
      "training: 44 batch 541 batch_loss: 0.18369805812835693\n",
      "training: 44 batch 542 batch_loss: 0.1859053373336792\n",
      "training: 44 batch 543 batch_loss: 0.18578386306762695\n",
      "training: 44 batch 544 batch_loss: 0.1852349042892456\n",
      "training: 44 batch 545 batch_loss: 0.1871241331100464\n",
      "training: 44 batch 546 batch_loss: 0.19072169065475464\n",
      "training: 44 batch 547 batch_loss: 0.18731296062469482\n",
      "training: 44 batch 548 batch_loss: 0.188439279794693\n",
      "training: 44 batch 549 batch_loss: 0.18914365768432617\n",
      "training: 44 batch 550 batch_loss: 0.18955743312835693\n",
      "training: 44 batch 551 batch_loss: 0.18513303995132446\n",
      "training: 44 batch 552 batch_loss: 0.18994948267936707\n",
      "training: 44 batch 553 batch_loss: 0.18615096807479858\n",
      "training: 44 batch 554 batch_loss: 0.18257135152816772\n",
      "training: 44 batch 555 batch_loss: 0.18807190656661987\n",
      "training: 44 batch 556 batch_loss: 0.18380871415138245\n",
      "training: 44 batch 557 batch_loss: 0.1841677725315094\n",
      "training: 44 batch 558 batch_loss: 0.18713179230690002\n",
      "training: 44 batch 559 batch_loss: 0.18916040658950806\n",
      "training: 44 batch 560 batch_loss: 0.18624234199523926\n",
      "training: 44 batch 561 batch_loss: 0.18877533078193665\n",
      "training: 44 batch 562 batch_loss: 0.1839401125907898\n",
      "training: 44 batch 563 batch_loss: 0.18918469548225403\n",
      "training: 44 batch 564 batch_loss: 0.18879595398902893\n",
      "training: 44 batch 565 batch_loss: 0.190005362033844\n",
      "training: 44 batch 566 batch_loss: 0.18664228916168213\n",
      "training: 44 batch 567 batch_loss: 0.18705961108207703\n",
      "training: 44 batch 568 batch_loss: 0.18875110149383545\n",
      "training: 44 batch 569 batch_loss: 0.1882724165916443\n",
      "training: 44 batch 570 batch_loss: 0.18713170289993286\n",
      "training: 44 batch 571 batch_loss: 0.18530067801475525\n",
      "training: 44 batch 572 batch_loss: 0.1894444227218628\n",
      "training: 44 batch 573 batch_loss: 0.1829575002193451\n",
      "training: 44 batch 574 batch_loss: 0.18429937958717346\n",
      "training: 44 batch 575 batch_loss: 0.18843793869018555\n",
      "training: 44 batch 576 batch_loss: 0.18419957160949707\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 44, Hit Ratio:0.03146642326769778 | Precision:0.046426816081785116 | Recall:0.061977529106228134 | NDCG:0.05988006322208036\n",
      "*Best Performance* \n",
      "Epoch: 43, Hit Ratio:0.03184951014847115 | Precision:0.04699203774697729 | Recall:0.06261647091061 | MDCG:0.0607415751955159\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 45 batch 0 batch_loss: 0.18741002678871155\n",
      "training: 45 batch 1 batch_loss: 0.18700677156448364\n",
      "training: 45 batch 2 batch_loss: 0.19104227423667908\n",
      "training: 45 batch 3 batch_loss: 0.18210569024085999\n",
      "training: 45 batch 4 batch_loss: 0.18455687165260315\n",
      "training: 45 batch 5 batch_loss: 0.18391454219818115\n",
      "training: 45 batch 6 batch_loss: 0.18160957098007202\n",
      "training: 45 batch 7 batch_loss: 0.18651217222213745\n",
      "training: 45 batch 8 batch_loss: 0.18671539425849915\n",
      "training: 45 batch 9 batch_loss: 0.18622618913650513\n",
      "training: 45 batch 10 batch_loss: 0.18610969185829163\n",
      "training: 45 batch 11 batch_loss: 0.18648988008499146\n",
      "training: 45 batch 12 batch_loss: 0.18379688262939453\n",
      "training: 45 batch 13 batch_loss: 0.1894567608833313\n",
      "training: 45 batch 14 batch_loss: 0.1827414333820343\n",
      "training: 45 batch 15 batch_loss: 0.1875547468662262\n",
      "training: 45 batch 16 batch_loss: 0.18727412819862366\n",
      "training: 45 batch 17 batch_loss: 0.18495476245880127\n",
      "training: 45 batch 18 batch_loss: 0.1805594265460968\n",
      "training: 45 batch 19 batch_loss: 0.18264231085777283\n",
      "training: 45 batch 20 batch_loss: 0.1863132119178772\n",
      "training: 45 batch 21 batch_loss: 0.18519651889801025\n",
      "training: 45 batch 22 batch_loss: 0.18270015716552734\n",
      "training: 45 batch 23 batch_loss: 0.18525448441505432\n",
      "training: 45 batch 24 batch_loss: 0.18533703684806824\n",
      "training: 45 batch 25 batch_loss: 0.18365660309791565\n",
      "training: 45 batch 26 batch_loss: 0.18256980180740356\n",
      "training: 45 batch 27 batch_loss: 0.18396762013435364\n",
      "training: 45 batch 28 batch_loss: 0.18720585107803345\n",
      "training: 45 batch 29 batch_loss: 0.187311589717865\n",
      "training: 45 batch 30 batch_loss: 0.18618014454841614\n",
      "training: 45 batch 31 batch_loss: 0.18776416778564453\n",
      "training: 45 batch 32 batch_loss: 0.1836949586868286\n",
      "training: 45 batch 33 batch_loss: 0.1820436716079712\n",
      "training: 45 batch 34 batch_loss: 0.18583279848098755\n",
      "training: 45 batch 35 batch_loss: 0.18199783563613892\n",
      "training: 45 batch 36 batch_loss: 0.1839536726474762\n",
      "training: 45 batch 37 batch_loss: 0.18416863679885864\n",
      "training: 45 batch 38 batch_loss: 0.18560856580734253\n",
      "training: 45 batch 39 batch_loss: 0.18621793389320374\n",
      "training: 45 batch 40 batch_loss: 0.18580660223960876\n",
      "training: 45 batch 41 batch_loss: 0.18240326642990112\n",
      "training: 45 batch 42 batch_loss: 0.18676969408988953\n",
      "training: 45 batch 43 batch_loss: 0.1858299970626831\n",
      "training: 45 batch 44 batch_loss: 0.1832377314567566\n",
      "training: 45 batch 45 batch_loss: 0.1867867410182953\n",
      "training: 45 batch 46 batch_loss: 0.18079078197479248\n",
      "training: 45 batch 47 batch_loss: 0.18631458282470703\n",
      "training: 45 batch 48 batch_loss: 0.1861954927444458\n",
      "training: 45 batch 49 batch_loss: 0.18510296940803528\n",
      "training: 45 batch 50 batch_loss: 0.18307271599769592\n",
      "training: 45 batch 51 batch_loss: 0.185592919588089\n",
      "training: 45 batch 52 batch_loss: 0.18685105443000793\n",
      "training: 45 batch 53 batch_loss: 0.18601897358894348\n",
      "training: 45 batch 54 batch_loss: 0.18279922008514404\n",
      "training: 45 batch 55 batch_loss: 0.18272006511688232\n",
      "training: 45 batch 56 batch_loss: 0.18543994426727295\n",
      "training: 45 batch 57 batch_loss: 0.18733513355255127\n",
      "training: 45 batch 58 batch_loss: 0.1820330023765564\n",
      "training: 45 batch 59 batch_loss: 0.18738210201263428\n",
      "training: 45 batch 60 batch_loss: 0.1875174343585968\n",
      "training: 45 batch 61 batch_loss: 0.18481484055519104\n",
      "training: 45 batch 62 batch_loss: 0.18492257595062256\n",
      "training: 45 batch 63 batch_loss: 0.18467512726783752\n",
      "training: 45 batch 64 batch_loss: 0.18485134840011597\n",
      "training: 45 batch 65 batch_loss: 0.18701103329658508\n",
      "training: 45 batch 66 batch_loss: 0.1881151795387268\n",
      "training: 45 batch 67 batch_loss: 0.18325355648994446\n",
      "training: 45 batch 68 batch_loss: 0.18382245302200317\n",
      "training: 45 batch 69 batch_loss: 0.18210700154304504\n",
      "training: 45 batch 70 batch_loss: 0.18115243315696716\n",
      "training: 45 batch 71 batch_loss: 0.1831878423690796\n",
      "training: 45 batch 72 batch_loss: 0.18051636219024658\n",
      "training: 45 batch 73 batch_loss: 0.18786382675170898\n",
      "training: 45 batch 74 batch_loss: 0.1800878643989563\n",
      "training: 45 batch 75 batch_loss: 0.18123936653137207\n",
      "training: 45 batch 76 batch_loss: 0.1855778694152832\n",
      "training: 45 batch 77 batch_loss: 0.18526804447174072\n",
      "training: 45 batch 78 batch_loss: 0.18290776014328003\n",
      "training: 45 batch 79 batch_loss: 0.18507787585258484\n",
      "training: 45 batch 80 batch_loss: 0.18797987699508667\n",
      "training: 45 batch 81 batch_loss: 0.18386626243591309\n",
      "training: 45 batch 82 batch_loss: 0.1845412254333496\n",
      "training: 45 batch 83 batch_loss: 0.18525037169456482\n",
      "training: 45 batch 84 batch_loss: 0.18273982405662537\n",
      "training: 45 batch 85 batch_loss: 0.18706610798835754\n",
      "training: 45 batch 86 batch_loss: 0.18871048092842102\n",
      "training: 45 batch 87 batch_loss: 0.18920743465423584\n",
      "training: 45 batch 88 batch_loss: 0.186308354139328\n",
      "training: 45 batch 89 batch_loss: 0.18720707297325134\n",
      "training: 45 batch 90 batch_loss: 0.1827142834663391\n",
      "training: 45 batch 91 batch_loss: 0.18742311000823975\n",
      "training: 45 batch 92 batch_loss: 0.18918126821517944\n",
      "training: 45 batch 93 batch_loss: 0.1831870973110199\n",
      "training: 45 batch 94 batch_loss: 0.1830347180366516\n",
      "training: 45 batch 95 batch_loss: 0.1875547468662262\n",
      "training: 45 batch 96 batch_loss: 0.18775564432144165\n",
      "training: 45 batch 97 batch_loss: 0.18778502941131592\n",
      "training: 45 batch 98 batch_loss: 0.1852951943874359\n",
      "training: 45 batch 99 batch_loss: 0.18241530656814575\n",
      "training: 45 batch 100 batch_loss: 0.18674781918525696\n",
      "training: 45 batch 101 batch_loss: 0.1877119541168213\n",
      "training: 45 batch 102 batch_loss: 0.18218588829040527\n",
      "training: 45 batch 103 batch_loss: 0.18477046489715576\n",
      "training: 45 batch 104 batch_loss: 0.18582883477210999\n",
      "training: 45 batch 105 batch_loss: 0.18257877230644226\n",
      "training: 45 batch 106 batch_loss: 0.1833725869655609\n",
      "training: 45 batch 107 batch_loss: 0.18818286061286926\n",
      "training: 45 batch 108 batch_loss: 0.1859460175037384\n",
      "training: 45 batch 109 batch_loss: 0.18521639704704285\n",
      "training: 45 batch 110 batch_loss: 0.18781200051307678\n",
      "training: 45 batch 111 batch_loss: 0.1883028745651245\n",
      "training: 45 batch 112 batch_loss: 0.18457534909248352\n",
      "training: 45 batch 113 batch_loss: 0.18720689415931702\n",
      "training: 45 batch 114 batch_loss: 0.18379530310630798\n",
      "training: 45 batch 115 batch_loss: 0.18585729598999023\n",
      "training: 45 batch 116 batch_loss: 0.18824219703674316\n",
      "training: 45 batch 117 batch_loss: 0.1887439489364624\n",
      "training: 45 batch 118 batch_loss: 0.18476513028144836\n",
      "training: 45 batch 119 batch_loss: 0.1834913194179535\n",
      "training: 45 batch 120 batch_loss: 0.18387848138809204\n",
      "training: 45 batch 121 batch_loss: 0.18387165665626526\n",
      "training: 45 batch 122 batch_loss: 0.18529516458511353\n",
      "training: 45 batch 123 batch_loss: 0.18514999747276306\n",
      "training: 45 batch 124 batch_loss: 0.1856716275215149\n",
      "training: 45 batch 125 batch_loss: 0.18493932485580444\n",
      "training: 45 batch 126 batch_loss: 0.18542855978012085\n",
      "training: 45 batch 127 batch_loss: 0.18692326545715332\n",
      "training: 45 batch 128 batch_loss: 0.18754327297210693\n",
      "training: 45 batch 129 batch_loss: 0.18731281161308289\n",
      "training: 45 batch 130 batch_loss: 0.18635696172714233\n",
      "training: 45 batch 131 batch_loss: 0.18323251605033875\n",
      "training: 45 batch 132 batch_loss: 0.18527042865753174\n",
      "training: 45 batch 133 batch_loss: 0.18484976887702942\n",
      "training: 45 batch 134 batch_loss: 0.18327349424362183\n",
      "training: 45 batch 135 batch_loss: 0.18268615007400513\n",
      "training: 45 batch 136 batch_loss: 0.1809217929840088\n",
      "training: 45 batch 137 batch_loss: 0.1828412115573883\n",
      "training: 45 batch 138 batch_loss: 0.1875801682472229\n",
      "training: 45 batch 139 batch_loss: 0.18344777822494507\n",
      "training: 45 batch 140 batch_loss: 0.18464279174804688\n",
      "training: 45 batch 141 batch_loss: 0.1843702793121338\n",
      "training: 45 batch 142 batch_loss: 0.18723899126052856\n",
      "training: 45 batch 143 batch_loss: 0.18857881426811218\n",
      "training: 45 batch 144 batch_loss: 0.18777188658714294\n",
      "training: 45 batch 145 batch_loss: 0.18294155597686768\n",
      "training: 45 batch 146 batch_loss: 0.18946444988250732\n",
      "training: 45 batch 147 batch_loss: 0.18394166231155396\n",
      "training: 45 batch 148 batch_loss: 0.18412238359451294\n",
      "training: 45 batch 149 batch_loss: 0.1870233416557312\n",
      "training: 45 batch 150 batch_loss: 0.1852726936340332\n",
      "training: 45 batch 151 batch_loss: 0.18580785393714905\n",
      "training: 45 batch 152 batch_loss: 0.18524298071861267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 45 batch 153 batch_loss: 0.18205049633979797\n",
      "training: 45 batch 154 batch_loss: 0.18377268314361572\n",
      "training: 45 batch 155 batch_loss: 0.18601146340370178\n",
      "training: 45 batch 156 batch_loss: 0.1859126091003418\n",
      "training: 45 batch 157 batch_loss: 0.18489742279052734\n",
      "training: 45 batch 158 batch_loss: 0.19066071510314941\n",
      "training: 45 batch 159 batch_loss: 0.18571433424949646\n",
      "training: 45 batch 160 batch_loss: 0.18777084350585938\n",
      "training: 45 batch 161 batch_loss: 0.18786677718162537\n",
      "training: 45 batch 162 batch_loss: 0.18858158588409424\n",
      "training: 45 batch 163 batch_loss: 0.18295198678970337\n",
      "training: 45 batch 164 batch_loss: 0.18215784430503845\n",
      "training: 45 batch 165 batch_loss: 0.18598029017448425\n",
      "training: 45 batch 166 batch_loss: 0.1869555413722992\n",
      "training: 45 batch 167 batch_loss: 0.18657562136650085\n",
      "training: 45 batch 168 batch_loss: 0.18548208475112915\n",
      "training: 45 batch 169 batch_loss: 0.1842850148677826\n",
      "training: 45 batch 170 batch_loss: 0.18640229105949402\n",
      "training: 45 batch 171 batch_loss: 0.1841559112071991\n",
      "training: 45 batch 172 batch_loss: 0.18424329161643982\n",
      "training: 45 batch 173 batch_loss: 0.18372631072998047\n",
      "training: 45 batch 174 batch_loss: 0.18626290559768677\n",
      "training: 45 batch 175 batch_loss: 0.18568596243858337\n",
      "training: 45 batch 176 batch_loss: 0.18614336848258972\n",
      "training: 45 batch 177 batch_loss: 0.18628442287445068\n",
      "training: 45 batch 178 batch_loss: 0.18876254558563232\n",
      "training: 45 batch 179 batch_loss: 0.18658903241157532\n",
      "training: 45 batch 180 batch_loss: 0.18505221605300903\n",
      "training: 45 batch 181 batch_loss: 0.18714341521263123\n",
      "training: 45 batch 182 batch_loss: 0.18241170048713684\n",
      "training: 45 batch 183 batch_loss: 0.18603870272636414\n",
      "training: 45 batch 184 batch_loss: 0.1888868510723114\n",
      "training: 45 batch 185 batch_loss: 0.18586942553520203\n",
      "training: 45 batch 186 batch_loss: 0.18426966667175293\n",
      "training: 45 batch 187 batch_loss: 0.1856498420238495\n",
      "training: 45 batch 188 batch_loss: 0.1855500042438507\n",
      "training: 45 batch 189 batch_loss: 0.18661075830459595\n",
      "training: 45 batch 190 batch_loss: 0.18361389636993408\n",
      "training: 45 batch 191 batch_loss: 0.18611767888069153\n",
      "training: 45 batch 192 batch_loss: 0.18844324350357056\n",
      "training: 45 batch 193 batch_loss: 0.18578389286994934\n",
      "training: 45 batch 194 batch_loss: 0.1855420470237732\n",
      "training: 45 batch 195 batch_loss: 0.18575960397720337\n",
      "training: 45 batch 196 batch_loss: 0.18459391593933105\n",
      "training: 45 batch 197 batch_loss: 0.1854577660560608\n",
      "training: 45 batch 198 batch_loss: 0.18349239230155945\n",
      "training: 45 batch 199 batch_loss: 0.18820789456367493\n",
      "training: 45 batch 200 batch_loss: 0.18746325373649597\n",
      "training: 45 batch 201 batch_loss: 0.18805310130119324\n",
      "training: 45 batch 202 batch_loss: 0.186600923538208\n",
      "training: 45 batch 203 batch_loss: 0.18520930409431458\n",
      "training: 45 batch 204 batch_loss: 0.18735140562057495\n",
      "training: 45 batch 205 batch_loss: 0.1858973205089569\n",
      "training: 45 batch 206 batch_loss: 0.1853073239326477\n",
      "training: 45 batch 207 batch_loss: 0.19019344449043274\n",
      "training: 45 batch 208 batch_loss: 0.18410563468933105\n",
      "training: 45 batch 209 batch_loss: 0.1867189109325409\n",
      "training: 45 batch 210 batch_loss: 0.1876542866230011\n",
      "training: 45 batch 211 batch_loss: 0.18795624375343323\n",
      "training: 45 batch 212 batch_loss: 0.18494105339050293\n",
      "training: 45 batch 213 batch_loss: 0.1835050880908966\n",
      "training: 45 batch 214 batch_loss: 0.1833532452583313\n",
      "training: 45 batch 215 batch_loss: 0.18578475713729858\n",
      "training: 45 batch 216 batch_loss: 0.18361660838127136\n",
      "training: 45 batch 217 batch_loss: 0.18732061982154846\n",
      "training: 45 batch 218 batch_loss: 0.18630892038345337\n",
      "training: 45 batch 219 batch_loss: 0.18680214881896973\n",
      "training: 45 batch 220 batch_loss: 0.1870918869972229\n",
      "training: 45 batch 221 batch_loss: 0.185918927192688\n",
      "training: 45 batch 222 batch_loss: 0.18421775102615356\n",
      "training: 45 batch 223 batch_loss: 0.1851215958595276\n",
      "training: 45 batch 224 batch_loss: 0.187547504901886\n",
      "training: 45 batch 225 batch_loss: 0.18390271067619324\n",
      "training: 45 batch 226 batch_loss: 0.18332284688949585\n",
      "training: 45 batch 227 batch_loss: 0.186631441116333\n",
      "training: 45 batch 228 batch_loss: 0.18408381938934326\n",
      "training: 45 batch 229 batch_loss: 0.1838848888874054\n",
      "training: 45 batch 230 batch_loss: 0.18346232175827026\n",
      "training: 45 batch 231 batch_loss: 0.18832072615623474\n",
      "training: 45 batch 232 batch_loss: 0.18260860443115234\n",
      "training: 45 batch 233 batch_loss: 0.18276181817054749\n",
      "training: 45 batch 234 batch_loss: 0.1858890950679779\n",
      "training: 45 batch 235 batch_loss: 0.1833115816116333\n",
      "training: 45 batch 236 batch_loss: 0.18346107006072998\n",
      "training: 45 batch 237 batch_loss: 0.187859445810318\n",
      "training: 45 batch 238 batch_loss: 0.18610289692878723\n",
      "training: 45 batch 239 batch_loss: 0.18660467863082886\n",
      "training: 45 batch 240 batch_loss: 0.19250044226646423\n",
      "training: 45 batch 241 batch_loss: 0.18638303875923157\n",
      "training: 45 batch 242 batch_loss: 0.18488821387290955\n",
      "training: 45 batch 243 batch_loss: 0.18572813272476196\n",
      "training: 45 batch 244 batch_loss: 0.1882493793964386\n",
      "training: 45 batch 245 batch_loss: 0.17963388562202454\n",
      "training: 45 batch 246 batch_loss: 0.1894451081752777\n",
      "training: 45 batch 247 batch_loss: 0.18398529291152954\n",
      "training: 45 batch 248 batch_loss: 0.18750298023223877\n",
      "training: 45 batch 249 batch_loss: 0.18497928977012634\n",
      "training: 45 batch 250 batch_loss: 0.18645811080932617\n",
      "training: 45 batch 251 batch_loss: 0.18923571705818176\n",
      "training: 45 batch 252 batch_loss: 0.18502312898635864\n",
      "training: 45 batch 253 batch_loss: 0.18354442715644836\n",
      "training: 45 batch 254 batch_loss: 0.18518182635307312\n",
      "training: 45 batch 255 batch_loss: 0.18556562066078186\n",
      "training: 45 batch 256 batch_loss: 0.18414023518562317\n",
      "training: 45 batch 257 batch_loss: 0.1867084503173828\n",
      "training: 45 batch 258 batch_loss: 0.18656420707702637\n",
      "training: 45 batch 259 batch_loss: 0.18487140536308289\n",
      "training: 45 batch 260 batch_loss: 0.18677860498428345\n",
      "training: 45 batch 261 batch_loss: 0.18557137250900269\n",
      "training: 45 batch 262 batch_loss: 0.19351720809936523\n",
      "training: 45 batch 263 batch_loss: 0.1883057951927185\n",
      "training: 45 batch 264 batch_loss: 0.18465659022331238\n",
      "training: 45 batch 265 batch_loss: 0.18747875094413757\n",
      "training: 45 batch 266 batch_loss: 0.18451759219169617\n",
      "training: 45 batch 267 batch_loss: 0.1854521930217743\n",
      "training: 45 batch 268 batch_loss: 0.18792468309402466\n",
      "training: 45 batch 269 batch_loss: 0.18811950087547302\n",
      "training: 45 batch 270 batch_loss: 0.18574589490890503\n",
      "training: 45 batch 271 batch_loss: 0.18818271160125732\n",
      "training: 45 batch 272 batch_loss: 0.18827596306800842\n",
      "training: 45 batch 273 batch_loss: 0.18379944562911987\n",
      "training: 45 batch 274 batch_loss: 0.185042142868042\n",
      "training: 45 batch 275 batch_loss: 0.18762683868408203\n",
      "training: 45 batch 276 batch_loss: 0.18777230381965637\n",
      "training: 45 batch 277 batch_loss: 0.1847972869873047\n",
      "training: 45 batch 278 batch_loss: 0.1842743456363678\n",
      "training: 45 batch 279 batch_loss: 0.18713638186454773\n",
      "training: 45 batch 280 batch_loss: 0.1830844283103943\n",
      "training: 45 batch 281 batch_loss: 0.1841350793838501\n",
      "training: 45 batch 282 batch_loss: 0.18741267919540405\n",
      "training: 45 batch 283 batch_loss: 0.1871984302997589\n",
      "training: 45 batch 284 batch_loss: 0.18509894609451294\n",
      "training: 45 batch 285 batch_loss: 0.18388110399246216\n",
      "training: 45 batch 286 batch_loss: 0.18753355741500854\n",
      "training: 45 batch 287 batch_loss: 0.18884164094924927\n",
      "training: 45 batch 288 batch_loss: 0.18392592668533325\n",
      "training: 45 batch 289 batch_loss: 0.1898830533027649\n",
      "training: 45 batch 290 batch_loss: 0.1842067539691925\n",
      "training: 45 batch 291 batch_loss: 0.18731772899627686\n",
      "training: 45 batch 292 batch_loss: 0.1868431568145752\n",
      "training: 45 batch 293 batch_loss: 0.18825694918632507\n",
      "training: 45 batch 294 batch_loss: 0.18480980396270752\n",
      "training: 45 batch 295 batch_loss: 0.18654003739356995\n",
      "training: 45 batch 296 batch_loss: 0.1834999918937683\n",
      "training: 45 batch 297 batch_loss: 0.18679335713386536\n",
      "training: 45 batch 298 batch_loss: 0.1897924542427063\n",
      "training: 45 batch 299 batch_loss: 0.19020050764083862\n",
      "training: 45 batch 300 batch_loss: 0.19272837042808533\n",
      "training: 45 batch 301 batch_loss: 0.18541455268859863\n",
      "training: 45 batch 302 batch_loss: 0.18475207686424255\n",
      "training: 45 batch 303 batch_loss: 0.18527254462242126\n",
      "training: 45 batch 304 batch_loss: 0.18521851301193237\n",
      "training: 45 batch 305 batch_loss: 0.1830165684223175\n",
      "training: 45 batch 306 batch_loss: 0.18825405836105347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 45 batch 307 batch_loss: 0.188108891248703\n",
      "training: 45 batch 308 batch_loss: 0.18685013055801392\n",
      "training: 45 batch 309 batch_loss: 0.18629035353660583\n",
      "training: 45 batch 310 batch_loss: 0.189063161611557\n",
      "training: 45 batch 311 batch_loss: 0.1838626265525818\n",
      "training: 45 batch 312 batch_loss: 0.1843796968460083\n",
      "training: 45 batch 313 batch_loss: 0.1811251938343048\n",
      "training: 45 batch 314 batch_loss: 0.18763279914855957\n",
      "training: 45 batch 315 batch_loss: 0.18694648146629333\n",
      "training: 45 batch 316 batch_loss: 0.18634328246116638\n",
      "training: 45 batch 317 batch_loss: 0.18575596809387207\n",
      "training: 45 batch 318 batch_loss: 0.1899699866771698\n",
      "training: 45 batch 319 batch_loss: 0.1850164830684662\n",
      "training: 45 batch 320 batch_loss: 0.18934589624404907\n",
      "training: 45 batch 321 batch_loss: 0.18551546335220337\n",
      "training: 45 batch 322 batch_loss: 0.18717887997627258\n",
      "training: 45 batch 323 batch_loss: 0.1868477761745453\n",
      "training: 45 batch 324 batch_loss: 0.1886734664440155\n",
      "training: 45 batch 325 batch_loss: 0.18443912267684937\n",
      "training: 45 batch 326 batch_loss: 0.1876247227191925\n",
      "training: 45 batch 327 batch_loss: 0.18268641829490662\n",
      "training: 45 batch 328 batch_loss: 0.1852799355983734\n",
      "training: 45 batch 329 batch_loss: 0.1827716827392578\n",
      "training: 45 batch 330 batch_loss: 0.18824127316474915\n",
      "training: 45 batch 331 batch_loss: 0.18925249576568604\n",
      "training: 45 batch 332 batch_loss: 0.18878650665283203\n",
      "training: 45 batch 333 batch_loss: 0.1867789328098297\n",
      "training: 45 batch 334 batch_loss: 0.18786495923995972\n",
      "training: 45 batch 335 batch_loss: 0.18803170323371887\n",
      "training: 45 batch 336 batch_loss: 0.1852082908153534\n",
      "training: 45 batch 337 batch_loss: 0.18605604767799377\n",
      "training: 45 batch 338 batch_loss: 0.1892862319946289\n",
      "training: 45 batch 339 batch_loss: 0.18649297952651978\n",
      "training: 45 batch 340 batch_loss: 0.1848897933959961\n",
      "training: 45 batch 341 batch_loss: 0.18478289246559143\n",
      "training: 45 batch 342 batch_loss: 0.18402248620986938\n",
      "training: 45 batch 343 batch_loss: 0.18939748406410217\n",
      "training: 45 batch 344 batch_loss: 0.18715354800224304\n",
      "training: 45 batch 345 batch_loss: 0.1866171956062317\n",
      "training: 45 batch 346 batch_loss: 0.19146719574928284\n",
      "training: 45 batch 347 batch_loss: 0.18379837274551392\n",
      "training: 45 batch 348 batch_loss: 0.18131408095359802\n",
      "training: 45 batch 349 batch_loss: 0.18552327156066895\n",
      "training: 45 batch 350 batch_loss: 0.18535065650939941\n",
      "training: 45 batch 351 batch_loss: 0.18608105182647705\n",
      "training: 45 batch 352 batch_loss: 0.1842079758644104\n",
      "training: 45 batch 353 batch_loss: 0.18316742777824402\n",
      "training: 45 batch 354 batch_loss: 0.19125860929489136\n",
      "training: 45 batch 355 batch_loss: 0.18425947427749634\n",
      "training: 45 batch 356 batch_loss: 0.18341216444969177\n",
      "training: 45 batch 357 batch_loss: 0.18772614002227783\n",
      "training: 45 batch 358 batch_loss: 0.18832695484161377\n",
      "training: 45 batch 359 batch_loss: 0.18439865112304688\n",
      "training: 45 batch 360 batch_loss: 0.18899273872375488\n",
      "training: 45 batch 361 batch_loss: 0.184880793094635\n",
      "training: 45 batch 362 batch_loss: 0.18785730004310608\n",
      "training: 45 batch 363 batch_loss: 0.18456098437309265\n",
      "training: 45 batch 364 batch_loss: 0.1872888207435608\n",
      "training: 45 batch 365 batch_loss: 0.1856193244457245\n",
      "training: 45 batch 366 batch_loss: 0.18365272879600525\n",
      "training: 45 batch 367 batch_loss: 0.1867423951625824\n",
      "training: 45 batch 368 batch_loss: 0.18634843826293945\n",
      "training: 45 batch 369 batch_loss: 0.18925949931144714\n",
      "training: 45 batch 370 batch_loss: 0.18648338317871094\n",
      "training: 45 batch 371 batch_loss: 0.18542015552520752\n",
      "training: 45 batch 372 batch_loss: 0.19050419330596924\n",
      "training: 45 batch 373 batch_loss: 0.18796226382255554\n",
      "training: 45 batch 374 batch_loss: 0.1895836591720581\n",
      "training: 45 batch 375 batch_loss: 0.18824392557144165\n",
      "training: 45 batch 376 batch_loss: 0.18745183944702148\n",
      "training: 45 batch 377 batch_loss: 0.18724390864372253\n",
      "training: 45 batch 378 batch_loss: 0.18840011954307556\n",
      "training: 45 batch 379 batch_loss: 0.19006648659706116\n",
      "training: 45 batch 380 batch_loss: 0.18572410941123962\n",
      "training: 45 batch 381 batch_loss: 0.18538212776184082\n",
      "training: 45 batch 382 batch_loss: 0.18532171845436096\n",
      "training: 45 batch 383 batch_loss: 0.18766874074935913\n",
      "training: 45 batch 384 batch_loss: 0.18759089708328247\n",
      "training: 45 batch 385 batch_loss: 0.18477290868759155\n",
      "training: 45 batch 386 batch_loss: 0.18340542912483215\n",
      "training: 45 batch 387 batch_loss: 0.18897369503974915\n",
      "training: 45 batch 388 batch_loss: 0.18499982357025146\n",
      "training: 45 batch 389 batch_loss: 0.18560022115707397\n",
      "training: 45 batch 390 batch_loss: 0.1898084282875061\n",
      "training: 45 batch 391 batch_loss: 0.18751880526542664\n",
      "training: 45 batch 392 batch_loss: 0.18888995051383972\n",
      "training: 45 batch 393 batch_loss: 0.18482249975204468\n",
      "training: 45 batch 394 batch_loss: 0.1876603662967682\n",
      "training: 45 batch 395 batch_loss: 0.18920540809631348\n",
      "training: 45 batch 396 batch_loss: 0.18436884880065918\n",
      "training: 45 batch 397 batch_loss: 0.18554383516311646\n",
      "training: 45 batch 398 batch_loss: 0.18304631114006042\n",
      "training: 45 batch 399 batch_loss: 0.18842840194702148\n",
      "training: 45 batch 400 batch_loss: 0.18838614225387573\n",
      "training: 45 batch 401 batch_loss: 0.18774470686912537\n",
      "training: 45 batch 402 batch_loss: 0.18619883060455322\n",
      "training: 45 batch 403 batch_loss: 0.1851731538772583\n",
      "training: 45 batch 404 batch_loss: 0.18675783276557922\n",
      "training: 45 batch 405 batch_loss: 0.18824082612991333\n",
      "training: 45 batch 406 batch_loss: 0.18782630562782288\n",
      "training: 45 batch 407 batch_loss: 0.19006633758544922\n",
      "training: 45 batch 408 batch_loss: 0.18339136242866516\n",
      "training: 45 batch 409 batch_loss: 0.19061440229415894\n",
      "training: 45 batch 410 batch_loss: 0.18943136930465698\n",
      "training: 45 batch 411 batch_loss: 0.1900639832019806\n",
      "training: 45 batch 412 batch_loss: 0.18994182348251343\n",
      "training: 45 batch 413 batch_loss: 0.18264338374137878\n",
      "training: 45 batch 414 batch_loss: 0.1857704222202301\n",
      "training: 45 batch 415 batch_loss: 0.1880948841571808\n",
      "training: 45 batch 416 batch_loss: 0.18552058935165405\n",
      "training: 45 batch 417 batch_loss: 0.18576449155807495\n",
      "training: 45 batch 418 batch_loss: 0.1877504587173462\n",
      "training: 45 batch 419 batch_loss: 0.1852804720401764\n",
      "training: 45 batch 420 batch_loss: 0.19091957807540894\n",
      "training: 45 batch 421 batch_loss: 0.18617531657218933\n",
      "training: 45 batch 422 batch_loss: 0.18744829297065735\n",
      "training: 45 batch 423 batch_loss: 0.18689319491386414\n",
      "training: 45 batch 424 batch_loss: 0.18422392010688782\n",
      "training: 45 batch 425 batch_loss: 0.18641459941864014\n",
      "training: 45 batch 426 batch_loss: 0.18628647923469543\n",
      "training: 45 batch 427 batch_loss: 0.18663746118545532\n",
      "training: 45 batch 428 batch_loss: 0.18412038683891296\n",
      "training: 45 batch 429 batch_loss: 0.190586656332016\n",
      "training: 45 batch 430 batch_loss: 0.1899520754814148\n",
      "training: 45 batch 431 batch_loss: 0.18993347883224487\n",
      "training: 45 batch 432 batch_loss: 0.18623679876327515\n",
      "training: 45 batch 433 batch_loss: 0.1889459788799286\n",
      "training: 45 batch 434 batch_loss: 0.18467625975608826\n",
      "training: 45 batch 435 batch_loss: 0.1857721507549286\n",
      "training: 45 batch 436 batch_loss: 0.19094648957252502\n",
      "training: 45 batch 437 batch_loss: 0.18649321794509888\n",
      "training: 45 batch 438 batch_loss: 0.1876087188720703\n",
      "training: 45 batch 439 batch_loss: 0.18878906965255737\n",
      "training: 45 batch 440 batch_loss: 0.18922945857048035\n",
      "training: 45 batch 441 batch_loss: 0.18925851583480835\n",
      "training: 45 batch 442 batch_loss: 0.1843649446964264\n",
      "training: 45 batch 443 batch_loss: 0.19057834148406982\n",
      "training: 45 batch 444 batch_loss: 0.1807360053062439\n",
      "training: 45 batch 445 batch_loss: 0.18799981474876404\n",
      "training: 45 batch 446 batch_loss: 0.18688663840293884\n",
      "training: 45 batch 447 batch_loss: 0.1868901550769806\n",
      "training: 45 batch 448 batch_loss: 0.18656787276268005\n",
      "training: 45 batch 449 batch_loss: 0.18518030643463135\n",
      "training: 45 batch 450 batch_loss: 0.18878304958343506\n",
      "training: 45 batch 451 batch_loss: 0.18708646297454834\n",
      "training: 45 batch 452 batch_loss: 0.1868421733379364\n",
      "training: 45 batch 453 batch_loss: 0.1837460696697235\n",
      "training: 45 batch 454 batch_loss: 0.19066840410232544\n",
      "training: 45 batch 455 batch_loss: 0.18624019622802734\n",
      "training: 45 batch 456 batch_loss: 0.18911269307136536\n",
      "training: 45 batch 457 batch_loss: 0.18568706512451172\n",
      "training: 45 batch 458 batch_loss: 0.18416768312454224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 45 batch 459 batch_loss: 0.1907702386379242\n",
      "training: 45 batch 460 batch_loss: 0.1899053454399109\n",
      "training: 45 batch 461 batch_loss: 0.18577489256858826\n",
      "training: 45 batch 462 batch_loss: 0.18592625856399536\n",
      "training: 45 batch 463 batch_loss: 0.18567389249801636\n",
      "training: 45 batch 464 batch_loss: 0.1831199824810028\n",
      "training: 45 batch 465 batch_loss: 0.18809732794761658\n",
      "training: 45 batch 466 batch_loss: 0.18138661980628967\n",
      "training: 45 batch 467 batch_loss: 0.18980374932289124\n",
      "training: 45 batch 468 batch_loss: 0.18669477105140686\n",
      "training: 45 batch 469 batch_loss: 0.18994617462158203\n",
      "training: 45 batch 470 batch_loss: 0.1866132915019989\n",
      "training: 45 batch 471 batch_loss: 0.18838539719581604\n",
      "training: 45 batch 472 batch_loss: 0.18780404329299927\n",
      "training: 45 batch 473 batch_loss: 0.18762800097465515\n",
      "training: 45 batch 474 batch_loss: 0.19154441356658936\n",
      "training: 45 batch 475 batch_loss: 0.18973714113235474\n",
      "training: 45 batch 476 batch_loss: 0.18569421768188477\n",
      "training: 45 batch 477 batch_loss: 0.18748506903648376\n",
      "training: 45 batch 478 batch_loss: 0.18948444724082947\n",
      "training: 45 batch 479 batch_loss: 0.18333211541175842\n",
      "training: 45 batch 480 batch_loss: 0.18614083528518677\n",
      "training: 45 batch 481 batch_loss: 0.19193628430366516\n",
      "training: 45 batch 482 batch_loss: 0.18566399812698364\n",
      "training: 45 batch 483 batch_loss: 0.1886523962020874\n",
      "training: 45 batch 484 batch_loss: 0.18888267874717712\n",
      "training: 45 batch 485 batch_loss: 0.18820586800575256\n",
      "training: 45 batch 486 batch_loss: 0.18802008032798767\n",
      "training: 45 batch 487 batch_loss: 0.19024741649627686\n",
      "training: 45 batch 488 batch_loss: 0.18625158071517944\n",
      "training: 45 batch 489 batch_loss: 0.18918338418006897\n",
      "training: 45 batch 490 batch_loss: 0.1885157823562622\n",
      "training: 45 batch 491 batch_loss: 0.1885843575000763\n",
      "training: 45 batch 492 batch_loss: 0.18685674667358398\n",
      "training: 45 batch 493 batch_loss: 0.18910661339759827\n",
      "training: 45 batch 494 batch_loss: 0.18719756603240967\n",
      "training: 45 batch 495 batch_loss: 0.1877775490283966\n",
      "training: 45 batch 496 batch_loss: 0.19138571619987488\n",
      "training: 45 batch 497 batch_loss: 0.1920143961906433\n",
      "training: 45 batch 498 batch_loss: 0.1883893609046936\n",
      "training: 45 batch 499 batch_loss: 0.1858455240726471\n",
      "training: 45 batch 500 batch_loss: 0.1879557967185974\n",
      "training: 45 batch 501 batch_loss: 0.1881057620048523\n",
      "training: 45 batch 502 batch_loss: 0.18810021877288818\n",
      "training: 45 batch 503 batch_loss: 0.18641307950019836\n",
      "training: 45 batch 504 batch_loss: 0.18610814213752747\n",
      "training: 45 batch 505 batch_loss: 0.18787038326263428\n",
      "training: 45 batch 506 batch_loss: 0.18511992692947388\n",
      "training: 45 batch 507 batch_loss: 0.1887470781803131\n",
      "training: 45 batch 508 batch_loss: 0.18711328506469727\n",
      "training: 45 batch 509 batch_loss: 0.18388161063194275\n",
      "training: 45 batch 510 batch_loss: 0.1892286241054535\n",
      "training: 45 batch 511 batch_loss: 0.18542057275772095\n",
      "training: 45 batch 512 batch_loss: 0.18890467286109924\n",
      "training: 45 batch 513 batch_loss: 0.1850455403327942\n",
      "training: 45 batch 514 batch_loss: 0.18681737780570984\n",
      "training: 45 batch 515 batch_loss: 0.18940001726150513\n",
      "training: 45 batch 516 batch_loss: 0.1862213909626007\n",
      "training: 45 batch 517 batch_loss: 0.1895456314086914\n",
      "training: 45 batch 518 batch_loss: 0.18570277094841003\n",
      "training: 45 batch 519 batch_loss: 0.19050458073616028\n",
      "training: 45 batch 520 batch_loss: 0.1880481243133545\n",
      "training: 45 batch 521 batch_loss: 0.18667209148406982\n",
      "training: 45 batch 522 batch_loss: 0.18457192182540894\n",
      "training: 45 batch 523 batch_loss: 0.18868324160575867\n",
      "training: 45 batch 524 batch_loss: 0.1892898678779602\n",
      "training: 45 batch 525 batch_loss: 0.18956884741783142\n",
      "training: 45 batch 526 batch_loss: 0.18989834189414978\n",
      "training: 45 batch 527 batch_loss: 0.18672305345535278\n",
      "training: 45 batch 528 batch_loss: 0.18880993127822876\n",
      "training: 45 batch 529 batch_loss: 0.18752479553222656\n",
      "training: 45 batch 530 batch_loss: 0.18732008337974548\n",
      "training: 45 batch 531 batch_loss: 0.1899431347846985\n",
      "training: 45 batch 532 batch_loss: 0.18504726886749268\n",
      "training: 45 batch 533 batch_loss: 0.18622058629989624\n",
      "training: 45 batch 534 batch_loss: 0.1820361316204071\n",
      "training: 45 batch 535 batch_loss: 0.18844866752624512\n",
      "training: 45 batch 536 batch_loss: 0.18842267990112305\n",
      "training: 45 batch 537 batch_loss: 0.1846032738685608\n",
      "training: 45 batch 538 batch_loss: 0.18741625547409058\n",
      "training: 45 batch 539 batch_loss: 0.18906345963478088\n",
      "training: 45 batch 540 batch_loss: 0.18695634603500366\n",
      "training: 45 batch 541 batch_loss: 0.18563571572303772\n",
      "training: 45 batch 542 batch_loss: 0.19111838936805725\n",
      "training: 45 batch 543 batch_loss: 0.18822509050369263\n",
      "training: 45 batch 544 batch_loss: 0.18668189644813538\n",
      "training: 45 batch 545 batch_loss: 0.18573516607284546\n",
      "training: 45 batch 546 batch_loss: 0.18584156036376953\n",
      "training: 45 batch 547 batch_loss: 0.1872277557849884\n",
      "training: 45 batch 548 batch_loss: 0.18709129095077515\n",
      "training: 45 batch 549 batch_loss: 0.1877533495426178\n",
      "training: 45 batch 550 batch_loss: 0.18942058086395264\n",
      "training: 45 batch 551 batch_loss: 0.18802234530448914\n",
      "training: 45 batch 552 batch_loss: 0.18629026412963867\n",
      "training: 45 batch 553 batch_loss: 0.1859472393989563\n",
      "training: 45 batch 554 batch_loss: 0.18996688723564148\n",
      "training: 45 batch 555 batch_loss: 0.18532836437225342\n",
      "training: 45 batch 556 batch_loss: 0.1829257607460022\n",
      "training: 45 batch 557 batch_loss: 0.18798336386680603\n",
      "training: 45 batch 558 batch_loss: 0.18596598505973816\n",
      "training: 45 batch 559 batch_loss: 0.1878543496131897\n",
      "training: 45 batch 560 batch_loss: 0.18701043725013733\n",
      "training: 45 batch 561 batch_loss: 0.1882849931716919\n",
      "training: 45 batch 562 batch_loss: 0.1867557168006897\n",
      "training: 45 batch 563 batch_loss: 0.1872151792049408\n",
      "training: 45 batch 564 batch_loss: 0.19017988443374634\n",
      "training: 45 batch 565 batch_loss: 0.18577492237091064\n",
      "training: 45 batch 566 batch_loss: 0.18544065952301025\n",
      "training: 45 batch 567 batch_loss: 0.1852843165397644\n",
      "training: 45 batch 568 batch_loss: 0.18866723775863647\n",
      "training: 45 batch 569 batch_loss: 0.18892717361450195\n",
      "training: 45 batch 570 batch_loss: 0.1858786642551422\n",
      "training: 45 batch 571 batch_loss: 0.1884770393371582\n",
      "training: 45 batch 572 batch_loss: 0.1860683560371399\n",
      "training: 45 batch 573 batch_loss: 0.1889851689338684\n",
      "training: 45 batch 574 batch_loss: 0.18751582503318787\n",
      "training: 45 batch 575 batch_loss: 0.19022005796432495\n",
      "training: 45 batch 576 batch_loss: 0.18777477741241455\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 45, Hit Ratio:0.031566358975725614 | Precision:0.04657426521183525 | Recall:0.062099197341415 | NDCG:0.06011183874505559\n",
      "*Best Performance* \n",
      "Epoch: 43, Hit Ratio:0.03184951014847115 | Precision:0.04699203774697729 | Recall:0.06261647091061 | MDCG:0.0607415751955159\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 46 batch 0 batch_loss: 0.18441656231880188\n",
      "training: 46 batch 1 batch_loss: 0.18465262651443481\n",
      "training: 46 batch 2 batch_loss: 0.18681678175926208\n",
      "training: 46 batch 3 batch_loss: 0.18622902035713196\n",
      "training: 46 batch 4 batch_loss: 0.18715530633926392\n",
      "training: 46 batch 5 batch_loss: 0.18277138471603394\n",
      "training: 46 batch 6 batch_loss: 0.1850123107433319\n",
      "training: 46 batch 7 batch_loss: 0.18370592594146729\n",
      "training: 46 batch 8 batch_loss: 0.18854913115501404\n",
      "training: 46 batch 9 batch_loss: 0.18047261238098145\n",
      "training: 46 batch 10 batch_loss: 0.18529292941093445\n",
      "training: 46 batch 11 batch_loss: 0.18703553080558777\n",
      "training: 46 batch 12 batch_loss: 0.18491822481155396\n",
      "training: 46 batch 13 batch_loss: 0.1856468915939331\n",
      "training: 46 batch 14 batch_loss: 0.18459942936897278\n",
      "training: 46 batch 15 batch_loss: 0.18381479382514954\n",
      "training: 46 batch 16 batch_loss: 0.1860816776752472\n",
      "training: 46 batch 17 batch_loss: 0.18398672342300415\n",
      "training: 46 batch 18 batch_loss: 0.18850600719451904\n",
      "training: 46 batch 19 batch_loss: 0.18879175186157227\n",
      "training: 46 batch 20 batch_loss: 0.18330049514770508\n",
      "training: 46 batch 21 batch_loss: 0.18224495649337769\n",
      "training: 46 batch 22 batch_loss: 0.1876402497291565\n",
      "training: 46 batch 23 batch_loss: 0.18398168683052063\n",
      "training: 46 batch 24 batch_loss: 0.19187819957733154\n",
      "training: 46 batch 25 batch_loss: 0.18642112612724304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 46 batch 26 batch_loss: 0.18672940135002136\n",
      "training: 46 batch 27 batch_loss: 0.18828755617141724\n",
      "training: 46 batch 28 batch_loss: 0.1850443184375763\n",
      "training: 46 batch 29 batch_loss: 0.18908146023750305\n",
      "training: 46 batch 30 batch_loss: 0.1842019259929657\n",
      "training: 46 batch 31 batch_loss: 0.1845337450504303\n",
      "training: 46 batch 32 batch_loss: 0.18694555759429932\n",
      "training: 46 batch 33 batch_loss: 0.18624535202980042\n",
      "training: 46 batch 34 batch_loss: 0.1852007508277893\n",
      "training: 46 batch 35 batch_loss: 0.18729928135871887\n",
      "training: 46 batch 36 batch_loss: 0.18695470690727234\n",
      "training: 46 batch 37 batch_loss: 0.1869409680366516\n",
      "training: 46 batch 38 batch_loss: 0.18552470207214355\n",
      "training: 46 batch 39 batch_loss: 0.18409156799316406\n",
      "training: 46 batch 40 batch_loss: 0.1882244050502777\n",
      "training: 46 batch 41 batch_loss: 0.1883130669593811\n",
      "training: 46 batch 42 batch_loss: 0.18618232011795044\n",
      "training: 46 batch 43 batch_loss: 0.1848016381263733\n",
      "training: 46 batch 44 batch_loss: 0.1876070499420166\n",
      "training: 46 batch 45 batch_loss: 0.18831205368041992\n",
      "training: 46 batch 46 batch_loss: 0.1825183629989624\n",
      "training: 46 batch 47 batch_loss: 0.1887257695198059\n",
      "training: 46 batch 48 batch_loss: 0.18663758039474487\n",
      "training: 46 batch 49 batch_loss: 0.18737322092056274\n",
      "training: 46 batch 50 batch_loss: 0.18818360567092896\n",
      "training: 46 batch 51 batch_loss: 0.1877911388874054\n",
      "training: 46 batch 52 batch_loss: 0.182148277759552\n",
      "training: 46 batch 53 batch_loss: 0.18456408381462097\n",
      "training: 46 batch 54 batch_loss: 0.1849282681941986\n",
      "training: 46 batch 55 batch_loss: 0.1882162094116211\n",
      "training: 46 batch 56 batch_loss: 0.18471276760101318\n",
      "training: 46 batch 57 batch_loss: 0.18366676568984985\n",
      "training: 46 batch 58 batch_loss: 0.18554463982582092\n",
      "training: 46 batch 59 batch_loss: 0.1879158616065979\n",
      "training: 46 batch 60 batch_loss: 0.1843758225440979\n",
      "training: 46 batch 61 batch_loss: 0.18571141362190247\n",
      "training: 46 batch 62 batch_loss: 0.18490540981292725\n",
      "training: 46 batch 63 batch_loss: 0.18887361884117126\n",
      "training: 46 batch 64 batch_loss: 0.18638238310813904\n",
      "training: 46 batch 65 batch_loss: 0.18652522563934326\n",
      "training: 46 batch 66 batch_loss: 0.19059643149375916\n",
      "training: 46 batch 67 batch_loss: 0.18544018268585205\n",
      "training: 46 batch 68 batch_loss: 0.18737828731536865\n",
      "training: 46 batch 69 batch_loss: 0.18520495295524597\n",
      "training: 46 batch 70 batch_loss: 0.18734225630760193\n",
      "training: 46 batch 71 batch_loss: 0.18567731976509094\n",
      "training: 46 batch 72 batch_loss: 0.18456032872200012\n",
      "training: 46 batch 73 batch_loss: 0.18819209933280945\n",
      "training: 46 batch 74 batch_loss: 0.18288612365722656\n",
      "training: 46 batch 75 batch_loss: 0.186275452375412\n",
      "training: 46 batch 76 batch_loss: 0.18646445870399475\n",
      "training: 46 batch 77 batch_loss: 0.1822221279144287\n",
      "training: 46 batch 78 batch_loss: 0.19086572527885437\n",
      "training: 46 batch 79 batch_loss: 0.1877846121788025\n",
      "training: 46 batch 80 batch_loss: 0.18371957540512085\n",
      "training: 46 batch 81 batch_loss: 0.188789963722229\n",
      "training: 46 batch 82 batch_loss: 0.1903189718723297\n",
      "training: 46 batch 83 batch_loss: 0.18550598621368408\n",
      "training: 46 batch 84 batch_loss: 0.1865064799785614\n",
      "training: 46 batch 85 batch_loss: 0.18414944410324097\n",
      "training: 46 batch 86 batch_loss: 0.1843615472316742\n",
      "training: 46 batch 87 batch_loss: 0.1845887005329132\n",
      "training: 46 batch 88 batch_loss: 0.18255862593650818\n",
      "training: 46 batch 89 batch_loss: 0.1870386302471161\n",
      "training: 46 batch 90 batch_loss: 0.18774518370628357\n",
      "training: 46 batch 91 batch_loss: 0.18334293365478516\n",
      "training: 46 batch 92 batch_loss: 0.18542927503585815\n",
      "training: 46 batch 93 batch_loss: 0.1872677206993103\n",
      "training: 46 batch 94 batch_loss: 0.1875324845314026\n",
      "training: 46 batch 95 batch_loss: 0.1865752935409546\n",
      "training: 46 batch 96 batch_loss: 0.18467244505882263\n",
      "training: 46 batch 97 batch_loss: 0.18632113933563232\n",
      "training: 46 batch 98 batch_loss: 0.1851787567138672\n",
      "training: 46 batch 99 batch_loss: 0.18514800071716309\n",
      "training: 46 batch 100 batch_loss: 0.18750882148742676\n",
      "training: 46 batch 101 batch_loss: 0.185726135969162\n",
      "training: 46 batch 102 batch_loss: 0.18108731508255005\n",
      "training: 46 batch 103 batch_loss: 0.1863185167312622\n",
      "training: 46 batch 104 batch_loss: 0.1854889690876007\n",
      "training: 46 batch 105 batch_loss: 0.19121640920639038\n",
      "training: 46 batch 106 batch_loss: 0.18284541368484497\n",
      "training: 46 batch 107 batch_loss: 0.1854259967803955\n",
      "training: 46 batch 108 batch_loss: 0.18786749243736267\n",
      "training: 46 batch 109 batch_loss: 0.18614080548286438\n",
      "training: 46 batch 110 batch_loss: 0.18524977564811707\n",
      "training: 46 batch 111 batch_loss: 0.1841033399105072\n",
      "training: 46 batch 112 batch_loss: 0.1887129545211792\n",
      "training: 46 batch 113 batch_loss: 0.18810832500457764\n",
      "training: 46 batch 114 batch_loss: 0.1826561987400055\n",
      "training: 46 batch 115 batch_loss: 0.18885409832000732\n",
      "training: 46 batch 116 batch_loss: 0.18797504901885986\n",
      "training: 46 batch 117 batch_loss: 0.18781957030296326\n",
      "training: 46 batch 118 batch_loss: 0.1865265965461731\n",
      "training: 46 batch 119 batch_loss: 0.18886810541152954\n",
      "training: 46 batch 120 batch_loss: 0.18454593420028687\n",
      "training: 46 batch 121 batch_loss: 0.1836453676223755\n",
      "training: 46 batch 122 batch_loss: 0.1833626925945282\n",
      "training: 46 batch 123 batch_loss: 0.18961802124977112\n",
      "training: 46 batch 124 batch_loss: 0.18761885166168213\n",
      "training: 46 batch 125 batch_loss: 0.18540173768997192\n",
      "training: 46 batch 126 batch_loss: 0.18840909004211426\n",
      "training: 46 batch 127 batch_loss: 0.1877802014350891\n",
      "training: 46 batch 128 batch_loss: 0.18296530842781067\n",
      "training: 46 batch 129 batch_loss: 0.18698012828826904\n",
      "training: 46 batch 130 batch_loss: 0.18590468168258667\n",
      "training: 46 batch 131 batch_loss: 0.1830725073814392\n",
      "training: 46 batch 132 batch_loss: 0.18848678469657898\n",
      "training: 46 batch 133 batch_loss: 0.1852109432220459\n",
      "training: 46 batch 134 batch_loss: 0.18428516387939453\n",
      "training: 46 batch 135 batch_loss: 0.1851830780506134\n",
      "training: 46 batch 136 batch_loss: 0.18656456470489502\n",
      "training: 46 batch 137 batch_loss: 0.18684890866279602\n",
      "training: 46 batch 138 batch_loss: 0.18375062942504883\n",
      "training: 46 batch 139 batch_loss: 0.18749678134918213\n",
      "training: 46 batch 140 batch_loss: 0.18891847133636475\n",
      "training: 46 batch 141 batch_loss: 0.1852281093597412\n",
      "training: 46 batch 142 batch_loss: 0.18493640422821045\n",
      "training: 46 batch 143 batch_loss: 0.1857272982597351\n",
      "training: 46 batch 144 batch_loss: 0.1898205280303955\n",
      "training: 46 batch 145 batch_loss: 0.18767613172531128\n",
      "training: 46 batch 146 batch_loss: 0.19068282842636108\n",
      "training: 46 batch 147 batch_loss: 0.18746092915534973\n",
      "training: 46 batch 148 batch_loss: 0.1891060471534729\n",
      "training: 46 batch 149 batch_loss: 0.18814051151275635\n",
      "training: 46 batch 150 batch_loss: 0.18752515316009521\n",
      "training: 46 batch 151 batch_loss: 0.18652698397636414\n",
      "training: 46 batch 152 batch_loss: 0.18504518270492554\n",
      "training: 46 batch 153 batch_loss: 0.18731656670570374\n",
      "training: 46 batch 154 batch_loss: 0.187354177236557\n",
      "training: 46 batch 155 batch_loss: 0.18460658192634583\n",
      "training: 46 batch 156 batch_loss: 0.18703263998031616\n",
      "training: 46 batch 157 batch_loss: 0.18691855669021606\n",
      "training: 46 batch 158 batch_loss: 0.19050905108451843\n",
      "training: 46 batch 159 batch_loss: 0.18812593817710876\n",
      "training: 46 batch 160 batch_loss: 0.18792879581451416\n",
      "training: 46 batch 161 batch_loss: 0.18339496850967407\n",
      "training: 46 batch 162 batch_loss: 0.1875169277191162\n",
      "training: 46 batch 163 batch_loss: 0.19027748703956604\n",
      "training: 46 batch 164 batch_loss: 0.18865400552749634\n",
      "training: 46 batch 165 batch_loss: 0.1874789595603943\n",
      "training: 46 batch 166 batch_loss: 0.18816104531288147\n",
      "training: 46 batch 167 batch_loss: 0.18729376792907715\n",
      "training: 46 batch 168 batch_loss: 0.18832916021347046\n",
      "training: 46 batch 169 batch_loss: 0.18802332878112793\n",
      "training: 46 batch 170 batch_loss: 0.18402189016342163\n",
      "training: 46 batch 171 batch_loss: 0.18773603439331055\n",
      "training: 46 batch 172 batch_loss: 0.18642663955688477\n",
      "training: 46 batch 173 batch_loss: 0.18850606679916382\n",
      "training: 46 batch 174 batch_loss: 0.18738287687301636\n",
      "training: 46 batch 175 batch_loss: 0.1878310739994049\n",
      "training: 46 batch 176 batch_loss: 0.18788856267929077\n",
      "training: 46 batch 177 batch_loss: 0.18711242079734802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 46 batch 178 batch_loss: 0.1868651807308197\n",
      "training: 46 batch 179 batch_loss: 0.1882248818874359\n",
      "training: 46 batch 180 batch_loss: 0.18917310237884521\n",
      "training: 46 batch 181 batch_loss: 0.19004660844802856\n",
      "training: 46 batch 182 batch_loss: 0.19322219491004944\n",
      "training: 46 batch 183 batch_loss: 0.18515706062316895\n",
      "training: 46 batch 184 batch_loss: 0.1887010633945465\n",
      "training: 46 batch 185 batch_loss: 0.18854328989982605\n",
      "training: 46 batch 186 batch_loss: 0.18970555067062378\n",
      "training: 46 batch 187 batch_loss: 0.1826169192790985\n",
      "training: 46 batch 188 batch_loss: 0.18615570664405823\n",
      "training: 46 batch 189 batch_loss: 0.18657538294792175\n",
      "training: 46 batch 190 batch_loss: 0.1884469985961914\n",
      "training: 46 batch 191 batch_loss: 0.19094181060791016\n",
      "training: 46 batch 192 batch_loss: 0.1853194236755371\n",
      "training: 46 batch 193 batch_loss: 0.18667158484458923\n",
      "training: 46 batch 194 batch_loss: 0.18654096126556396\n",
      "training: 46 batch 195 batch_loss: 0.18563979864120483\n",
      "training: 46 batch 196 batch_loss: 0.18656441569328308\n",
      "training: 46 batch 197 batch_loss: 0.19149142503738403\n",
      "training: 46 batch 198 batch_loss: 0.186283677816391\n",
      "training: 46 batch 199 batch_loss: 0.19092920422554016\n",
      "training: 46 batch 200 batch_loss: 0.18698999285697937\n",
      "training: 46 batch 201 batch_loss: 0.18017831444740295\n",
      "training: 46 batch 202 batch_loss: 0.18829813599586487\n",
      "training: 46 batch 203 batch_loss: 0.19163894653320312\n",
      "training: 46 batch 204 batch_loss: 0.18342429399490356\n",
      "training: 46 batch 205 batch_loss: 0.1888708472251892\n",
      "training: 46 batch 206 batch_loss: 0.18447750806808472\n",
      "training: 46 batch 207 batch_loss: 0.18564823269844055\n",
      "training: 46 batch 208 batch_loss: 0.18772819638252258\n",
      "training: 46 batch 209 batch_loss: 0.18869030475616455\n",
      "training: 46 batch 210 batch_loss: 0.18797814846038818\n",
      "training: 46 batch 211 batch_loss: 0.18668010830879211\n",
      "training: 46 batch 212 batch_loss: 0.18457448482513428\n",
      "training: 46 batch 213 batch_loss: 0.18655139207839966\n",
      "training: 46 batch 214 batch_loss: 0.18353834748268127\n",
      "training: 46 batch 215 batch_loss: 0.18743982911109924\n",
      "training: 46 batch 216 batch_loss: 0.18648892641067505\n",
      "training: 46 batch 217 batch_loss: 0.19045013189315796\n",
      "training: 46 batch 218 batch_loss: 0.18851959705352783\n",
      "training: 46 batch 219 batch_loss: 0.19113096594810486\n",
      "training: 46 batch 220 batch_loss: 0.18767455220222473\n",
      "training: 46 batch 221 batch_loss: 0.18850237131118774\n",
      "training: 46 batch 222 batch_loss: 0.1906147599220276\n",
      "training: 46 batch 223 batch_loss: 0.18619120121002197\n",
      "training: 46 batch 224 batch_loss: 0.1843624711036682\n",
      "training: 46 batch 225 batch_loss: 0.18781626224517822\n",
      "training: 46 batch 226 batch_loss: 0.18831861019134521\n",
      "training: 46 batch 227 batch_loss: 0.18711405992507935\n",
      "training: 46 batch 228 batch_loss: 0.18914520740509033\n",
      "training: 46 batch 229 batch_loss: 0.1857602596282959\n",
      "training: 46 batch 230 batch_loss: 0.18972763419151306\n",
      "training: 46 batch 231 batch_loss: 0.1875918209552765\n",
      "training: 46 batch 232 batch_loss: 0.1890164017677307\n",
      "training: 46 batch 233 batch_loss: 0.18443357944488525\n",
      "training: 46 batch 234 batch_loss: 0.1894046664237976\n",
      "training: 46 batch 235 batch_loss: 0.18677383661270142\n",
      "training: 46 batch 236 batch_loss: 0.18773585557937622\n",
      "training: 46 batch 237 batch_loss: 0.18890947103500366\n",
      "training: 46 batch 238 batch_loss: 0.18755057454109192\n",
      "training: 46 batch 239 batch_loss: 0.18723899126052856\n",
      "training: 46 batch 240 batch_loss: 0.18641549348831177\n",
      "training: 46 batch 241 batch_loss: 0.18719130754470825\n",
      "training: 46 batch 242 batch_loss: 0.19240233302116394\n",
      "training: 46 batch 243 batch_loss: 0.18743354082107544\n",
      "training: 46 batch 244 batch_loss: 0.18801096081733704\n",
      "training: 46 batch 245 batch_loss: 0.18458911776542664\n",
      "training: 46 batch 246 batch_loss: 0.1861780285835266\n",
      "training: 46 batch 247 batch_loss: 0.18800774216651917\n",
      "training: 46 batch 248 batch_loss: 0.1865450143814087\n",
      "training: 46 batch 249 batch_loss: 0.18862780928611755\n",
      "training: 46 batch 250 batch_loss: 0.1873132288455963\n",
      "training: 46 batch 251 batch_loss: 0.18919965624809265\n",
      "training: 46 batch 252 batch_loss: 0.18930020928382874\n",
      "training: 46 batch 253 batch_loss: 0.1884661614894867\n",
      "training: 46 batch 254 batch_loss: 0.19009116291999817\n",
      "training: 46 batch 255 batch_loss: 0.18653899431228638\n",
      "training: 46 batch 256 batch_loss: 0.1909169852733612\n",
      "training: 46 batch 257 batch_loss: 0.1876945197582245\n",
      "training: 46 batch 258 batch_loss: 0.18987685441970825\n",
      "training: 46 batch 259 batch_loss: 0.183312326669693\n",
      "training: 46 batch 260 batch_loss: 0.18697547912597656\n",
      "training: 46 batch 261 batch_loss: 0.18349051475524902\n",
      "training: 46 batch 262 batch_loss: 0.18907466530799866\n",
      "training: 46 batch 263 batch_loss: 0.18804621696472168\n",
      "training: 46 batch 264 batch_loss: 0.19209599494934082\n",
      "training: 46 batch 265 batch_loss: 0.1885353922843933\n",
      "training: 46 batch 266 batch_loss: 0.18394088745117188\n",
      "training: 46 batch 267 batch_loss: 0.1855265498161316\n",
      "training: 46 batch 268 batch_loss: 0.18840819597244263\n",
      "training: 46 batch 269 batch_loss: 0.18923592567443848\n",
      "training: 46 batch 270 batch_loss: 0.1896916627883911\n",
      "training: 46 batch 271 batch_loss: 0.18809980154037476\n",
      "training: 46 batch 272 batch_loss: 0.18486452102661133\n",
      "training: 46 batch 273 batch_loss: 0.18867522478103638\n",
      "training: 46 batch 274 batch_loss: 0.18712782859802246\n",
      "training: 46 batch 275 batch_loss: 0.18401244282722473\n",
      "training: 46 batch 276 batch_loss: 0.1849116086959839\n",
      "training: 46 batch 277 batch_loss: 0.18400025367736816\n",
      "training: 46 batch 278 batch_loss: 0.19160214066505432\n",
      "training: 46 batch 279 batch_loss: 0.18838539719581604\n",
      "training: 46 batch 280 batch_loss: 0.1920185089111328\n",
      "training: 46 batch 281 batch_loss: 0.18872404098510742\n",
      "training: 46 batch 282 batch_loss: 0.18990743160247803\n",
      "training: 46 batch 283 batch_loss: 0.18730667233467102\n",
      "training: 46 batch 284 batch_loss: 0.18891307711601257\n",
      "training: 46 batch 285 batch_loss: 0.18700799345970154\n",
      "training: 46 batch 286 batch_loss: 0.1896107792854309\n",
      "training: 46 batch 287 batch_loss: 0.18942585587501526\n",
      "training: 46 batch 288 batch_loss: 0.18525737524032593\n",
      "training: 46 batch 289 batch_loss: 0.1882527768611908\n",
      "training: 46 batch 290 batch_loss: 0.19152012467384338\n",
      "training: 46 batch 291 batch_loss: 0.18946504592895508\n",
      "training: 46 batch 292 batch_loss: 0.18576765060424805\n",
      "training: 46 batch 293 batch_loss: 0.1851319670677185\n",
      "training: 46 batch 294 batch_loss: 0.18675363063812256\n",
      "training: 46 batch 295 batch_loss: 0.18579509854316711\n",
      "training: 46 batch 296 batch_loss: 0.18882030248641968\n",
      "training: 46 batch 297 batch_loss: 0.18712735176086426\n",
      "training: 46 batch 298 batch_loss: 0.19014614820480347\n",
      "training: 46 batch 299 batch_loss: 0.1842675805091858\n",
      "training: 46 batch 300 batch_loss: 0.18679988384246826\n",
      "training: 46 batch 301 batch_loss: 0.1895090937614441\n",
      "training: 46 batch 302 batch_loss: 0.19079875946044922\n",
      "training: 46 batch 303 batch_loss: 0.18500882387161255\n",
      "training: 46 batch 304 batch_loss: 0.18913143873214722\n",
      "training: 46 batch 305 batch_loss: 0.18849298357963562\n",
      "training: 46 batch 306 batch_loss: 0.1900022029876709\n",
      "training: 46 batch 307 batch_loss: 0.18683680891990662\n",
      "training: 46 batch 308 batch_loss: 0.18641024827957153\n",
      "training: 46 batch 309 batch_loss: 0.1895664930343628\n",
      "training: 46 batch 310 batch_loss: 0.1867884397506714\n",
      "training: 46 batch 311 batch_loss: 0.18676525354385376\n",
      "training: 46 batch 312 batch_loss: 0.19304382801055908\n",
      "training: 46 batch 313 batch_loss: 0.18941229581832886\n",
      "training: 46 batch 314 batch_loss: 0.1869024634361267\n",
      "training: 46 batch 315 batch_loss: 0.190898597240448\n",
      "training: 46 batch 316 batch_loss: 0.1850886046886444\n",
      "training: 46 batch 317 batch_loss: 0.18627959489822388\n",
      "training: 46 batch 318 batch_loss: 0.1863829791545868\n",
      "training: 46 batch 319 batch_loss: 0.1886734962463379\n",
      "training: 46 batch 320 batch_loss: 0.187211275100708\n",
      "training: 46 batch 321 batch_loss: 0.18597674369812012\n",
      "training: 46 batch 322 batch_loss: 0.18954873085021973\n",
      "training: 46 batch 323 batch_loss: 0.1902974247932434\n",
      "training: 46 batch 324 batch_loss: 0.18732917308807373\n",
      "training: 46 batch 325 batch_loss: 0.18478763103485107\n",
      "training: 46 batch 326 batch_loss: 0.1839768886566162\n",
      "training: 46 batch 327 batch_loss: 0.18783265352249146\n",
      "training: 46 batch 328 batch_loss: 0.18733501434326172\n",
      "training: 46 batch 329 batch_loss: 0.1903335452079773\n",
      "training: 46 batch 330 batch_loss: 0.18902820348739624\n",
      "training: 46 batch 331 batch_loss: 0.1898173987865448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 46 batch 332 batch_loss: 0.18859952688217163\n",
      "training: 46 batch 333 batch_loss: 0.19015520811080933\n",
      "training: 46 batch 334 batch_loss: 0.18453651666641235\n",
      "training: 46 batch 335 batch_loss: 0.1896226406097412\n",
      "training: 46 batch 336 batch_loss: 0.1894458532333374\n",
      "training: 46 batch 337 batch_loss: 0.18825334310531616\n",
      "training: 46 batch 338 batch_loss: 0.18838223814964294\n",
      "training: 46 batch 339 batch_loss: 0.1881130337715149\n",
      "training: 46 batch 340 batch_loss: 0.18802854418754578\n",
      "training: 46 batch 341 batch_loss: 0.19013690948486328\n",
      "training: 46 batch 342 batch_loss: 0.1877748966217041\n",
      "training: 46 batch 343 batch_loss: 0.1894986629486084\n",
      "training: 46 batch 344 batch_loss: 0.18761005997657776\n",
      "training: 46 batch 345 batch_loss: 0.1900905966758728\n",
      "training: 46 batch 346 batch_loss: 0.18718761205673218\n",
      "training: 46 batch 347 batch_loss: 0.18803101778030396\n",
      "training: 46 batch 348 batch_loss: 0.18620210886001587\n",
      "training: 46 batch 349 batch_loss: 0.18773293495178223\n",
      "training: 46 batch 350 batch_loss: 0.19038963317871094\n",
      "training: 46 batch 351 batch_loss: 0.18713441491127014\n",
      "training: 46 batch 352 batch_loss: 0.18774771690368652\n",
      "training: 46 batch 353 batch_loss: 0.18234488368034363\n",
      "training: 46 batch 354 batch_loss: 0.19043239951133728\n",
      "training: 46 batch 355 batch_loss: 0.18876993656158447\n",
      "training: 46 batch 356 batch_loss: 0.18786543607711792\n",
      "training: 46 batch 357 batch_loss: 0.18793287873268127\n",
      "training: 46 batch 358 batch_loss: 0.18715044856071472\n",
      "training: 46 batch 359 batch_loss: 0.1863504946231842\n",
      "training: 46 batch 360 batch_loss: 0.19081348180770874\n",
      "training: 46 batch 361 batch_loss: 0.18316760659217834\n",
      "training: 46 batch 362 batch_loss: 0.18987822532653809\n",
      "training: 46 batch 363 batch_loss: 0.18786156177520752\n",
      "training: 46 batch 364 batch_loss: 0.18605244159698486\n",
      "training: 46 batch 365 batch_loss: 0.1872817873954773\n",
      "training: 46 batch 366 batch_loss: 0.18668699264526367\n",
      "training: 46 batch 367 batch_loss: 0.18768423795700073\n",
      "training: 46 batch 368 batch_loss: 0.18736866116523743\n",
      "training: 46 batch 369 batch_loss: 0.1892363429069519\n",
      "training: 46 batch 370 batch_loss: 0.18729335069656372\n",
      "training: 46 batch 371 batch_loss: 0.18907424807548523\n",
      "training: 46 batch 372 batch_loss: 0.19359058141708374\n",
      "training: 46 batch 373 batch_loss: 0.18683254718780518\n",
      "training: 46 batch 374 batch_loss: 0.1881663203239441\n",
      "training: 46 batch 375 batch_loss: 0.18643900752067566\n",
      "training: 46 batch 376 batch_loss: 0.18713536858558655\n",
      "training: 46 batch 377 batch_loss: 0.18686619400978088\n",
      "training: 46 batch 378 batch_loss: 0.18820494413375854\n",
      "training: 46 batch 379 batch_loss: 0.18339309096336365\n",
      "training: 46 batch 380 batch_loss: 0.18931937217712402\n",
      "training: 46 batch 381 batch_loss: 0.1874239146709442\n",
      "training: 46 batch 382 batch_loss: 0.19237422943115234\n",
      "training: 46 batch 383 batch_loss: 0.18940001726150513\n",
      "training: 46 batch 384 batch_loss: 0.1885298490524292\n",
      "training: 46 batch 385 batch_loss: 0.19140732288360596\n",
      "training: 46 batch 386 batch_loss: 0.19170498847961426\n",
      "training: 46 batch 387 batch_loss: 0.18859928846359253\n",
      "training: 46 batch 388 batch_loss: 0.1840967833995819\n",
      "training: 46 batch 389 batch_loss: 0.1881168782711029\n",
      "training: 46 batch 390 batch_loss: 0.19108501076698303\n",
      "training: 46 batch 391 batch_loss: 0.1912459135055542\n",
      "training: 46 batch 392 batch_loss: 0.1900462508201599\n",
      "training: 46 batch 393 batch_loss: 0.18881243467330933\n",
      "training: 46 batch 394 batch_loss: 0.18899497389793396\n",
      "training: 46 batch 395 batch_loss: 0.18768310546875\n",
      "training: 46 batch 396 batch_loss: 0.19095274806022644\n",
      "training: 46 batch 397 batch_loss: 0.18742012977600098\n",
      "training: 46 batch 398 batch_loss: 0.1890961229801178\n",
      "training: 46 batch 399 batch_loss: 0.18639269471168518\n",
      "training: 46 batch 400 batch_loss: 0.18659639358520508\n",
      "training: 46 batch 401 batch_loss: 0.19410359859466553\n",
      "training: 46 batch 402 batch_loss: 0.19339585304260254\n",
      "training: 46 batch 403 batch_loss: 0.18948182463645935\n",
      "training: 46 batch 404 batch_loss: 0.19155669212341309\n",
      "training: 46 batch 405 batch_loss: 0.18538117408752441\n",
      "training: 46 batch 406 batch_loss: 0.18418431282043457\n",
      "training: 46 batch 407 batch_loss: 0.1923964023590088\n",
      "training: 46 batch 408 batch_loss: 0.18807286024093628\n",
      "training: 46 batch 409 batch_loss: 0.18672919273376465\n",
      "training: 46 batch 410 batch_loss: 0.18757152557373047\n",
      "training: 46 batch 411 batch_loss: 0.1879664659500122\n",
      "training: 46 batch 412 batch_loss: 0.1872166395187378\n",
      "training: 46 batch 413 batch_loss: 0.18641313910484314\n",
      "training: 46 batch 414 batch_loss: 0.18955522775650024\n",
      "training: 46 batch 415 batch_loss: 0.1882394552230835\n",
      "training: 46 batch 416 batch_loss: 0.19011467695236206\n",
      "training: 46 batch 417 batch_loss: 0.19038954377174377\n",
      "training: 46 batch 418 batch_loss: 0.18787690997123718\n",
      "training: 46 batch 419 batch_loss: 0.19125273823738098\n",
      "training: 46 batch 420 batch_loss: 0.18427780270576477\n",
      "training: 46 batch 421 batch_loss: 0.1895538866519928\n",
      "training: 46 batch 422 batch_loss: 0.18766900897026062\n",
      "training: 46 batch 423 batch_loss: 0.18411779403686523\n",
      "training: 46 batch 424 batch_loss: 0.1875283122062683\n",
      "training: 46 batch 425 batch_loss: 0.1900768280029297\n",
      "training: 46 batch 426 batch_loss: 0.18443724513053894\n",
      "training: 46 batch 427 batch_loss: 0.19340628385543823\n",
      "training: 46 batch 428 batch_loss: 0.18706801533699036\n",
      "training: 46 batch 429 batch_loss: 0.18534687161445618\n",
      "training: 46 batch 430 batch_loss: 0.1874154806137085\n",
      "training: 46 batch 431 batch_loss: 0.18836060166358948\n",
      "training: 46 batch 432 batch_loss: 0.18717336654663086\n",
      "training: 46 batch 433 batch_loss: 0.18826666474342346\n",
      "training: 46 batch 434 batch_loss: 0.18822339177131653\n",
      "training: 46 batch 435 batch_loss: 0.1883041262626648\n",
      "training: 46 batch 436 batch_loss: 0.18729066848754883\n",
      "training: 46 batch 437 batch_loss: 0.1890178620815277\n",
      "training: 46 batch 438 batch_loss: 0.18902158737182617\n",
      "training: 46 batch 439 batch_loss: 0.1861647665500641\n",
      "training: 46 batch 440 batch_loss: 0.18966513872146606\n",
      "training: 46 batch 441 batch_loss: 0.18793854117393494\n",
      "training: 46 batch 442 batch_loss: 0.18947267532348633\n",
      "training: 46 batch 443 batch_loss: 0.1876891851425171\n",
      "training: 46 batch 444 batch_loss: 0.18893295526504517\n",
      "training: 46 batch 445 batch_loss: 0.1857338547706604\n",
      "training: 46 batch 446 batch_loss: 0.1876145899295807\n",
      "training: 46 batch 447 batch_loss: 0.1907377541065216\n",
      "training: 46 batch 448 batch_loss: 0.18999835848808289\n",
      "training: 46 batch 449 batch_loss: 0.18559443950653076\n",
      "training: 46 batch 450 batch_loss: 0.19105306267738342\n",
      "training: 46 batch 451 batch_loss: 0.18986093997955322\n",
      "training: 46 batch 452 batch_loss: 0.18828648328781128\n",
      "training: 46 batch 453 batch_loss: 0.18910735845565796\n",
      "training: 46 batch 454 batch_loss: 0.19132313132286072\n",
      "training: 46 batch 455 batch_loss: 0.18878042697906494\n",
      "training: 46 batch 456 batch_loss: 0.19255614280700684\n",
      "training: 46 batch 457 batch_loss: 0.19032779335975647\n",
      "training: 46 batch 458 batch_loss: 0.18758991360664368\n",
      "training: 46 batch 459 batch_loss: 0.1881604790687561\n",
      "training: 46 batch 460 batch_loss: 0.19024226069450378\n",
      "training: 46 batch 461 batch_loss: 0.1890663206577301\n",
      "training: 46 batch 462 batch_loss: 0.18734923005104065\n",
      "training: 46 batch 463 batch_loss: 0.1887061893939972\n",
      "training: 46 batch 464 batch_loss: 0.19118642807006836\n",
      "training: 46 batch 465 batch_loss: 0.19293400645256042\n",
      "training: 46 batch 466 batch_loss: 0.18660205602645874\n",
      "training: 46 batch 467 batch_loss: 0.18612927198410034\n",
      "training: 46 batch 468 batch_loss: 0.18721646070480347\n",
      "training: 46 batch 469 batch_loss: 0.18927761912345886\n",
      "training: 46 batch 470 batch_loss: 0.18659216165542603\n",
      "training: 46 batch 471 batch_loss: 0.18558120727539062\n",
      "training: 46 batch 472 batch_loss: 0.18884125351905823\n",
      "training: 46 batch 473 batch_loss: 0.1892578899860382\n",
      "training: 46 batch 474 batch_loss: 0.19030475616455078\n",
      "training: 46 batch 475 batch_loss: 0.18629804253578186\n",
      "training: 46 batch 476 batch_loss: 0.18931090831756592\n",
      "training: 46 batch 477 batch_loss: 0.18514761328697205\n",
      "training: 46 batch 478 batch_loss: 0.1866092085838318\n",
      "training: 46 batch 479 batch_loss: 0.18915602564811707\n",
      "training: 46 batch 480 batch_loss: 0.19265484809875488\n",
      "training: 46 batch 481 batch_loss: 0.1855575144290924\n",
      "training: 46 batch 482 batch_loss: 0.18993765115737915\n",
      "training: 46 batch 483 batch_loss: 0.18332970142364502\n",
      "training: 46 batch 484 batch_loss: 0.1888938844203949\n",
      "training: 46 batch 485 batch_loss: 0.19182753562927246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 46 batch 486 batch_loss: 0.19081833958625793\n",
      "training: 46 batch 487 batch_loss: 0.19035661220550537\n",
      "training: 46 batch 488 batch_loss: 0.19062989950180054\n",
      "training: 46 batch 489 batch_loss: 0.1897948980331421\n",
      "training: 46 batch 490 batch_loss: 0.1891273558139801\n",
      "training: 46 batch 491 batch_loss: 0.1881343424320221\n",
      "training: 46 batch 492 batch_loss: 0.18712332844734192\n",
      "training: 46 batch 493 batch_loss: 0.18594369292259216\n",
      "training: 46 batch 494 batch_loss: 0.18624353408813477\n",
      "training: 46 batch 495 batch_loss: 0.1881726086139679\n",
      "training: 46 batch 496 batch_loss: 0.18821585178375244\n",
      "training: 46 batch 497 batch_loss: 0.19106131792068481\n",
      "training: 46 batch 498 batch_loss: 0.1899397373199463\n",
      "training: 46 batch 499 batch_loss: 0.1907818615436554\n",
      "training: 46 batch 500 batch_loss: 0.18921595811843872\n",
      "training: 46 batch 501 batch_loss: 0.18873363733291626\n",
      "training: 46 batch 502 batch_loss: 0.1914633810520172\n",
      "training: 46 batch 503 batch_loss: 0.18643662333488464\n",
      "training: 46 batch 504 batch_loss: 0.18823543190956116\n",
      "training: 46 batch 505 batch_loss: 0.18711647391319275\n",
      "training: 46 batch 506 batch_loss: 0.18804579973220825\n",
      "training: 46 batch 507 batch_loss: 0.18905746936798096\n",
      "training: 46 batch 508 batch_loss: 0.18883571028709412\n",
      "training: 46 batch 509 batch_loss: 0.18397608399391174\n",
      "training: 46 batch 510 batch_loss: 0.19089236855506897\n",
      "training: 46 batch 511 batch_loss: 0.1888786256313324\n",
      "training: 46 batch 512 batch_loss: 0.18873736262321472\n",
      "training: 46 batch 513 batch_loss: 0.18857908248901367\n",
      "training: 46 batch 514 batch_loss: 0.1862671971321106\n",
      "training: 46 batch 515 batch_loss: 0.191117525100708\n",
      "training: 46 batch 516 batch_loss: 0.19172403216362\n",
      "training: 46 batch 517 batch_loss: 0.18636661767959595\n",
      "training: 46 batch 518 batch_loss: 0.1863495111465454\n",
      "training: 46 batch 519 batch_loss: 0.19272848963737488\n",
      "training: 46 batch 520 batch_loss: 0.19259241223335266\n",
      "training: 46 batch 521 batch_loss: 0.18643543124198914\n",
      "training: 46 batch 522 batch_loss: 0.1893083155155182\n",
      "training: 46 batch 523 batch_loss: 0.18864759802818298\n",
      "training: 46 batch 524 batch_loss: 0.19184374809265137\n",
      "training: 46 batch 525 batch_loss: 0.18756970763206482\n",
      "training: 46 batch 526 batch_loss: 0.18929624557495117\n",
      "training: 46 batch 527 batch_loss: 0.18927285075187683\n",
      "training: 46 batch 528 batch_loss: 0.19496697187423706\n",
      "training: 46 batch 529 batch_loss: 0.18721774220466614\n",
      "training: 46 batch 530 batch_loss: 0.18649062514305115\n",
      "training: 46 batch 531 batch_loss: 0.19188883900642395\n",
      "training: 46 batch 532 batch_loss: 0.18735992908477783\n",
      "training: 46 batch 533 batch_loss: 0.19129538536071777\n",
      "training: 46 batch 534 batch_loss: 0.19286280870437622\n",
      "training: 46 batch 535 batch_loss: 0.18717479705810547\n",
      "training: 46 batch 536 batch_loss: 0.18741238117218018\n",
      "training: 46 batch 537 batch_loss: 0.1849902868270874\n",
      "training: 46 batch 538 batch_loss: 0.18892839550971985\n",
      "training: 46 batch 539 batch_loss: 0.1920468807220459\n",
      "training: 46 batch 540 batch_loss: 0.19124048948287964\n",
      "training: 46 batch 541 batch_loss: 0.1871095597743988\n",
      "training: 46 batch 542 batch_loss: 0.1864762306213379\n",
      "training: 46 batch 543 batch_loss: 0.1906827688217163\n",
      "training: 46 batch 544 batch_loss: 0.18798846006393433\n",
      "training: 46 batch 545 batch_loss: 0.19052329659461975\n",
      "training: 46 batch 546 batch_loss: 0.18772587180137634\n",
      "training: 46 batch 547 batch_loss: 0.1886933445930481\n",
      "training: 46 batch 548 batch_loss: 0.19015786051750183\n",
      "training: 46 batch 549 batch_loss: 0.19180640578269958\n",
      "training: 46 batch 550 batch_loss: 0.18923509120941162\n",
      "training: 46 batch 551 batch_loss: 0.1887211799621582\n",
      "training: 46 batch 552 batch_loss: 0.1907353401184082\n",
      "training: 46 batch 553 batch_loss: 0.1859627366065979\n",
      "training: 46 batch 554 batch_loss: 0.19152110815048218\n",
      "training: 46 batch 555 batch_loss: 0.19210657477378845\n",
      "training: 46 batch 556 batch_loss: 0.1866423487663269\n",
      "training: 46 batch 557 batch_loss: 0.18846842646598816\n",
      "training: 46 batch 558 batch_loss: 0.188473641872406\n",
      "training: 46 batch 559 batch_loss: 0.19224613904953003\n",
      "training: 46 batch 560 batch_loss: 0.18726018071174622\n",
      "training: 46 batch 561 batch_loss: 0.1886157989501953\n",
      "training: 46 batch 562 batch_loss: 0.18982622027397156\n",
      "training: 46 batch 563 batch_loss: 0.18905624747276306\n",
      "training: 46 batch 564 batch_loss: 0.18729031085968018\n",
      "training: 46 batch 565 batch_loss: 0.19041478633880615\n",
      "training: 46 batch 566 batch_loss: 0.19265949726104736\n",
      "training: 46 batch 567 batch_loss: 0.19219312071800232\n",
      "training: 46 batch 568 batch_loss: 0.19133207201957703\n",
      "training: 46 batch 569 batch_loss: 0.1883867383003235\n",
      "training: 46 batch 570 batch_loss: 0.18994253873825073\n",
      "training: 46 batch 571 batch_loss: 0.18981778621673584\n",
      "training: 46 batch 572 batch_loss: 0.18969571590423584\n",
      "training: 46 batch 573 batch_loss: 0.18548977375030518\n",
      "training: 46 batch 574 batch_loss: 0.1869518756866455\n",
      "training: 46 batch 575 batch_loss: 0.18943709135055542\n",
      "training: 46 batch 576 batch_loss: 0.18805882334709167\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 46, Hit Ratio:0.03176623039178129 | Precision:0.04686916347193552 | Recall:0.06247812413368426 | NDCG:0.06049014725473591\n",
      "*Best Performance* \n",
      "Epoch: 43, Hit Ratio:0.03184951014847115 | Precision:0.04699203774697729 | Recall:0.06261647091061 | MDCG:0.0607415751955159\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 47 batch 0 batch_loss: 0.18366524577140808\n",
      "training: 47 batch 1 batch_loss: 0.185733824968338\n",
      "training: 47 batch 2 batch_loss: 0.18647700548171997\n",
      "training: 47 batch 3 batch_loss: 0.18575933575630188\n",
      "training: 47 batch 4 batch_loss: 0.18816393613815308\n",
      "training: 47 batch 5 batch_loss: 0.1882554590702057\n",
      "training: 47 batch 6 batch_loss: 0.18403780460357666\n",
      "training: 47 batch 7 batch_loss: 0.18425974249839783\n",
      "training: 47 batch 8 batch_loss: 0.18752866983413696\n",
      "training: 47 batch 9 batch_loss: 0.18719011545181274\n",
      "training: 47 batch 10 batch_loss: 0.18728014826774597\n",
      "training: 47 batch 11 batch_loss: 0.1878833770751953\n",
      "training: 47 batch 12 batch_loss: 0.1870836317539215\n",
      "training: 47 batch 13 batch_loss: 0.19072148203849792\n",
      "training: 47 batch 14 batch_loss: 0.19291159510612488\n",
      "training: 47 batch 15 batch_loss: 0.18833482265472412\n",
      "training: 47 batch 16 batch_loss: 0.18522727489471436\n",
      "training: 47 batch 17 batch_loss: 0.18535566329956055\n",
      "training: 47 batch 18 batch_loss: 0.18501722812652588\n",
      "training: 47 batch 19 batch_loss: 0.18663957715034485\n",
      "training: 47 batch 20 batch_loss: 0.1863013207912445\n",
      "training: 47 batch 21 batch_loss: 0.18395739793777466\n",
      "training: 47 batch 22 batch_loss: 0.18644320964813232\n",
      "training: 47 batch 23 batch_loss: 0.1869787573814392\n",
      "training: 47 batch 24 batch_loss: 0.18806421756744385\n",
      "training: 47 batch 25 batch_loss: 0.19277137517929077\n",
      "training: 47 batch 26 batch_loss: 0.1879781186580658\n",
      "training: 47 batch 27 batch_loss: 0.18824616074562073\n",
      "training: 47 batch 28 batch_loss: 0.18695801496505737\n",
      "training: 47 batch 29 batch_loss: 0.18762710690498352\n",
      "training: 47 batch 30 batch_loss: 0.18728002905845642\n",
      "training: 47 batch 31 batch_loss: 0.18596374988555908\n",
      "training: 47 batch 32 batch_loss: 0.188797265291214\n",
      "training: 47 batch 33 batch_loss: 0.1874856948852539\n",
      "training: 47 batch 34 batch_loss: 0.1877676248550415\n",
      "training: 47 batch 35 batch_loss: 0.18610066175460815\n",
      "training: 47 batch 36 batch_loss: 0.18995258212089539\n",
      "training: 47 batch 37 batch_loss: 0.18898159265518188\n",
      "training: 47 batch 38 batch_loss: 0.18705815076828003\n",
      "training: 47 batch 39 batch_loss: 0.18969488143920898\n",
      "training: 47 batch 40 batch_loss: 0.1893060803413391\n",
      "training: 47 batch 41 batch_loss: 0.185549795627594\n",
      "training: 47 batch 42 batch_loss: 0.19148334860801697\n",
      "training: 47 batch 43 batch_loss: 0.191543847322464\n",
      "training: 47 batch 44 batch_loss: 0.18909454345703125\n",
      "training: 47 batch 45 batch_loss: 0.1843821108341217\n",
      "training: 47 batch 46 batch_loss: 0.1868172287940979\n",
      "training: 47 batch 47 batch_loss: 0.18752574920654297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 47 batch 48 batch_loss: 0.1900063157081604\n",
      "training: 47 batch 49 batch_loss: 0.18608376383781433\n",
      "training: 47 batch 50 batch_loss: 0.1842554211616516\n",
      "training: 47 batch 51 batch_loss: 0.1908252239227295\n",
      "training: 47 batch 52 batch_loss: 0.1862783432006836\n",
      "training: 47 batch 53 batch_loss: 0.18867012858390808\n",
      "training: 47 batch 54 batch_loss: 0.18682724237442017\n",
      "training: 47 batch 55 batch_loss: 0.18907895684242249\n",
      "training: 47 batch 56 batch_loss: 0.18826359510421753\n",
      "training: 47 batch 57 batch_loss: 0.1860552728176117\n",
      "training: 47 batch 58 batch_loss: 0.18799853324890137\n",
      "training: 47 batch 59 batch_loss: 0.1865728497505188\n",
      "training: 47 batch 60 batch_loss: 0.18936508893966675\n",
      "training: 47 batch 61 batch_loss: 0.18746140599250793\n",
      "training: 47 batch 62 batch_loss: 0.1867239773273468\n",
      "training: 47 batch 63 batch_loss: 0.18834072351455688\n",
      "training: 47 batch 64 batch_loss: 0.18671277165412903\n",
      "training: 47 batch 65 batch_loss: 0.18796730041503906\n",
      "training: 47 batch 66 batch_loss: 0.18781417608261108\n",
      "training: 47 batch 67 batch_loss: 0.18743294477462769\n",
      "training: 47 batch 68 batch_loss: 0.19027644395828247\n",
      "training: 47 batch 69 batch_loss: 0.1903509795665741\n",
      "training: 47 batch 70 batch_loss: 0.18381109833717346\n",
      "training: 47 batch 71 batch_loss: 0.18744805455207825\n",
      "training: 47 batch 72 batch_loss: 0.1905984878540039\n",
      "training: 47 batch 73 batch_loss: 0.18993008136749268\n",
      "training: 47 batch 74 batch_loss: 0.1881537139415741\n",
      "training: 47 batch 75 batch_loss: 0.19219109416007996\n",
      "training: 47 batch 76 batch_loss: 0.1860385239124298\n",
      "training: 47 batch 77 batch_loss: 0.1853625476360321\n",
      "training: 47 batch 78 batch_loss: 0.18623051047325134\n",
      "training: 47 batch 79 batch_loss: 0.1855149269104004\n",
      "training: 47 batch 80 batch_loss: 0.19106709957122803\n",
      "training: 47 batch 81 batch_loss: 0.19184789061546326\n",
      "training: 47 batch 82 batch_loss: 0.18633103370666504\n",
      "training: 47 batch 83 batch_loss: 0.18650585412979126\n",
      "training: 47 batch 84 batch_loss: 0.18368381261825562\n",
      "training: 47 batch 85 batch_loss: 0.18967658281326294\n",
      "training: 47 batch 86 batch_loss: 0.18418726325035095\n",
      "training: 47 batch 87 batch_loss: 0.18705695867538452\n",
      "training: 47 batch 88 batch_loss: 0.1871793270111084\n",
      "training: 47 batch 89 batch_loss: 0.18986842036247253\n",
      "training: 47 batch 90 batch_loss: 0.18666231632232666\n",
      "training: 47 batch 91 batch_loss: 0.18715021014213562\n",
      "training: 47 batch 92 batch_loss: 0.1858418583869934\n",
      "training: 47 batch 93 batch_loss: 0.18781331181526184\n",
      "training: 47 batch 94 batch_loss: 0.18921950459480286\n",
      "training: 47 batch 95 batch_loss: 0.19115200638771057\n",
      "training: 47 batch 96 batch_loss: 0.18870744109153748\n",
      "training: 47 batch 97 batch_loss: 0.1885349452495575\n",
      "training: 47 batch 98 batch_loss: 0.18822786211967468\n",
      "training: 47 batch 99 batch_loss: 0.18984773755073547\n",
      "training: 47 batch 100 batch_loss: 0.18465641140937805\n",
      "training: 47 batch 101 batch_loss: 0.1871037483215332\n",
      "training: 47 batch 102 batch_loss: 0.18978539109230042\n",
      "training: 47 batch 103 batch_loss: 0.18668639659881592\n",
      "training: 47 batch 104 batch_loss: 0.1948307454586029\n",
      "training: 47 batch 105 batch_loss: 0.1920945942401886\n",
      "training: 47 batch 106 batch_loss: 0.1880674660205841\n",
      "training: 47 batch 107 batch_loss: 0.1865772008895874\n",
      "training: 47 batch 108 batch_loss: 0.18881183862686157\n",
      "training: 47 batch 109 batch_loss: 0.18773427605628967\n",
      "training: 47 batch 110 batch_loss: 0.1880958378314972\n",
      "training: 47 batch 111 batch_loss: 0.1851661503314972\n",
      "training: 47 batch 112 batch_loss: 0.18930953741073608\n",
      "training: 47 batch 113 batch_loss: 0.1889396607875824\n",
      "training: 47 batch 114 batch_loss: 0.1873094141483307\n",
      "training: 47 batch 115 batch_loss: 0.1862177848815918\n",
      "training: 47 batch 116 batch_loss: 0.1893766224384308\n",
      "training: 47 batch 117 batch_loss: 0.1859385073184967\n",
      "training: 47 batch 118 batch_loss: 0.19023969769477844\n",
      "training: 47 batch 119 batch_loss: 0.187375009059906\n",
      "training: 47 batch 120 batch_loss: 0.18645232915878296\n",
      "training: 47 batch 121 batch_loss: 0.1849282681941986\n",
      "training: 47 batch 122 batch_loss: 0.18745258450508118\n",
      "training: 47 batch 123 batch_loss: 0.18691498041152954\n",
      "training: 47 batch 124 batch_loss: 0.18677538633346558\n",
      "training: 47 batch 125 batch_loss: 0.1867021918296814\n",
      "training: 47 batch 126 batch_loss: 0.1867561936378479\n",
      "training: 47 batch 127 batch_loss: 0.19207170605659485\n",
      "training: 47 batch 128 batch_loss: 0.18720924854278564\n",
      "training: 47 batch 129 batch_loss: 0.1875089406967163\n",
      "training: 47 batch 130 batch_loss: 0.18784308433532715\n",
      "training: 47 batch 131 batch_loss: 0.18947935104370117\n",
      "training: 47 batch 132 batch_loss: 0.18683379888534546\n",
      "training: 47 batch 133 batch_loss: 0.18552610278129578\n",
      "training: 47 batch 134 batch_loss: 0.18976891040802002\n",
      "training: 47 batch 135 batch_loss: 0.188128262758255\n",
      "training: 47 batch 136 batch_loss: 0.18676498532295227\n",
      "training: 47 batch 137 batch_loss: 0.18994209170341492\n",
      "training: 47 batch 138 batch_loss: 0.18602001667022705\n",
      "training: 47 batch 139 batch_loss: 0.1870778203010559\n",
      "training: 47 batch 140 batch_loss: 0.19144636392593384\n",
      "training: 47 batch 141 batch_loss: 0.18618535995483398\n",
      "training: 47 batch 142 batch_loss: 0.18932566046714783\n",
      "training: 47 batch 143 batch_loss: 0.1902136206626892\n",
      "training: 47 batch 144 batch_loss: 0.19001701474189758\n",
      "training: 47 batch 145 batch_loss: 0.18830060958862305\n",
      "training: 47 batch 146 batch_loss: 0.18570971488952637\n",
      "training: 47 batch 147 batch_loss: 0.1891365945339203\n",
      "training: 47 batch 148 batch_loss: 0.19187197089195251\n",
      "training: 47 batch 149 batch_loss: 0.18709373474121094\n",
      "training: 47 batch 150 batch_loss: 0.18579509854316711\n",
      "training: 47 batch 151 batch_loss: 0.18942701816558838\n",
      "training: 47 batch 152 batch_loss: 0.19212377071380615\n",
      "training: 47 batch 153 batch_loss: 0.19032350182533264\n",
      "training: 47 batch 154 batch_loss: 0.18887028098106384\n",
      "training: 47 batch 155 batch_loss: 0.19299504160881042\n",
      "training: 47 batch 156 batch_loss: 0.1890375018119812\n",
      "training: 47 batch 157 batch_loss: 0.18542581796646118\n",
      "training: 47 batch 158 batch_loss: 0.18341553211212158\n",
      "training: 47 batch 159 batch_loss: 0.19009095430374146\n",
      "training: 47 batch 160 batch_loss: 0.1878819763660431\n",
      "training: 47 batch 161 batch_loss: 0.19051384925842285\n",
      "training: 47 batch 162 batch_loss: 0.19057011604309082\n",
      "training: 47 batch 163 batch_loss: 0.18946826457977295\n",
      "training: 47 batch 164 batch_loss: 0.18905863165855408\n",
      "training: 47 batch 165 batch_loss: 0.18544316291809082\n",
      "training: 47 batch 166 batch_loss: 0.18667110800743103\n",
      "training: 47 batch 167 batch_loss: 0.19023120403289795\n",
      "training: 47 batch 168 batch_loss: 0.19265994429588318\n",
      "training: 47 batch 169 batch_loss: 0.18749010562896729\n",
      "training: 47 batch 170 batch_loss: 0.187531977891922\n",
      "training: 47 batch 171 batch_loss: 0.18892303109169006\n",
      "training: 47 batch 172 batch_loss: 0.18630245327949524\n",
      "training: 47 batch 173 batch_loss: 0.1880323886871338\n",
      "training: 47 batch 174 batch_loss: 0.1892586052417755\n",
      "training: 47 batch 175 batch_loss: 0.18795889616012573\n",
      "training: 47 batch 176 batch_loss: 0.1876320242881775\n",
      "training: 47 batch 177 batch_loss: 0.187225341796875\n",
      "training: 47 batch 178 batch_loss: 0.19104599952697754\n",
      "training: 47 batch 179 batch_loss: 0.18956565856933594\n",
      "training: 47 batch 180 batch_loss: 0.18880212306976318\n",
      "training: 47 batch 181 batch_loss: 0.18986985087394714\n",
      "training: 47 batch 182 batch_loss: 0.19116440415382385\n",
      "training: 47 batch 183 batch_loss: 0.18470248579978943\n",
      "training: 47 batch 184 batch_loss: 0.18590524792671204\n",
      "training: 47 batch 185 batch_loss: 0.18518540263175964\n",
      "training: 47 batch 186 batch_loss: 0.1889660656452179\n",
      "training: 47 batch 187 batch_loss: 0.18953591585159302\n",
      "training: 47 batch 188 batch_loss: 0.18937677145004272\n",
      "training: 47 batch 189 batch_loss: 0.1896822452545166\n",
      "training: 47 batch 190 batch_loss: 0.18695759773254395\n",
      "training: 47 batch 191 batch_loss: 0.19175812602043152\n",
      "training: 47 batch 192 batch_loss: 0.18676188588142395\n",
      "training: 47 batch 193 batch_loss: 0.19406720995903015\n",
      "training: 47 batch 194 batch_loss: 0.18927353620529175\n",
      "training: 47 batch 195 batch_loss: 0.18730944395065308\n",
      "training: 47 batch 196 batch_loss: 0.18809479475021362\n",
      "training: 47 batch 197 batch_loss: 0.18565094470977783\n",
      "training: 47 batch 198 batch_loss: 0.1872929334640503\n",
      "training: 47 batch 199 batch_loss: 0.1879924237728119\n",
      "training: 47 batch 200 batch_loss: 0.1899707317352295\n",
      "training: 47 batch 201 batch_loss: 0.18928325176239014\n",
      "training: 47 batch 202 batch_loss: 0.18875709176063538\n",
      "training: 47 batch 203 batch_loss: 0.19131594896316528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 47 batch 204 batch_loss: 0.1878003180027008\n",
      "training: 47 batch 205 batch_loss: 0.18678155541419983\n",
      "training: 47 batch 206 batch_loss: 0.18691283464431763\n",
      "training: 47 batch 207 batch_loss: 0.1861836016178131\n",
      "training: 47 batch 208 batch_loss: 0.18684405088424683\n",
      "training: 47 batch 209 batch_loss: 0.19435566663742065\n",
      "training: 47 batch 210 batch_loss: 0.19196593761444092\n",
      "training: 47 batch 211 batch_loss: 0.1912398636341095\n",
      "training: 47 batch 212 batch_loss: 0.18562474846839905\n",
      "training: 47 batch 213 batch_loss: 0.18964529037475586\n",
      "training: 47 batch 214 batch_loss: 0.18551889061927795\n",
      "training: 47 batch 215 batch_loss: 0.18445512652397156\n",
      "training: 47 batch 216 batch_loss: 0.19061315059661865\n",
      "training: 47 batch 217 batch_loss: 0.18728163838386536\n",
      "training: 47 batch 218 batch_loss: 0.19288519024848938\n",
      "training: 47 batch 219 batch_loss: 0.18401449918746948\n",
      "training: 47 batch 220 batch_loss: 0.19078335165977478\n",
      "training: 47 batch 221 batch_loss: 0.19034752249717712\n",
      "training: 47 batch 222 batch_loss: 0.18862318992614746\n",
      "training: 47 batch 223 batch_loss: 0.19041591882705688\n",
      "training: 47 batch 224 batch_loss: 0.1845518946647644\n",
      "training: 47 batch 225 batch_loss: 0.18726694583892822\n",
      "training: 47 batch 226 batch_loss: 0.1876750886440277\n",
      "training: 47 batch 227 batch_loss: 0.19286203384399414\n",
      "training: 47 batch 228 batch_loss: 0.18703797459602356\n",
      "training: 47 batch 229 batch_loss: 0.19045916199684143\n",
      "training: 47 batch 230 batch_loss: 0.18531915545463562\n",
      "training: 47 batch 231 batch_loss: 0.18605047464370728\n",
      "training: 47 batch 232 batch_loss: 0.1913205087184906\n",
      "training: 47 batch 233 batch_loss: 0.19023537635803223\n",
      "training: 47 batch 234 batch_loss: 0.18998169898986816\n",
      "training: 47 batch 235 batch_loss: 0.18752259016036987\n",
      "training: 47 batch 236 batch_loss: 0.18767240643501282\n",
      "training: 47 batch 237 batch_loss: 0.18533474206924438\n",
      "training: 47 batch 238 batch_loss: 0.19184201955795288\n",
      "training: 47 batch 239 batch_loss: 0.1916693150997162\n",
      "training: 47 batch 240 batch_loss: 0.19243577122688293\n",
      "training: 47 batch 241 batch_loss: 0.18881407380104065\n",
      "training: 47 batch 242 batch_loss: 0.19092243909835815\n",
      "training: 47 batch 243 batch_loss: 0.19126605987548828\n",
      "training: 47 batch 244 batch_loss: 0.1871122419834137\n",
      "training: 47 batch 245 batch_loss: 0.19224503636360168\n",
      "training: 47 batch 246 batch_loss: 0.18819653987884521\n",
      "training: 47 batch 247 batch_loss: 0.18575650453567505\n",
      "training: 47 batch 248 batch_loss: 0.18761947751045227\n",
      "training: 47 batch 249 batch_loss: 0.1895100474357605\n",
      "training: 47 batch 250 batch_loss: 0.18489372730255127\n",
      "training: 47 batch 251 batch_loss: 0.19284841418266296\n",
      "training: 47 batch 252 batch_loss: 0.18787544965744019\n",
      "training: 47 batch 253 batch_loss: 0.18810993432998657\n",
      "training: 47 batch 254 batch_loss: 0.1898137331008911\n",
      "training: 47 batch 255 batch_loss: 0.18710950016975403\n",
      "training: 47 batch 256 batch_loss: 0.18911588191986084\n",
      "training: 47 batch 257 batch_loss: 0.1876145899295807\n",
      "training: 47 batch 258 batch_loss: 0.1890234351158142\n",
      "training: 47 batch 259 batch_loss: 0.18706676363945007\n",
      "training: 47 batch 260 batch_loss: 0.19171062111854553\n",
      "training: 47 batch 261 batch_loss: 0.1886405050754547\n",
      "training: 47 batch 262 batch_loss: 0.18904456496238708\n",
      "training: 47 batch 263 batch_loss: 0.18681347370147705\n",
      "training: 47 batch 264 batch_loss: 0.1884756088256836\n",
      "training: 47 batch 265 batch_loss: 0.19071945548057556\n",
      "training: 47 batch 266 batch_loss: 0.18555176258087158\n",
      "training: 47 batch 267 batch_loss: 0.1912904679775238\n",
      "training: 47 batch 268 batch_loss: 0.18665754795074463\n",
      "training: 47 batch 269 batch_loss: 0.1911965012550354\n",
      "training: 47 batch 270 batch_loss: 0.18609866499900818\n",
      "training: 47 batch 271 batch_loss: 0.1887873411178589\n",
      "training: 47 batch 272 batch_loss: 0.18968713283538818\n",
      "training: 47 batch 273 batch_loss: 0.19307589530944824\n",
      "training: 47 batch 274 batch_loss: 0.18999344110488892\n",
      "training: 47 batch 275 batch_loss: 0.18711882829666138\n",
      "training: 47 batch 276 batch_loss: 0.18759658932685852\n",
      "training: 47 batch 277 batch_loss: 0.18936312198638916\n",
      "training: 47 batch 278 batch_loss: 0.1952936351299286\n",
      "training: 47 batch 279 batch_loss: 0.18734604120254517\n",
      "training: 47 batch 280 batch_loss: 0.18670374155044556\n",
      "training: 47 batch 281 batch_loss: 0.18780040740966797\n",
      "training: 47 batch 282 batch_loss: 0.1873353123664856\n",
      "training: 47 batch 283 batch_loss: 0.19087767601013184\n",
      "training: 47 batch 284 batch_loss: 0.19296690821647644\n",
      "training: 47 batch 285 batch_loss: 0.18759897351264954\n",
      "training: 47 batch 286 batch_loss: 0.18536615371704102\n",
      "training: 47 batch 287 batch_loss: 0.18877673149108887\n",
      "training: 47 batch 288 batch_loss: 0.1910153031349182\n",
      "training: 47 batch 289 batch_loss: 0.18832853436470032\n",
      "training: 47 batch 290 batch_loss: 0.1878066062927246\n",
      "training: 47 batch 291 batch_loss: 0.1894056797027588\n",
      "training: 47 batch 292 batch_loss: 0.1871216893196106\n",
      "training: 47 batch 293 batch_loss: 0.19112426042556763\n",
      "training: 47 batch 294 batch_loss: 0.19269782304763794\n",
      "training: 47 batch 295 batch_loss: 0.19199565052986145\n",
      "training: 47 batch 296 batch_loss: 0.19197487831115723\n",
      "training: 47 batch 297 batch_loss: 0.18890011310577393\n",
      "training: 47 batch 298 batch_loss: 0.190052330493927\n",
      "training: 47 batch 299 batch_loss: 0.18773192167282104\n",
      "training: 47 batch 300 batch_loss: 0.18907293677330017\n",
      "training: 47 batch 301 batch_loss: 0.19207528233528137\n",
      "training: 47 batch 302 batch_loss: 0.1880081295967102\n",
      "training: 47 batch 303 batch_loss: 0.1891002058982849\n",
      "training: 47 batch 304 batch_loss: 0.18892133235931396\n",
      "training: 47 batch 305 batch_loss: 0.19193723797798157\n",
      "training: 47 batch 306 batch_loss: 0.18528535962104797\n",
      "training: 47 batch 307 batch_loss: 0.19091251492500305\n",
      "training: 47 batch 308 batch_loss: 0.19060862064361572\n",
      "training: 47 batch 309 batch_loss: 0.19126296043395996\n",
      "training: 47 batch 310 batch_loss: 0.1869528889656067\n",
      "training: 47 batch 311 batch_loss: 0.18842655420303345\n",
      "training: 47 batch 312 batch_loss: 0.1905500888824463\n",
      "training: 47 batch 313 batch_loss: 0.18452894687652588\n",
      "training: 47 batch 314 batch_loss: 0.1864524781703949\n",
      "training: 47 batch 315 batch_loss: 0.190593421459198\n",
      "training: 47 batch 316 batch_loss: 0.1891389787197113\n",
      "training: 47 batch 317 batch_loss: 0.1914522349834442\n",
      "training: 47 batch 318 batch_loss: 0.18641823530197144\n",
      "training: 47 batch 319 batch_loss: 0.1895695924758911\n",
      "training: 47 batch 320 batch_loss: 0.1897968053817749\n",
      "training: 47 batch 321 batch_loss: 0.19373074173927307\n",
      "training: 47 batch 322 batch_loss: 0.19187098741531372\n",
      "training: 47 batch 323 batch_loss: 0.1909133493900299\n",
      "training: 47 batch 324 batch_loss: 0.1902252733707428\n",
      "training: 47 batch 325 batch_loss: 0.19273900985717773\n",
      "training: 47 batch 326 batch_loss: 0.19026005268096924\n",
      "training: 47 batch 327 batch_loss: 0.18980780243873596\n",
      "training: 47 batch 328 batch_loss: 0.18970009684562683\n",
      "training: 47 batch 329 batch_loss: 0.18996673822402954\n",
      "training: 47 batch 330 batch_loss: 0.1866455078125\n",
      "training: 47 batch 331 batch_loss: 0.18861213326454163\n",
      "training: 47 batch 332 batch_loss: 0.1893499195575714\n",
      "training: 47 batch 333 batch_loss: 0.1910620629787445\n",
      "training: 47 batch 334 batch_loss: 0.18956905603408813\n",
      "training: 47 batch 335 batch_loss: 0.1876593828201294\n",
      "training: 47 batch 336 batch_loss: 0.19020181894302368\n",
      "training: 47 batch 337 batch_loss: 0.19129860401153564\n",
      "training: 47 batch 338 batch_loss: 0.186477929353714\n",
      "training: 47 batch 339 batch_loss: 0.18618664145469666\n",
      "training: 47 batch 340 batch_loss: 0.18773189187049866\n",
      "training: 47 batch 341 batch_loss: 0.19114950299263\n",
      "training: 47 batch 342 batch_loss: 0.18999814987182617\n",
      "training: 47 batch 343 batch_loss: 0.18960905075073242\n",
      "training: 47 batch 344 batch_loss: 0.18802189826965332\n",
      "training: 47 batch 345 batch_loss: 0.1895124614238739\n",
      "training: 47 batch 346 batch_loss: 0.18901100754737854\n",
      "training: 47 batch 347 batch_loss: 0.18965014815330505\n",
      "training: 47 batch 348 batch_loss: 0.18585503101348877\n",
      "training: 47 batch 349 batch_loss: 0.18909475207328796\n",
      "training: 47 batch 350 batch_loss: 0.18996793031692505\n",
      "training: 47 batch 351 batch_loss: 0.18864497542381287\n",
      "training: 47 batch 352 batch_loss: 0.19091901183128357\n",
      "training: 47 batch 353 batch_loss: 0.18800988793373108\n",
      "training: 47 batch 354 batch_loss: 0.1926846206188202\n",
      "training: 47 batch 355 batch_loss: 0.19416767358779907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 47 batch 356 batch_loss: 0.18790686130523682\n",
      "training: 47 batch 357 batch_loss: 0.18780744075775146\n",
      "training: 47 batch 358 batch_loss: 0.1914462447166443\n",
      "training: 47 batch 359 batch_loss: 0.19061583280563354\n",
      "training: 47 batch 360 batch_loss: 0.1875084638595581\n",
      "training: 47 batch 361 batch_loss: 0.1898018717765808\n",
      "training: 47 batch 362 batch_loss: 0.19250330328941345\n",
      "training: 47 batch 363 batch_loss: 0.18612295389175415\n",
      "training: 47 batch 364 batch_loss: 0.1876726746559143\n",
      "training: 47 batch 365 batch_loss: 0.18781229853630066\n",
      "training: 47 batch 366 batch_loss: 0.18875929713249207\n",
      "training: 47 batch 367 batch_loss: 0.1897231936454773\n",
      "training: 47 batch 368 batch_loss: 0.18789470195770264\n",
      "training: 47 batch 369 batch_loss: 0.18638551235198975\n",
      "training: 47 batch 370 batch_loss: 0.19240319728851318\n",
      "training: 47 batch 371 batch_loss: 0.19007405638694763\n",
      "training: 47 batch 372 batch_loss: 0.18949660658836365\n",
      "training: 47 batch 373 batch_loss: 0.18891406059265137\n",
      "training: 47 batch 374 batch_loss: 0.19154292345046997\n",
      "training: 47 batch 375 batch_loss: 0.18916568160057068\n",
      "training: 47 batch 376 batch_loss: 0.18428641557693481\n",
      "training: 47 batch 377 batch_loss: 0.19384419918060303\n",
      "training: 47 batch 378 batch_loss: 0.18939003348350525\n",
      "training: 47 batch 379 batch_loss: 0.18992871046066284\n",
      "training: 47 batch 380 batch_loss: 0.19112852215766907\n",
      "training: 47 batch 381 batch_loss: 0.19546270370483398\n",
      "training: 47 batch 382 batch_loss: 0.1898953914642334\n",
      "training: 47 batch 383 batch_loss: 0.19248583912849426\n",
      "training: 47 batch 384 batch_loss: 0.19295650720596313\n",
      "training: 47 batch 385 batch_loss: 0.1919262409210205\n",
      "training: 47 batch 386 batch_loss: 0.19203481078147888\n",
      "training: 47 batch 387 batch_loss: 0.1909043788909912\n",
      "training: 47 batch 388 batch_loss: 0.18881535530090332\n",
      "training: 47 batch 389 batch_loss: 0.1860930621623993\n",
      "training: 47 batch 390 batch_loss: 0.1893918216228485\n",
      "training: 47 batch 391 batch_loss: 0.1873902678489685\n",
      "training: 47 batch 392 batch_loss: 0.18795517086982727\n",
      "training: 47 batch 393 batch_loss: 0.19278380274772644\n",
      "training: 47 batch 394 batch_loss: 0.19029775261878967\n",
      "training: 47 batch 395 batch_loss: 0.19328680634498596\n",
      "training: 47 batch 396 batch_loss: 0.18953785300254822\n",
      "training: 47 batch 397 batch_loss: 0.1880151629447937\n",
      "training: 47 batch 398 batch_loss: 0.19241881370544434\n",
      "training: 47 batch 399 batch_loss: 0.1902374029159546\n",
      "training: 47 batch 400 batch_loss: 0.1860373616218567\n",
      "training: 47 batch 401 batch_loss: 0.1907491385936737\n",
      "training: 47 batch 402 batch_loss: 0.19141808152198792\n",
      "training: 47 batch 403 batch_loss: 0.18593227863311768\n",
      "training: 47 batch 404 batch_loss: 0.18559539318084717\n",
      "training: 47 batch 405 batch_loss: 0.18692979216575623\n",
      "training: 47 batch 406 batch_loss: 0.1902732253074646\n",
      "training: 47 batch 407 batch_loss: 0.18826237320899963\n",
      "training: 47 batch 408 batch_loss: 0.1898287534713745\n",
      "training: 47 batch 409 batch_loss: 0.189509779214859\n",
      "training: 47 batch 410 batch_loss: 0.19276201725006104\n",
      "training: 47 batch 411 batch_loss: 0.19173812866210938\n",
      "training: 47 batch 412 batch_loss: 0.1887749433517456\n",
      "training: 47 batch 413 batch_loss: 0.18751588463783264\n",
      "training: 47 batch 414 batch_loss: 0.19210520386695862\n",
      "training: 47 batch 415 batch_loss: 0.18880590796470642\n",
      "training: 47 batch 416 batch_loss: 0.19163286685943604\n",
      "training: 47 batch 417 batch_loss: 0.19135445356369019\n",
      "training: 47 batch 418 batch_loss: 0.18887212872505188\n",
      "training: 47 batch 419 batch_loss: 0.18863576650619507\n",
      "training: 47 batch 420 batch_loss: 0.19052326679229736\n",
      "training: 47 batch 421 batch_loss: 0.18845811486244202\n",
      "training: 47 batch 422 batch_loss: 0.18914419412612915\n",
      "training: 47 batch 423 batch_loss: 0.1884205937385559\n",
      "training: 47 batch 424 batch_loss: 0.19058820605278015\n",
      "training: 47 batch 425 batch_loss: 0.18744325637817383\n",
      "training: 47 batch 426 batch_loss: 0.19179421663284302\n",
      "training: 47 batch 427 batch_loss: 0.1870104968547821\n",
      "training: 47 batch 428 batch_loss: 0.1889553964138031\n",
      "training: 47 batch 429 batch_loss: 0.1889418363571167\n",
      "training: 47 batch 430 batch_loss: 0.1904064118862152\n",
      "training: 47 batch 431 batch_loss: 0.189165860414505\n",
      "training: 47 batch 432 batch_loss: 0.19106286764144897\n",
      "training: 47 batch 433 batch_loss: 0.19204124808311462\n",
      "training: 47 batch 434 batch_loss: 0.19070759415626526\n",
      "training: 47 batch 435 batch_loss: 0.18841218948364258\n",
      "training: 47 batch 436 batch_loss: 0.19114217162132263\n",
      "training: 47 batch 437 batch_loss: 0.1878550946712494\n",
      "training: 47 batch 438 batch_loss: 0.1899777054786682\n",
      "training: 47 batch 439 batch_loss: 0.1892637014389038\n",
      "training: 47 batch 440 batch_loss: 0.1881292462348938\n",
      "training: 47 batch 441 batch_loss: 0.1860518455505371\n",
      "training: 47 batch 442 batch_loss: 0.19452428817749023\n",
      "training: 47 batch 443 batch_loss: 0.19095450639724731\n",
      "training: 47 batch 444 batch_loss: 0.1917349100112915\n",
      "training: 47 batch 445 batch_loss: 0.18630248308181763\n",
      "training: 47 batch 446 batch_loss: 0.18928909301757812\n",
      "training: 47 batch 447 batch_loss: 0.1887136697769165\n",
      "training: 47 batch 448 batch_loss: 0.19409611821174622\n",
      "training: 47 batch 449 batch_loss: 0.18884509801864624\n",
      "training: 47 batch 450 batch_loss: 0.19022712111473083\n",
      "training: 47 batch 451 batch_loss: 0.19634705781936646\n",
      "training: 47 batch 452 batch_loss: 0.18837952613830566\n",
      "training: 47 batch 453 batch_loss: 0.190654456615448\n",
      "training: 47 batch 454 batch_loss: 0.18797963857650757\n",
      "training: 47 batch 455 batch_loss: 0.19370347261428833\n",
      "training: 47 batch 456 batch_loss: 0.19077682495117188\n",
      "training: 47 batch 457 batch_loss: 0.1886129379272461\n",
      "training: 47 batch 458 batch_loss: 0.19114333391189575\n",
      "training: 47 batch 459 batch_loss: 0.19241440296173096\n",
      "training: 47 batch 460 batch_loss: 0.19301319122314453\n",
      "training: 47 batch 461 batch_loss: 0.19015058875083923\n",
      "training: 47 batch 462 batch_loss: 0.18962198495864868\n",
      "training: 47 batch 463 batch_loss: 0.18407484889030457\n",
      "training: 47 batch 464 batch_loss: 0.1909095048904419\n",
      "training: 47 batch 465 batch_loss: 0.19049608707427979\n",
      "training: 47 batch 466 batch_loss: 0.18734273314476013\n",
      "training: 47 batch 467 batch_loss: 0.1917145848274231\n",
      "training: 47 batch 468 batch_loss: 0.1901128888130188\n",
      "training: 47 batch 469 batch_loss: 0.1889764368534088\n",
      "training: 47 batch 470 batch_loss: 0.19076475501060486\n",
      "training: 47 batch 471 batch_loss: 0.19314530491828918\n",
      "training: 47 batch 472 batch_loss: 0.1881682276725769\n",
      "training: 47 batch 473 batch_loss: 0.18660953640937805\n",
      "training: 47 batch 474 batch_loss: 0.18833643198013306\n",
      "training: 47 batch 475 batch_loss: 0.19105666875839233\n",
      "training: 47 batch 476 batch_loss: 0.1897432804107666\n",
      "training: 47 batch 477 batch_loss: 0.19247332215309143\n",
      "training: 47 batch 478 batch_loss: 0.18996313214302063\n",
      "training: 47 batch 479 batch_loss: 0.1910908818244934\n",
      "training: 47 batch 480 batch_loss: 0.19193106889724731\n",
      "training: 47 batch 481 batch_loss: 0.18961668014526367\n",
      "training: 47 batch 482 batch_loss: 0.19031858444213867\n",
      "training: 47 batch 483 batch_loss: 0.19082897901535034\n",
      "training: 47 batch 484 batch_loss: 0.1912868320941925\n",
      "training: 47 batch 485 batch_loss: 0.18952882289886475\n",
      "training: 47 batch 486 batch_loss: 0.19014739990234375\n",
      "training: 47 batch 487 batch_loss: 0.18829146027565002\n",
      "training: 47 batch 488 batch_loss: 0.19196584820747375\n",
      "training: 47 batch 489 batch_loss: 0.1901281774044037\n",
      "training: 47 batch 490 batch_loss: 0.18739014863967896\n",
      "training: 47 batch 491 batch_loss: 0.19176730513572693\n",
      "training: 47 batch 492 batch_loss: 0.19088971614837646\n",
      "training: 47 batch 493 batch_loss: 0.19107720255851746\n",
      "training: 47 batch 494 batch_loss: 0.19532856345176697\n",
      "training: 47 batch 495 batch_loss: 0.186862051486969\n",
      "training: 47 batch 496 batch_loss: 0.19034600257873535\n",
      "training: 47 batch 497 batch_loss: 0.19014444947242737\n",
      "training: 47 batch 498 batch_loss: 0.19264522194862366\n",
      "training: 47 batch 499 batch_loss: 0.1894087791442871\n",
      "training: 47 batch 500 batch_loss: 0.1887434422969818\n",
      "training: 47 batch 501 batch_loss: 0.18896639347076416\n",
      "training: 47 batch 502 batch_loss: 0.1924538016319275\n",
      "training: 47 batch 503 batch_loss: 0.1882495880126953\n",
      "training: 47 batch 504 batch_loss: 0.19455605745315552\n",
      "training: 47 batch 505 batch_loss: 0.186007559299469\n",
      "training: 47 batch 506 batch_loss: 0.18806874752044678\n",
      "training: 47 batch 507 batch_loss: 0.19338712096214294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 47 batch 508 batch_loss: 0.1905738115310669\n",
      "training: 47 batch 509 batch_loss: 0.18868526816368103\n",
      "training: 47 batch 510 batch_loss: 0.19062787294387817\n",
      "training: 47 batch 511 batch_loss: 0.19015240669250488\n",
      "training: 47 batch 512 batch_loss: 0.18936610221862793\n",
      "training: 47 batch 513 batch_loss: 0.19220766425132751\n",
      "training: 47 batch 514 batch_loss: 0.19193121790885925\n",
      "training: 47 batch 515 batch_loss: 0.19088327884674072\n",
      "training: 47 batch 516 batch_loss: 0.18882927298545837\n",
      "training: 47 batch 517 batch_loss: 0.1914278268814087\n",
      "training: 47 batch 518 batch_loss: 0.19139942526817322\n",
      "training: 47 batch 519 batch_loss: 0.18845140933990479\n",
      "training: 47 batch 520 batch_loss: 0.18926504254341125\n",
      "training: 47 batch 521 batch_loss: 0.19258087873458862\n",
      "training: 47 batch 522 batch_loss: 0.18885475397109985\n",
      "training: 47 batch 523 batch_loss: 0.19118550419807434\n",
      "training: 47 batch 524 batch_loss: 0.1885778307914734\n",
      "training: 47 batch 525 batch_loss: 0.18681323528289795\n",
      "training: 47 batch 526 batch_loss: 0.19352298974990845\n",
      "training: 47 batch 527 batch_loss: 0.18626582622528076\n",
      "training: 47 batch 528 batch_loss: 0.19113260507583618\n",
      "training: 47 batch 529 batch_loss: 0.19085532426834106\n",
      "training: 47 batch 530 batch_loss: 0.1852259635925293\n",
      "training: 47 batch 531 batch_loss: 0.1895139217376709\n",
      "training: 47 batch 532 batch_loss: 0.1889173984527588\n",
      "training: 47 batch 533 batch_loss: 0.18866828083992004\n",
      "training: 47 batch 534 batch_loss: 0.19168883562088013\n",
      "training: 47 batch 535 batch_loss: 0.18717104196548462\n",
      "training: 47 batch 536 batch_loss: 0.19242244958877563\n",
      "training: 47 batch 537 batch_loss: 0.18777957558631897\n",
      "training: 47 batch 538 batch_loss: 0.1903703510761261\n",
      "training: 47 batch 539 batch_loss: 0.19190454483032227\n",
      "training: 47 batch 540 batch_loss: 0.19053608179092407\n",
      "training: 47 batch 541 batch_loss: 0.18739888072013855\n",
      "training: 47 batch 542 batch_loss: 0.1885908544063568\n",
      "training: 47 batch 543 batch_loss: 0.18960607051849365\n",
      "training: 47 batch 544 batch_loss: 0.18561536073684692\n",
      "training: 47 batch 545 batch_loss: 0.18727844953536987\n",
      "training: 47 batch 546 batch_loss: 0.1882675588130951\n",
      "training: 47 batch 547 batch_loss: 0.19183838367462158\n",
      "training: 47 batch 548 batch_loss: 0.1904158592224121\n",
      "training: 47 batch 549 batch_loss: 0.18858814239501953\n",
      "training: 47 batch 550 batch_loss: 0.1873185634613037\n",
      "training: 47 batch 551 batch_loss: 0.19352596998214722\n",
      "training: 47 batch 552 batch_loss: 0.19113382697105408\n",
      "training: 47 batch 553 batch_loss: 0.19113188982009888\n",
      "training: 47 batch 554 batch_loss: 0.18824642896652222\n",
      "training: 47 batch 555 batch_loss: 0.19095689058303833\n",
      "training: 47 batch 556 batch_loss: 0.19506478309631348\n",
      "training: 47 batch 557 batch_loss: 0.1879500448703766\n",
      "training: 47 batch 558 batch_loss: 0.18894600868225098\n",
      "training: 47 batch 559 batch_loss: 0.18457260727882385\n",
      "training: 47 batch 560 batch_loss: 0.18394172191619873\n",
      "training: 47 batch 561 batch_loss: 0.18902698159217834\n",
      "training: 47 batch 562 batch_loss: 0.19066813588142395\n",
      "training: 47 batch 563 batch_loss: 0.19449082016944885\n",
      "training: 47 batch 564 batch_loss: 0.1879609227180481\n",
      "training: 47 batch 565 batch_loss: 0.18789222836494446\n",
      "training: 47 batch 566 batch_loss: 0.19013914465904236\n",
      "training: 47 batch 567 batch_loss: 0.19152775406837463\n",
      "training: 47 batch 568 batch_loss: 0.18762880563735962\n",
      "training: 47 batch 569 batch_loss: 0.1903030276298523\n",
      "training: 47 batch 570 batch_loss: 0.1903553009033203\n",
      "training: 47 batch 571 batch_loss: 0.19181957840919495\n",
      "training: 47 batch 572 batch_loss: 0.1891792118549347\n",
      "training: 47 batch 573 batch_loss: 0.18704509735107422\n",
      "training: 47 batch 574 batch_loss: 0.18592679500579834\n",
      "training: 47 batch 575 batch_loss: 0.1863013207912445\n",
      "training: 47 batch 576 batch_loss: 0.19533470273017883\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 47, Hit Ratio:0.03210934298934352 | Precision:0.04737540548510764 | Recall:0.06309003839007817 | NDCG:0.0611395723667628\n",
      "*Best Performance* \n",
      "Epoch: 47, Hit Ratio:0.03210934298934352 | Precision:0.04737540548510764 | Recall:0.06309003839007817 | MDCG:0.0611395723667628\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 48 batch 0 batch_loss: 0.18867957592010498\n",
      "training: 48 batch 1 batch_loss: 0.19061940908432007\n",
      "training: 48 batch 2 batch_loss: 0.18745151162147522\n",
      "training: 48 batch 3 batch_loss: 0.18745124340057373\n",
      "training: 48 batch 4 batch_loss: 0.1866077482700348\n",
      "training: 48 batch 5 batch_loss: 0.19079327583312988\n",
      "training: 48 batch 6 batch_loss: 0.19407624006271362\n",
      "training: 48 batch 7 batch_loss: 0.18953266739845276\n",
      "training: 48 batch 8 batch_loss: 0.1883062720298767\n",
      "training: 48 batch 9 batch_loss: 0.18776822090148926\n",
      "training: 48 batch 10 batch_loss: 0.18945744633674622\n",
      "training: 48 batch 11 batch_loss: 0.19173285365104675\n",
      "training: 48 batch 12 batch_loss: 0.18832018971443176\n",
      "training: 48 batch 13 batch_loss: 0.18645066022872925\n",
      "training: 48 batch 14 batch_loss: 0.18467563390731812\n",
      "training: 48 batch 15 batch_loss: 0.1894260048866272\n",
      "training: 48 batch 16 batch_loss: 0.18886026740074158\n",
      "training: 48 batch 17 batch_loss: 0.18708857893943787\n",
      "training: 48 batch 18 batch_loss: 0.18857792019844055\n",
      "training: 48 batch 19 batch_loss: 0.1886090636253357\n",
      "training: 48 batch 20 batch_loss: 0.18888205289840698\n",
      "training: 48 batch 21 batch_loss: 0.18644201755523682\n",
      "training: 48 batch 22 batch_loss: 0.18701386451721191\n",
      "training: 48 batch 23 batch_loss: 0.19271782040596008\n",
      "training: 48 batch 24 batch_loss: 0.1906549036502838\n",
      "training: 48 batch 25 batch_loss: 0.18493971228599548\n",
      "training: 48 batch 26 batch_loss: 0.18665918707847595\n",
      "training: 48 batch 27 batch_loss: 0.18857964873313904\n",
      "training: 48 batch 28 batch_loss: 0.19220948219299316\n",
      "training: 48 batch 29 batch_loss: 0.19125968217849731\n",
      "training: 48 batch 30 batch_loss: 0.1870625913143158\n",
      "training: 48 batch 31 batch_loss: 0.18890148401260376\n",
      "training: 48 batch 32 batch_loss: 0.18990525603294373\n",
      "training: 48 batch 33 batch_loss: 0.18978017568588257\n",
      "training: 48 batch 34 batch_loss: 0.18616408109664917\n",
      "training: 48 batch 35 batch_loss: 0.18951871991157532\n",
      "training: 48 batch 36 batch_loss: 0.188652902841568\n",
      "training: 48 batch 37 batch_loss: 0.18723514676094055\n",
      "training: 48 batch 38 batch_loss: 0.18452033400535583\n",
      "training: 48 batch 39 batch_loss: 0.1845405101776123\n",
      "training: 48 batch 40 batch_loss: 0.1878555715084076\n",
      "training: 48 batch 41 batch_loss: 0.18863683938980103\n",
      "training: 48 batch 42 batch_loss: 0.18931472301483154\n",
      "training: 48 batch 43 batch_loss: 0.18853861093521118\n",
      "training: 48 batch 44 batch_loss: 0.190954327583313\n",
      "training: 48 batch 45 batch_loss: 0.18766170740127563\n",
      "training: 48 batch 46 batch_loss: 0.19169703125953674\n",
      "training: 48 batch 47 batch_loss: 0.18815773725509644\n",
      "training: 48 batch 48 batch_loss: 0.1882959008216858\n",
      "training: 48 batch 49 batch_loss: 0.18524032831192017\n",
      "training: 48 batch 50 batch_loss: 0.18911036849021912\n",
      "training: 48 batch 51 batch_loss: 0.18968382477760315\n",
      "training: 48 batch 52 batch_loss: 0.19450163841247559\n",
      "training: 48 batch 53 batch_loss: 0.1911265254020691\n",
      "training: 48 batch 54 batch_loss: 0.18900823593139648\n",
      "training: 48 batch 55 batch_loss: 0.18967798352241516\n",
      "training: 48 batch 56 batch_loss: 0.1915249526500702\n",
      "training: 48 batch 57 batch_loss: 0.19037944078445435\n",
      "training: 48 batch 58 batch_loss: 0.1860089898109436\n",
      "training: 48 batch 59 batch_loss: 0.19081860780715942\n",
      "training: 48 batch 60 batch_loss: 0.1865851879119873\n",
      "training: 48 batch 61 batch_loss: 0.18674737215042114\n",
      "training: 48 batch 62 batch_loss: 0.18956926465034485\n",
      "training: 48 batch 63 batch_loss: 0.19027000665664673\n",
      "training: 48 batch 64 batch_loss: 0.18940985202789307\n",
      "training: 48 batch 65 batch_loss: 0.19038286805152893\n",
      "training: 48 batch 66 batch_loss: 0.19296890497207642\n",
      "training: 48 batch 67 batch_loss: 0.19039377570152283\n",
      "training: 48 batch 68 batch_loss: 0.18639692664146423\n",
      "training: 48 batch 69 batch_loss: 0.195607990026474\n",
      "training: 48 batch 70 batch_loss: 0.1899941861629486\n",
      "training: 48 batch 71 batch_loss: 0.1901731789112091\n",
      "training: 48 batch 72 batch_loss: 0.19049525260925293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 48 batch 73 batch_loss: 0.19146007299423218\n",
      "training: 48 batch 74 batch_loss: 0.1916303038597107\n",
      "training: 48 batch 75 batch_loss: 0.18829426169395447\n",
      "training: 48 batch 76 batch_loss: 0.19108325242996216\n",
      "training: 48 batch 77 batch_loss: 0.19136059284210205\n",
      "training: 48 batch 78 batch_loss: 0.1924232840538025\n",
      "training: 48 batch 79 batch_loss: 0.18949395418167114\n",
      "training: 48 batch 80 batch_loss: 0.1906300187110901\n",
      "training: 48 batch 81 batch_loss: 0.19088202714920044\n",
      "training: 48 batch 82 batch_loss: 0.1913619339466095\n",
      "training: 48 batch 83 batch_loss: 0.18375632166862488\n",
      "training: 48 batch 84 batch_loss: 0.18815350532531738\n",
      "training: 48 batch 85 batch_loss: 0.1889311671257019\n",
      "training: 48 batch 86 batch_loss: 0.18774741888046265\n",
      "training: 48 batch 87 batch_loss: 0.19120275974273682\n",
      "training: 48 batch 88 batch_loss: 0.18743154406547546\n",
      "training: 48 batch 89 batch_loss: 0.191489577293396\n",
      "training: 48 batch 90 batch_loss: 0.18789350986480713\n",
      "training: 48 batch 91 batch_loss: 0.19380807876586914\n",
      "training: 48 batch 92 batch_loss: 0.1886800229549408\n",
      "training: 48 batch 93 batch_loss: 0.19043976068496704\n",
      "training: 48 batch 94 batch_loss: 0.19171562790870667\n",
      "training: 48 batch 95 batch_loss: 0.1921156346797943\n",
      "training: 48 batch 96 batch_loss: 0.1910497546195984\n",
      "training: 48 batch 97 batch_loss: 0.1861063838005066\n",
      "training: 48 batch 98 batch_loss: 0.18624821305274963\n",
      "training: 48 batch 99 batch_loss: 0.1898643672466278\n",
      "training: 48 batch 100 batch_loss: 0.18931159377098083\n",
      "training: 48 batch 101 batch_loss: 0.18829423189163208\n",
      "training: 48 batch 102 batch_loss: 0.19088387489318848\n",
      "training: 48 batch 103 batch_loss: 0.19302889704704285\n",
      "training: 48 batch 104 batch_loss: 0.18841278553009033\n",
      "training: 48 batch 105 batch_loss: 0.18907758593559265\n",
      "training: 48 batch 106 batch_loss: 0.1916578710079193\n",
      "training: 48 batch 107 batch_loss: 0.18880385160446167\n",
      "training: 48 batch 108 batch_loss: 0.18742722272872925\n",
      "training: 48 batch 109 batch_loss: 0.18302148580551147\n",
      "training: 48 batch 110 batch_loss: 0.19029831886291504\n",
      "training: 48 batch 111 batch_loss: 0.1930106282234192\n",
      "training: 48 batch 112 batch_loss: 0.18920403718948364\n",
      "training: 48 batch 113 batch_loss: 0.18800222873687744\n",
      "training: 48 batch 114 batch_loss: 0.190993070602417\n",
      "training: 48 batch 115 batch_loss: 0.1840960681438446\n",
      "training: 48 batch 116 batch_loss: 0.18790248036384583\n",
      "training: 48 batch 117 batch_loss: 0.189204603433609\n",
      "training: 48 batch 118 batch_loss: 0.18829143047332764\n",
      "training: 48 batch 119 batch_loss: 0.1859269142150879\n",
      "training: 48 batch 120 batch_loss: 0.187543123960495\n",
      "training: 48 batch 121 batch_loss: 0.1873975396156311\n",
      "training: 48 batch 122 batch_loss: 0.19068026542663574\n",
      "training: 48 batch 123 batch_loss: 0.18534645438194275\n",
      "training: 48 batch 124 batch_loss: 0.18910852074623108\n",
      "training: 48 batch 125 batch_loss: 0.19220691919326782\n",
      "training: 48 batch 126 batch_loss: 0.18914464116096497\n",
      "training: 48 batch 127 batch_loss: 0.18790742754936218\n",
      "training: 48 batch 128 batch_loss: 0.18798944354057312\n",
      "training: 48 batch 129 batch_loss: 0.1872713267803192\n",
      "training: 48 batch 130 batch_loss: 0.18885689973831177\n",
      "training: 48 batch 131 batch_loss: 0.18651962280273438\n",
      "training: 48 batch 132 batch_loss: 0.18654155731201172\n",
      "training: 48 batch 133 batch_loss: 0.1886851191520691\n",
      "training: 48 batch 134 batch_loss: 0.18822795152664185\n",
      "training: 48 batch 135 batch_loss: 0.1895422339439392\n",
      "training: 48 batch 136 batch_loss: 0.18740934133529663\n",
      "training: 48 batch 137 batch_loss: 0.1872255504131317\n",
      "training: 48 batch 138 batch_loss: 0.18910646438598633\n",
      "training: 48 batch 139 batch_loss: 0.18548470735549927\n",
      "training: 48 batch 140 batch_loss: 0.19060412049293518\n",
      "training: 48 batch 141 batch_loss: 0.1897181272506714\n",
      "training: 48 batch 142 batch_loss: 0.18996891379356384\n",
      "training: 48 batch 143 batch_loss: 0.18887171149253845\n",
      "training: 48 batch 144 batch_loss: 0.18614089488983154\n",
      "training: 48 batch 145 batch_loss: 0.1904979646205902\n",
      "training: 48 batch 146 batch_loss: 0.18875637650489807\n",
      "training: 48 batch 147 batch_loss: 0.1895158290863037\n",
      "training: 48 batch 148 batch_loss: 0.18615177273750305\n",
      "training: 48 batch 149 batch_loss: 0.1945931315422058\n",
      "training: 48 batch 150 batch_loss: 0.1930127739906311\n",
      "training: 48 batch 151 batch_loss: 0.18919578194618225\n",
      "training: 48 batch 152 batch_loss: 0.1854444146156311\n",
      "training: 48 batch 153 batch_loss: 0.19048082828521729\n",
      "training: 48 batch 154 batch_loss: 0.18982496857643127\n",
      "training: 48 batch 155 batch_loss: 0.1906408667564392\n",
      "training: 48 batch 156 batch_loss: 0.18980708718299866\n",
      "training: 48 batch 157 batch_loss: 0.18680402636528015\n",
      "training: 48 batch 158 batch_loss: 0.19223150610923767\n",
      "training: 48 batch 159 batch_loss: 0.19069576263427734\n",
      "training: 48 batch 160 batch_loss: 0.1914973258972168\n",
      "training: 48 batch 161 batch_loss: 0.19325745105743408\n",
      "training: 48 batch 162 batch_loss: 0.19352084398269653\n",
      "training: 48 batch 163 batch_loss: 0.18539148569107056\n",
      "training: 48 batch 164 batch_loss: 0.18837404251098633\n",
      "training: 48 batch 165 batch_loss: 0.1889076828956604\n",
      "training: 48 batch 166 batch_loss: 0.19151932001113892\n",
      "training: 48 batch 167 batch_loss: 0.1892075538635254\n",
      "training: 48 batch 168 batch_loss: 0.18974906206130981\n",
      "training: 48 batch 169 batch_loss: 0.19110116362571716\n",
      "training: 48 batch 170 batch_loss: 0.1914413571357727\n",
      "training: 48 batch 171 batch_loss: 0.19106435775756836\n",
      "training: 48 batch 172 batch_loss: 0.19123217463493347\n",
      "training: 48 batch 173 batch_loss: 0.19019246101379395\n",
      "training: 48 batch 174 batch_loss: 0.18673056364059448\n",
      "training: 48 batch 175 batch_loss: 0.19125604629516602\n",
      "training: 48 batch 176 batch_loss: 0.1875995695590973\n",
      "training: 48 batch 177 batch_loss: 0.189886212348938\n",
      "training: 48 batch 178 batch_loss: 0.1899031698703766\n",
      "training: 48 batch 179 batch_loss: 0.1899033486843109\n",
      "training: 48 batch 180 batch_loss: 0.19152116775512695\n",
      "training: 48 batch 181 batch_loss: 0.19055044651031494\n",
      "training: 48 batch 182 batch_loss: 0.19437956809997559\n",
      "training: 48 batch 183 batch_loss: 0.18931007385253906\n",
      "training: 48 batch 184 batch_loss: 0.19300535321235657\n",
      "training: 48 batch 185 batch_loss: 0.18823015689849854\n",
      "training: 48 batch 186 batch_loss: 0.191206157207489\n",
      "training: 48 batch 187 batch_loss: 0.189846009016037\n",
      "training: 48 batch 188 batch_loss: 0.1848306655883789\n",
      "training: 48 batch 189 batch_loss: 0.1915597915649414\n",
      "training: 48 batch 190 batch_loss: 0.19021880626678467\n",
      "training: 48 batch 191 batch_loss: 0.18821477890014648\n",
      "training: 48 batch 192 batch_loss: 0.18989568948745728\n",
      "training: 48 batch 193 batch_loss: 0.18868014216423035\n",
      "training: 48 batch 194 batch_loss: 0.18865656852722168\n",
      "training: 48 batch 195 batch_loss: 0.18924972414970398\n",
      "training: 48 batch 196 batch_loss: 0.19573399424552917\n",
      "training: 48 batch 197 batch_loss: 0.1903373897075653\n",
      "training: 48 batch 198 batch_loss: 0.18651527166366577\n",
      "training: 48 batch 199 batch_loss: 0.18545743823051453\n",
      "training: 48 batch 200 batch_loss: 0.18536216020584106\n",
      "training: 48 batch 201 batch_loss: 0.18696147203445435\n",
      "training: 48 batch 202 batch_loss: 0.1943289339542389\n",
      "training: 48 batch 203 batch_loss: 0.19170436263084412\n",
      "training: 48 batch 204 batch_loss: 0.19399410486221313\n",
      "training: 48 batch 205 batch_loss: 0.18979638814926147\n",
      "training: 48 batch 206 batch_loss: 0.18966981768608093\n",
      "training: 48 batch 207 batch_loss: 0.19098567962646484\n",
      "training: 48 batch 208 batch_loss: 0.19253221154212952\n",
      "training: 48 batch 209 batch_loss: 0.1938958764076233\n",
      "training: 48 batch 210 batch_loss: 0.18840304017066956\n",
      "training: 48 batch 211 batch_loss: 0.1912045180797577\n",
      "training: 48 batch 212 batch_loss: 0.1880556344985962\n",
      "training: 48 batch 213 batch_loss: 0.1868859827518463\n",
      "training: 48 batch 214 batch_loss: 0.18765366077423096\n",
      "training: 48 batch 215 batch_loss: 0.18944522738456726\n",
      "training: 48 batch 216 batch_loss: 0.19191434979438782\n",
      "training: 48 batch 217 batch_loss: 0.19071868062019348\n",
      "training: 48 batch 218 batch_loss: 0.18801754713058472\n",
      "training: 48 batch 219 batch_loss: 0.18986788392066956\n",
      "training: 48 batch 220 batch_loss: 0.1931837499141693\n",
      "training: 48 batch 221 batch_loss: 0.19131454825401306\n",
      "training: 48 batch 222 batch_loss: 0.18940436840057373\n",
      "training: 48 batch 223 batch_loss: 0.18723255395889282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 48 batch 224 batch_loss: 0.18855121731758118\n",
      "training: 48 batch 225 batch_loss: 0.18860593438148499\n",
      "training: 48 batch 226 batch_loss: 0.19047144055366516\n",
      "training: 48 batch 227 batch_loss: 0.19156453013420105\n",
      "training: 48 batch 228 batch_loss: 0.18858882784843445\n",
      "training: 48 batch 229 batch_loss: 0.19179871678352356\n",
      "training: 48 batch 230 batch_loss: 0.1873130202293396\n",
      "training: 48 batch 231 batch_loss: 0.19387933611869812\n",
      "training: 48 batch 232 batch_loss: 0.18808802962303162\n",
      "training: 48 batch 233 batch_loss: 0.19188374280929565\n",
      "training: 48 batch 234 batch_loss: 0.1917075216770172\n",
      "training: 48 batch 235 batch_loss: 0.19036173820495605\n",
      "training: 48 batch 236 batch_loss: 0.18943318724632263\n",
      "training: 48 batch 237 batch_loss: 0.19020068645477295\n",
      "training: 48 batch 238 batch_loss: 0.1896105408668518\n",
      "training: 48 batch 239 batch_loss: 0.19262373447418213\n",
      "training: 48 batch 240 batch_loss: 0.1898130178451538\n",
      "training: 48 batch 241 batch_loss: 0.18721914291381836\n",
      "training: 48 batch 242 batch_loss: 0.19036069512367249\n",
      "training: 48 batch 243 batch_loss: 0.18764832615852356\n",
      "training: 48 batch 244 batch_loss: 0.18949899077415466\n",
      "training: 48 batch 245 batch_loss: 0.18739813566207886\n",
      "training: 48 batch 246 batch_loss: 0.1901979148387909\n",
      "training: 48 batch 247 batch_loss: 0.19173967838287354\n",
      "training: 48 batch 248 batch_loss: 0.18887928128242493\n",
      "training: 48 batch 249 batch_loss: 0.18861505389213562\n",
      "training: 48 batch 250 batch_loss: 0.1910751461982727\n",
      "training: 48 batch 251 batch_loss: 0.18709659576416016\n",
      "training: 48 batch 252 batch_loss: 0.18925032019615173\n",
      "training: 48 batch 253 batch_loss: 0.19304582476615906\n",
      "training: 48 batch 254 batch_loss: 0.18950355052947998\n",
      "training: 48 batch 255 batch_loss: 0.19189122319221497\n",
      "training: 48 batch 256 batch_loss: 0.18651044368743896\n",
      "training: 48 batch 257 batch_loss: 0.18916049599647522\n",
      "training: 48 batch 258 batch_loss: 0.19011971354484558\n",
      "training: 48 batch 259 batch_loss: 0.19033050537109375\n",
      "training: 48 batch 260 batch_loss: 0.1903684139251709\n",
      "training: 48 batch 261 batch_loss: 0.18846797943115234\n",
      "training: 48 batch 262 batch_loss: 0.1895618438720703\n",
      "training: 48 batch 263 batch_loss: 0.18958619236946106\n",
      "training: 48 batch 264 batch_loss: 0.18773838877677917\n",
      "training: 48 batch 265 batch_loss: 0.18516376614570618\n",
      "training: 48 batch 266 batch_loss: 0.19211140275001526\n",
      "training: 48 batch 267 batch_loss: 0.19217082858085632\n",
      "training: 48 batch 268 batch_loss: 0.19069820642471313\n",
      "training: 48 batch 269 batch_loss: 0.1893283724784851\n",
      "training: 48 batch 270 batch_loss: 0.18905529379844666\n",
      "training: 48 batch 271 batch_loss: 0.18856137990951538\n",
      "training: 48 batch 272 batch_loss: 0.1899007260799408\n",
      "training: 48 batch 273 batch_loss: 0.19462057948112488\n",
      "training: 48 batch 274 batch_loss: 0.18989220261573792\n",
      "training: 48 batch 275 batch_loss: 0.19328483939170837\n",
      "training: 48 batch 276 batch_loss: 0.1900215446949005\n",
      "training: 48 batch 277 batch_loss: 0.18863022327423096\n",
      "training: 48 batch 278 batch_loss: 0.189243346452713\n",
      "training: 48 batch 279 batch_loss: 0.19027626514434814\n",
      "training: 48 batch 280 batch_loss: 0.18956568837165833\n",
      "training: 48 batch 281 batch_loss: 0.19289639592170715\n",
      "training: 48 batch 282 batch_loss: 0.19090792536735535\n",
      "training: 48 batch 283 batch_loss: 0.19340017437934875\n",
      "training: 48 batch 284 batch_loss: 0.18786680698394775\n",
      "training: 48 batch 285 batch_loss: 0.18744519352912903\n",
      "training: 48 batch 286 batch_loss: 0.19203919172286987\n",
      "training: 48 batch 287 batch_loss: 0.18964093923568726\n",
      "training: 48 batch 288 batch_loss: 0.18998882174491882\n",
      "training: 48 batch 289 batch_loss: 0.18577662110328674\n",
      "training: 48 batch 290 batch_loss: 0.19261080026626587\n",
      "training: 48 batch 291 batch_loss: 0.1915804147720337\n",
      "training: 48 batch 292 batch_loss: 0.18803906440734863\n",
      "training: 48 batch 293 batch_loss: 0.1942284107208252\n",
      "training: 48 batch 294 batch_loss: 0.19375678896903992\n",
      "training: 48 batch 295 batch_loss: 0.18885508179664612\n",
      "training: 48 batch 296 batch_loss: 0.19377705454826355\n",
      "training: 48 batch 297 batch_loss: 0.1871623396873474\n",
      "training: 48 batch 298 batch_loss: 0.191273033618927\n",
      "training: 48 batch 299 batch_loss: 0.19051924347877502\n",
      "training: 48 batch 300 batch_loss: 0.19097715616226196\n",
      "training: 48 batch 301 batch_loss: 0.1910136342048645\n",
      "training: 48 batch 302 batch_loss: 0.19210955500602722\n",
      "training: 48 batch 303 batch_loss: 0.19084253907203674\n",
      "training: 48 batch 304 batch_loss: 0.18912264704704285\n",
      "training: 48 batch 305 batch_loss: 0.1888713836669922\n",
      "training: 48 batch 306 batch_loss: 0.19185978174209595\n",
      "training: 48 batch 307 batch_loss: 0.1871861219406128\n",
      "training: 48 batch 308 batch_loss: 0.19294482469558716\n",
      "training: 48 batch 309 batch_loss: 0.18929478526115417\n",
      "training: 48 batch 310 batch_loss: 0.189081072807312\n",
      "training: 48 batch 311 batch_loss: 0.19242137670516968\n",
      "training: 48 batch 312 batch_loss: 0.19140145182609558\n",
      "training: 48 batch 313 batch_loss: 0.1910760998725891\n",
      "training: 48 batch 314 batch_loss: 0.1918332278728485\n",
      "training: 48 batch 315 batch_loss: 0.18927890062332153\n",
      "training: 48 batch 316 batch_loss: 0.1909145712852478\n",
      "training: 48 batch 317 batch_loss: 0.19412776827812195\n",
      "training: 48 batch 318 batch_loss: 0.18928757309913635\n",
      "training: 48 batch 319 batch_loss: 0.19309380650520325\n",
      "training: 48 batch 320 batch_loss: 0.19371697306632996\n",
      "training: 48 batch 321 batch_loss: 0.1918262541294098\n",
      "training: 48 batch 322 batch_loss: 0.1902453899383545\n",
      "training: 48 batch 323 batch_loss: 0.18839800357818604\n",
      "training: 48 batch 324 batch_loss: 0.1866905391216278\n",
      "training: 48 batch 325 batch_loss: 0.18966126441955566\n",
      "training: 48 batch 326 batch_loss: 0.18979018926620483\n",
      "training: 48 batch 327 batch_loss: 0.18736276030540466\n",
      "training: 48 batch 328 batch_loss: 0.1911972165107727\n",
      "training: 48 batch 329 batch_loss: 0.1919812262058258\n",
      "training: 48 batch 330 batch_loss: 0.18950265645980835\n",
      "training: 48 batch 331 batch_loss: 0.18826887011528015\n",
      "training: 48 batch 332 batch_loss: 0.19149291515350342\n",
      "training: 48 batch 333 batch_loss: 0.19464629888534546\n",
      "training: 48 batch 334 batch_loss: 0.19293132424354553\n",
      "training: 48 batch 335 batch_loss: 0.19123616814613342\n",
      "training: 48 batch 336 batch_loss: 0.1916711926460266\n",
      "training: 48 batch 337 batch_loss: 0.18863895535469055\n",
      "training: 48 batch 338 batch_loss: 0.19474256038665771\n",
      "training: 48 batch 339 batch_loss: 0.18850237131118774\n",
      "training: 48 batch 340 batch_loss: 0.19122013449668884\n",
      "training: 48 batch 341 batch_loss: 0.19307774305343628\n",
      "training: 48 batch 342 batch_loss: 0.1879895031452179\n",
      "training: 48 batch 343 batch_loss: 0.19346246123313904\n",
      "training: 48 batch 344 batch_loss: 0.18919062614440918\n",
      "training: 48 batch 345 batch_loss: 0.19093909859657288\n",
      "training: 48 batch 346 batch_loss: 0.18916916847229004\n",
      "training: 48 batch 347 batch_loss: 0.18982267379760742\n",
      "training: 48 batch 348 batch_loss: 0.192704439163208\n",
      "training: 48 batch 349 batch_loss: 0.18940019607543945\n",
      "training: 48 batch 350 batch_loss: 0.19582059979438782\n",
      "training: 48 batch 351 batch_loss: 0.18862205743789673\n",
      "training: 48 batch 352 batch_loss: 0.18922138214111328\n",
      "training: 48 batch 353 batch_loss: 0.18828749656677246\n",
      "training: 48 batch 354 batch_loss: 0.18811798095703125\n",
      "training: 48 batch 355 batch_loss: 0.18561610579490662\n",
      "training: 48 batch 356 batch_loss: 0.19103336334228516\n",
      "training: 48 batch 357 batch_loss: 0.188534677028656\n",
      "training: 48 batch 358 batch_loss: 0.1924050748348236\n",
      "training: 48 batch 359 batch_loss: 0.1901143193244934\n",
      "training: 48 batch 360 batch_loss: 0.19292914867401123\n",
      "training: 48 batch 361 batch_loss: 0.18770962953567505\n",
      "training: 48 batch 362 batch_loss: 0.19366642832756042\n",
      "training: 48 batch 363 batch_loss: 0.19259989261627197\n",
      "training: 48 batch 364 batch_loss: 0.18440675735473633\n",
      "training: 48 batch 365 batch_loss: 0.18755057454109192\n",
      "training: 48 batch 366 batch_loss: 0.1892651915550232\n",
      "training: 48 batch 367 batch_loss: 0.19049715995788574\n",
      "training: 48 batch 368 batch_loss: 0.19447004795074463\n",
      "training: 48 batch 369 batch_loss: 0.1903190016746521\n",
      "training: 48 batch 370 batch_loss: 0.1905105710029602\n",
      "training: 48 batch 371 batch_loss: 0.19361022114753723\n",
      "training: 48 batch 372 batch_loss: 0.1915547251701355\n",
      "training: 48 batch 373 batch_loss: 0.18911713361740112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 48 batch 374 batch_loss: 0.1927075982093811\n",
      "training: 48 batch 375 batch_loss: 0.19230765104293823\n",
      "training: 48 batch 376 batch_loss: 0.18708878755569458\n",
      "training: 48 batch 377 batch_loss: 0.19325289130210876\n",
      "training: 48 batch 378 batch_loss: 0.18962553143501282\n",
      "training: 48 batch 379 batch_loss: 0.19154956936836243\n",
      "training: 48 batch 380 batch_loss: 0.19245082139968872\n",
      "training: 48 batch 381 batch_loss: 0.19476646184921265\n",
      "training: 48 batch 382 batch_loss: 0.1894942820072174\n",
      "training: 48 batch 383 batch_loss: 0.18981662392616272\n",
      "training: 48 batch 384 batch_loss: 0.19578957557678223\n",
      "training: 48 batch 385 batch_loss: 0.18671327829360962\n",
      "training: 48 batch 386 batch_loss: 0.19071567058563232\n",
      "training: 48 batch 387 batch_loss: 0.1934886872768402\n",
      "training: 48 batch 388 batch_loss: 0.19431719183921814\n",
      "training: 48 batch 389 batch_loss: 0.18996453285217285\n",
      "training: 48 batch 390 batch_loss: 0.19106525182724\n",
      "training: 48 batch 391 batch_loss: 0.1939728558063507\n",
      "training: 48 batch 392 batch_loss: 0.19271054863929749\n",
      "training: 48 batch 393 batch_loss: 0.1904422640800476\n",
      "training: 48 batch 394 batch_loss: 0.1876802146434784\n",
      "training: 48 batch 395 batch_loss: 0.19132530689239502\n",
      "training: 48 batch 396 batch_loss: 0.19109150767326355\n",
      "training: 48 batch 397 batch_loss: 0.18773925304412842\n",
      "training: 48 batch 398 batch_loss: 0.19193965196609497\n",
      "training: 48 batch 399 batch_loss: 0.19643914699554443\n",
      "training: 48 batch 400 batch_loss: 0.19202664494514465\n",
      "training: 48 batch 401 batch_loss: 0.18879178166389465\n",
      "training: 48 batch 402 batch_loss: 0.1946558952331543\n",
      "training: 48 batch 403 batch_loss: 0.19341808557510376\n",
      "training: 48 batch 404 batch_loss: 0.19074809551239014\n",
      "training: 48 batch 405 batch_loss: 0.1907307505607605\n",
      "training: 48 batch 406 batch_loss: 0.19385716319084167\n",
      "training: 48 batch 407 batch_loss: 0.18547707796096802\n",
      "training: 48 batch 408 batch_loss: 0.18731939792633057\n",
      "training: 48 batch 409 batch_loss: 0.19293656945228577\n",
      "training: 48 batch 410 batch_loss: 0.19195812940597534\n",
      "training: 48 batch 411 batch_loss: 0.1928558349609375\n",
      "training: 48 batch 412 batch_loss: 0.19072794914245605\n",
      "training: 48 batch 413 batch_loss: 0.19504764676094055\n",
      "training: 48 batch 414 batch_loss: 0.19060379266738892\n",
      "training: 48 batch 415 batch_loss: 0.1889021098613739\n",
      "training: 48 batch 416 batch_loss: 0.19114306569099426\n",
      "training: 48 batch 417 batch_loss: 0.19011548161506653\n",
      "training: 48 batch 418 batch_loss: 0.1928471326828003\n",
      "training: 48 batch 419 batch_loss: 0.19179239869117737\n",
      "training: 48 batch 420 batch_loss: 0.19001835584640503\n",
      "training: 48 batch 421 batch_loss: 0.18867400288581848\n",
      "training: 48 batch 422 batch_loss: 0.19030430912971497\n",
      "training: 48 batch 423 batch_loss: 0.1885494589805603\n",
      "training: 48 batch 424 batch_loss: 0.18804219365119934\n",
      "training: 48 batch 425 batch_loss: 0.1862468123435974\n",
      "training: 48 batch 426 batch_loss: 0.19300758838653564\n",
      "training: 48 batch 427 batch_loss: 0.19365838170051575\n",
      "training: 48 batch 428 batch_loss: 0.1901504099369049\n",
      "training: 48 batch 429 batch_loss: 0.18856573104858398\n",
      "training: 48 batch 430 batch_loss: 0.18850818276405334\n",
      "training: 48 batch 431 batch_loss: 0.18524661660194397\n",
      "training: 48 batch 432 batch_loss: 0.1878572404384613\n",
      "training: 48 batch 433 batch_loss: 0.19063806533813477\n",
      "training: 48 batch 434 batch_loss: 0.19146394729614258\n",
      "training: 48 batch 435 batch_loss: 0.19099286198616028\n",
      "training: 48 batch 436 batch_loss: 0.18996357917785645\n",
      "training: 48 batch 437 batch_loss: 0.19160622358322144\n",
      "training: 48 batch 438 batch_loss: 0.19251400232315063\n",
      "training: 48 batch 439 batch_loss: 0.19320818781852722\n",
      "training: 48 batch 440 batch_loss: 0.18933671712875366\n",
      "training: 48 batch 441 batch_loss: 0.19078052043914795\n",
      "training: 48 batch 442 batch_loss: 0.18797141313552856\n",
      "training: 48 batch 443 batch_loss: 0.19000089168548584\n",
      "training: 48 batch 444 batch_loss: 0.19196182489395142\n",
      "training: 48 batch 445 batch_loss: 0.18922379612922668\n",
      "training: 48 batch 446 batch_loss: 0.19209638237953186\n",
      "training: 48 batch 447 batch_loss: 0.1879832148551941\n",
      "training: 48 batch 448 batch_loss: 0.19068938493728638\n",
      "training: 48 batch 449 batch_loss: 0.19099891185760498\n",
      "training: 48 batch 450 batch_loss: 0.19691160321235657\n",
      "training: 48 batch 451 batch_loss: 0.19345879554748535\n",
      "training: 48 batch 452 batch_loss: 0.18519741296768188\n",
      "training: 48 batch 453 batch_loss: 0.18706658482551575\n",
      "training: 48 batch 454 batch_loss: 0.1914549469947815\n",
      "training: 48 batch 455 batch_loss: 0.19295939803123474\n",
      "training: 48 batch 456 batch_loss: 0.1900438368320465\n",
      "training: 48 batch 457 batch_loss: 0.19351941347122192\n",
      "training: 48 batch 458 batch_loss: 0.19359511137008667\n",
      "training: 48 batch 459 batch_loss: 0.19194349646568298\n",
      "training: 48 batch 460 batch_loss: 0.19262152910232544\n",
      "training: 48 batch 461 batch_loss: 0.1911560297012329\n",
      "training: 48 batch 462 batch_loss: 0.1930254101753235\n",
      "training: 48 batch 463 batch_loss: 0.19133979082107544\n",
      "training: 48 batch 464 batch_loss: 0.19170397520065308\n",
      "training: 48 batch 465 batch_loss: 0.1971648931503296\n",
      "training: 48 batch 466 batch_loss: 0.19217449426651\n",
      "training: 48 batch 467 batch_loss: 0.19338810443878174\n",
      "training: 48 batch 468 batch_loss: 0.1923944056034088\n",
      "training: 48 batch 469 batch_loss: 0.1919093132019043\n",
      "training: 48 batch 470 batch_loss: 0.18431735038757324\n",
      "training: 48 batch 471 batch_loss: 0.19236257672309875\n",
      "training: 48 batch 472 batch_loss: 0.19003379344940186\n",
      "training: 48 batch 473 batch_loss: 0.19332557916641235\n",
      "training: 48 batch 474 batch_loss: 0.19140291213989258\n",
      "training: 48 batch 475 batch_loss: 0.18791669607162476\n",
      "training: 48 batch 476 batch_loss: 0.19497615098953247\n",
      "training: 48 batch 477 batch_loss: 0.19430622458457947\n",
      "training: 48 batch 478 batch_loss: 0.19472461938858032\n",
      "training: 48 batch 479 batch_loss: 0.19215476512908936\n",
      "training: 48 batch 480 batch_loss: 0.1864548623561859\n",
      "training: 48 batch 481 batch_loss: 0.19079601764678955\n",
      "training: 48 batch 482 batch_loss: 0.19445723295211792\n",
      "training: 48 batch 483 batch_loss: 0.18939200043678284\n",
      "training: 48 batch 484 batch_loss: 0.1893041729927063\n",
      "training: 48 batch 485 batch_loss: 0.19004645943641663\n",
      "training: 48 batch 486 batch_loss: 0.18852025270462036\n",
      "training: 48 batch 487 batch_loss: 0.19225695729255676\n",
      "training: 48 batch 488 batch_loss: 0.19298705458641052\n",
      "training: 48 batch 489 batch_loss: 0.188992440700531\n",
      "training: 48 batch 490 batch_loss: 0.19386613368988037\n",
      "training: 48 batch 491 batch_loss: 0.19260892271995544\n",
      "training: 48 batch 492 batch_loss: 0.19184857606887817\n",
      "training: 48 batch 493 batch_loss: 0.1899215281009674\n",
      "training: 48 batch 494 batch_loss: 0.1918405294418335\n",
      "training: 48 batch 495 batch_loss: 0.19367673993110657\n",
      "training: 48 batch 496 batch_loss: 0.19323104619979858\n",
      "training: 48 batch 497 batch_loss: 0.19015496969223022\n",
      "training: 48 batch 498 batch_loss: 0.19286900758743286\n",
      "training: 48 batch 499 batch_loss: 0.19654437899589539\n",
      "training: 48 batch 500 batch_loss: 0.191936194896698\n",
      "training: 48 batch 501 batch_loss: 0.19411784410476685\n",
      "training: 48 batch 502 batch_loss: 0.18965071439743042\n",
      "training: 48 batch 503 batch_loss: 0.192344069480896\n",
      "training: 48 batch 504 batch_loss: 0.18882837891578674\n",
      "training: 48 batch 505 batch_loss: 0.1929250955581665\n",
      "training: 48 batch 506 batch_loss: 0.18780404329299927\n",
      "training: 48 batch 507 batch_loss: 0.19128385186195374\n",
      "training: 48 batch 508 batch_loss: 0.18989530205726624\n",
      "training: 48 batch 509 batch_loss: 0.19219905138015747\n",
      "training: 48 batch 510 batch_loss: 0.1938989758491516\n",
      "training: 48 batch 511 batch_loss: 0.1909884214401245\n",
      "training: 48 batch 512 batch_loss: 0.19028621912002563\n",
      "training: 48 batch 513 batch_loss: 0.1899484097957611\n",
      "training: 48 batch 514 batch_loss: 0.1912727653980255\n",
      "training: 48 batch 515 batch_loss: 0.19111338257789612\n",
      "training: 48 batch 516 batch_loss: 0.19669485092163086\n",
      "training: 48 batch 517 batch_loss: 0.19501587748527527\n",
      "training: 48 batch 518 batch_loss: 0.19239473342895508\n",
      "training: 48 batch 519 batch_loss: 0.19311746954917908\n",
      "training: 48 batch 520 batch_loss: 0.19317910075187683\n",
      "training: 48 batch 521 batch_loss: 0.19184786081314087\n",
      "training: 48 batch 522 batch_loss: 0.19213107228279114\n",
      "training: 48 batch 523 batch_loss: 0.18891030550003052\n",
      "training: 48 batch 524 batch_loss: 0.19117900729179382\n",
      "training: 48 batch 525 batch_loss: 0.18919533491134644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 48 batch 526 batch_loss: 0.18858635425567627\n",
      "training: 48 batch 527 batch_loss: 0.1916140913963318\n",
      "training: 48 batch 528 batch_loss: 0.1889924705028534\n",
      "training: 48 batch 529 batch_loss: 0.1927710771560669\n",
      "training: 48 batch 530 batch_loss: 0.19107258319854736\n",
      "training: 48 batch 531 batch_loss: 0.19388240575790405\n",
      "training: 48 batch 532 batch_loss: 0.19005140662193298\n",
      "training: 48 batch 533 batch_loss: 0.18527168035507202\n",
      "training: 48 batch 534 batch_loss: 0.19205719232559204\n",
      "training: 48 batch 535 batch_loss: 0.19085979461669922\n",
      "training: 48 batch 536 batch_loss: 0.19080010056495667\n",
      "training: 48 batch 537 batch_loss: 0.19032329320907593\n",
      "training: 48 batch 538 batch_loss: 0.18909740447998047\n",
      "training: 48 batch 539 batch_loss: 0.1935066282749176\n",
      "training: 48 batch 540 batch_loss: 0.1922871470451355\n",
      "training: 48 batch 541 batch_loss: 0.1918271780014038\n",
      "training: 48 batch 542 batch_loss: 0.1911921203136444\n",
      "training: 48 batch 543 batch_loss: 0.19121873378753662\n",
      "training: 48 batch 544 batch_loss: 0.19088733196258545\n",
      "training: 48 batch 545 batch_loss: 0.1898612082004547\n",
      "training: 48 batch 546 batch_loss: 0.19303542375564575\n",
      "training: 48 batch 547 batch_loss: 0.1875825822353363\n",
      "training: 48 batch 548 batch_loss: 0.18515649437904358\n",
      "training: 48 batch 549 batch_loss: 0.18729308247566223\n",
      "training: 48 batch 550 batch_loss: 0.19061747193336487\n",
      "training: 48 batch 551 batch_loss: 0.1908578872680664\n",
      "training: 48 batch 552 batch_loss: 0.19120800495147705\n",
      "training: 48 batch 553 batch_loss: 0.1921491026878357\n",
      "training: 48 batch 554 batch_loss: 0.1899799108505249\n",
      "training: 48 batch 555 batch_loss: 0.1919565200805664\n",
      "training: 48 batch 556 batch_loss: 0.1944323182106018\n",
      "training: 48 batch 557 batch_loss: 0.19370734691619873\n",
      "training: 48 batch 558 batch_loss: 0.18888536095619202\n",
      "training: 48 batch 559 batch_loss: 0.19153493642807007\n",
      "training: 48 batch 560 batch_loss: 0.190131276845932\n",
      "training: 48 batch 561 batch_loss: 0.19633430242538452\n",
      "training: 48 batch 562 batch_loss: 0.19249457120895386\n",
      "training: 48 batch 563 batch_loss: 0.18934860825538635\n",
      "training: 48 batch 564 batch_loss: 0.18961584568023682\n",
      "training: 48 batch 565 batch_loss: 0.19124102592468262\n",
      "training: 48 batch 566 batch_loss: 0.19021737575531006\n",
      "training: 48 batch 567 batch_loss: 0.1924653947353363\n",
      "training: 48 batch 568 batch_loss: 0.19124165177345276\n",
      "training: 48 batch 569 batch_loss: 0.1880912184715271\n",
      "training: 48 batch 570 batch_loss: 0.19231432676315308\n",
      "training: 48 batch 571 batch_loss: 0.19126302003860474\n",
      "training: 48 batch 572 batch_loss: 0.19210785627365112\n",
      "training: 48 batch 573 batch_loss: 0.19115939736366272\n",
      "training: 48 batch 574 batch_loss: 0.18822762370109558\n",
      "training: 48 batch 575 batch_loss: 0.19022366404533386\n",
      "training: 48 batch 576 batch_loss: 0.18672645092010498\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 48, Hit Ratio:0.03223259702924452 | Precision:0.04755725941216947 | Recall:0.06286491907939709 | NDCG:0.06136863679648068\n",
      "*Best Performance* \n",
      "Epoch: 48, Hit Ratio:0.03223259702924452 | Precision:0.04755725941216947 | Recall:0.06286491907939709 | MDCG:0.06136863679648068\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 49 batch 0 batch_loss: 0.1915472149848938\n",
      "training: 49 batch 1 batch_loss: 0.18700364232063293\n",
      "training: 49 batch 2 batch_loss: 0.19072991609573364\n",
      "training: 49 batch 3 batch_loss: 0.18681713938713074\n",
      "training: 49 batch 4 batch_loss: 0.18494468927383423\n",
      "training: 49 batch 5 batch_loss: 0.19055521488189697\n",
      "training: 49 batch 6 batch_loss: 0.18974724411964417\n",
      "training: 49 batch 7 batch_loss: 0.18985670804977417\n",
      "training: 49 batch 8 batch_loss: 0.19183549284934998\n",
      "training: 49 batch 9 batch_loss: 0.1861858069896698\n",
      "training: 49 batch 10 batch_loss: 0.19365784525871277\n",
      "training: 49 batch 11 batch_loss: 0.1899496614933014\n",
      "training: 49 batch 12 batch_loss: 0.1902693808078766\n",
      "training: 49 batch 13 batch_loss: 0.19125109910964966\n",
      "training: 49 batch 14 batch_loss: 0.19233301281929016\n",
      "training: 49 batch 15 batch_loss: 0.18641871213912964\n",
      "training: 49 batch 16 batch_loss: 0.18558195233345032\n",
      "training: 49 batch 17 batch_loss: 0.19166845083236694\n",
      "training: 49 batch 18 batch_loss: 0.19241845607757568\n",
      "training: 49 batch 19 batch_loss: 0.18554621934890747\n",
      "training: 49 batch 20 batch_loss: 0.1847900152206421\n",
      "training: 49 batch 21 batch_loss: 0.18902772665023804\n",
      "training: 49 batch 22 batch_loss: 0.19121119379997253\n",
      "training: 49 batch 23 batch_loss: 0.1900126338005066\n",
      "training: 49 batch 24 batch_loss: 0.19090613722801208\n",
      "training: 49 batch 25 batch_loss: 0.19075527787208557\n",
      "training: 49 batch 26 batch_loss: 0.18957334756851196\n",
      "training: 49 batch 27 batch_loss: 0.1889628767967224\n",
      "training: 49 batch 28 batch_loss: 0.1883813738822937\n",
      "training: 49 batch 29 batch_loss: 0.18934696912765503\n",
      "training: 49 batch 30 batch_loss: 0.18699872493743896\n",
      "training: 49 batch 31 batch_loss: 0.18943217396736145\n",
      "training: 49 batch 32 batch_loss: 0.1878989040851593\n",
      "training: 49 batch 33 batch_loss: 0.1888677477836609\n",
      "training: 49 batch 34 batch_loss: 0.19147947430610657\n",
      "training: 49 batch 35 batch_loss: 0.19517838954925537\n",
      "training: 49 batch 36 batch_loss: 0.19140714406967163\n",
      "training: 49 batch 37 batch_loss: 0.1919803023338318\n",
      "training: 49 batch 38 batch_loss: 0.18910586833953857\n",
      "training: 49 batch 39 batch_loss: 0.18522429466247559\n",
      "training: 49 batch 40 batch_loss: 0.18906649947166443\n",
      "training: 49 batch 41 batch_loss: 0.18934667110443115\n",
      "training: 49 batch 42 batch_loss: 0.19139695167541504\n",
      "training: 49 batch 43 batch_loss: 0.18867963552474976\n",
      "training: 49 batch 44 batch_loss: 0.18919217586517334\n",
      "training: 49 batch 45 batch_loss: 0.19124919176101685\n",
      "training: 49 batch 46 batch_loss: 0.186703622341156\n",
      "training: 49 batch 47 batch_loss: 0.1880742907524109\n",
      "training: 49 batch 48 batch_loss: 0.18936878442764282\n",
      "training: 49 batch 49 batch_loss: 0.18861466646194458\n",
      "training: 49 batch 50 batch_loss: 0.19115427136421204\n",
      "training: 49 batch 51 batch_loss: 0.18902435898780823\n",
      "training: 49 batch 52 batch_loss: 0.19017991423606873\n",
      "training: 49 batch 53 batch_loss: 0.19021129608154297\n",
      "training: 49 batch 54 batch_loss: 0.19096237421035767\n",
      "training: 49 batch 55 batch_loss: 0.1901494562625885\n",
      "training: 49 batch 56 batch_loss: 0.1926664412021637\n",
      "training: 49 batch 57 batch_loss: 0.18996360898017883\n",
      "training: 49 batch 58 batch_loss: 0.19268500804901123\n",
      "training: 49 batch 59 batch_loss: 0.19192904233932495\n",
      "training: 49 batch 60 batch_loss: 0.1867268681526184\n",
      "training: 49 batch 61 batch_loss: 0.19243896007537842\n",
      "training: 49 batch 62 batch_loss: 0.18885892629623413\n",
      "training: 49 batch 63 batch_loss: 0.19208770990371704\n",
      "training: 49 batch 64 batch_loss: 0.19175249338150024\n",
      "training: 49 batch 65 batch_loss: 0.1852850615978241\n",
      "training: 49 batch 66 batch_loss: 0.18930160999298096\n",
      "training: 49 batch 67 batch_loss: 0.19157475233078003\n",
      "training: 49 batch 68 batch_loss: 0.18729692697525024\n",
      "training: 49 batch 69 batch_loss: 0.18686825037002563\n",
      "training: 49 batch 70 batch_loss: 0.18574339151382446\n",
      "training: 49 batch 71 batch_loss: 0.19166797399520874\n",
      "training: 49 batch 72 batch_loss: 0.19105923175811768\n",
      "training: 49 batch 73 batch_loss: 0.19292211532592773\n",
      "training: 49 batch 74 batch_loss: 0.1864141821861267\n",
      "training: 49 batch 75 batch_loss: 0.1907406449317932\n",
      "training: 49 batch 76 batch_loss: 0.19173264503479004\n",
      "training: 49 batch 77 batch_loss: 0.193162739276886\n",
      "training: 49 batch 78 batch_loss: 0.1896967887878418\n",
      "training: 49 batch 79 batch_loss: 0.18946880102157593\n",
      "training: 49 batch 80 batch_loss: 0.191911518573761\n",
      "training: 49 batch 81 batch_loss: 0.19122067093849182\n",
      "training: 49 batch 82 batch_loss: 0.19189757108688354\n",
      "training: 49 batch 83 batch_loss: 0.1922680139541626\n",
      "training: 49 batch 84 batch_loss: 0.18780431151390076\n",
      "training: 49 batch 85 batch_loss: 0.1878657341003418\n",
      "training: 49 batch 86 batch_loss: 0.19048306345939636\n",
      "training: 49 batch 87 batch_loss: 0.1923888921737671\n",
      "training: 49 batch 88 batch_loss: 0.19425195455551147\n",
      "training: 49 batch 89 batch_loss: 0.19373223185539246\n",
      "training: 49 batch 90 batch_loss: 0.1898219883441925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 49 batch 91 batch_loss: 0.190153568983078\n",
      "training: 49 batch 92 batch_loss: 0.18845432996749878\n",
      "training: 49 batch 93 batch_loss: 0.18797630071640015\n",
      "training: 49 batch 94 batch_loss: 0.1904115080833435\n",
      "training: 49 batch 95 batch_loss: 0.1891133189201355\n",
      "training: 49 batch 96 batch_loss: 0.18572759628295898\n",
      "training: 49 batch 97 batch_loss: 0.19139590859413147\n",
      "training: 49 batch 98 batch_loss: 0.1924300491809845\n",
      "training: 49 batch 99 batch_loss: 0.1912769079208374\n",
      "training: 49 batch 100 batch_loss: 0.18954697251319885\n",
      "training: 49 batch 101 batch_loss: 0.1900375485420227\n",
      "training: 49 batch 102 batch_loss: 0.1906907558441162\n",
      "training: 49 batch 103 batch_loss: 0.1895541548728943\n",
      "training: 49 batch 104 batch_loss: 0.19397124648094177\n",
      "training: 49 batch 105 batch_loss: 0.1943957507610321\n",
      "training: 49 batch 106 batch_loss: 0.18934527039527893\n",
      "training: 49 batch 107 batch_loss: 0.18810224533081055\n",
      "training: 49 batch 108 batch_loss: 0.19087141752243042\n",
      "training: 49 batch 109 batch_loss: 0.19506427645683289\n",
      "training: 49 batch 110 batch_loss: 0.1900307536125183\n",
      "training: 49 batch 111 batch_loss: 0.19089281558990479\n",
      "training: 49 batch 112 batch_loss: 0.19351303577423096\n",
      "training: 49 batch 113 batch_loss: 0.18990162014961243\n",
      "training: 49 batch 114 batch_loss: 0.1913178563117981\n",
      "training: 49 batch 115 batch_loss: 0.18993085622787476\n",
      "training: 49 batch 116 batch_loss: 0.19007956981658936\n",
      "training: 49 batch 117 batch_loss: 0.18944606184959412\n",
      "training: 49 batch 118 batch_loss: 0.1920507252216339\n",
      "training: 49 batch 119 batch_loss: 0.19001251459121704\n",
      "training: 49 batch 120 batch_loss: 0.18793627619743347\n",
      "training: 49 batch 121 batch_loss: 0.18658697605133057\n",
      "training: 49 batch 122 batch_loss: 0.18834370374679565\n",
      "training: 49 batch 123 batch_loss: 0.1883755922317505\n",
      "training: 49 batch 124 batch_loss: 0.19039052724838257\n",
      "training: 49 batch 125 batch_loss: 0.19427472352981567\n",
      "training: 49 batch 126 batch_loss: 0.19298136234283447\n",
      "training: 49 batch 127 batch_loss: 0.1895841360092163\n",
      "training: 49 batch 128 batch_loss: 0.18952935934066772\n",
      "training: 49 batch 129 batch_loss: 0.19115647673606873\n",
      "training: 49 batch 130 batch_loss: 0.18919184803962708\n",
      "training: 49 batch 131 batch_loss: 0.19093653559684753\n",
      "training: 49 batch 132 batch_loss: 0.19352781772613525\n",
      "training: 49 batch 133 batch_loss: 0.18866443634033203\n",
      "training: 49 batch 134 batch_loss: 0.19029471278190613\n",
      "training: 49 batch 135 batch_loss: 0.19044837355613708\n",
      "training: 49 batch 136 batch_loss: 0.19019749760627747\n",
      "training: 49 batch 137 batch_loss: 0.1903468668460846\n",
      "training: 49 batch 138 batch_loss: 0.18723046779632568\n",
      "training: 49 batch 139 batch_loss: 0.18919134140014648\n",
      "training: 49 batch 140 batch_loss: 0.18892869353294373\n",
      "training: 49 batch 141 batch_loss: 0.19143962860107422\n",
      "training: 49 batch 142 batch_loss: 0.19145047664642334\n",
      "training: 49 batch 143 batch_loss: 0.1900101900100708\n",
      "training: 49 batch 144 batch_loss: 0.19596827030181885\n",
      "training: 49 batch 145 batch_loss: 0.19248706102371216\n",
      "training: 49 batch 146 batch_loss: 0.19209080934524536\n",
      "training: 49 batch 147 batch_loss: 0.18697398900985718\n",
      "training: 49 batch 148 batch_loss: 0.19462397694587708\n",
      "training: 49 batch 149 batch_loss: 0.1923452913761139\n",
      "training: 49 batch 150 batch_loss: 0.18566304445266724\n",
      "training: 49 batch 151 batch_loss: 0.18954318761825562\n",
      "training: 49 batch 152 batch_loss: 0.1955270767211914\n",
      "training: 49 batch 153 batch_loss: 0.1875072717666626\n",
      "training: 49 batch 154 batch_loss: 0.19007301330566406\n",
      "training: 49 batch 155 batch_loss: 0.1897648274898529\n",
      "training: 49 batch 156 batch_loss: 0.1881568431854248\n",
      "training: 49 batch 157 batch_loss: 0.18749874830245972\n",
      "training: 49 batch 158 batch_loss: 0.1938069760799408\n",
      "training: 49 batch 159 batch_loss: 0.19187018275260925\n",
      "training: 49 batch 160 batch_loss: 0.19298654794692993\n",
      "training: 49 batch 161 batch_loss: 0.18872475624084473\n",
      "training: 49 batch 162 batch_loss: 0.1954515278339386\n",
      "training: 49 batch 163 batch_loss: 0.18896949291229248\n",
      "training: 49 batch 164 batch_loss: 0.19227921962738037\n",
      "training: 49 batch 165 batch_loss: 0.19051405787467957\n",
      "training: 49 batch 166 batch_loss: 0.1915200650691986\n",
      "training: 49 batch 167 batch_loss: 0.19302836060523987\n",
      "training: 49 batch 168 batch_loss: 0.19142639636993408\n",
      "training: 49 batch 169 batch_loss: 0.1862974464893341\n",
      "training: 49 batch 170 batch_loss: 0.1908576786518097\n",
      "training: 49 batch 171 batch_loss: 0.19162911176681519\n",
      "training: 49 batch 172 batch_loss: 0.18905478715896606\n",
      "training: 49 batch 173 batch_loss: 0.19185125827789307\n",
      "training: 49 batch 174 batch_loss: 0.1940554678440094\n",
      "training: 49 batch 175 batch_loss: 0.1876559853553772\n",
      "training: 49 batch 176 batch_loss: 0.1907312572002411\n",
      "training: 49 batch 177 batch_loss: 0.190914124250412\n",
      "training: 49 batch 178 batch_loss: 0.1897560954093933\n",
      "training: 49 batch 179 batch_loss: 0.1927621066570282\n",
      "training: 49 batch 180 batch_loss: 0.19149261713027954\n",
      "training: 49 batch 181 batch_loss: 0.19119617342948914\n",
      "training: 49 batch 182 batch_loss: 0.19165921211242676\n",
      "training: 49 batch 183 batch_loss: 0.190651535987854\n",
      "training: 49 batch 184 batch_loss: 0.1883731484413147\n",
      "training: 49 batch 185 batch_loss: 0.19158899784088135\n",
      "training: 49 batch 186 batch_loss: 0.18992263078689575\n",
      "training: 49 batch 187 batch_loss: 0.19435322284698486\n",
      "training: 49 batch 188 batch_loss: 0.18903756141662598\n",
      "training: 49 batch 189 batch_loss: 0.18893706798553467\n",
      "training: 49 batch 190 batch_loss: 0.19158807396888733\n",
      "training: 49 batch 191 batch_loss: 0.19292211532592773\n",
      "training: 49 batch 192 batch_loss: 0.1934720277786255\n",
      "training: 49 batch 193 batch_loss: 0.19469738006591797\n",
      "training: 49 batch 194 batch_loss: 0.18859252333641052\n",
      "training: 49 batch 195 batch_loss: 0.19008177518844604\n",
      "training: 49 batch 196 batch_loss: 0.18691569566726685\n",
      "training: 49 batch 197 batch_loss: 0.19170454144477844\n",
      "training: 49 batch 198 batch_loss: 0.19376984238624573\n",
      "training: 49 batch 199 batch_loss: 0.18564140796661377\n",
      "training: 49 batch 200 batch_loss: 0.1915748119354248\n",
      "training: 49 batch 201 batch_loss: 0.19428810477256775\n",
      "training: 49 batch 202 batch_loss: 0.18920141458511353\n",
      "training: 49 batch 203 batch_loss: 0.18895182013511658\n",
      "training: 49 batch 204 batch_loss: 0.1919441521167755\n",
      "training: 49 batch 205 batch_loss: 0.19412600994110107\n",
      "training: 49 batch 206 batch_loss: 0.19043931365013123\n",
      "training: 49 batch 207 batch_loss: 0.19131487607955933\n",
      "training: 49 batch 208 batch_loss: 0.19105657935142517\n",
      "training: 49 batch 209 batch_loss: 0.18840622901916504\n",
      "training: 49 batch 210 batch_loss: 0.1965731382369995\n",
      "training: 49 batch 211 batch_loss: 0.19262221455574036\n",
      "training: 49 batch 212 batch_loss: 0.19367730617523193\n",
      "training: 49 batch 213 batch_loss: 0.19298341870307922\n",
      "training: 49 batch 214 batch_loss: 0.1879240870475769\n",
      "training: 49 batch 215 batch_loss: 0.19500741362571716\n",
      "training: 49 batch 216 batch_loss: 0.19147083163261414\n",
      "training: 49 batch 217 batch_loss: 0.1902981698513031\n",
      "training: 49 batch 218 batch_loss: 0.19251853227615356\n",
      "training: 49 batch 219 batch_loss: 0.1883680820465088\n",
      "training: 49 batch 220 batch_loss: 0.1908179521560669\n",
      "training: 49 batch 221 batch_loss: 0.1929037868976593\n",
      "training: 49 batch 222 batch_loss: 0.18848076462745667\n",
      "training: 49 batch 223 batch_loss: 0.19206377863883972\n",
      "training: 49 batch 224 batch_loss: 0.1907079517841339\n",
      "training: 49 batch 225 batch_loss: 0.18781062960624695\n",
      "training: 49 batch 226 batch_loss: 0.19282802939414978\n",
      "training: 49 batch 227 batch_loss: 0.19537395238876343\n",
      "training: 49 batch 228 batch_loss: 0.19622290134429932\n",
      "training: 49 batch 229 batch_loss: 0.18892920017242432\n",
      "training: 49 batch 230 batch_loss: 0.19265642762184143\n",
      "training: 49 batch 231 batch_loss: 0.19430306553840637\n",
      "training: 49 batch 232 batch_loss: 0.18872672319412231\n",
      "training: 49 batch 233 batch_loss: 0.1914639174938202\n",
      "training: 49 batch 234 batch_loss: 0.19225996732711792\n",
      "training: 49 batch 235 batch_loss: 0.19378411769866943\n",
      "training: 49 batch 236 batch_loss: 0.18916857242584229\n",
      "training: 49 batch 237 batch_loss: 0.19061622023582458\n",
      "training: 49 batch 238 batch_loss: 0.19367355108261108\n",
      "training: 49 batch 239 batch_loss: 0.19022178649902344\n",
      "training: 49 batch 240 batch_loss: 0.19259467720985413\n",
      "training: 49 batch 241 batch_loss: 0.19507604837417603\n",
      "training: 49 batch 242 batch_loss: 0.192744642496109\n",
      "training: 49 batch 243 batch_loss: 0.19058841466903687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 49 batch 244 batch_loss: 0.19534605741500854\n",
      "training: 49 batch 245 batch_loss: 0.19152602553367615\n",
      "training: 49 batch 246 batch_loss: 0.18733572959899902\n",
      "training: 49 batch 247 batch_loss: 0.19276541471481323\n",
      "training: 49 batch 248 batch_loss: 0.19135481119155884\n",
      "training: 49 batch 249 batch_loss: 0.1891794502735138\n",
      "training: 49 batch 250 batch_loss: 0.1899719536304474\n",
      "training: 49 batch 251 batch_loss: 0.18803715705871582\n",
      "training: 49 batch 252 batch_loss: 0.1954047977924347\n",
      "training: 49 batch 253 batch_loss: 0.18716838955879211\n",
      "training: 49 batch 254 batch_loss: 0.19266006350517273\n",
      "training: 49 batch 255 batch_loss: 0.19194462895393372\n",
      "training: 49 batch 256 batch_loss: 0.19193768501281738\n",
      "training: 49 batch 257 batch_loss: 0.19081228971481323\n",
      "training: 49 batch 258 batch_loss: 0.19222718477249146\n",
      "training: 49 batch 259 batch_loss: 0.19236060976982117\n",
      "training: 49 batch 260 batch_loss: 0.1893416941165924\n",
      "training: 49 batch 261 batch_loss: 0.1894652247428894\n",
      "training: 49 batch 262 batch_loss: 0.18698060512542725\n",
      "training: 49 batch 263 batch_loss: 0.18728476762771606\n",
      "training: 49 batch 264 batch_loss: 0.19221821427345276\n",
      "training: 49 batch 265 batch_loss: 0.19136688113212585\n",
      "training: 49 batch 266 batch_loss: 0.1908118724822998\n",
      "training: 49 batch 267 batch_loss: 0.19375604391098022\n",
      "training: 49 batch 268 batch_loss: 0.1947983205318451\n",
      "training: 49 batch 269 batch_loss: 0.18872931599617004\n",
      "training: 49 batch 270 batch_loss: 0.19134163856506348\n",
      "training: 49 batch 271 batch_loss: 0.1917213499546051\n",
      "training: 49 batch 272 batch_loss: 0.19150087237358093\n",
      "training: 49 batch 273 batch_loss: 0.18949374556541443\n",
      "training: 49 batch 274 batch_loss: 0.19135865569114685\n",
      "training: 49 batch 275 batch_loss: 0.18930375576019287\n",
      "training: 49 batch 276 batch_loss: 0.19367817044258118\n",
      "training: 49 batch 277 batch_loss: 0.1875668168067932\n",
      "training: 49 batch 278 batch_loss: 0.19175207614898682\n",
      "training: 49 batch 279 batch_loss: 0.1938752830028534\n",
      "training: 49 batch 280 batch_loss: 0.18977922201156616\n",
      "training: 49 batch 281 batch_loss: 0.19497260451316833\n",
      "training: 49 batch 282 batch_loss: 0.19167187809944153\n",
      "training: 49 batch 283 batch_loss: 0.1930069625377655\n",
      "training: 49 batch 284 batch_loss: 0.1888575255870819\n",
      "training: 49 batch 285 batch_loss: 0.18580880761146545\n",
      "training: 49 batch 286 batch_loss: 0.1952614188194275\n",
      "training: 49 batch 287 batch_loss: 0.19318166375160217\n",
      "training: 49 batch 288 batch_loss: 0.19114333391189575\n",
      "training: 49 batch 289 batch_loss: 0.1901746392250061\n",
      "training: 49 batch 290 batch_loss: 0.18992239236831665\n",
      "training: 49 batch 291 batch_loss: 0.18900996446609497\n",
      "training: 49 batch 292 batch_loss: 0.18842920660972595\n",
      "training: 49 batch 293 batch_loss: 0.19284480810165405\n",
      "training: 49 batch 294 batch_loss: 0.1919071078300476\n",
      "training: 49 batch 295 batch_loss: 0.1935466229915619\n",
      "training: 49 batch 296 batch_loss: 0.1940779685974121\n",
      "training: 49 batch 297 batch_loss: 0.18788927793502808\n",
      "training: 49 batch 298 batch_loss: 0.19528120756149292\n",
      "training: 49 batch 299 batch_loss: 0.19469603896141052\n",
      "training: 49 batch 300 batch_loss: 0.1887928545475006\n",
      "training: 49 batch 301 batch_loss: 0.1943737268447876\n",
      "training: 49 batch 302 batch_loss: 0.1903705894947052\n",
      "training: 49 batch 303 batch_loss: 0.19057494401931763\n",
      "training: 49 batch 304 batch_loss: 0.19327238202095032\n",
      "training: 49 batch 305 batch_loss: 0.18915045261383057\n",
      "training: 49 batch 306 batch_loss: 0.19151073694229126\n",
      "training: 49 batch 307 batch_loss: 0.19230502843856812\n",
      "training: 49 batch 308 batch_loss: 0.19333717226982117\n",
      "training: 49 batch 309 batch_loss: 0.18824192881584167\n",
      "training: 49 batch 310 batch_loss: 0.19554302096366882\n",
      "training: 49 batch 311 batch_loss: 0.19095468521118164\n",
      "training: 49 batch 312 batch_loss: 0.19373643398284912\n",
      "training: 49 batch 313 batch_loss: 0.1919286549091339\n",
      "training: 49 batch 314 batch_loss: 0.19421467185020447\n",
      "training: 49 batch 315 batch_loss: 0.19230914115905762\n",
      "training: 49 batch 316 batch_loss: 0.19110536575317383\n",
      "training: 49 batch 317 batch_loss: 0.19290781021118164\n",
      "training: 49 batch 318 batch_loss: 0.19374078512191772\n",
      "training: 49 batch 319 batch_loss: 0.19380038976669312\n",
      "training: 49 batch 320 batch_loss: 0.19047406315803528\n",
      "training: 49 batch 321 batch_loss: 0.19568336009979248\n",
      "training: 49 batch 322 batch_loss: 0.19415658712387085\n",
      "training: 49 batch 323 batch_loss: 0.19764035940170288\n",
      "training: 49 batch 324 batch_loss: 0.19334053993225098\n",
      "training: 49 batch 325 batch_loss: 0.19170495867729187\n",
      "training: 49 batch 326 batch_loss: 0.19110926985740662\n",
      "training: 49 batch 327 batch_loss: 0.19088050723075867\n",
      "training: 49 batch 328 batch_loss: 0.19188526272773743\n",
      "training: 49 batch 329 batch_loss: 0.19139662384986877\n",
      "training: 49 batch 330 batch_loss: 0.19173747301101685\n",
      "training: 49 batch 331 batch_loss: 0.1945597529411316\n",
      "training: 49 batch 332 batch_loss: 0.19022440910339355\n",
      "training: 49 batch 333 batch_loss: 0.19261938333511353\n",
      "training: 49 batch 334 batch_loss: 0.18912416696548462\n",
      "training: 49 batch 335 batch_loss: 0.1920381784439087\n",
      "training: 49 batch 336 batch_loss: 0.1869526505470276\n",
      "training: 49 batch 337 batch_loss: 0.19360074400901794\n",
      "training: 49 batch 338 batch_loss: 0.1954735517501831\n",
      "training: 49 batch 339 batch_loss: 0.1934691071510315\n",
      "training: 49 batch 340 batch_loss: 0.18861228227615356\n",
      "training: 49 batch 341 batch_loss: 0.1924435794353485\n",
      "training: 49 batch 342 batch_loss: 0.19595187902450562\n",
      "training: 49 batch 343 batch_loss: 0.19428080320358276\n",
      "training: 49 batch 344 batch_loss: 0.190929114818573\n",
      "training: 49 batch 345 batch_loss: 0.1900286078453064\n",
      "training: 49 batch 346 batch_loss: 0.18855249881744385\n",
      "training: 49 batch 347 batch_loss: 0.1900079846382141\n",
      "training: 49 batch 348 batch_loss: 0.19064423441886902\n",
      "training: 49 batch 349 batch_loss: 0.1955874264240265\n",
      "training: 49 batch 350 batch_loss: 0.1967075765132904\n",
      "training: 49 batch 351 batch_loss: 0.1932814121246338\n",
      "training: 49 batch 352 batch_loss: 0.19232556223869324\n",
      "training: 49 batch 353 batch_loss: 0.1922977864742279\n",
      "training: 49 batch 354 batch_loss: 0.1894819140434265\n",
      "training: 49 batch 355 batch_loss: 0.18868952989578247\n",
      "training: 49 batch 356 batch_loss: 0.18831020593643188\n",
      "training: 49 batch 357 batch_loss: 0.1902455985546112\n",
      "training: 49 batch 358 batch_loss: 0.19519823789596558\n",
      "training: 49 batch 359 batch_loss: 0.19415691494941711\n",
      "training: 49 batch 360 batch_loss: 0.19552060961723328\n",
      "training: 49 batch 361 batch_loss: 0.19626963138580322\n",
      "training: 49 batch 362 batch_loss: 0.18931031227111816\n",
      "training: 49 batch 363 batch_loss: 0.18885329365730286\n",
      "training: 49 batch 364 batch_loss: 0.19709047675132751\n",
      "training: 49 batch 365 batch_loss: 0.18844333291053772\n",
      "training: 49 batch 366 batch_loss: 0.1929687261581421\n",
      "training: 49 batch 367 batch_loss: 0.19333937764167786\n",
      "training: 49 batch 368 batch_loss: 0.1905013620853424\n",
      "training: 49 batch 369 batch_loss: 0.19343355298042297\n",
      "training: 49 batch 370 batch_loss: 0.1888270378112793\n",
      "training: 49 batch 371 batch_loss: 0.19244828820228577\n",
      "training: 49 batch 372 batch_loss: 0.18994271755218506\n",
      "training: 49 batch 373 batch_loss: 0.19029676914215088\n",
      "training: 49 batch 374 batch_loss: 0.19042044878005981\n",
      "training: 49 batch 375 batch_loss: 0.19037240743637085\n",
      "training: 49 batch 376 batch_loss: 0.19070249795913696\n",
      "training: 49 batch 377 batch_loss: 0.19129157066345215\n",
      "training: 49 batch 378 batch_loss: 0.19443809986114502\n",
      "training: 49 batch 379 batch_loss: 0.19303163886070251\n",
      "training: 49 batch 380 batch_loss: 0.18876159191131592\n",
      "training: 49 batch 381 batch_loss: 0.19105151295661926\n",
      "training: 49 batch 382 batch_loss: 0.19121336936950684\n",
      "training: 49 batch 383 batch_loss: 0.1881905496120453\n",
      "training: 49 batch 384 batch_loss: 0.19030582904815674\n",
      "training: 49 batch 385 batch_loss: 0.19350194931030273\n",
      "training: 49 batch 386 batch_loss: 0.19043803215026855\n",
      "training: 49 batch 387 batch_loss: 0.1890832483768463\n",
      "training: 49 batch 388 batch_loss: 0.19300538301467896\n",
      "training: 49 batch 389 batch_loss: 0.194338858127594\n",
      "training: 49 batch 390 batch_loss: 0.19246819615364075\n",
      "training: 49 batch 391 batch_loss: 0.19759994745254517\n",
      "training: 49 batch 392 batch_loss: 0.1917993128299713\n",
      "training: 49 batch 393 batch_loss: 0.19415315985679626\n",
      "training: 49 batch 394 batch_loss: 0.1895555853843689\n",
      "training: 49 batch 395 batch_loss: 0.1931651532649994\n",
      "training: 49 batch 396 batch_loss: 0.19606411457061768\n",
      "training: 49 batch 397 batch_loss: 0.1892349123954773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 49 batch 398 batch_loss: 0.19060194492340088\n",
      "training: 49 batch 399 batch_loss: 0.19379201531410217\n",
      "training: 49 batch 400 batch_loss: 0.1947985589504242\n",
      "training: 49 batch 401 batch_loss: 0.19469648599624634\n",
      "training: 49 batch 402 batch_loss: 0.19174176454544067\n",
      "training: 49 batch 403 batch_loss: 0.19576221704483032\n",
      "training: 49 batch 404 batch_loss: 0.19424381852149963\n",
      "training: 49 batch 405 batch_loss: 0.18750375509262085\n",
      "training: 49 batch 406 batch_loss: 0.19285541772842407\n",
      "training: 49 batch 407 batch_loss: 0.19096583127975464\n",
      "training: 49 batch 408 batch_loss: 0.19078528881072998\n",
      "training: 49 batch 409 batch_loss: 0.19253703951835632\n",
      "training: 49 batch 410 batch_loss: 0.18927806615829468\n",
      "training: 49 batch 411 batch_loss: 0.19220000505447388\n",
      "training: 49 batch 412 batch_loss: 0.18971660733222961\n",
      "training: 49 batch 413 batch_loss: 0.19116699695587158\n",
      "training: 49 batch 414 batch_loss: 0.1876518726348877\n",
      "training: 49 batch 415 batch_loss: 0.19146224856376648\n",
      "training: 49 batch 416 batch_loss: 0.18931663036346436\n",
      "training: 49 batch 417 batch_loss: 0.1926853060722351\n",
      "training: 49 batch 418 batch_loss: 0.1885853111743927\n",
      "training: 49 batch 419 batch_loss: 0.19105881452560425\n",
      "training: 49 batch 420 batch_loss: 0.19214272499084473\n",
      "training: 49 batch 421 batch_loss: 0.19032615423202515\n",
      "training: 49 batch 422 batch_loss: 0.1944068968296051\n",
      "training: 49 batch 423 batch_loss: 0.19122055172920227\n",
      "training: 49 batch 424 batch_loss: 0.1914425492286682\n",
      "training: 49 batch 425 batch_loss: 0.18993890285491943\n",
      "training: 49 batch 426 batch_loss: 0.19255167245864868\n",
      "training: 49 batch 427 batch_loss: 0.19279363751411438\n",
      "training: 49 batch 428 batch_loss: 0.19077712297439575\n",
      "training: 49 batch 429 batch_loss: 0.18890157341957092\n",
      "training: 49 batch 430 batch_loss: 0.19060716032981873\n",
      "training: 49 batch 431 batch_loss: 0.1896762251853943\n",
      "training: 49 batch 432 batch_loss: 0.18769317865371704\n",
      "training: 49 batch 433 batch_loss: 0.1910175085067749\n",
      "training: 49 batch 434 batch_loss: 0.19230103492736816\n",
      "training: 49 batch 435 batch_loss: 0.19152256846427917\n",
      "training: 49 batch 436 batch_loss: 0.1899447739124298\n",
      "training: 49 batch 437 batch_loss: 0.19289156794548035\n",
      "training: 49 batch 438 batch_loss: 0.19016796350479126\n",
      "training: 49 batch 439 batch_loss: 0.190818190574646\n",
      "training: 49 batch 440 batch_loss: 0.19027620553970337\n",
      "training: 49 batch 441 batch_loss: 0.1938876509666443\n",
      "training: 49 batch 442 batch_loss: 0.19503647089004517\n",
      "training: 49 batch 443 batch_loss: 0.19774234294891357\n",
      "training: 49 batch 444 batch_loss: 0.18960750102996826\n",
      "training: 49 batch 445 batch_loss: 0.19560092687606812\n",
      "training: 49 batch 446 batch_loss: 0.19076210260391235\n",
      "training: 49 batch 447 batch_loss: 0.1963597536087036\n",
      "training: 49 batch 448 batch_loss: 0.19208687543869019\n",
      "training: 49 batch 449 batch_loss: 0.1947159767150879\n",
      "training: 49 batch 450 batch_loss: 0.19366243481636047\n",
      "training: 49 batch 451 batch_loss: 0.1926925778388977\n",
      "training: 49 batch 452 batch_loss: 0.1949443519115448\n",
      "training: 49 batch 453 batch_loss: 0.19212177395820618\n",
      "training: 49 batch 454 batch_loss: 0.1933380663394928\n",
      "training: 49 batch 455 batch_loss: 0.19186371564865112\n",
      "training: 49 batch 456 batch_loss: 0.19260486960411072\n",
      "training: 49 batch 457 batch_loss: 0.1940399408340454\n",
      "training: 49 batch 458 batch_loss: 0.19181746244430542\n",
      "training: 49 batch 459 batch_loss: 0.18575984239578247\n",
      "training: 49 batch 460 batch_loss: 0.18993842601776123\n",
      "training: 49 batch 461 batch_loss: 0.1919330358505249\n",
      "training: 49 batch 462 batch_loss: 0.18970340490341187\n",
      "training: 49 batch 463 batch_loss: 0.19232308864593506\n",
      "training: 49 batch 464 batch_loss: 0.18866193294525146\n",
      "training: 49 batch 465 batch_loss: 0.19519221782684326\n",
      "training: 49 batch 466 batch_loss: 0.18951967358589172\n",
      "training: 49 batch 467 batch_loss: 0.19295772910118103\n",
      "training: 49 batch 468 batch_loss: 0.19616511464118958\n",
      "training: 49 batch 469 batch_loss: 0.1937868893146515\n",
      "training: 49 batch 470 batch_loss: 0.19219177961349487\n",
      "training: 49 batch 471 batch_loss: 0.19224262237548828\n",
      "training: 49 batch 472 batch_loss: 0.19332921504974365\n",
      "training: 49 batch 473 batch_loss: 0.18978074193000793\n",
      "training: 49 batch 474 batch_loss: 0.19381999969482422\n",
      "training: 49 batch 475 batch_loss: 0.1971634328365326\n",
      "training: 49 batch 476 batch_loss: 0.19093048572540283\n",
      "training: 49 batch 477 batch_loss: 0.19277822971343994\n",
      "training: 49 batch 478 batch_loss: 0.19192036986351013\n",
      "training: 49 batch 479 batch_loss: 0.1902725100517273\n",
      "training: 49 batch 480 batch_loss: 0.19071784615516663\n",
      "training: 49 batch 481 batch_loss: 0.19211304187774658\n",
      "training: 49 batch 482 batch_loss: 0.19486543536186218\n",
      "training: 49 batch 483 batch_loss: 0.1924670934677124\n",
      "training: 49 batch 484 batch_loss: 0.1907760500907898\n",
      "training: 49 batch 485 batch_loss: 0.19383496046066284\n",
      "training: 49 batch 486 batch_loss: 0.19272685050964355\n",
      "training: 49 batch 487 batch_loss: 0.1910475492477417\n",
      "training: 49 batch 488 batch_loss: 0.19349172711372375\n",
      "training: 49 batch 489 batch_loss: 0.19163480401039124\n",
      "training: 49 batch 490 batch_loss: 0.19451135396957397\n",
      "training: 49 batch 491 batch_loss: 0.19201812148094177\n",
      "training: 49 batch 492 batch_loss: 0.19293266534805298\n",
      "training: 49 batch 493 batch_loss: 0.1936742663383484\n",
      "training: 49 batch 494 batch_loss: 0.19004809856414795\n",
      "training: 49 batch 495 batch_loss: 0.19117987155914307\n",
      "training: 49 batch 496 batch_loss: 0.18648114800453186\n",
      "training: 49 batch 497 batch_loss: 0.19104614853858948\n",
      "training: 49 batch 498 batch_loss: 0.18973901867866516\n",
      "training: 49 batch 499 batch_loss: 0.19279277324676514\n",
      "training: 49 batch 500 batch_loss: 0.1944090723991394\n",
      "training: 49 batch 501 batch_loss: 0.1921127438545227\n",
      "training: 49 batch 502 batch_loss: 0.1909676194190979\n",
      "training: 49 batch 503 batch_loss: 0.1935192048549652\n",
      "training: 49 batch 504 batch_loss: 0.19254282116889954\n",
      "training: 49 batch 505 batch_loss: 0.19693568348884583\n",
      "training: 49 batch 506 batch_loss: 0.19287702441215515\n",
      "training: 49 batch 507 batch_loss: 0.19442009925842285\n",
      "training: 49 batch 508 batch_loss: 0.19650068879127502\n",
      "training: 49 batch 509 batch_loss: 0.19515502452850342\n",
      "training: 49 batch 510 batch_loss: 0.19119125604629517\n",
      "training: 49 batch 511 batch_loss: 0.19246405363082886\n",
      "training: 49 batch 512 batch_loss: 0.1916201114654541\n",
      "training: 49 batch 513 batch_loss: 0.19236141443252563\n",
      "training: 49 batch 514 batch_loss: 0.19233077764511108\n",
      "training: 49 batch 515 batch_loss: 0.19473224878311157\n",
      "training: 49 batch 516 batch_loss: 0.19290509819984436\n",
      "training: 49 batch 517 batch_loss: 0.19168147444725037\n",
      "training: 49 batch 518 batch_loss: 0.18927645683288574\n",
      "training: 49 batch 519 batch_loss: 0.19116461277008057\n",
      "training: 49 batch 520 batch_loss: 0.1930575966835022\n",
      "training: 49 batch 521 batch_loss: 0.1911473274230957\n",
      "training: 49 batch 522 batch_loss: 0.19231003522872925\n",
      "training: 49 batch 523 batch_loss: 0.19104063510894775\n",
      "training: 49 batch 524 batch_loss: 0.19510847330093384\n",
      "training: 49 batch 525 batch_loss: 0.1936226487159729\n",
      "training: 49 batch 526 batch_loss: 0.19513201713562012\n",
      "training: 49 batch 527 batch_loss: 0.19509893655776978\n",
      "training: 49 batch 528 batch_loss: 0.19282415509223938\n",
      "training: 49 batch 529 batch_loss: 0.19507169723510742\n",
      "training: 49 batch 530 batch_loss: 0.19500529766082764\n",
      "training: 49 batch 531 batch_loss: 0.1904579997062683\n",
      "training: 49 batch 532 batch_loss: 0.1919737160205841\n",
      "training: 49 batch 533 batch_loss: 0.19240829348564148\n",
      "training: 49 batch 534 batch_loss: 0.1950421929359436\n",
      "training: 49 batch 535 batch_loss: 0.19404616951942444\n",
      "training: 49 batch 536 batch_loss: 0.19524043798446655\n",
      "training: 49 batch 537 batch_loss: 0.19339773058891296\n",
      "training: 49 batch 538 batch_loss: 0.19119250774383545\n",
      "training: 49 batch 539 batch_loss: 0.19092363119125366\n",
      "training: 49 batch 540 batch_loss: 0.1915270984172821\n",
      "training: 49 batch 541 batch_loss: 0.19246715307235718\n",
      "training: 49 batch 542 batch_loss: 0.19069835543632507\n",
      "training: 49 batch 543 batch_loss: 0.1981310248374939\n",
      "training: 49 batch 544 batch_loss: 0.1953178346157074\n",
      "training: 49 batch 545 batch_loss: 0.19175797700881958\n",
      "training: 49 batch 546 batch_loss: 0.1934087574481964\n",
      "training: 49 batch 547 batch_loss: 0.19266372919082642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 49 batch 548 batch_loss: 0.1916581392288208\n",
      "training: 49 batch 549 batch_loss: 0.19163841009140015\n",
      "training: 49 batch 550 batch_loss: 0.1940290927886963\n",
      "training: 49 batch 551 batch_loss: 0.19501462578773499\n",
      "training: 49 batch 552 batch_loss: 0.19393420219421387\n",
      "training: 49 batch 553 batch_loss: 0.1942131221294403\n",
      "training: 49 batch 554 batch_loss: 0.19357702136039734\n",
      "training: 49 batch 555 batch_loss: 0.19342359900474548\n",
      "training: 49 batch 556 batch_loss: 0.1907220482826233\n",
      "training: 49 batch 557 batch_loss: 0.19209188222885132\n",
      "training: 49 batch 558 batch_loss: 0.19463101029396057\n",
      "training: 49 batch 559 batch_loss: 0.19437822699546814\n",
      "training: 49 batch 560 batch_loss: 0.19221588969230652\n",
      "training: 49 batch 561 batch_loss: 0.19439709186553955\n",
      "training: 49 batch 562 batch_loss: 0.19191810488700867\n",
      "training: 49 batch 563 batch_loss: 0.19322621822357178\n",
      "training: 49 batch 564 batch_loss: 0.1935429871082306\n",
      "training: 49 batch 565 batch_loss: 0.19474613666534424\n",
      "training: 49 batch 566 batch_loss: 0.19423338770866394\n",
      "training: 49 batch 567 batch_loss: 0.19394657015800476\n",
      "training: 49 batch 568 batch_loss: 0.1940276026725769\n",
      "training: 49 batch 569 batch_loss: 0.19330978393554688\n",
      "training: 49 batch 570 batch_loss: 0.19240784645080566\n",
      "training: 49 batch 571 batch_loss: 0.1903843879699707\n",
      "training: 49 batch 572 batch_loss: 0.1905146837234497\n",
      "training: 49 batch 573 batch_loss: 0.19549942016601562\n",
      "training: 49 batch 574 batch_loss: 0.18927878141403198\n",
      "training: 49 batch 575 batch_loss: 0.19073212146759033\n",
      "training: 49 batch 576 batch_loss: 0.18880555033683777\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 49, Hit Ratio:0.032479105109046516 | Precision:0.04792096726629313 | Recall:0.06353025789214156 | NDCG:0.061899475319776374\n",
      "*Best Performance* \n",
      "Epoch: 49, Hit Ratio:0.032479105109046516 | Precision:0.04792096726629313 | Recall:0.06353025789214156 | MDCG:0.061899475319776374\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 50 batch 0 batch_loss: 0.1919371485710144\n",
      "training: 50 batch 1 batch_loss: 0.18856659531593323\n",
      "training: 50 batch 2 batch_loss: 0.19155848026275635\n",
      "training: 50 batch 3 batch_loss: 0.1917763650417328\n",
      "training: 50 batch 4 batch_loss: 0.18789547681808472\n",
      "training: 50 batch 5 batch_loss: 0.1890476644039154\n",
      "training: 50 batch 6 batch_loss: 0.193928062915802\n",
      "training: 50 batch 7 batch_loss: 0.19017699360847473\n",
      "training: 50 batch 8 batch_loss: 0.19198352098464966\n",
      "training: 50 batch 9 batch_loss: 0.1902363896369934\n",
      "training: 50 batch 10 batch_loss: 0.187136709690094\n",
      "training: 50 batch 11 batch_loss: 0.18956854939460754\n",
      "training: 50 batch 12 batch_loss: 0.1932784616947174\n",
      "training: 50 batch 13 batch_loss: 0.18922823667526245\n",
      "training: 50 batch 14 batch_loss: 0.19360342621803284\n",
      "training: 50 batch 15 batch_loss: 0.1905830204486847\n",
      "training: 50 batch 16 batch_loss: 0.19003808498382568\n",
      "training: 50 batch 17 batch_loss: 0.19095146656036377\n",
      "training: 50 batch 18 batch_loss: 0.1907879114151001\n",
      "training: 50 batch 19 batch_loss: 0.19181504845619202\n",
      "training: 50 batch 20 batch_loss: 0.19350850582122803\n",
      "training: 50 batch 21 batch_loss: 0.1925867199897766\n",
      "training: 50 batch 22 batch_loss: 0.1939023733139038\n",
      "training: 50 batch 23 batch_loss: 0.1918548047542572\n",
      "training: 50 batch 24 batch_loss: 0.19228294491767883\n",
      "training: 50 batch 25 batch_loss: 0.19316330552101135\n",
      "training: 50 batch 26 batch_loss: 0.19449633359909058\n",
      "training: 50 batch 27 batch_loss: 0.19077587127685547\n",
      "training: 50 batch 28 batch_loss: 0.19002068042755127\n",
      "training: 50 batch 29 batch_loss: 0.1885395646095276\n",
      "training: 50 batch 30 batch_loss: 0.1914914846420288\n",
      "training: 50 batch 31 batch_loss: 0.19261494278907776\n",
      "training: 50 batch 32 batch_loss: 0.1913226842880249\n",
      "training: 50 batch 33 batch_loss: 0.1890849471092224\n",
      "training: 50 batch 34 batch_loss: 0.19425812363624573\n",
      "training: 50 batch 35 batch_loss: 0.1909644603729248\n",
      "training: 50 batch 36 batch_loss: 0.19125956296920776\n",
      "training: 50 batch 37 batch_loss: 0.19287839531898499\n",
      "training: 50 batch 38 batch_loss: 0.1933409869670868\n",
      "training: 50 batch 39 batch_loss: 0.19104018807411194\n",
      "training: 50 batch 40 batch_loss: 0.18899694085121155\n",
      "training: 50 batch 41 batch_loss: 0.18887344002723694\n",
      "training: 50 batch 42 batch_loss: 0.19334089756011963\n",
      "training: 50 batch 43 batch_loss: 0.19118034839630127\n",
      "training: 50 batch 44 batch_loss: 0.1891964077949524\n",
      "training: 50 batch 45 batch_loss: 0.19084399938583374\n",
      "training: 50 batch 46 batch_loss: 0.1897226870059967\n",
      "training: 50 batch 47 batch_loss: 0.1907537579536438\n",
      "training: 50 batch 48 batch_loss: 0.19094979763031006\n",
      "training: 50 batch 49 batch_loss: 0.1911468803882599\n",
      "training: 50 batch 50 batch_loss: 0.19562989473342896\n",
      "training: 50 batch 51 batch_loss: 0.19232481718063354\n",
      "training: 50 batch 52 batch_loss: 0.19429662823677063\n",
      "training: 50 batch 53 batch_loss: 0.18977072834968567\n",
      "training: 50 batch 54 batch_loss: 0.18803834915161133\n",
      "training: 50 batch 55 batch_loss: 0.1920013427734375\n",
      "training: 50 batch 56 batch_loss: 0.19195878505706787\n",
      "training: 50 batch 57 batch_loss: 0.18952980637550354\n",
      "training: 50 batch 58 batch_loss: 0.19120126962661743\n",
      "training: 50 batch 59 batch_loss: 0.19170749187469482\n",
      "training: 50 batch 60 batch_loss: 0.19293212890625\n",
      "training: 50 batch 61 batch_loss: 0.19202297925949097\n",
      "training: 50 batch 62 batch_loss: 0.19086936116218567\n",
      "training: 50 batch 63 batch_loss: 0.19400876760482788\n",
      "training: 50 batch 64 batch_loss: 0.18955284357070923\n",
      "training: 50 batch 65 batch_loss: 0.19056618213653564\n",
      "training: 50 batch 66 batch_loss: 0.190743088722229\n",
      "training: 50 batch 67 batch_loss: 0.19283205270767212\n",
      "training: 50 batch 68 batch_loss: 0.19201132655143738\n",
      "training: 50 batch 69 batch_loss: 0.19150784611701965\n",
      "training: 50 batch 70 batch_loss: 0.19379237294197083\n",
      "training: 50 batch 71 batch_loss: 0.19580650329589844\n",
      "training: 50 batch 72 batch_loss: 0.1891627311706543\n",
      "training: 50 batch 73 batch_loss: 0.1907140016555786\n",
      "training: 50 batch 74 batch_loss: 0.18833136558532715\n",
      "training: 50 batch 75 batch_loss: 0.18903928995132446\n",
      "training: 50 batch 76 batch_loss: 0.1963104009628296\n",
      "training: 50 batch 77 batch_loss: 0.19538235664367676\n",
      "training: 50 batch 78 batch_loss: 0.1927821934223175\n",
      "training: 50 batch 79 batch_loss: 0.19038763642311096\n",
      "training: 50 batch 80 batch_loss: 0.19049334526062012\n",
      "training: 50 batch 81 batch_loss: 0.19235986471176147\n",
      "training: 50 batch 82 batch_loss: 0.18800723552703857\n",
      "training: 50 batch 83 batch_loss: 0.1909109354019165\n",
      "training: 50 batch 84 batch_loss: 0.19164425134658813\n",
      "training: 50 batch 85 batch_loss: 0.19015076756477356\n",
      "training: 50 batch 86 batch_loss: 0.18905198574066162\n",
      "training: 50 batch 87 batch_loss: 0.19034907221794128\n",
      "training: 50 batch 88 batch_loss: 0.19382315874099731\n",
      "training: 50 batch 89 batch_loss: 0.19027107954025269\n",
      "training: 50 batch 90 batch_loss: 0.19210585951805115\n",
      "training: 50 batch 91 batch_loss: 0.18681874871253967\n",
      "training: 50 batch 92 batch_loss: 0.19257959723472595\n",
      "training: 50 batch 93 batch_loss: 0.19420304894447327\n",
      "training: 50 batch 94 batch_loss: 0.194488525390625\n",
      "training: 50 batch 95 batch_loss: 0.19219332933425903\n",
      "training: 50 batch 96 batch_loss: 0.19145208597183228\n",
      "training: 50 batch 97 batch_loss: 0.18854832649230957\n",
      "training: 50 batch 98 batch_loss: 0.19181501865386963\n",
      "training: 50 batch 99 batch_loss: 0.18840932846069336\n",
      "training: 50 batch 100 batch_loss: 0.19164606928825378\n",
      "training: 50 batch 101 batch_loss: 0.19220459461212158\n",
      "training: 50 batch 102 batch_loss: 0.19141051173210144\n",
      "training: 50 batch 103 batch_loss: 0.19395962357521057\n",
      "training: 50 batch 104 batch_loss: 0.19224295020103455\n",
      "training: 50 batch 105 batch_loss: 0.18998625874519348\n",
      "training: 50 batch 106 batch_loss: 0.1899283528327942\n",
      "training: 50 batch 107 batch_loss: 0.19699251651763916\n",
      "training: 50 batch 108 batch_loss: 0.1929970383644104\n",
      "training: 50 batch 109 batch_loss: 0.18967479467391968\n",
      "training: 50 batch 110 batch_loss: 0.19114550948143005\n",
      "training: 50 batch 111 batch_loss: 0.19224438071250916\n",
      "training: 50 batch 112 batch_loss: 0.19173261523246765\n",
      "training: 50 batch 113 batch_loss: 0.1911819577217102\n",
      "training: 50 batch 114 batch_loss: 0.19578859210014343\n",
      "training: 50 batch 115 batch_loss: 0.19352242350578308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 50 batch 116 batch_loss: 0.1895434856414795\n",
      "training: 50 batch 117 batch_loss: 0.19195714592933655\n",
      "training: 50 batch 118 batch_loss: 0.1920126974582672\n",
      "training: 50 batch 119 batch_loss: 0.19044053554534912\n",
      "training: 50 batch 120 batch_loss: 0.18981099128723145\n",
      "training: 50 batch 121 batch_loss: 0.1897430717945099\n",
      "training: 50 batch 122 batch_loss: 0.1892644762992859\n",
      "training: 50 batch 123 batch_loss: 0.19355350732803345\n",
      "training: 50 batch 124 batch_loss: 0.18985527753829956\n",
      "training: 50 batch 125 batch_loss: 0.19689828157424927\n",
      "training: 50 batch 126 batch_loss: 0.19042688608169556\n",
      "training: 50 batch 127 batch_loss: 0.1920146942138672\n",
      "training: 50 batch 128 batch_loss: 0.19316190481185913\n",
      "training: 50 batch 129 batch_loss: 0.1954376995563507\n",
      "training: 50 batch 130 batch_loss: 0.18766456842422485\n",
      "training: 50 batch 131 batch_loss: 0.18881219625473022\n",
      "training: 50 batch 132 batch_loss: 0.19121816754341125\n",
      "training: 50 batch 133 batch_loss: 0.18789345026016235\n",
      "training: 50 batch 134 batch_loss: 0.19583767652511597\n",
      "training: 50 batch 135 batch_loss: 0.1922948956489563\n",
      "training: 50 batch 136 batch_loss: 0.19228854775428772\n",
      "training: 50 batch 137 batch_loss: 0.19171735644340515\n",
      "training: 50 batch 138 batch_loss: 0.19139546155929565\n",
      "training: 50 batch 139 batch_loss: 0.1932660937309265\n",
      "training: 50 batch 140 batch_loss: 0.19383400678634644\n",
      "training: 50 batch 141 batch_loss: 0.19430717825889587\n",
      "training: 50 batch 142 batch_loss: 0.18839487433433533\n",
      "training: 50 batch 143 batch_loss: 0.1925957202911377\n",
      "training: 50 batch 144 batch_loss: 0.1935860812664032\n",
      "training: 50 batch 145 batch_loss: 0.1907520294189453\n",
      "training: 50 batch 146 batch_loss: 0.19156986474990845\n",
      "training: 50 batch 147 batch_loss: 0.19400081038475037\n",
      "training: 50 batch 148 batch_loss: 0.19140303134918213\n",
      "training: 50 batch 149 batch_loss: 0.19408395886421204\n",
      "training: 50 batch 150 batch_loss: 0.19192415475845337\n",
      "training: 50 batch 151 batch_loss: 0.19273489713668823\n",
      "training: 50 batch 152 batch_loss: 0.18995895981788635\n",
      "training: 50 batch 153 batch_loss: 0.19226998090744019\n",
      "training: 50 batch 154 batch_loss: 0.194219708442688\n",
      "training: 50 batch 155 batch_loss: 0.1909102201461792\n",
      "training: 50 batch 156 batch_loss: 0.19629645347595215\n",
      "training: 50 batch 157 batch_loss: 0.19235724210739136\n",
      "training: 50 batch 158 batch_loss: 0.1945141851902008\n",
      "training: 50 batch 159 batch_loss: 0.19336175918579102\n",
      "training: 50 batch 160 batch_loss: 0.19293835759162903\n",
      "training: 50 batch 161 batch_loss: 0.1911117434501648\n",
      "training: 50 batch 162 batch_loss: 0.19359904527664185\n",
      "training: 50 batch 163 batch_loss: 0.19569966197013855\n",
      "training: 50 batch 164 batch_loss: 0.19241195917129517\n",
      "training: 50 batch 165 batch_loss: 0.19362086057662964\n",
      "training: 50 batch 166 batch_loss: 0.19015643000602722\n",
      "training: 50 batch 167 batch_loss: 0.1897030770778656\n",
      "training: 50 batch 168 batch_loss: 0.1920613944530487\n",
      "training: 50 batch 169 batch_loss: 0.1961132287979126\n",
      "training: 50 batch 170 batch_loss: 0.18960893154144287\n",
      "training: 50 batch 171 batch_loss: 0.1887010633945465\n",
      "training: 50 batch 172 batch_loss: 0.1936270296573639\n",
      "training: 50 batch 173 batch_loss: 0.1937810778617859\n",
      "training: 50 batch 174 batch_loss: 0.18915393948554993\n",
      "training: 50 batch 175 batch_loss: 0.19164234399795532\n",
      "training: 50 batch 176 batch_loss: 0.1932482123374939\n",
      "training: 50 batch 177 batch_loss: 0.18842238187789917\n",
      "training: 50 batch 178 batch_loss: 0.19366547465324402\n",
      "training: 50 batch 179 batch_loss: 0.18595778942108154\n",
      "training: 50 batch 180 batch_loss: 0.19186222553253174\n",
      "training: 50 batch 181 batch_loss: 0.19291743636131287\n",
      "training: 50 batch 182 batch_loss: 0.19509515166282654\n",
      "training: 50 batch 183 batch_loss: 0.19254335761070251\n",
      "training: 50 batch 184 batch_loss: 0.18832850456237793\n",
      "training: 50 batch 185 batch_loss: 0.19367951154708862\n",
      "training: 50 batch 186 batch_loss: 0.19312253594398499\n",
      "training: 50 batch 187 batch_loss: 0.1897335648536682\n",
      "training: 50 batch 188 batch_loss: 0.19267398118972778\n",
      "training: 50 batch 189 batch_loss: 0.19003397226333618\n",
      "training: 50 batch 190 batch_loss: 0.19223445653915405\n",
      "training: 50 batch 191 batch_loss: 0.1918802559375763\n",
      "training: 50 batch 192 batch_loss: 0.19432345032691956\n",
      "training: 50 batch 193 batch_loss: 0.194549560546875\n",
      "training: 50 batch 194 batch_loss: 0.18910357356071472\n",
      "training: 50 batch 195 batch_loss: 0.1915816366672516\n",
      "training: 50 batch 196 batch_loss: 0.19184160232543945\n",
      "training: 50 batch 197 batch_loss: 0.1911269724369049\n",
      "training: 50 batch 198 batch_loss: 0.18859249353408813\n",
      "training: 50 batch 199 batch_loss: 0.19272801280021667\n",
      "training: 50 batch 200 batch_loss: 0.19333791732788086\n",
      "training: 50 batch 201 batch_loss: 0.1907077431678772\n",
      "training: 50 batch 202 batch_loss: 0.19314640760421753\n",
      "training: 50 batch 203 batch_loss: 0.19255995750427246\n",
      "training: 50 batch 204 batch_loss: 0.19060879945755005\n",
      "training: 50 batch 205 batch_loss: 0.19173145294189453\n",
      "training: 50 batch 206 batch_loss: 0.19313660264015198\n",
      "training: 50 batch 207 batch_loss: 0.19412800669670105\n",
      "training: 50 batch 208 batch_loss: 0.18971660733222961\n",
      "training: 50 batch 209 batch_loss: 0.1935957670211792\n",
      "training: 50 batch 210 batch_loss: 0.19508841633796692\n",
      "training: 50 batch 211 batch_loss: 0.19792702794075012\n",
      "training: 50 batch 212 batch_loss: 0.19122880697250366\n",
      "training: 50 batch 213 batch_loss: 0.1899568736553192\n",
      "training: 50 batch 214 batch_loss: 0.1853865385055542\n",
      "training: 50 batch 215 batch_loss: 0.19810032844543457\n",
      "training: 50 batch 216 batch_loss: 0.18798178434371948\n",
      "training: 50 batch 217 batch_loss: 0.19944316148757935\n",
      "training: 50 batch 218 batch_loss: 0.19311225414276123\n",
      "training: 50 batch 219 batch_loss: 0.18930155038833618\n",
      "training: 50 batch 220 batch_loss: 0.19229257106781006\n",
      "training: 50 batch 221 batch_loss: 0.1907660961151123\n",
      "training: 50 batch 222 batch_loss: 0.1955876648426056\n",
      "training: 50 batch 223 batch_loss: 0.192994624376297\n",
      "training: 50 batch 224 batch_loss: 0.1911277174949646\n",
      "training: 50 batch 225 batch_loss: 0.1914859414100647\n",
      "training: 50 batch 226 batch_loss: 0.1963042914867401\n",
      "training: 50 batch 227 batch_loss: 0.1912890374660492\n",
      "training: 50 batch 228 batch_loss: 0.19124382734298706\n",
      "training: 50 batch 229 batch_loss: 0.1942862868309021\n",
      "training: 50 batch 230 batch_loss: 0.19012987613677979\n",
      "training: 50 batch 231 batch_loss: 0.19604095816612244\n",
      "training: 50 batch 232 batch_loss: 0.1915251910686493\n",
      "training: 50 batch 233 batch_loss: 0.19265002012252808\n",
      "training: 50 batch 234 batch_loss: 0.19505718350410461\n",
      "training: 50 batch 235 batch_loss: 0.19157513976097107\n",
      "training: 50 batch 236 batch_loss: 0.19079676270484924\n",
      "training: 50 batch 237 batch_loss: 0.18888568878173828\n",
      "training: 50 batch 238 batch_loss: 0.19231608510017395\n",
      "training: 50 batch 239 batch_loss: 0.18852031230926514\n",
      "training: 50 batch 240 batch_loss: 0.1929478645324707\n",
      "training: 50 batch 241 batch_loss: 0.19463887810707092\n",
      "training: 50 batch 242 batch_loss: 0.19693276286125183\n",
      "training: 50 batch 243 batch_loss: 0.19429606199264526\n",
      "training: 50 batch 244 batch_loss: 0.19336751103401184\n",
      "training: 50 batch 245 batch_loss: 0.19302767515182495\n",
      "training: 50 batch 246 batch_loss: 0.19107338786125183\n",
      "training: 50 batch 247 batch_loss: 0.19407376646995544\n",
      "training: 50 batch 248 batch_loss: 0.1935161054134369\n",
      "training: 50 batch 249 batch_loss: 0.1908971667289734\n",
      "training: 50 batch 250 batch_loss: 0.19162264466285706\n",
      "training: 50 batch 251 batch_loss: 0.19104334712028503\n",
      "training: 50 batch 252 batch_loss: 0.18958815932273865\n",
      "training: 50 batch 253 batch_loss: 0.192929208278656\n",
      "training: 50 batch 254 batch_loss: 0.19339504837989807\n",
      "training: 50 batch 255 batch_loss: 0.19213220477104187\n",
      "training: 50 batch 256 batch_loss: 0.19149842858314514\n",
      "training: 50 batch 257 batch_loss: 0.1894332766532898\n",
      "training: 50 batch 258 batch_loss: 0.19371077418327332\n",
      "training: 50 batch 259 batch_loss: 0.193560391664505\n",
      "training: 50 batch 260 batch_loss: 0.19187450408935547\n",
      "training: 50 batch 261 batch_loss: 0.19223853945732117\n",
      "training: 50 batch 262 batch_loss: 0.19110137224197388\n",
      "training: 50 batch 263 batch_loss: 0.1883101761341095\n",
      "training: 50 batch 264 batch_loss: 0.18746718764305115\n",
      "training: 50 batch 265 batch_loss: 0.192880779504776\n",
      "training: 50 batch 266 batch_loss: 0.19492149353027344\n",
      "training: 50 batch 267 batch_loss: 0.19368606805801392\n",
      "training: 50 batch 268 batch_loss: 0.1950886845588684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 50 batch 269 batch_loss: 0.19096916913986206\n",
      "training: 50 batch 270 batch_loss: 0.19156873226165771\n",
      "training: 50 batch 271 batch_loss: 0.18949806690216064\n",
      "training: 50 batch 272 batch_loss: 0.18980377912521362\n",
      "training: 50 batch 273 batch_loss: 0.19217750430107117\n",
      "training: 50 batch 274 batch_loss: 0.19126784801483154\n",
      "training: 50 batch 275 batch_loss: 0.1937832236289978\n",
      "training: 50 batch 276 batch_loss: 0.19487887620925903\n",
      "training: 50 batch 277 batch_loss: 0.19543710350990295\n",
      "training: 50 batch 278 batch_loss: 0.19186770915985107\n",
      "training: 50 batch 279 batch_loss: 0.18975847959518433\n",
      "training: 50 batch 280 batch_loss: 0.1898919939994812\n",
      "training: 50 batch 281 batch_loss: 0.19572702050209045\n",
      "training: 50 batch 282 batch_loss: 0.19535014033317566\n",
      "training: 50 batch 283 batch_loss: 0.19276392459869385\n",
      "training: 50 batch 284 batch_loss: 0.19223174452781677\n",
      "training: 50 batch 285 batch_loss: 0.19153833389282227\n",
      "training: 50 batch 286 batch_loss: 0.19239014387130737\n",
      "training: 50 batch 287 batch_loss: 0.19316762685775757\n",
      "training: 50 batch 288 batch_loss: 0.19739454984664917\n",
      "training: 50 batch 289 batch_loss: 0.19319790601730347\n",
      "training: 50 batch 290 batch_loss: 0.19025832414627075\n",
      "training: 50 batch 291 batch_loss: 0.19066613912582397\n",
      "training: 50 batch 292 batch_loss: 0.195470929145813\n",
      "training: 50 batch 293 batch_loss: 0.1979772448539734\n",
      "training: 50 batch 294 batch_loss: 0.1893804967403412\n",
      "training: 50 batch 295 batch_loss: 0.1893015205860138\n",
      "training: 50 batch 296 batch_loss: 0.19408240914344788\n",
      "training: 50 batch 297 batch_loss: 0.1924234926700592\n",
      "training: 50 batch 298 batch_loss: 0.19446036219596863\n",
      "training: 50 batch 299 batch_loss: 0.19260916113853455\n",
      "training: 50 batch 300 batch_loss: 0.19055891036987305\n",
      "training: 50 batch 301 batch_loss: 0.1907350718975067\n",
      "training: 50 batch 302 batch_loss: 0.1935993731021881\n",
      "training: 50 batch 303 batch_loss: 0.19458192586898804\n",
      "training: 50 batch 304 batch_loss: 0.1932617425918579\n",
      "training: 50 batch 305 batch_loss: 0.19121474027633667\n",
      "training: 50 batch 306 batch_loss: 0.1931016743183136\n",
      "training: 50 batch 307 batch_loss: 0.18912318348884583\n",
      "training: 50 batch 308 batch_loss: 0.18796414136886597\n",
      "training: 50 batch 309 batch_loss: 0.19431766867637634\n",
      "training: 50 batch 310 batch_loss: 0.19428634643554688\n",
      "training: 50 batch 311 batch_loss: 0.19382032752037048\n",
      "training: 50 batch 312 batch_loss: 0.19149985909461975\n",
      "training: 50 batch 313 batch_loss: 0.19720888137817383\n",
      "training: 50 batch 314 batch_loss: 0.19556990265846252\n",
      "training: 50 batch 315 batch_loss: 0.19534721970558167\n",
      "training: 50 batch 316 batch_loss: 0.19465327262878418\n",
      "training: 50 batch 317 batch_loss: 0.1948593258857727\n",
      "training: 50 batch 318 batch_loss: 0.19027268886566162\n",
      "training: 50 batch 319 batch_loss: 0.19000035524368286\n",
      "training: 50 batch 320 batch_loss: 0.19332310557365417\n",
      "training: 50 batch 321 batch_loss: 0.1934448778629303\n",
      "training: 50 batch 322 batch_loss: 0.19338658452033997\n",
      "training: 50 batch 323 batch_loss: 0.194463312625885\n",
      "training: 50 batch 324 batch_loss: 0.19191047549247742\n",
      "training: 50 batch 325 batch_loss: 0.19304850697517395\n",
      "training: 50 batch 326 batch_loss: 0.1918727159500122\n",
      "training: 50 batch 327 batch_loss: 0.19415098428726196\n",
      "training: 50 batch 328 batch_loss: 0.18951880931854248\n",
      "training: 50 batch 329 batch_loss: 0.19334676861763\n",
      "training: 50 batch 330 batch_loss: 0.1949559450149536\n",
      "training: 50 batch 331 batch_loss: 0.1924819052219391\n",
      "training: 50 batch 332 batch_loss: 0.19554054737091064\n",
      "training: 50 batch 333 batch_loss: 0.1926605999469757\n",
      "training: 50 batch 334 batch_loss: 0.19609475135803223\n",
      "training: 50 batch 335 batch_loss: 0.1942850947380066\n",
      "training: 50 batch 336 batch_loss: 0.19519802927970886\n",
      "training: 50 batch 337 batch_loss: 0.1931666135787964\n",
      "training: 50 batch 338 batch_loss: 0.1948869824409485\n",
      "training: 50 batch 339 batch_loss: 0.19295993447303772\n",
      "training: 50 batch 340 batch_loss: 0.19217464327812195\n",
      "training: 50 batch 341 batch_loss: 0.1925564706325531\n",
      "training: 50 batch 342 batch_loss: 0.19399699568748474\n",
      "training: 50 batch 343 batch_loss: 0.19287109375\n",
      "training: 50 batch 344 batch_loss: 0.19241908192634583\n",
      "training: 50 batch 345 batch_loss: 0.1948704719543457\n",
      "training: 50 batch 346 batch_loss: 0.1925027072429657\n",
      "training: 50 batch 347 batch_loss: 0.1910800337791443\n",
      "training: 50 batch 348 batch_loss: 0.19648051261901855\n",
      "training: 50 batch 349 batch_loss: 0.1938551962375641\n",
      "training: 50 batch 350 batch_loss: 0.19191434979438782\n",
      "training: 50 batch 351 batch_loss: 0.1950719952583313\n",
      "training: 50 batch 352 batch_loss: 0.18990564346313477\n",
      "training: 50 batch 353 batch_loss: 0.18802767992019653\n",
      "training: 50 batch 354 batch_loss: 0.19420453906059265\n",
      "training: 50 batch 355 batch_loss: 0.19164353609085083\n",
      "training: 50 batch 356 batch_loss: 0.19295647740364075\n",
      "training: 50 batch 357 batch_loss: 0.19193780422210693\n",
      "training: 50 batch 358 batch_loss: 0.19345173239707947\n",
      "training: 50 batch 359 batch_loss: 0.19196093082427979\n",
      "training: 50 batch 360 batch_loss: 0.1913260817527771\n",
      "training: 50 batch 361 batch_loss: 0.19214749336242676\n",
      "training: 50 batch 362 batch_loss: 0.19194361567497253\n",
      "training: 50 batch 363 batch_loss: 0.20022624731063843\n",
      "training: 50 batch 364 batch_loss: 0.19339460134506226\n",
      "training: 50 batch 365 batch_loss: 0.19255396723747253\n",
      "training: 50 batch 366 batch_loss: 0.19246551394462585\n",
      "training: 50 batch 367 batch_loss: 0.1937706470489502\n",
      "training: 50 batch 368 batch_loss: 0.18952178955078125\n",
      "training: 50 batch 369 batch_loss: 0.192123144865036\n",
      "training: 50 batch 370 batch_loss: 0.19373342394828796\n",
      "training: 50 batch 371 batch_loss: 0.18869662284851074\n",
      "training: 50 batch 372 batch_loss: 0.19579029083251953\n",
      "training: 50 batch 373 batch_loss: 0.18959301710128784\n",
      "training: 50 batch 374 batch_loss: 0.1933717131614685\n",
      "training: 50 batch 375 batch_loss: 0.18966025114059448\n",
      "training: 50 batch 376 batch_loss: 0.19197878241539001\n",
      "training: 50 batch 377 batch_loss: 0.19421792030334473\n",
      "training: 50 batch 378 batch_loss: 0.19532924890518188\n",
      "training: 50 batch 379 batch_loss: 0.19528737664222717\n",
      "training: 50 batch 380 batch_loss: 0.19366541504859924\n",
      "training: 50 batch 381 batch_loss: 0.1928592026233673\n",
      "training: 50 batch 382 batch_loss: 0.19267317652702332\n",
      "training: 50 batch 383 batch_loss: 0.1921207308769226\n",
      "training: 50 batch 384 batch_loss: 0.19108733534812927\n",
      "training: 50 batch 385 batch_loss: 0.19181591272354126\n",
      "training: 50 batch 386 batch_loss: 0.1959262192249298\n",
      "training: 50 batch 387 batch_loss: 0.19282293319702148\n",
      "training: 50 batch 388 batch_loss: 0.1953032910823822\n",
      "training: 50 batch 389 batch_loss: 0.19425112009048462\n",
      "training: 50 batch 390 batch_loss: 0.19338127970695496\n",
      "training: 50 batch 391 batch_loss: 0.19487398862838745\n",
      "training: 50 batch 392 batch_loss: 0.1901630163192749\n",
      "training: 50 batch 393 batch_loss: 0.19332298636436462\n",
      "training: 50 batch 394 batch_loss: 0.1918589472770691\n",
      "training: 50 batch 395 batch_loss: 0.19047591090202332\n",
      "training: 50 batch 396 batch_loss: 0.19176599383354187\n",
      "training: 50 batch 397 batch_loss: 0.19225919246673584\n",
      "training: 50 batch 398 batch_loss: 0.19653385877609253\n",
      "training: 50 batch 399 batch_loss: 0.19336208701133728\n",
      "training: 50 batch 400 batch_loss: 0.18820416927337646\n",
      "training: 50 batch 401 batch_loss: 0.1955585479736328\n",
      "training: 50 batch 402 batch_loss: 0.19202125072479248\n",
      "training: 50 batch 403 batch_loss: 0.1902795135974884\n",
      "training: 50 batch 404 batch_loss: 0.1918729841709137\n",
      "training: 50 batch 405 batch_loss: 0.19438815116882324\n",
      "training: 50 batch 406 batch_loss: 0.1944078803062439\n",
      "training: 50 batch 407 batch_loss: 0.19278153777122498\n",
      "training: 50 batch 408 batch_loss: 0.19387927651405334\n",
      "training: 50 batch 409 batch_loss: 0.18982341885566711\n",
      "training: 50 batch 410 batch_loss: 0.19289112091064453\n",
      "training: 50 batch 411 batch_loss: 0.19192349910736084\n",
      "training: 50 batch 412 batch_loss: 0.19462242722511292\n",
      "training: 50 batch 413 batch_loss: 0.1974511444568634\n",
      "training: 50 batch 414 batch_loss: 0.1920807957649231\n",
      "training: 50 batch 415 batch_loss: 0.19435063004493713\n",
      "training: 50 batch 416 batch_loss: 0.1930922269821167\n",
      "training: 50 batch 417 batch_loss: 0.1892758309841156\n",
      "training: 50 batch 418 batch_loss: 0.191852867603302\n",
      "training: 50 batch 419 batch_loss: 0.19035109877586365\n",
      "training: 50 batch 420 batch_loss: 0.19159594178199768\n",
      "training: 50 batch 421 batch_loss: 0.19695210456848145\n",
      "training: 50 batch 422 batch_loss: 0.19491294026374817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 50 batch 423 batch_loss: 0.19536089897155762\n",
      "training: 50 batch 424 batch_loss: 0.18900138139724731\n",
      "training: 50 batch 425 batch_loss: 0.19336950778961182\n",
      "training: 50 batch 426 batch_loss: 0.19692513346672058\n",
      "training: 50 batch 427 batch_loss: 0.19746631383895874\n",
      "training: 50 batch 428 batch_loss: 0.19552838802337646\n",
      "training: 50 batch 429 batch_loss: 0.19482824206352234\n",
      "training: 50 batch 430 batch_loss: 0.1917707920074463\n",
      "training: 50 batch 431 batch_loss: 0.19652047753334045\n",
      "training: 50 batch 432 batch_loss: 0.19190621376037598\n",
      "training: 50 batch 433 batch_loss: 0.19334527850151062\n",
      "training: 50 batch 434 batch_loss: 0.19372758269309998\n",
      "training: 50 batch 435 batch_loss: 0.19581863284111023\n",
      "training: 50 batch 436 batch_loss: 0.1886318325996399\n",
      "training: 50 batch 437 batch_loss: 0.18746355175971985\n",
      "training: 50 batch 438 batch_loss: 0.1906459927558899\n",
      "training: 50 batch 439 batch_loss: 0.19599467515945435\n",
      "training: 50 batch 440 batch_loss: 0.19721576571464539\n",
      "training: 50 batch 441 batch_loss: 0.19584280252456665\n",
      "training: 50 batch 442 batch_loss: 0.1919047236442566\n",
      "training: 50 batch 443 batch_loss: 0.19898730516433716\n",
      "training: 50 batch 444 batch_loss: 0.19008785486221313\n",
      "training: 50 batch 445 batch_loss: 0.19144988059997559\n",
      "training: 50 batch 446 batch_loss: 0.19313842058181763\n",
      "training: 50 batch 447 batch_loss: 0.19337761402130127\n",
      "training: 50 batch 448 batch_loss: 0.19577279686927795\n",
      "training: 50 batch 449 batch_loss: 0.18865031003952026\n",
      "training: 50 batch 450 batch_loss: 0.19283193349838257\n",
      "training: 50 batch 451 batch_loss: 0.1923966407775879\n",
      "training: 50 batch 452 batch_loss: 0.19473475217819214\n",
      "training: 50 batch 453 batch_loss: 0.1885271966457367\n",
      "training: 50 batch 454 batch_loss: 0.19396725296974182\n",
      "training: 50 batch 455 batch_loss: 0.19464921951293945\n",
      "training: 50 batch 456 batch_loss: 0.1994915008544922\n",
      "training: 50 batch 457 batch_loss: 0.1921410858631134\n",
      "training: 50 batch 458 batch_loss: 0.19949829578399658\n",
      "training: 50 batch 459 batch_loss: 0.19508838653564453\n",
      "training: 50 batch 460 batch_loss: 0.19172322750091553\n",
      "training: 50 batch 461 batch_loss: 0.1931571066379547\n",
      "training: 50 batch 462 batch_loss: 0.19400906562805176\n",
      "training: 50 batch 463 batch_loss: 0.19737085700035095\n",
      "training: 50 batch 464 batch_loss: 0.1963939368724823\n",
      "training: 50 batch 465 batch_loss: 0.190497487783432\n",
      "training: 50 batch 466 batch_loss: 0.19819870591163635\n",
      "training: 50 batch 467 batch_loss: 0.19191312789916992\n",
      "training: 50 batch 468 batch_loss: 0.18818256258964539\n",
      "training: 50 batch 469 batch_loss: 0.19305747747421265\n",
      "training: 50 batch 470 batch_loss: 0.19447895884513855\n",
      "training: 50 batch 471 batch_loss: 0.19356930255889893\n",
      "training: 50 batch 472 batch_loss: 0.19463777542114258\n",
      "training: 50 batch 473 batch_loss: 0.19164180755615234\n",
      "training: 50 batch 474 batch_loss: 0.1957409679889679\n",
      "training: 50 batch 475 batch_loss: 0.19363585114479065\n",
      "training: 50 batch 476 batch_loss: 0.19312134385108948\n",
      "training: 50 batch 477 batch_loss: 0.19470256567001343\n",
      "training: 50 batch 478 batch_loss: 0.19421729445457458\n",
      "training: 50 batch 479 batch_loss: 0.19279655814170837\n",
      "training: 50 batch 480 batch_loss: 0.19197291135787964\n",
      "training: 50 batch 481 batch_loss: 0.19515907764434814\n",
      "training: 50 batch 482 batch_loss: 0.19353485107421875\n",
      "training: 50 batch 483 batch_loss: 0.19553056359291077\n",
      "training: 50 batch 484 batch_loss: 0.19184496998786926\n",
      "training: 50 batch 485 batch_loss: 0.19412770867347717\n",
      "training: 50 batch 486 batch_loss: 0.19719311594963074\n",
      "training: 50 batch 487 batch_loss: 0.19762635231018066\n",
      "training: 50 batch 488 batch_loss: 0.18899792432785034\n",
      "training: 50 batch 489 batch_loss: 0.19476556777954102\n",
      "training: 50 batch 490 batch_loss: 0.19822415709495544\n",
      "training: 50 batch 491 batch_loss: 0.18861877918243408\n",
      "training: 50 batch 492 batch_loss: 0.19410017132759094\n",
      "training: 50 batch 493 batch_loss: 0.19299039244651794\n",
      "training: 50 batch 494 batch_loss: 0.19448095560073853\n",
      "training: 50 batch 495 batch_loss: 0.19195875525474548\n",
      "training: 50 batch 496 batch_loss: 0.19276148080825806\n",
      "training: 50 batch 497 batch_loss: 0.1941579282283783\n",
      "training: 50 batch 498 batch_loss: 0.19159731268882751\n",
      "training: 50 batch 499 batch_loss: 0.19501203298568726\n",
      "training: 50 batch 500 batch_loss: 0.19656133651733398\n",
      "training: 50 batch 501 batch_loss: 0.19414448738098145\n",
      "training: 50 batch 502 batch_loss: 0.1938333511352539\n",
      "training: 50 batch 503 batch_loss: 0.18892240524291992\n",
      "training: 50 batch 504 batch_loss: 0.1947024166584015\n",
      "training: 50 batch 505 batch_loss: 0.1901424527168274\n",
      "training: 50 batch 506 batch_loss: 0.19304513931274414\n",
      "training: 50 batch 507 batch_loss: 0.1908811330795288\n",
      "training: 50 batch 508 batch_loss: 0.19218537211418152\n",
      "training: 50 batch 509 batch_loss: 0.1911403238773346\n",
      "training: 50 batch 510 batch_loss: 0.19355279207229614\n",
      "training: 50 batch 511 batch_loss: 0.19122570753097534\n",
      "training: 50 batch 512 batch_loss: 0.19286566972732544\n",
      "training: 50 batch 513 batch_loss: 0.19551846385002136\n",
      "training: 50 batch 514 batch_loss: 0.194219172000885\n",
      "training: 50 batch 515 batch_loss: 0.19628748297691345\n",
      "training: 50 batch 516 batch_loss: 0.19456344842910767\n",
      "training: 50 batch 517 batch_loss: 0.1928134262561798\n",
      "training: 50 batch 518 batch_loss: 0.19575577974319458\n",
      "training: 50 batch 519 batch_loss: 0.19620442390441895\n",
      "training: 50 batch 520 batch_loss: 0.19375953078269958\n",
      "training: 50 batch 521 batch_loss: 0.19064170122146606\n",
      "training: 50 batch 522 batch_loss: 0.19432112574577332\n",
      "training: 50 batch 523 batch_loss: 0.1921069324016571\n",
      "training: 50 batch 524 batch_loss: 0.19639089703559875\n",
      "training: 50 batch 525 batch_loss: 0.19565096497535706\n",
      "training: 50 batch 526 batch_loss: 0.19581115245819092\n",
      "training: 50 batch 527 batch_loss: 0.19275251030921936\n",
      "training: 50 batch 528 batch_loss: 0.1933247447013855\n",
      "training: 50 batch 529 batch_loss: 0.19867897033691406\n",
      "training: 50 batch 530 batch_loss: 0.19228321313858032\n",
      "training: 50 batch 531 batch_loss: 0.19638153910636902\n",
      "training: 50 batch 532 batch_loss: 0.1924443244934082\n",
      "training: 50 batch 533 batch_loss: 0.19275003671646118\n",
      "training: 50 batch 534 batch_loss: 0.19172945618629456\n",
      "training: 50 batch 535 batch_loss: 0.19006377458572388\n",
      "training: 50 batch 536 batch_loss: 0.19283443689346313\n",
      "training: 50 batch 537 batch_loss: 0.19425606727600098\n",
      "training: 50 batch 538 batch_loss: 0.1965540647506714\n",
      "training: 50 batch 539 batch_loss: 0.19011938571929932\n",
      "training: 50 batch 540 batch_loss: 0.19517090916633606\n",
      "training: 50 batch 541 batch_loss: 0.19567042589187622\n",
      "training: 50 batch 542 batch_loss: 0.19570130109786987\n",
      "training: 50 batch 543 batch_loss: 0.196304053068161\n",
      "training: 50 batch 544 batch_loss: 0.19819846749305725\n",
      "training: 50 batch 545 batch_loss: 0.19615188241004944\n",
      "training: 50 batch 546 batch_loss: 0.1942722201347351\n",
      "training: 50 batch 547 batch_loss: 0.19385915994644165\n",
      "training: 50 batch 548 batch_loss: 0.19387787580490112\n",
      "training: 50 batch 549 batch_loss: 0.194549560546875\n",
      "training: 50 batch 550 batch_loss: 0.19697478413581848\n",
      "training: 50 batch 551 batch_loss: 0.18956518173217773\n",
      "training: 50 batch 552 batch_loss: 0.19572380185127258\n",
      "training: 50 batch 553 batch_loss: 0.19741755723953247\n",
      "training: 50 batch 554 batch_loss: 0.1950942575931549\n",
      "training: 50 batch 555 batch_loss: 0.1979677975177765\n",
      "training: 50 batch 556 batch_loss: 0.19516265392303467\n",
      "training: 50 batch 557 batch_loss: 0.19411876797676086\n",
      "training: 50 batch 558 batch_loss: 0.1921634078025818\n",
      "training: 50 batch 559 batch_loss: 0.1924392282962799\n",
      "training: 50 batch 560 batch_loss: 0.1956084966659546\n",
      "training: 50 batch 561 batch_loss: 0.19339463114738464\n",
      "training: 50 batch 562 batch_loss: 0.19253787398338318\n",
      "training: 50 batch 563 batch_loss: 0.19233235716819763\n",
      "training: 50 batch 564 batch_loss: 0.19555553793907166\n",
      "training: 50 batch 565 batch_loss: 0.1985614001750946\n",
      "training: 50 batch 566 batch_loss: 0.19353356957435608\n",
      "training: 50 batch 567 batch_loss: 0.19236579537391663\n",
      "training: 50 batch 568 batch_loss: 0.19812482595443726\n",
      "training: 50 batch 569 batch_loss: 0.1941809058189392\n",
      "training: 50 batch 570 batch_loss: 0.19226038455963135\n",
      "training: 50 batch 571 batch_loss: 0.19323596358299255\n",
      "training: 50 batch 572 batch_loss: 0.19019603729248047\n",
      "training: 50 batch 573 batch_loss: 0.19230714440345764\n",
      "training: 50 batch 574 batch_loss: 0.19584167003631592\n",
      "training: 50 batch 575 batch_loss: 0.1967831254005432\n",
      "training: 50 batch 576 batch_loss: 0.19553855061531067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 50, Hit Ratio:0.032395825352356654 | Precision:0.04779809299125135 | Recall:0.06357051459988748 | NDCG:0.06169761783246518\n",
      "*Best Performance* \n",
      "Epoch: 49, Hit Ratio:0.032479105109046516 | Precision:0.04792096726629313 | Recall:0.06353025789214156 | MDCG:0.061899475319776374\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 51 batch 0 batch_loss: 0.19290077686309814\n",
      "training: 51 batch 1 batch_loss: 0.18952631950378418\n",
      "training: 51 batch 2 batch_loss: 0.18755698204040527\n",
      "training: 51 batch 3 batch_loss: 0.19539862871170044\n",
      "training: 51 batch 4 batch_loss: 0.19196143746376038\n",
      "training: 51 batch 5 batch_loss: 0.1948431134223938\n",
      "training: 51 batch 6 batch_loss: 0.19189155101776123\n",
      "training: 51 batch 7 batch_loss: 0.1911008059978485\n",
      "training: 51 batch 8 batch_loss: 0.19189047813415527\n",
      "training: 51 batch 9 batch_loss: 0.19103574752807617\n",
      "training: 51 batch 10 batch_loss: 0.1913687288761139\n",
      "training: 51 batch 11 batch_loss: 0.19053536653518677\n",
      "training: 51 batch 12 batch_loss: 0.19381576776504517\n",
      "training: 51 batch 13 batch_loss: 0.1879866123199463\n",
      "training: 51 batch 14 batch_loss: 0.19040650129318237\n",
      "training: 51 batch 15 batch_loss: 0.19251596927642822\n",
      "training: 51 batch 16 batch_loss: 0.19533848762512207\n",
      "training: 51 batch 17 batch_loss: 0.1934572160243988\n",
      "training: 51 batch 18 batch_loss: 0.1962025761604309\n",
      "training: 51 batch 19 batch_loss: 0.1936700940132141\n",
      "training: 51 batch 20 batch_loss: 0.19605296850204468\n",
      "training: 51 batch 21 batch_loss: 0.18932375311851501\n",
      "training: 51 batch 22 batch_loss: 0.19235515594482422\n",
      "training: 51 batch 23 batch_loss: 0.19147351384162903\n",
      "training: 51 batch 24 batch_loss: 0.18906763195991516\n",
      "training: 51 batch 25 batch_loss: 0.19178569316864014\n",
      "training: 51 batch 26 batch_loss: 0.1923212707042694\n",
      "training: 51 batch 27 batch_loss: 0.19690030813217163\n",
      "training: 51 batch 28 batch_loss: 0.18798327445983887\n",
      "training: 51 batch 29 batch_loss: 0.19343698024749756\n",
      "training: 51 batch 30 batch_loss: 0.19398516416549683\n",
      "training: 51 batch 31 batch_loss: 0.19186162948608398\n",
      "training: 51 batch 32 batch_loss: 0.19009685516357422\n",
      "training: 51 batch 33 batch_loss: 0.19113770127296448\n",
      "training: 51 batch 34 batch_loss: 0.19397851824760437\n",
      "training: 51 batch 35 batch_loss: 0.19002032279968262\n",
      "training: 51 batch 36 batch_loss: 0.1925521194934845\n",
      "training: 51 batch 37 batch_loss: 0.19398725032806396\n",
      "training: 51 batch 38 batch_loss: 0.19165927171707153\n",
      "training: 51 batch 39 batch_loss: 0.191150963306427\n",
      "training: 51 batch 40 batch_loss: 0.1910575032234192\n",
      "training: 51 batch 41 batch_loss: 0.19079110026359558\n",
      "training: 51 batch 42 batch_loss: 0.18855983018875122\n",
      "training: 51 batch 43 batch_loss: 0.1950705647468567\n",
      "training: 51 batch 44 batch_loss: 0.193660706281662\n",
      "training: 51 batch 45 batch_loss: 0.1931762397289276\n",
      "training: 51 batch 46 batch_loss: 0.188579261302948\n",
      "training: 51 batch 47 batch_loss: 0.1967896819114685\n",
      "training: 51 batch 48 batch_loss: 0.19463253021240234\n",
      "training: 51 batch 49 batch_loss: 0.19212713837623596\n",
      "training: 51 batch 50 batch_loss: 0.18821164965629578\n",
      "training: 51 batch 51 batch_loss: 0.1933995485305786\n",
      "training: 51 batch 52 batch_loss: 0.19217056035995483\n",
      "training: 51 batch 53 batch_loss: 0.19209760427474976\n",
      "training: 51 batch 54 batch_loss: 0.19324004650115967\n",
      "training: 51 batch 55 batch_loss: 0.18965664505958557\n",
      "training: 51 batch 56 batch_loss: 0.19028428196907043\n",
      "training: 51 batch 57 batch_loss: 0.1901634931564331\n",
      "training: 51 batch 58 batch_loss: 0.19032740592956543\n",
      "training: 51 batch 59 batch_loss: 0.19410431385040283\n",
      "training: 51 batch 60 batch_loss: 0.19211691617965698\n",
      "training: 51 batch 61 batch_loss: 0.1944742500782013\n",
      "training: 51 batch 62 batch_loss: 0.1965426206588745\n",
      "training: 51 batch 63 batch_loss: 0.19599959254264832\n",
      "training: 51 batch 64 batch_loss: 0.19631361961364746\n",
      "training: 51 batch 65 batch_loss: 0.19434431195259094\n",
      "training: 51 batch 66 batch_loss: 0.19530275464057922\n",
      "training: 51 batch 67 batch_loss: 0.19207200407981873\n",
      "training: 51 batch 68 batch_loss: 0.19559571146965027\n",
      "training: 51 batch 69 batch_loss: 0.19396129250526428\n",
      "training: 51 batch 70 batch_loss: 0.19155776500701904\n",
      "training: 51 batch 71 batch_loss: 0.19528141617774963\n",
      "training: 51 batch 72 batch_loss: 0.18972879648208618\n",
      "training: 51 batch 73 batch_loss: 0.1928841471672058\n",
      "training: 51 batch 74 batch_loss: 0.19105660915374756\n",
      "training: 51 batch 75 batch_loss: 0.18791010975837708\n",
      "training: 51 batch 76 batch_loss: 0.19160562753677368\n",
      "training: 51 batch 77 batch_loss: 0.19210892915725708\n",
      "training: 51 batch 78 batch_loss: 0.19476640224456787\n",
      "training: 51 batch 79 batch_loss: 0.1887764036655426\n",
      "training: 51 batch 80 batch_loss: 0.19155102968215942\n",
      "training: 51 batch 81 batch_loss: 0.1910514533519745\n",
      "training: 51 batch 82 batch_loss: 0.19075250625610352\n",
      "training: 51 batch 83 batch_loss: 0.19750776886940002\n",
      "training: 51 batch 84 batch_loss: 0.19647875428199768\n",
      "training: 51 batch 85 batch_loss: 0.1926194727420807\n",
      "training: 51 batch 86 batch_loss: 0.19339534640312195\n",
      "training: 51 batch 87 batch_loss: 0.1928090751171112\n",
      "training: 51 batch 88 batch_loss: 0.18676364421844482\n",
      "training: 51 batch 89 batch_loss: 0.1932094693183899\n",
      "training: 51 batch 90 batch_loss: 0.18864157795906067\n",
      "training: 51 batch 91 batch_loss: 0.19373860955238342\n",
      "training: 51 batch 92 batch_loss: 0.19744494557380676\n",
      "training: 51 batch 93 batch_loss: 0.19689956307411194\n",
      "training: 51 batch 94 batch_loss: 0.19146135449409485\n",
      "training: 51 batch 95 batch_loss: 0.1933082938194275\n",
      "training: 51 batch 96 batch_loss: 0.1961337924003601\n",
      "training: 51 batch 97 batch_loss: 0.19456902146339417\n",
      "training: 51 batch 98 batch_loss: 0.19131526350975037\n",
      "training: 51 batch 99 batch_loss: 0.19177651405334473\n",
      "training: 51 batch 100 batch_loss: 0.1969362497329712\n",
      "training: 51 batch 101 batch_loss: 0.19325131177902222\n",
      "training: 51 batch 102 batch_loss: 0.19354784488677979\n",
      "training: 51 batch 103 batch_loss: 0.19093850255012512\n",
      "training: 51 batch 104 batch_loss: 0.1948249638080597\n",
      "training: 51 batch 105 batch_loss: 0.18934115767478943\n",
      "training: 51 batch 106 batch_loss: 0.18903398513793945\n",
      "training: 51 batch 107 batch_loss: 0.19164186716079712\n",
      "training: 51 batch 108 batch_loss: 0.19558459520339966\n",
      "training: 51 batch 109 batch_loss: 0.1933867335319519\n",
      "training: 51 batch 110 batch_loss: 0.19353985786437988\n",
      "training: 51 batch 111 batch_loss: 0.19507735967636108\n",
      "training: 51 batch 112 batch_loss: 0.19337588548660278\n",
      "training: 51 batch 113 batch_loss: 0.1923164427280426\n",
      "training: 51 batch 114 batch_loss: 0.1934063732624054\n",
      "training: 51 batch 115 batch_loss: 0.19183191657066345\n",
      "training: 51 batch 116 batch_loss: 0.19506624341011047\n",
      "training: 51 batch 117 batch_loss: 0.19430756568908691\n",
      "training: 51 batch 118 batch_loss: 0.19599205255508423\n",
      "training: 51 batch 119 batch_loss: 0.19149771332740784\n",
      "training: 51 batch 120 batch_loss: 0.1963699460029602\n",
      "training: 51 batch 121 batch_loss: 0.19383695721626282\n",
      "training: 51 batch 122 batch_loss: 0.19160601496696472\n",
      "training: 51 batch 123 batch_loss: 0.19506987929344177\n",
      "training: 51 batch 124 batch_loss: 0.19512394070625305\n",
      "training: 51 batch 125 batch_loss: 0.1940220594406128\n",
      "training: 51 batch 126 batch_loss: 0.19334572553634644\n",
      "training: 51 batch 127 batch_loss: 0.19141840934753418\n",
      "training: 51 batch 128 batch_loss: 0.19066232442855835\n",
      "training: 51 batch 129 batch_loss: 0.19357353448867798\n",
      "training: 51 batch 130 batch_loss: 0.1921379566192627\n",
      "training: 51 batch 131 batch_loss: 0.19807985424995422\n",
      "training: 51 batch 132 batch_loss: 0.19479608535766602\n",
      "training: 51 batch 133 batch_loss: 0.19675633311271667\n",
      "training: 51 batch 134 batch_loss: 0.18960940837860107\n",
      "training: 51 batch 135 batch_loss: 0.19541436433792114\n",
      "training: 51 batch 136 batch_loss: 0.1939331293106079\n",
      "training: 51 batch 137 batch_loss: 0.19094905257225037\n",
      "training: 51 batch 138 batch_loss: 0.19864904880523682\n",
      "training: 51 batch 139 batch_loss: 0.1912274956703186\n",
      "training: 51 batch 140 batch_loss: 0.1902886927127838\n",
      "training: 51 batch 141 batch_loss: 0.19492578506469727\n",
      "training: 51 batch 142 batch_loss: 0.19422724843025208\n",
      "training: 51 batch 143 batch_loss: 0.19059798121452332\n",
      "training: 51 batch 144 batch_loss: 0.1930752992630005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 51 batch 145 batch_loss: 0.19106724858283997\n",
      "training: 51 batch 146 batch_loss: 0.19158512353897095\n",
      "training: 51 batch 147 batch_loss: 0.19224703311920166\n",
      "training: 51 batch 148 batch_loss: 0.1919316053390503\n",
      "training: 51 batch 149 batch_loss: 0.19520077109336853\n",
      "training: 51 batch 150 batch_loss: 0.19082000851631165\n",
      "training: 51 batch 151 batch_loss: 0.19343465566635132\n",
      "training: 51 batch 152 batch_loss: 0.19225746393203735\n",
      "training: 51 batch 153 batch_loss: 0.19083231687545776\n",
      "training: 51 batch 154 batch_loss: 0.1958640217781067\n",
      "training: 51 batch 155 batch_loss: 0.19285917282104492\n",
      "training: 51 batch 156 batch_loss: 0.1931355893611908\n",
      "training: 51 batch 157 batch_loss: 0.19020462036132812\n",
      "training: 51 batch 158 batch_loss: 0.19255352020263672\n",
      "training: 51 batch 159 batch_loss: 0.19285288453102112\n",
      "training: 51 batch 160 batch_loss: 0.1887187659740448\n",
      "training: 51 batch 161 batch_loss: 0.19420921802520752\n",
      "training: 51 batch 162 batch_loss: 0.19428500533103943\n",
      "training: 51 batch 163 batch_loss: 0.1899753212928772\n",
      "training: 51 batch 164 batch_loss: 0.19447168707847595\n",
      "training: 51 batch 165 batch_loss: 0.1931443214416504\n",
      "training: 51 batch 166 batch_loss: 0.1909732222557068\n",
      "training: 51 batch 167 batch_loss: 0.19376283884048462\n",
      "training: 51 batch 168 batch_loss: 0.19610494375228882\n",
      "training: 51 batch 169 batch_loss: 0.19509655237197876\n",
      "training: 51 batch 170 batch_loss: 0.19394266605377197\n",
      "training: 51 batch 171 batch_loss: 0.19087880849838257\n",
      "training: 51 batch 172 batch_loss: 0.19492310285568237\n",
      "training: 51 batch 173 batch_loss: 0.19999587535858154\n",
      "training: 51 batch 174 batch_loss: 0.1910582184791565\n",
      "training: 51 batch 175 batch_loss: 0.19506677985191345\n",
      "training: 51 batch 176 batch_loss: 0.19452032446861267\n",
      "training: 51 batch 177 batch_loss: 0.19173747301101685\n",
      "training: 51 batch 178 batch_loss: 0.19234460592269897\n",
      "training: 51 batch 179 batch_loss: 0.19625836610794067\n",
      "training: 51 batch 180 batch_loss: 0.19388774037361145\n",
      "training: 51 batch 181 batch_loss: 0.19138243794441223\n",
      "training: 51 batch 182 batch_loss: 0.19406428933143616\n",
      "training: 51 batch 183 batch_loss: 0.19118371605873108\n",
      "training: 51 batch 184 batch_loss: 0.19490525126457214\n",
      "training: 51 batch 185 batch_loss: 0.19434931874275208\n",
      "training: 51 batch 186 batch_loss: 0.19256559014320374\n",
      "training: 51 batch 187 batch_loss: 0.19297140836715698\n",
      "training: 51 batch 188 batch_loss: 0.19311442971229553\n",
      "training: 51 batch 189 batch_loss: 0.19341826438903809\n",
      "training: 51 batch 190 batch_loss: 0.18885236978530884\n",
      "training: 51 batch 191 batch_loss: 0.19818589091300964\n",
      "training: 51 batch 192 batch_loss: 0.19447413086891174\n",
      "training: 51 batch 193 batch_loss: 0.1934470534324646\n",
      "training: 51 batch 194 batch_loss: 0.1952005922794342\n",
      "training: 51 batch 195 batch_loss: 0.19280487298965454\n",
      "training: 51 batch 196 batch_loss: 0.19196459650993347\n",
      "training: 51 batch 197 batch_loss: 0.19285616278648376\n",
      "training: 51 batch 198 batch_loss: 0.19130012392997742\n",
      "training: 51 batch 199 batch_loss: 0.19549086689949036\n",
      "training: 51 batch 200 batch_loss: 0.19270145893096924\n",
      "training: 51 batch 201 batch_loss: 0.19168949127197266\n",
      "training: 51 batch 202 batch_loss: 0.19252285361289978\n",
      "training: 51 batch 203 batch_loss: 0.19320115447044373\n",
      "training: 51 batch 204 batch_loss: 0.1909509003162384\n",
      "training: 51 batch 205 batch_loss: 0.1917763650417328\n",
      "training: 51 batch 206 batch_loss: 0.1943875551223755\n",
      "training: 51 batch 207 batch_loss: 0.1954866349697113\n",
      "training: 51 batch 208 batch_loss: 0.19280719757080078\n",
      "training: 51 batch 209 batch_loss: 0.19282665848731995\n",
      "training: 51 batch 210 batch_loss: 0.1948242485523224\n",
      "training: 51 batch 211 batch_loss: 0.19443878531455994\n",
      "training: 51 batch 212 batch_loss: 0.1952163577079773\n",
      "training: 51 batch 213 batch_loss: 0.1928466260433197\n",
      "training: 51 batch 214 batch_loss: 0.19454163312911987\n",
      "training: 51 batch 215 batch_loss: 0.191606342792511\n",
      "training: 51 batch 216 batch_loss: 0.19202548265457153\n",
      "training: 51 batch 217 batch_loss: 0.19453439116477966\n",
      "training: 51 batch 218 batch_loss: 0.19233494997024536\n",
      "training: 51 batch 219 batch_loss: 0.1916182041168213\n",
      "training: 51 batch 220 batch_loss: 0.19192442297935486\n",
      "training: 51 batch 221 batch_loss: 0.19890734553337097\n",
      "training: 51 batch 222 batch_loss: 0.19366413354873657\n",
      "training: 51 batch 223 batch_loss: 0.19075676798820496\n",
      "training: 51 batch 224 batch_loss: 0.1942775547504425\n",
      "training: 51 batch 225 batch_loss: 0.19036424160003662\n",
      "training: 51 batch 226 batch_loss: 0.19423604011535645\n",
      "training: 51 batch 227 batch_loss: 0.18888196349143982\n",
      "training: 51 batch 228 batch_loss: 0.19280153512954712\n",
      "training: 51 batch 229 batch_loss: 0.19836533069610596\n",
      "training: 51 batch 230 batch_loss: 0.1954323649406433\n",
      "training: 51 batch 231 batch_loss: 0.1925804615020752\n",
      "training: 51 batch 232 batch_loss: 0.1971733570098877\n",
      "training: 51 batch 233 batch_loss: 0.19297361373901367\n",
      "training: 51 batch 234 batch_loss: 0.19309493899345398\n",
      "training: 51 batch 235 batch_loss: 0.19054767489433289\n",
      "training: 51 batch 236 batch_loss: 0.19259729981422424\n",
      "training: 51 batch 237 batch_loss: 0.18863216042518616\n",
      "training: 51 batch 238 batch_loss: 0.19239541888237\n",
      "training: 51 batch 239 batch_loss: 0.19077017903327942\n",
      "training: 51 batch 240 batch_loss: 0.19800913333892822\n",
      "training: 51 batch 241 batch_loss: 0.19726508855819702\n",
      "training: 51 batch 242 batch_loss: 0.19715315103530884\n",
      "training: 51 batch 243 batch_loss: 0.19217392802238464\n",
      "training: 51 batch 244 batch_loss: 0.19292256236076355\n",
      "training: 51 batch 245 batch_loss: 0.1931973099708557\n",
      "training: 51 batch 246 batch_loss: 0.19515585899353027\n",
      "training: 51 batch 247 batch_loss: 0.1925683617591858\n",
      "training: 51 batch 248 batch_loss: 0.19195842742919922\n",
      "training: 51 batch 249 batch_loss: 0.193223774433136\n",
      "training: 51 batch 250 batch_loss: 0.19600561261177063\n",
      "training: 51 batch 251 batch_loss: 0.19514325261116028\n",
      "training: 51 batch 252 batch_loss: 0.19441521167755127\n",
      "training: 51 batch 253 batch_loss: 0.19050753116607666\n",
      "training: 51 batch 254 batch_loss: 0.19190073013305664\n",
      "training: 51 batch 255 batch_loss: 0.19023004174232483\n",
      "training: 51 batch 256 batch_loss: 0.19572210311889648\n",
      "training: 51 batch 257 batch_loss: 0.19394296407699585\n",
      "training: 51 batch 258 batch_loss: 0.19461321830749512\n",
      "training: 51 batch 259 batch_loss: 0.1907060146331787\n",
      "training: 51 batch 260 batch_loss: 0.1934119164943695\n",
      "training: 51 batch 261 batch_loss: 0.19571584463119507\n",
      "training: 51 batch 262 batch_loss: 0.19223493337631226\n",
      "training: 51 batch 263 batch_loss: 0.19298434257507324\n",
      "training: 51 batch 264 batch_loss: 0.1936841607093811\n",
      "training: 51 batch 265 batch_loss: 0.19055268168449402\n",
      "training: 51 batch 266 batch_loss: 0.19399753212928772\n",
      "training: 51 batch 267 batch_loss: 0.1950993537902832\n",
      "training: 51 batch 268 batch_loss: 0.19403356313705444\n",
      "training: 51 batch 269 batch_loss: 0.19437530636787415\n",
      "training: 51 batch 270 batch_loss: 0.19120612740516663\n",
      "training: 51 batch 271 batch_loss: 0.1942472755908966\n",
      "training: 51 batch 272 batch_loss: 0.194644033908844\n",
      "training: 51 batch 273 batch_loss: 0.1955055594444275\n",
      "training: 51 batch 274 batch_loss: 0.1911654770374298\n",
      "training: 51 batch 275 batch_loss: 0.19135016202926636\n",
      "training: 51 batch 276 batch_loss: 0.19245368242263794\n",
      "training: 51 batch 277 batch_loss: 0.19833680987358093\n",
      "training: 51 batch 278 batch_loss: 0.19230246543884277\n",
      "training: 51 batch 279 batch_loss: 0.19103792309761047\n",
      "training: 51 batch 280 batch_loss: 0.19321012496948242\n",
      "training: 51 batch 281 batch_loss: 0.196419358253479\n",
      "training: 51 batch 282 batch_loss: 0.1944979429244995\n",
      "training: 51 batch 283 batch_loss: 0.1979924440383911\n",
      "training: 51 batch 284 batch_loss: 0.19639143347740173\n",
      "training: 51 batch 285 batch_loss: 0.19348716735839844\n",
      "training: 51 batch 286 batch_loss: 0.1976233422756195\n",
      "training: 51 batch 287 batch_loss: 0.19246691465377808\n",
      "training: 51 batch 288 batch_loss: 0.19514918327331543\n",
      "training: 51 batch 289 batch_loss: 0.19494974613189697\n",
      "training: 51 batch 290 batch_loss: 0.194955974817276\n",
      "training: 51 batch 291 batch_loss: 0.19463565945625305\n",
      "training: 51 batch 292 batch_loss: 0.19061297178268433\n",
      "training: 51 batch 293 batch_loss: 0.1927720606327057\n",
      "training: 51 batch 294 batch_loss: 0.19235733151435852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 51 batch 295 batch_loss: 0.18978509306907654\n",
      "training: 51 batch 296 batch_loss: 0.19633865356445312\n",
      "training: 51 batch 297 batch_loss: 0.19923090934753418\n",
      "training: 51 batch 298 batch_loss: 0.19589489698410034\n",
      "training: 51 batch 299 batch_loss: 0.1944800317287445\n",
      "training: 51 batch 300 batch_loss: 0.19545182585716248\n",
      "training: 51 batch 301 batch_loss: 0.19645866751670837\n",
      "training: 51 batch 302 batch_loss: 0.19143563508987427\n",
      "training: 51 batch 303 batch_loss: 0.1947193443775177\n",
      "training: 51 batch 304 batch_loss: 0.1933078169822693\n",
      "training: 51 batch 305 batch_loss: 0.1976759135723114\n",
      "training: 51 batch 306 batch_loss: 0.19776561856269836\n",
      "training: 51 batch 307 batch_loss: 0.19406437873840332\n",
      "training: 51 batch 308 batch_loss: 0.19797298312187195\n",
      "training: 51 batch 309 batch_loss: 0.19735386967658997\n",
      "training: 51 batch 310 batch_loss: 0.19234168529510498\n",
      "training: 51 batch 311 batch_loss: 0.19811120629310608\n",
      "training: 51 batch 312 batch_loss: 0.19500961899757385\n",
      "training: 51 batch 313 batch_loss: 0.18937426805496216\n",
      "training: 51 batch 314 batch_loss: 0.19282060861587524\n",
      "training: 51 batch 315 batch_loss: 0.19410276412963867\n",
      "training: 51 batch 316 batch_loss: 0.19259744882583618\n",
      "training: 51 batch 317 batch_loss: 0.19533607363700867\n",
      "training: 51 batch 318 batch_loss: 0.19508406519889832\n",
      "training: 51 batch 319 batch_loss: 0.19585919380187988\n",
      "training: 51 batch 320 batch_loss: 0.19476202130317688\n",
      "training: 51 batch 321 batch_loss: 0.19681069254875183\n",
      "training: 51 batch 322 batch_loss: 0.19259214401245117\n",
      "training: 51 batch 323 batch_loss: 0.19233807921409607\n",
      "training: 51 batch 324 batch_loss: 0.1944267749786377\n",
      "training: 51 batch 325 batch_loss: 0.2000339925289154\n",
      "training: 51 batch 326 batch_loss: 0.19029384851455688\n",
      "training: 51 batch 327 batch_loss: 0.19259178638458252\n",
      "training: 51 batch 328 batch_loss: 0.19719302654266357\n",
      "training: 51 batch 329 batch_loss: 0.1925716996192932\n",
      "training: 51 batch 330 batch_loss: 0.1916590929031372\n",
      "training: 51 batch 331 batch_loss: 0.1971939504146576\n",
      "training: 51 batch 332 batch_loss: 0.19608262181282043\n",
      "training: 51 batch 333 batch_loss: 0.19212591648101807\n",
      "training: 51 batch 334 batch_loss: 0.1961137056350708\n",
      "training: 51 batch 335 batch_loss: 0.19621676206588745\n",
      "training: 51 batch 336 batch_loss: 0.188083678483963\n",
      "training: 51 batch 337 batch_loss: 0.195607990026474\n",
      "training: 51 batch 338 batch_loss: 0.1944882571697235\n",
      "training: 51 batch 339 batch_loss: 0.19259732961654663\n",
      "training: 51 batch 340 batch_loss: 0.19845250248908997\n",
      "training: 51 batch 341 batch_loss: 0.19369614124298096\n",
      "training: 51 batch 342 batch_loss: 0.1960906684398651\n",
      "training: 51 batch 343 batch_loss: 0.19237208366394043\n",
      "training: 51 batch 344 batch_loss: 0.19603478908538818\n",
      "training: 51 batch 345 batch_loss: 0.19714221358299255\n",
      "training: 51 batch 346 batch_loss: 0.1973905861377716\n",
      "training: 51 batch 347 batch_loss: 0.19577166438102722\n",
      "training: 51 batch 348 batch_loss: 0.19609642028808594\n",
      "training: 51 batch 349 batch_loss: 0.19577765464782715\n",
      "training: 51 batch 350 batch_loss: 0.1955813765525818\n",
      "training: 51 batch 351 batch_loss: 0.19406795501708984\n",
      "training: 51 batch 352 batch_loss: 0.19661319255828857\n",
      "training: 51 batch 353 batch_loss: 0.19744330644607544\n",
      "training: 51 batch 354 batch_loss: 0.19601893424987793\n",
      "training: 51 batch 355 batch_loss: 0.1941395103931427\n",
      "training: 51 batch 356 batch_loss: 0.19112437963485718\n",
      "training: 51 batch 357 batch_loss: 0.19598954916000366\n",
      "training: 51 batch 358 batch_loss: 0.19281244277954102\n",
      "training: 51 batch 359 batch_loss: 0.18961963057518005\n",
      "training: 51 batch 360 batch_loss: 0.19386547803878784\n",
      "training: 51 batch 361 batch_loss: 0.1962076723575592\n",
      "training: 51 batch 362 batch_loss: 0.19287487864494324\n",
      "training: 51 batch 363 batch_loss: 0.19541636109352112\n",
      "training: 51 batch 364 batch_loss: 0.1944270133972168\n",
      "training: 51 batch 365 batch_loss: 0.1965685486793518\n",
      "training: 51 batch 366 batch_loss: 0.19149091839790344\n",
      "training: 51 batch 367 batch_loss: 0.1969151496887207\n",
      "training: 51 batch 368 batch_loss: 0.19574755430221558\n",
      "training: 51 batch 369 batch_loss: 0.1971733570098877\n",
      "training: 51 batch 370 batch_loss: 0.1955975890159607\n",
      "training: 51 batch 371 batch_loss: 0.1959649622440338\n",
      "training: 51 batch 372 batch_loss: 0.19370439648628235\n",
      "training: 51 batch 373 batch_loss: 0.19654172658920288\n",
      "training: 51 batch 374 batch_loss: 0.19205522537231445\n",
      "training: 51 batch 375 batch_loss: 0.19086188077926636\n",
      "training: 51 batch 376 batch_loss: 0.19487783312797546\n",
      "training: 51 batch 377 batch_loss: 0.19695842266082764\n",
      "training: 51 batch 378 batch_loss: 0.1954500377178192\n",
      "training: 51 batch 379 batch_loss: 0.19491583108901978\n",
      "training: 51 batch 380 batch_loss: 0.1937052607536316\n",
      "training: 51 batch 381 batch_loss: 0.19868487119674683\n",
      "training: 51 batch 382 batch_loss: 0.19582289457321167\n",
      "training: 51 batch 383 batch_loss: 0.1957125961780548\n",
      "training: 51 batch 384 batch_loss: 0.19314411282539368\n",
      "training: 51 batch 385 batch_loss: 0.19724053144454956\n",
      "training: 51 batch 386 batch_loss: 0.19404876232147217\n",
      "training: 51 batch 387 batch_loss: 0.19779589772224426\n",
      "training: 51 batch 388 batch_loss: 0.19803497195243835\n",
      "training: 51 batch 389 batch_loss: 0.19683939218521118\n",
      "training: 51 batch 390 batch_loss: 0.18964770436286926\n",
      "training: 51 batch 391 batch_loss: 0.19238385558128357\n",
      "training: 51 batch 392 batch_loss: 0.19597429037094116\n",
      "training: 51 batch 393 batch_loss: 0.1962367296218872\n",
      "training: 51 batch 394 batch_loss: 0.19373661279678345\n",
      "training: 51 batch 395 batch_loss: 0.19437360763549805\n",
      "training: 51 batch 396 batch_loss: 0.19794782996177673\n",
      "training: 51 batch 397 batch_loss: 0.1921497881412506\n",
      "training: 51 batch 398 batch_loss: 0.1937001645565033\n",
      "training: 51 batch 399 batch_loss: 0.1976909041404724\n",
      "training: 51 batch 400 batch_loss: 0.1943415105342865\n",
      "training: 51 batch 401 batch_loss: 0.19533342123031616\n",
      "training: 51 batch 402 batch_loss: 0.19423839449882507\n",
      "training: 51 batch 403 batch_loss: 0.19565600156784058\n",
      "training: 51 batch 404 batch_loss: 0.19355088472366333\n",
      "training: 51 batch 405 batch_loss: 0.19655820727348328\n",
      "training: 51 batch 406 batch_loss: 0.1921733021736145\n",
      "training: 51 batch 407 batch_loss: 0.19161254167556763\n",
      "training: 51 batch 408 batch_loss: 0.19630125164985657\n",
      "training: 51 batch 409 batch_loss: 0.19541749358177185\n",
      "training: 51 batch 410 batch_loss: 0.1942225694656372\n",
      "training: 51 batch 411 batch_loss: 0.1956387758255005\n",
      "training: 51 batch 412 batch_loss: 0.19101232290267944\n",
      "training: 51 batch 413 batch_loss: 0.19185703992843628\n",
      "training: 51 batch 414 batch_loss: 0.19449004530906677\n",
      "training: 51 batch 415 batch_loss: 0.19351327419281006\n",
      "training: 51 batch 416 batch_loss: 0.19609487056732178\n",
      "training: 51 batch 417 batch_loss: 0.19121116399765015\n",
      "training: 51 batch 418 batch_loss: 0.19411271810531616\n",
      "training: 51 batch 419 batch_loss: 0.19464057683944702\n",
      "training: 51 batch 420 batch_loss: 0.19336634874343872\n",
      "training: 51 batch 421 batch_loss: 0.1916911005973816\n",
      "training: 51 batch 422 batch_loss: 0.1927729845046997\n",
      "training: 51 batch 423 batch_loss: 0.19312673807144165\n",
      "training: 51 batch 424 batch_loss: 0.19366037845611572\n",
      "training: 51 batch 425 batch_loss: 0.1951529085636139\n",
      "training: 51 batch 426 batch_loss: 0.19668304920196533\n",
      "training: 51 batch 427 batch_loss: 0.19730645418167114\n",
      "training: 51 batch 428 batch_loss: 0.19184613227844238\n",
      "training: 51 batch 429 batch_loss: 0.1997317671775818\n",
      "training: 51 batch 430 batch_loss: 0.19233286380767822\n",
      "training: 51 batch 431 batch_loss: 0.1921396553516388\n",
      "training: 51 batch 432 batch_loss: 0.19860714673995972\n",
      "training: 51 batch 433 batch_loss: 0.19875910878181458\n",
      "training: 51 batch 434 batch_loss: 0.19164162874221802\n",
      "training: 51 batch 435 batch_loss: 0.19778886437416077\n",
      "training: 51 batch 436 batch_loss: 0.19491800665855408\n",
      "training: 51 batch 437 batch_loss: 0.1911267638206482\n",
      "training: 51 batch 438 batch_loss: 0.1958276629447937\n",
      "training: 51 batch 439 batch_loss: 0.19463041424751282\n",
      "training: 51 batch 440 batch_loss: 0.19286060333251953\n",
      "training: 51 batch 441 batch_loss: 0.1958024799823761\n",
      "training: 51 batch 442 batch_loss: 0.1940574049949646\n",
      "training: 51 batch 443 batch_loss: 0.19390901923179626\n",
      "training: 51 batch 444 batch_loss: 0.1941666603088379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 51 batch 445 batch_loss: 0.19453668594360352\n",
      "training: 51 batch 446 batch_loss: 0.19508576393127441\n",
      "training: 51 batch 447 batch_loss: 0.19389036297798157\n",
      "training: 51 batch 448 batch_loss: 0.1967736780643463\n",
      "training: 51 batch 449 batch_loss: 0.19374608993530273\n",
      "training: 51 batch 450 batch_loss: 0.19480350613594055\n",
      "training: 51 batch 451 batch_loss: 0.19390517473220825\n",
      "training: 51 batch 452 batch_loss: 0.19733655452728271\n",
      "training: 51 batch 453 batch_loss: 0.19659310579299927\n",
      "training: 51 batch 454 batch_loss: 0.19636020064353943\n",
      "training: 51 batch 455 batch_loss: 0.19861769676208496\n",
      "training: 51 batch 456 batch_loss: 0.19838181138038635\n",
      "training: 51 batch 457 batch_loss: 0.1950690746307373\n",
      "training: 51 batch 458 batch_loss: 0.1936742067337036\n",
      "training: 51 batch 459 batch_loss: 0.1938864290714264\n",
      "training: 51 batch 460 batch_loss: 0.1949423849582672\n",
      "training: 51 batch 461 batch_loss: 0.1964329183101654\n",
      "training: 51 batch 462 batch_loss: 0.19055435061454773\n",
      "training: 51 batch 463 batch_loss: 0.19412961602210999\n",
      "training: 51 batch 464 batch_loss: 0.19323396682739258\n",
      "training: 51 batch 465 batch_loss: 0.1958514153957367\n",
      "training: 51 batch 466 batch_loss: 0.1931191384792328\n",
      "training: 51 batch 467 batch_loss: 0.19526687264442444\n",
      "training: 51 batch 468 batch_loss: 0.1970379650592804\n",
      "training: 51 batch 469 batch_loss: 0.19225654006004333\n",
      "training: 51 batch 470 batch_loss: 0.1952228546142578\n",
      "training: 51 batch 471 batch_loss: 0.19581201672554016\n",
      "training: 51 batch 472 batch_loss: 0.19707810878753662\n",
      "training: 51 batch 473 batch_loss: 0.19414570927619934\n",
      "training: 51 batch 474 batch_loss: 0.19792860746383667\n",
      "training: 51 batch 475 batch_loss: 0.1923253834247589\n",
      "training: 51 batch 476 batch_loss: 0.19476675987243652\n",
      "training: 51 batch 477 batch_loss: 0.19614839553833008\n",
      "training: 51 batch 478 batch_loss: 0.1900826394557953\n",
      "training: 51 batch 479 batch_loss: 0.19207122921943665\n",
      "training: 51 batch 480 batch_loss: 0.19197097420692444\n",
      "training: 51 batch 481 batch_loss: 0.1932634711265564\n",
      "training: 51 batch 482 batch_loss: 0.1937733292579651\n",
      "training: 51 batch 483 batch_loss: 0.19378066062927246\n",
      "training: 51 batch 484 batch_loss: 0.1977047324180603\n",
      "training: 51 batch 485 batch_loss: 0.19584736227989197\n",
      "training: 51 batch 486 batch_loss: 0.19530561566352844\n",
      "training: 51 batch 487 batch_loss: 0.19653159379959106\n",
      "training: 51 batch 488 batch_loss: 0.19936031103134155\n",
      "training: 51 batch 489 batch_loss: 0.19483771920204163\n",
      "training: 51 batch 490 batch_loss: 0.1950710117816925\n",
      "training: 51 batch 491 batch_loss: 0.19699108600616455\n",
      "training: 51 batch 492 batch_loss: 0.19230878353118896\n",
      "training: 51 batch 493 batch_loss: 0.19704192876815796\n",
      "training: 51 batch 494 batch_loss: 0.19736742973327637\n",
      "training: 51 batch 495 batch_loss: 0.19136518239974976\n",
      "training: 51 batch 496 batch_loss: 0.19705313444137573\n",
      "training: 51 batch 497 batch_loss: 0.1958981454372406\n",
      "training: 51 batch 498 batch_loss: 0.19507235288619995\n",
      "training: 51 batch 499 batch_loss: 0.19090163707733154\n",
      "training: 51 batch 500 batch_loss: 0.19706469774246216\n",
      "training: 51 batch 501 batch_loss: 0.19356778264045715\n",
      "training: 51 batch 502 batch_loss: 0.19279074668884277\n",
      "training: 51 batch 503 batch_loss: 0.19401311874389648\n",
      "training: 51 batch 504 batch_loss: 0.19456741213798523\n",
      "training: 51 batch 505 batch_loss: 0.1963212490081787\n",
      "training: 51 batch 506 batch_loss: 0.19505396485328674\n",
      "training: 51 batch 507 batch_loss: 0.19269898533821106\n",
      "training: 51 batch 508 batch_loss: 0.19604924321174622\n",
      "training: 51 batch 509 batch_loss: 0.19182199239730835\n",
      "training: 51 batch 510 batch_loss: 0.20026156306266785\n",
      "training: 51 batch 511 batch_loss: 0.19555619359016418\n",
      "training: 51 batch 512 batch_loss: 0.1939445436000824\n",
      "training: 51 batch 513 batch_loss: 0.19444477558135986\n",
      "training: 51 batch 514 batch_loss: 0.19726821780204773\n",
      "training: 51 batch 515 batch_loss: 0.19841450452804565\n",
      "training: 51 batch 516 batch_loss: 0.18996188044548035\n",
      "training: 51 batch 517 batch_loss: 0.19248715043067932\n",
      "training: 51 batch 518 batch_loss: 0.19466927647590637\n",
      "training: 51 batch 519 batch_loss: 0.19648736715316772\n",
      "training: 51 batch 520 batch_loss: 0.19028636813163757\n",
      "training: 51 batch 521 batch_loss: 0.19628393650054932\n",
      "training: 51 batch 522 batch_loss: 0.19630426168441772\n",
      "training: 51 batch 523 batch_loss: 0.1881476640701294\n",
      "training: 51 batch 524 batch_loss: 0.19560647010803223\n",
      "training: 51 batch 525 batch_loss: 0.19692599773406982\n",
      "training: 51 batch 526 batch_loss: 0.19310539960861206\n",
      "training: 51 batch 527 batch_loss: 0.19603592157363892\n",
      "training: 51 batch 528 batch_loss: 0.1969614028930664\n",
      "training: 51 batch 529 batch_loss: 0.1920669972896576\n",
      "training: 51 batch 530 batch_loss: 0.19653522968292236\n",
      "training: 51 batch 531 batch_loss: 0.19584709405899048\n",
      "training: 51 batch 532 batch_loss: 0.1930147111415863\n",
      "training: 51 batch 533 batch_loss: 0.19616267085075378\n",
      "training: 51 batch 534 batch_loss: 0.19365879893302917\n",
      "training: 51 batch 535 batch_loss: 0.19536235928535461\n",
      "training: 51 batch 536 batch_loss: 0.191741943359375\n",
      "training: 51 batch 537 batch_loss: 0.199239581823349\n",
      "training: 51 batch 538 batch_loss: 0.1956711709499359\n",
      "training: 51 batch 539 batch_loss: 0.1967828869819641\n",
      "training: 51 batch 540 batch_loss: 0.191865473985672\n",
      "training: 51 batch 541 batch_loss: 0.19741076231002808\n",
      "training: 51 batch 542 batch_loss: 0.19444158673286438\n",
      "training: 51 batch 543 batch_loss: 0.1962493360042572\n",
      "training: 51 batch 544 batch_loss: 0.1910223364830017\n",
      "training: 51 batch 545 batch_loss: 0.1975436806678772\n",
      "training: 51 batch 546 batch_loss: 0.19457519054412842\n",
      "training: 51 batch 547 batch_loss: 0.19344821572303772\n",
      "training: 51 batch 548 batch_loss: 0.19452297687530518\n",
      "training: 51 batch 549 batch_loss: 0.19500595331192017\n",
      "training: 51 batch 550 batch_loss: 0.19819426536560059\n",
      "training: 51 batch 551 batch_loss: 0.19328472018241882\n",
      "training: 51 batch 552 batch_loss: 0.1922437846660614\n",
      "training: 51 batch 553 batch_loss: 0.19380557537078857\n",
      "training: 51 batch 554 batch_loss: 0.19708150625228882\n",
      "training: 51 batch 555 batch_loss: 0.19706958532333374\n",
      "training: 51 batch 556 batch_loss: 0.19429102540016174\n",
      "training: 51 batch 557 batch_loss: 0.1952688992023468\n",
      "training: 51 batch 558 batch_loss: 0.19817495346069336\n",
      "training: 51 batch 559 batch_loss: 0.18965458869934082\n",
      "training: 51 batch 560 batch_loss: 0.19299417734146118\n",
      "training: 51 batch 561 batch_loss: 0.1968708634376526\n",
      "training: 51 batch 562 batch_loss: 0.19107460975646973\n",
      "training: 51 batch 563 batch_loss: 0.1978607177734375\n",
      "training: 51 batch 564 batch_loss: 0.1948654055595398\n",
      "training: 51 batch 565 batch_loss: 0.19818422198295593\n",
      "training: 51 batch 566 batch_loss: 0.19542527198791504\n",
      "training: 51 batch 567 batch_loss: 0.1948724091053009\n",
      "training: 51 batch 568 batch_loss: 0.19430339336395264\n",
      "training: 51 batch 569 batch_loss: 0.19695410132408142\n",
      "training: 51 batch 570 batch_loss: 0.19492456316947937\n",
      "training: 51 batch 571 batch_loss: 0.19824030995368958\n",
      "training: 51 batch 572 batch_loss: 0.19171929359436035\n",
      "training: 51 batch 573 batch_loss: 0.19752022624015808\n",
      "training: 51 batch 574 batch_loss: 0.19673532247543335\n",
      "training: 51 batch 575 batch_loss: 0.19382458925247192\n",
      "training: 51 batch 576 batch_loss: 0.19719049334526062\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 51, Hit Ratio:0.03229255845406122 | Precision:0.04764572889019955 | Recall:0.06340327244936822 | NDCG:0.06159198958990449\n",
      "*Best Performance* \n",
      "Epoch: 49, Hit Ratio:0.032479105109046516 | Precision:0.04792096726629313 | Recall:0.06353025789214156 | MDCG:0.061899475319776374\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 52 batch 0 batch_loss: 0.19710713624954224\n",
      "training: 52 batch 1 batch_loss: 0.19399729371070862\n",
      "training: 52 batch 2 batch_loss: 0.1899641752243042\n",
      "training: 52 batch 3 batch_loss: 0.18925127387046814\n",
      "training: 52 batch 4 batch_loss: 0.1909063458442688\n",
      "training: 52 batch 5 batch_loss: 0.1934153437614441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 52 batch 6 batch_loss: 0.1921616494655609\n",
      "training: 52 batch 7 batch_loss: 0.19453322887420654\n",
      "training: 52 batch 8 batch_loss: 0.19342106580734253\n",
      "training: 52 batch 9 batch_loss: 0.19419437646865845\n",
      "training: 52 batch 10 batch_loss: 0.196844220161438\n",
      "training: 52 batch 11 batch_loss: 0.1954096555709839\n",
      "training: 52 batch 12 batch_loss: 0.19227206707000732\n",
      "training: 52 batch 13 batch_loss: 0.19175374507904053\n",
      "training: 52 batch 14 batch_loss: 0.18823659420013428\n",
      "training: 52 batch 15 batch_loss: 0.193070650100708\n",
      "training: 52 batch 16 batch_loss: 0.19200950860977173\n",
      "training: 52 batch 17 batch_loss: 0.19320854544639587\n",
      "training: 52 batch 18 batch_loss: 0.1911695897579193\n",
      "training: 52 batch 19 batch_loss: 0.19134339690208435\n",
      "training: 52 batch 20 batch_loss: 0.19232821464538574\n",
      "training: 52 batch 21 batch_loss: 0.19384783506393433\n",
      "training: 52 batch 22 batch_loss: 0.1958479881286621\n",
      "training: 52 batch 23 batch_loss: 0.19417688250541687\n",
      "training: 52 batch 24 batch_loss: 0.19626176357269287\n",
      "training: 52 batch 25 batch_loss: 0.1942688524723053\n",
      "training: 52 batch 26 batch_loss: 0.1922922432422638\n",
      "training: 52 batch 27 batch_loss: 0.19337090849876404\n",
      "training: 52 batch 28 batch_loss: 0.1940218210220337\n",
      "training: 52 batch 29 batch_loss: 0.19185060262680054\n",
      "training: 52 batch 30 batch_loss: 0.19423726201057434\n",
      "training: 52 batch 31 batch_loss: 0.19477438926696777\n",
      "training: 52 batch 32 batch_loss: 0.1937655210494995\n",
      "training: 52 batch 33 batch_loss: 0.19396209716796875\n",
      "training: 52 batch 34 batch_loss: 0.19438040256500244\n",
      "training: 52 batch 35 batch_loss: 0.19391494989395142\n",
      "training: 52 batch 36 batch_loss: 0.19326341152191162\n",
      "training: 52 batch 37 batch_loss: 0.1936206817626953\n",
      "training: 52 batch 38 batch_loss: 0.19111615419387817\n",
      "training: 52 batch 39 batch_loss: 0.194421648979187\n",
      "training: 52 batch 40 batch_loss: 0.19638392329216003\n",
      "training: 52 batch 41 batch_loss: 0.1960822343826294\n",
      "training: 52 batch 42 batch_loss: 0.19317346811294556\n",
      "training: 52 batch 43 batch_loss: 0.19466117024421692\n",
      "training: 52 batch 44 batch_loss: 0.19257569313049316\n",
      "training: 52 batch 45 batch_loss: 0.19391417503356934\n",
      "training: 52 batch 46 batch_loss: 0.18986022472381592\n",
      "training: 52 batch 47 batch_loss: 0.1924828290939331\n",
      "training: 52 batch 48 batch_loss: 0.19523885846138\n",
      "training: 52 batch 49 batch_loss: 0.19360029697418213\n",
      "training: 52 batch 50 batch_loss: 0.1943330466747284\n",
      "training: 52 batch 51 batch_loss: 0.19364237785339355\n",
      "training: 52 batch 52 batch_loss: 0.19565734267234802\n",
      "training: 52 batch 53 batch_loss: 0.19652941823005676\n",
      "training: 52 batch 54 batch_loss: 0.19591593742370605\n",
      "training: 52 batch 55 batch_loss: 0.19202524423599243\n",
      "training: 52 batch 56 batch_loss: 0.19544562697410583\n",
      "training: 52 batch 57 batch_loss: 0.1958080232143402\n",
      "training: 52 batch 58 batch_loss: 0.19461244344711304\n",
      "training: 52 batch 59 batch_loss: 0.19028741121292114\n",
      "training: 52 batch 60 batch_loss: 0.19275805354118347\n",
      "training: 52 batch 61 batch_loss: 0.18958306312561035\n",
      "training: 52 batch 62 batch_loss: 0.19179797172546387\n",
      "training: 52 batch 63 batch_loss: 0.1931518316268921\n",
      "training: 52 batch 64 batch_loss: 0.19495230913162231\n",
      "training: 52 batch 65 batch_loss: 0.19396990537643433\n",
      "training: 52 batch 66 batch_loss: 0.1926020085811615\n",
      "training: 52 batch 67 batch_loss: 0.19170385599136353\n",
      "training: 52 batch 68 batch_loss: 0.1931111216545105\n",
      "training: 52 batch 69 batch_loss: 0.191819429397583\n",
      "training: 52 batch 70 batch_loss: 0.19043776392936707\n",
      "training: 52 batch 71 batch_loss: 0.19634371995925903\n",
      "training: 52 batch 72 batch_loss: 0.1954309046268463\n",
      "training: 52 batch 73 batch_loss: 0.1965872049331665\n",
      "training: 52 batch 74 batch_loss: 0.193907231092453\n",
      "training: 52 batch 75 batch_loss: 0.19339025020599365\n",
      "training: 52 batch 76 batch_loss: 0.19412913918495178\n",
      "training: 52 batch 77 batch_loss: 0.19277700781822205\n",
      "training: 52 batch 78 batch_loss: 0.19617131352424622\n",
      "training: 52 batch 79 batch_loss: 0.19243204593658447\n",
      "training: 52 batch 80 batch_loss: 0.1937817633152008\n",
      "training: 52 batch 81 batch_loss: 0.19685479998588562\n",
      "training: 52 batch 82 batch_loss: 0.19112077355384827\n",
      "training: 52 batch 83 batch_loss: 0.1952463984489441\n",
      "training: 52 batch 84 batch_loss: 0.1971358060836792\n",
      "training: 52 batch 85 batch_loss: 0.19468796253204346\n",
      "training: 52 batch 86 batch_loss: 0.1962149441242218\n",
      "training: 52 batch 87 batch_loss: 0.19299101829528809\n",
      "training: 52 batch 88 batch_loss: 0.194888174533844\n",
      "training: 52 batch 89 batch_loss: 0.19918084144592285\n",
      "training: 52 batch 90 batch_loss: 0.19277030229568481\n",
      "training: 52 batch 91 batch_loss: 0.1992722749710083\n",
      "training: 52 batch 92 batch_loss: 0.19758734107017517\n",
      "training: 52 batch 93 batch_loss: 0.1892888844013214\n",
      "training: 52 batch 94 batch_loss: 0.19597160816192627\n",
      "training: 52 batch 95 batch_loss: 0.19651547074317932\n",
      "training: 52 batch 96 batch_loss: 0.19064006209373474\n",
      "training: 52 batch 97 batch_loss: 0.19201722741127014\n",
      "training: 52 batch 98 batch_loss: 0.19482079148292542\n",
      "training: 52 batch 99 batch_loss: 0.19471219182014465\n",
      "training: 52 batch 100 batch_loss: 0.1911696493625641\n",
      "training: 52 batch 101 batch_loss: 0.1969275176525116\n",
      "training: 52 batch 102 batch_loss: 0.18798187375068665\n",
      "training: 52 batch 103 batch_loss: 0.18980970978736877\n",
      "training: 52 batch 104 batch_loss: 0.19801896810531616\n",
      "training: 52 batch 105 batch_loss: 0.19061297178268433\n",
      "training: 52 batch 106 batch_loss: 0.19168347120285034\n",
      "training: 52 batch 107 batch_loss: 0.19473066926002502\n",
      "training: 52 batch 108 batch_loss: 0.1941385567188263\n",
      "training: 52 batch 109 batch_loss: 0.1931777000427246\n",
      "training: 52 batch 110 batch_loss: 0.196848064661026\n",
      "training: 52 batch 111 batch_loss: 0.19038888812065125\n",
      "training: 52 batch 112 batch_loss: 0.19613707065582275\n",
      "training: 52 batch 113 batch_loss: 0.1930449903011322\n",
      "training: 52 batch 114 batch_loss: 0.19191139936447144\n",
      "training: 52 batch 115 batch_loss: 0.19495552778244019\n",
      "training: 52 batch 116 batch_loss: 0.1936223804950714\n",
      "training: 52 batch 117 batch_loss: 0.1971774697303772\n",
      "training: 52 batch 118 batch_loss: 0.19486269354820251\n",
      "training: 52 batch 119 batch_loss: 0.19749760627746582\n",
      "training: 52 batch 120 batch_loss: 0.19496679306030273\n",
      "training: 52 batch 121 batch_loss: 0.19556578993797302\n",
      "training: 52 batch 122 batch_loss: 0.19261851906776428\n",
      "training: 52 batch 123 batch_loss: 0.19460174441337585\n",
      "training: 52 batch 124 batch_loss: 0.19595718383789062\n",
      "training: 52 batch 125 batch_loss: 0.19006618857383728\n",
      "training: 52 batch 126 batch_loss: 0.19523370265960693\n",
      "training: 52 batch 127 batch_loss: 0.1946353018283844\n",
      "training: 52 batch 128 batch_loss: 0.1910497546195984\n",
      "training: 52 batch 129 batch_loss: 0.19075271487236023\n",
      "training: 52 batch 130 batch_loss: 0.19406747817993164\n",
      "training: 52 batch 131 batch_loss: 0.19568684697151184\n",
      "training: 52 batch 132 batch_loss: 0.1947614848613739\n",
      "training: 52 batch 133 batch_loss: 0.1946195363998413\n",
      "training: 52 batch 134 batch_loss: 0.19468185305595398\n",
      "training: 52 batch 135 batch_loss: 0.19907888770103455\n",
      "training: 52 batch 136 batch_loss: 0.19249671697616577\n",
      "training: 52 batch 137 batch_loss: 0.19757458567619324\n",
      "training: 52 batch 138 batch_loss: 0.19650545716285706\n",
      "training: 52 batch 139 batch_loss: 0.19573014974594116\n",
      "training: 52 batch 140 batch_loss: 0.19497519731521606\n",
      "training: 52 batch 141 batch_loss: 0.19562962651252747\n",
      "training: 52 batch 142 batch_loss: 0.19071656465530396\n",
      "training: 52 batch 143 batch_loss: 0.19238942861557007\n",
      "training: 52 batch 144 batch_loss: 0.19361194968223572\n",
      "training: 52 batch 145 batch_loss: 0.1922711431980133\n",
      "training: 52 batch 146 batch_loss: 0.18936687707901\n",
      "training: 52 batch 147 batch_loss: 0.1935001015663147\n",
      "training: 52 batch 148 batch_loss: 0.1917904019355774\n",
      "training: 52 batch 149 batch_loss: 0.19235077500343323\n",
      "training: 52 batch 150 batch_loss: 0.19482183456420898\n",
      "training: 52 batch 151 batch_loss: 0.19200947880744934\n",
      "training: 52 batch 152 batch_loss: 0.19419068098068237\n",
      "training: 52 batch 153 batch_loss: 0.19474130868911743\n",
      "training: 52 batch 154 batch_loss: 0.19553130865097046\n",
      "training: 52 batch 155 batch_loss: 0.19158941507339478\n",
      "training: 52 batch 156 batch_loss: 0.1964496672153473\n",
      "training: 52 batch 157 batch_loss: 0.19006580114364624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 52 batch 158 batch_loss: 0.1956518292427063\n",
      "training: 52 batch 159 batch_loss: 0.19203993678092957\n",
      "training: 52 batch 160 batch_loss: 0.19665101170539856\n",
      "training: 52 batch 161 batch_loss: 0.19362959265708923\n",
      "training: 52 batch 162 batch_loss: 0.1932550072669983\n",
      "training: 52 batch 163 batch_loss: 0.19533562660217285\n",
      "training: 52 batch 164 batch_loss: 0.19128158688545227\n",
      "training: 52 batch 165 batch_loss: 0.19399914145469666\n",
      "training: 52 batch 166 batch_loss: 0.1954115629196167\n",
      "training: 52 batch 167 batch_loss: 0.1917659342288971\n",
      "training: 52 batch 168 batch_loss: 0.19466090202331543\n",
      "training: 52 batch 169 batch_loss: 0.19359830021858215\n",
      "training: 52 batch 170 batch_loss: 0.19583845138549805\n",
      "training: 52 batch 171 batch_loss: 0.19626620411872864\n",
      "training: 52 batch 172 batch_loss: 0.1944139301776886\n",
      "training: 52 batch 173 batch_loss: 0.1968679428100586\n",
      "training: 52 batch 174 batch_loss: 0.19474953413009644\n",
      "training: 52 batch 175 batch_loss: 0.1949613094329834\n",
      "training: 52 batch 176 batch_loss: 0.19245436787605286\n",
      "training: 52 batch 177 batch_loss: 0.1943354606628418\n",
      "training: 52 batch 178 batch_loss: 0.19270473718643188\n",
      "training: 52 batch 179 batch_loss: 0.19385090470314026\n",
      "training: 52 batch 180 batch_loss: 0.19829055666923523\n",
      "training: 52 batch 181 batch_loss: 0.19500386714935303\n",
      "training: 52 batch 182 batch_loss: 0.1958560049533844\n",
      "training: 52 batch 183 batch_loss: 0.19611522555351257\n",
      "training: 52 batch 184 batch_loss: 0.19682738184928894\n",
      "training: 52 batch 185 batch_loss: 0.1953945755958557\n",
      "training: 52 batch 186 batch_loss: 0.19703146815299988\n",
      "training: 52 batch 187 batch_loss: 0.1958995759487152\n",
      "training: 52 batch 188 batch_loss: 0.19332122802734375\n",
      "training: 52 batch 189 batch_loss: 0.18867814540863037\n",
      "training: 52 batch 190 batch_loss: 0.19664597511291504\n",
      "training: 52 batch 191 batch_loss: 0.19292905926704407\n",
      "training: 52 batch 192 batch_loss: 0.19449856877326965\n",
      "training: 52 batch 193 batch_loss: 0.19294357299804688\n",
      "training: 52 batch 194 batch_loss: 0.1951296627521515\n",
      "training: 52 batch 195 batch_loss: 0.19061949849128723\n",
      "training: 52 batch 196 batch_loss: 0.1941872239112854\n",
      "training: 52 batch 197 batch_loss: 0.19584739208221436\n",
      "training: 52 batch 198 batch_loss: 0.19107279181480408\n",
      "training: 52 batch 199 batch_loss: 0.1960843801498413\n",
      "training: 52 batch 200 batch_loss: 0.1952032744884491\n",
      "training: 52 batch 201 batch_loss: 0.19618558883666992\n",
      "training: 52 batch 202 batch_loss: 0.19533663988113403\n",
      "training: 52 batch 203 batch_loss: 0.19631516933441162\n",
      "training: 52 batch 204 batch_loss: 0.19208520650863647\n",
      "training: 52 batch 205 batch_loss: 0.2014276385307312\n",
      "training: 52 batch 206 batch_loss: 0.1940072774887085\n",
      "training: 52 batch 207 batch_loss: 0.19275525212287903\n",
      "training: 52 batch 208 batch_loss: 0.19675058126449585\n",
      "training: 52 batch 209 batch_loss: 0.19418439269065857\n",
      "training: 52 batch 210 batch_loss: 0.1943768858909607\n",
      "training: 52 batch 211 batch_loss: 0.19227272272109985\n",
      "training: 52 batch 212 batch_loss: 0.19621780514717102\n",
      "training: 52 batch 213 batch_loss: 0.19281449913978577\n",
      "training: 52 batch 214 batch_loss: 0.19200760126113892\n",
      "training: 52 batch 215 batch_loss: 0.19707417488098145\n",
      "training: 52 batch 216 batch_loss: 0.19186821579933167\n",
      "training: 52 batch 217 batch_loss: 0.19495266675949097\n",
      "training: 52 batch 218 batch_loss: 0.19334203004837036\n",
      "training: 52 batch 219 batch_loss: 0.1927642524242401\n",
      "training: 52 batch 220 batch_loss: 0.19364014267921448\n",
      "training: 52 batch 221 batch_loss: 0.1970125138759613\n",
      "training: 52 batch 222 batch_loss: 0.1957416534423828\n",
      "training: 52 batch 223 batch_loss: 0.19646874070167542\n",
      "training: 52 batch 224 batch_loss: 0.19621720910072327\n",
      "training: 52 batch 225 batch_loss: 0.19773611426353455\n",
      "training: 52 batch 226 batch_loss: 0.1944076418876648\n",
      "training: 52 batch 227 batch_loss: 0.19364064931869507\n",
      "training: 52 batch 228 batch_loss: 0.1977819800376892\n",
      "training: 52 batch 229 batch_loss: 0.19652417302131653\n",
      "training: 52 batch 230 batch_loss: 0.19513875246047974\n",
      "training: 52 batch 231 batch_loss: 0.19817718863487244\n",
      "training: 52 batch 232 batch_loss: 0.19517263770103455\n",
      "training: 52 batch 233 batch_loss: 0.192789226770401\n",
      "training: 52 batch 234 batch_loss: 0.19536659121513367\n",
      "training: 52 batch 235 batch_loss: 0.18904298543930054\n",
      "training: 52 batch 236 batch_loss: 0.19391107559204102\n",
      "training: 52 batch 237 batch_loss: 0.19501447677612305\n",
      "training: 52 batch 238 batch_loss: 0.19326895475387573\n",
      "training: 52 batch 239 batch_loss: 0.19143494963645935\n",
      "training: 52 batch 240 batch_loss: 0.1978098452091217\n",
      "training: 52 batch 241 batch_loss: 0.19507452845573425\n",
      "training: 52 batch 242 batch_loss: 0.19463831186294556\n",
      "training: 52 batch 243 batch_loss: 0.19482585787773132\n",
      "training: 52 batch 244 batch_loss: 0.19624540209770203\n",
      "training: 52 batch 245 batch_loss: 0.19377321004867554\n",
      "training: 52 batch 246 batch_loss: 0.1923535168170929\n",
      "training: 52 batch 247 batch_loss: 0.19359290599822998\n",
      "training: 52 batch 248 batch_loss: 0.1947396695613861\n",
      "training: 52 batch 249 batch_loss: 0.1929522454738617\n",
      "training: 52 batch 250 batch_loss: 0.1923350989818573\n",
      "training: 52 batch 251 batch_loss: 0.19342607259750366\n",
      "training: 52 batch 252 batch_loss: 0.19264346361160278\n",
      "training: 52 batch 253 batch_loss: 0.19520550966262817\n",
      "training: 52 batch 254 batch_loss: 0.19292837381362915\n",
      "training: 52 batch 255 batch_loss: 0.19551169872283936\n",
      "training: 52 batch 256 batch_loss: 0.19487199187278748\n",
      "training: 52 batch 257 batch_loss: 0.1962105631828308\n",
      "training: 52 batch 258 batch_loss: 0.1944478154182434\n",
      "training: 52 batch 259 batch_loss: 0.19039785861968994\n",
      "training: 52 batch 260 batch_loss: 0.19335970282554626\n",
      "training: 52 batch 261 batch_loss: 0.20034357905387878\n",
      "training: 52 batch 262 batch_loss: 0.19489073753356934\n",
      "training: 52 batch 263 batch_loss: 0.1911630928516388\n",
      "training: 52 batch 264 batch_loss: 0.19986721873283386\n",
      "training: 52 batch 265 batch_loss: 0.19525805115699768\n",
      "training: 52 batch 266 batch_loss: 0.19329845905303955\n",
      "training: 52 batch 267 batch_loss: 0.19621625542640686\n",
      "training: 52 batch 268 batch_loss: 0.1928987205028534\n",
      "training: 52 batch 269 batch_loss: 0.19168171286582947\n",
      "training: 52 batch 270 batch_loss: 0.19639018177986145\n",
      "training: 52 batch 271 batch_loss: 0.19857817888259888\n",
      "training: 52 batch 272 batch_loss: 0.1940639317035675\n",
      "training: 52 batch 273 batch_loss: 0.19379839301109314\n",
      "training: 52 batch 274 batch_loss: 0.19780665636062622\n",
      "training: 52 batch 275 batch_loss: 0.19486406445503235\n",
      "training: 52 batch 276 batch_loss: 0.1941339671611786\n",
      "training: 52 batch 277 batch_loss: 0.1952754259109497\n",
      "training: 52 batch 278 batch_loss: 0.19432109594345093\n",
      "training: 52 batch 279 batch_loss: 0.1909864842891693\n",
      "training: 52 batch 280 batch_loss: 0.19047030806541443\n",
      "training: 52 batch 281 batch_loss: 0.19065052270889282\n",
      "training: 52 batch 282 batch_loss: 0.1958516240119934\n",
      "training: 52 batch 283 batch_loss: 0.1941939890384674\n",
      "training: 52 batch 284 batch_loss: 0.19258245825767517\n",
      "training: 52 batch 285 batch_loss: 0.19447758793830872\n",
      "training: 52 batch 286 batch_loss: 0.19473984837532043\n",
      "training: 52 batch 287 batch_loss: 0.1940203309059143\n",
      "training: 52 batch 288 batch_loss: 0.1958548128604889\n",
      "training: 52 batch 289 batch_loss: 0.19306743144989014\n",
      "training: 52 batch 290 batch_loss: 0.19497811794281006\n",
      "training: 52 batch 291 batch_loss: 0.1962515413761139\n",
      "training: 52 batch 292 batch_loss: 0.1974157691001892\n",
      "training: 52 batch 293 batch_loss: 0.19870403409004211\n",
      "training: 52 batch 294 batch_loss: 0.19603636860847473\n",
      "training: 52 batch 295 batch_loss: 0.19885286688804626\n",
      "training: 52 batch 296 batch_loss: 0.19317463040351868\n",
      "training: 52 batch 297 batch_loss: 0.195499449968338\n",
      "training: 52 batch 298 batch_loss: 0.19623184204101562\n",
      "training: 52 batch 299 batch_loss: 0.19344675540924072\n",
      "training: 52 batch 300 batch_loss: 0.1967894732952118\n",
      "training: 52 batch 301 batch_loss: 0.19533511996269226\n",
      "training: 52 batch 302 batch_loss: 0.1925831437110901\n",
      "training: 52 batch 303 batch_loss: 0.1960902214050293\n",
      "training: 52 batch 304 batch_loss: 0.19479531049728394\n",
      "training: 52 batch 305 batch_loss: 0.1944909393787384\n",
      "training: 52 batch 306 batch_loss: 0.19871419668197632\n",
      "training: 52 batch 307 batch_loss: 0.1903902292251587\n",
      "training: 52 batch 308 batch_loss: 0.1924028992652893\n",
      "training: 52 batch 309 batch_loss: 0.1942172348499298\n",
      "training: 52 batch 310 batch_loss: 0.19579964876174927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 52 batch 311 batch_loss: 0.1930178999900818\n",
      "training: 52 batch 312 batch_loss: 0.19579756259918213\n",
      "training: 52 batch 313 batch_loss: 0.19511276483535767\n",
      "training: 52 batch 314 batch_loss: 0.19487124681472778\n",
      "training: 52 batch 315 batch_loss: 0.19740349054336548\n",
      "training: 52 batch 316 batch_loss: 0.19415605068206787\n",
      "training: 52 batch 317 batch_loss: 0.19463825225830078\n",
      "training: 52 batch 318 batch_loss: 0.19509026408195496\n",
      "training: 52 batch 319 batch_loss: 0.19241243600845337\n",
      "training: 52 batch 320 batch_loss: 0.19328376650810242\n",
      "training: 52 batch 321 batch_loss: 0.19396567344665527\n",
      "training: 52 batch 322 batch_loss: 0.19468209147453308\n",
      "training: 52 batch 323 batch_loss: 0.20166954398155212\n",
      "training: 52 batch 324 batch_loss: 0.1931338906288147\n",
      "training: 52 batch 325 batch_loss: 0.2004481554031372\n",
      "training: 52 batch 326 batch_loss: 0.19584441184997559\n",
      "training: 52 batch 327 batch_loss: 0.19848871231079102\n",
      "training: 52 batch 328 batch_loss: 0.19693827629089355\n",
      "training: 52 batch 329 batch_loss: 0.19254964590072632\n",
      "training: 52 batch 330 batch_loss: 0.19702404737472534\n",
      "training: 52 batch 331 batch_loss: 0.1988285779953003\n",
      "training: 52 batch 332 batch_loss: 0.19556254148483276\n",
      "training: 52 batch 333 batch_loss: 0.19361180067062378\n",
      "training: 52 batch 334 batch_loss: 0.19214779138565063\n",
      "training: 52 batch 335 batch_loss: 0.19417425990104675\n",
      "training: 52 batch 336 batch_loss: 0.19570407271385193\n",
      "training: 52 batch 337 batch_loss: 0.1996060609817505\n",
      "training: 52 batch 338 batch_loss: 0.19500714540481567\n",
      "training: 52 batch 339 batch_loss: 0.19412675499916077\n",
      "training: 52 batch 340 batch_loss: 0.19684237241744995\n",
      "training: 52 batch 341 batch_loss: 0.19579827785491943\n",
      "training: 52 batch 342 batch_loss: 0.19600218534469604\n",
      "training: 52 batch 343 batch_loss: 0.20355236530303955\n",
      "training: 52 batch 344 batch_loss: 0.19611874222755432\n",
      "training: 52 batch 345 batch_loss: 0.20059221982955933\n",
      "training: 52 batch 346 batch_loss: 0.19553765654563904\n",
      "training: 52 batch 347 batch_loss: 0.19650280475616455\n",
      "training: 52 batch 348 batch_loss: 0.1941736340522766\n",
      "training: 52 batch 349 batch_loss: 0.18939533829689026\n",
      "training: 52 batch 350 batch_loss: 0.19655382633209229\n",
      "training: 52 batch 351 batch_loss: 0.19811725616455078\n",
      "training: 52 batch 352 batch_loss: 0.196143239736557\n",
      "training: 52 batch 353 batch_loss: 0.19446223974227905\n",
      "training: 52 batch 354 batch_loss: 0.1950770616531372\n",
      "training: 52 batch 355 batch_loss: 0.19385266304016113\n",
      "training: 52 batch 356 batch_loss: 0.19482553005218506\n",
      "training: 52 batch 357 batch_loss: 0.1954757571220398\n",
      "training: 52 batch 358 batch_loss: 0.19585952162742615\n",
      "training: 52 batch 359 batch_loss: 0.19657960534095764\n",
      "training: 52 batch 360 batch_loss: 0.19294029474258423\n",
      "training: 52 batch 361 batch_loss: 0.1966131329536438\n",
      "training: 52 batch 362 batch_loss: 0.19360536336898804\n",
      "training: 52 batch 363 batch_loss: 0.19596290588378906\n",
      "training: 52 batch 364 batch_loss: 0.19478321075439453\n",
      "training: 52 batch 365 batch_loss: 0.19749200344085693\n",
      "training: 52 batch 366 batch_loss: 0.19767224788665771\n",
      "training: 52 batch 367 batch_loss: 0.19595322012901306\n",
      "training: 52 batch 368 batch_loss: 0.19403454661369324\n",
      "training: 52 batch 369 batch_loss: 0.19190159440040588\n",
      "training: 52 batch 370 batch_loss: 0.1927100419998169\n",
      "training: 52 batch 371 batch_loss: 0.1966196596622467\n",
      "training: 52 batch 372 batch_loss: 0.19192582368850708\n",
      "training: 52 batch 373 batch_loss: 0.19383671879768372\n",
      "training: 52 batch 374 batch_loss: 0.1975090503692627\n",
      "training: 52 batch 375 batch_loss: 0.19647732377052307\n",
      "training: 52 batch 376 batch_loss: 0.19367393851280212\n",
      "training: 52 batch 377 batch_loss: 0.19350138306617737\n",
      "training: 52 batch 378 batch_loss: 0.19612571597099304\n",
      "training: 52 batch 379 batch_loss: 0.196437269449234\n",
      "training: 52 batch 380 batch_loss: 0.1950574815273285\n",
      "training: 52 batch 381 batch_loss: 0.19374409317970276\n",
      "training: 52 batch 382 batch_loss: 0.1922779679298401\n",
      "training: 52 batch 383 batch_loss: 0.19551852345466614\n",
      "training: 52 batch 384 batch_loss: 0.19662171602249146\n",
      "training: 52 batch 385 batch_loss: 0.19898459315299988\n",
      "training: 52 batch 386 batch_loss: 0.193854421377182\n",
      "training: 52 batch 387 batch_loss: 0.19211751222610474\n",
      "training: 52 batch 388 batch_loss: 0.19733721017837524\n",
      "training: 52 batch 389 batch_loss: 0.19903305172920227\n",
      "training: 52 batch 390 batch_loss: 0.19772130250930786\n",
      "training: 52 batch 391 batch_loss: 0.19464963674545288\n",
      "training: 52 batch 392 batch_loss: 0.1976529061794281\n",
      "training: 52 batch 393 batch_loss: 0.19312909245491028\n",
      "training: 52 batch 394 batch_loss: 0.19698217511177063\n",
      "training: 52 batch 395 batch_loss: 0.19450491666793823\n",
      "training: 52 batch 396 batch_loss: 0.1915619969367981\n",
      "training: 52 batch 397 batch_loss: 0.19266971945762634\n",
      "training: 52 batch 398 batch_loss: 0.1960146427154541\n",
      "training: 52 batch 399 batch_loss: 0.1931731402873993\n",
      "training: 52 batch 400 batch_loss: 0.19487693905830383\n",
      "training: 52 batch 401 batch_loss: 0.1996387243270874\n",
      "training: 52 batch 402 batch_loss: 0.1943313479423523\n",
      "training: 52 batch 403 batch_loss: 0.1938180923461914\n",
      "training: 52 batch 404 batch_loss: 0.19918924570083618\n",
      "training: 52 batch 405 batch_loss: 0.19702908396720886\n",
      "training: 52 batch 406 batch_loss: 0.1959792971611023\n",
      "training: 52 batch 407 batch_loss: 0.19773700833320618\n",
      "training: 52 batch 408 batch_loss: 0.19229909777641296\n",
      "training: 52 batch 409 batch_loss: 0.20142707228660583\n",
      "training: 52 batch 410 batch_loss: 0.19275468587875366\n",
      "training: 52 batch 411 batch_loss: 0.19220203161239624\n",
      "training: 52 batch 412 batch_loss: 0.19655489921569824\n",
      "training: 52 batch 413 batch_loss: 0.19280225038528442\n",
      "training: 52 batch 414 batch_loss: 0.19923242926597595\n",
      "training: 52 batch 415 batch_loss: 0.19884276390075684\n",
      "training: 52 batch 416 batch_loss: 0.1981736123561859\n",
      "training: 52 batch 417 batch_loss: 0.19686713814735413\n",
      "training: 52 batch 418 batch_loss: 0.197361558675766\n",
      "training: 52 batch 419 batch_loss: 0.1992504596710205\n",
      "training: 52 batch 420 batch_loss: 0.19295620918273926\n",
      "training: 52 batch 421 batch_loss: 0.1975691020488739\n",
      "training: 52 batch 422 batch_loss: 0.1973390281200409\n",
      "training: 52 batch 423 batch_loss: 0.19188162684440613\n",
      "training: 52 batch 424 batch_loss: 0.19736036658287048\n",
      "training: 52 batch 425 batch_loss: 0.19200918078422546\n",
      "training: 52 batch 426 batch_loss: 0.19534972310066223\n",
      "training: 52 batch 427 batch_loss: 0.19658160209655762\n",
      "training: 52 batch 428 batch_loss: 0.19630181789398193\n",
      "training: 52 batch 429 batch_loss: 0.19624504446983337\n",
      "training: 52 batch 430 batch_loss: 0.1955726146697998\n",
      "training: 52 batch 431 batch_loss: 0.19532793760299683\n",
      "training: 52 batch 432 batch_loss: 0.19231095910072327\n",
      "training: 52 batch 433 batch_loss: 0.19491982460021973\n",
      "training: 52 batch 434 batch_loss: 0.19618839025497437\n",
      "training: 52 batch 435 batch_loss: 0.19806036353111267\n",
      "training: 52 batch 436 batch_loss: 0.19430309534072876\n",
      "training: 52 batch 437 batch_loss: 0.19789499044418335\n",
      "training: 52 batch 438 batch_loss: 0.1949526071548462\n",
      "training: 52 batch 439 batch_loss: 0.19525033235549927\n",
      "training: 52 batch 440 batch_loss: 0.19650787115097046\n",
      "training: 52 batch 441 batch_loss: 0.1957682967185974\n",
      "training: 52 batch 442 batch_loss: 0.19704824686050415\n",
      "training: 52 batch 443 batch_loss: 0.1960393488407135\n",
      "training: 52 batch 444 batch_loss: 0.19474762678146362\n",
      "training: 52 batch 445 batch_loss: 0.19298386573791504\n",
      "training: 52 batch 446 batch_loss: 0.19377344846725464\n",
      "training: 52 batch 447 batch_loss: 0.19882163405418396\n",
      "training: 52 batch 448 batch_loss: 0.19432798027992249\n",
      "training: 52 batch 449 batch_loss: 0.19610977172851562\n",
      "training: 52 batch 450 batch_loss: 0.20014795660972595\n",
      "training: 52 batch 451 batch_loss: 0.20061379671096802\n",
      "training: 52 batch 452 batch_loss: 0.19780993461608887\n",
      "training: 52 batch 453 batch_loss: 0.19677317142486572\n",
      "training: 52 batch 454 batch_loss: 0.19274434447288513\n",
      "training: 52 batch 455 batch_loss: 0.19448378682136536\n",
      "training: 52 batch 456 batch_loss: 0.19822412729263306\n",
      "training: 52 batch 457 batch_loss: 0.19792509078979492\n",
      "training: 52 batch 458 batch_loss: 0.19663619995117188\n",
      "training: 52 batch 459 batch_loss: 0.20247554779052734\n",
      "training: 52 batch 460 batch_loss: 0.19710618257522583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 52 batch 461 batch_loss: 0.19522926211357117\n",
      "training: 52 batch 462 batch_loss: 0.2014482319355011\n",
      "training: 52 batch 463 batch_loss: 0.1935042142868042\n",
      "training: 52 batch 464 batch_loss: 0.1983078122138977\n",
      "training: 52 batch 465 batch_loss: 0.1917518675327301\n",
      "training: 52 batch 466 batch_loss: 0.19757667183876038\n",
      "training: 52 batch 467 batch_loss: 0.1968795359134674\n",
      "training: 52 batch 468 batch_loss: 0.19429710507392883\n",
      "training: 52 batch 469 batch_loss: 0.19394239783287048\n",
      "training: 52 batch 470 batch_loss: 0.19269421696662903\n",
      "training: 52 batch 471 batch_loss: 0.19882771372795105\n",
      "training: 52 batch 472 batch_loss: 0.19660449028015137\n",
      "training: 52 batch 473 batch_loss: 0.1966174840927124\n",
      "training: 52 batch 474 batch_loss: 0.19087186455726624\n",
      "training: 52 batch 475 batch_loss: 0.19732600450515747\n",
      "training: 52 batch 476 batch_loss: 0.1956557333469391\n",
      "training: 52 batch 477 batch_loss: 0.19746246933937073\n",
      "training: 52 batch 478 batch_loss: 0.1947559416294098\n",
      "training: 52 batch 479 batch_loss: 0.19625556468963623\n",
      "training: 52 batch 480 batch_loss: 0.19429942965507507\n",
      "training: 52 batch 481 batch_loss: 0.19628795981407166\n",
      "training: 52 batch 482 batch_loss: 0.19763267040252686\n",
      "training: 52 batch 483 batch_loss: 0.20269590616226196\n",
      "training: 52 batch 484 batch_loss: 0.1945786476135254\n",
      "training: 52 batch 485 batch_loss: 0.19523656368255615\n",
      "training: 52 batch 486 batch_loss: 0.1962672472000122\n",
      "training: 52 batch 487 batch_loss: 0.19826790690422058\n",
      "training: 52 batch 488 batch_loss: 0.1964685320854187\n",
      "training: 52 batch 489 batch_loss: 0.19396722316741943\n",
      "training: 52 batch 490 batch_loss: 0.19568628072738647\n",
      "training: 52 batch 491 batch_loss: 0.1958131492137909\n",
      "training: 52 batch 492 batch_loss: 0.19359499216079712\n",
      "training: 52 batch 493 batch_loss: 0.2005223035812378\n",
      "training: 52 batch 494 batch_loss: 0.19467127323150635\n",
      "training: 52 batch 495 batch_loss: 0.19253602623939514\n",
      "training: 52 batch 496 batch_loss: 0.1969851553440094\n",
      "training: 52 batch 497 batch_loss: 0.19897136092185974\n",
      "training: 52 batch 498 batch_loss: 0.19305282831192017\n",
      "training: 52 batch 499 batch_loss: 0.1962333619594574\n",
      "training: 52 batch 500 batch_loss: 0.19661223888397217\n",
      "training: 52 batch 501 batch_loss: 0.19651636481285095\n",
      "training: 52 batch 502 batch_loss: 0.19455358386039734\n",
      "training: 52 batch 503 batch_loss: 0.19613435864448547\n",
      "training: 52 batch 504 batch_loss: 0.19461512565612793\n",
      "training: 52 batch 505 batch_loss: 0.19505944848060608\n",
      "training: 52 batch 506 batch_loss: 0.19232001900672913\n",
      "training: 52 batch 507 batch_loss: 0.19174852967262268\n",
      "training: 52 batch 508 batch_loss: 0.1928359568119049\n",
      "training: 52 batch 509 batch_loss: 0.1962452232837677\n",
      "training: 52 batch 510 batch_loss: 0.19536104798316956\n",
      "training: 52 batch 511 batch_loss: 0.19578340649604797\n",
      "training: 52 batch 512 batch_loss: 0.19735431671142578\n",
      "training: 52 batch 513 batch_loss: 0.19569003582000732\n",
      "training: 52 batch 514 batch_loss: 0.1955164670944214\n",
      "training: 52 batch 515 batch_loss: 0.19254153966903687\n",
      "training: 52 batch 516 batch_loss: 0.19466474652290344\n",
      "training: 52 batch 517 batch_loss: 0.200118750333786\n",
      "training: 52 batch 518 batch_loss: 0.1950514018535614\n",
      "training: 52 batch 519 batch_loss: 0.19779962301254272\n",
      "training: 52 batch 520 batch_loss: 0.19867557287216187\n",
      "training: 52 batch 521 batch_loss: 0.1974799633026123\n",
      "training: 52 batch 522 batch_loss: 0.19780123233795166\n",
      "training: 52 batch 523 batch_loss: 0.20028221607208252\n",
      "training: 52 batch 524 batch_loss: 0.19624269008636475\n",
      "training: 52 batch 525 batch_loss: 0.19569998979568481\n",
      "training: 52 batch 526 batch_loss: 0.19932571053504944\n",
      "training: 52 batch 527 batch_loss: 0.19804030656814575\n",
      "training: 52 batch 528 batch_loss: 0.19632568955421448\n",
      "training: 52 batch 529 batch_loss: 0.19868382811546326\n",
      "training: 52 batch 530 batch_loss: 0.19235560297966003\n",
      "training: 52 batch 531 batch_loss: 0.19493263959884644\n",
      "training: 52 batch 532 batch_loss: 0.19605958461761475\n",
      "training: 52 batch 533 batch_loss: 0.19774901866912842\n",
      "training: 52 batch 534 batch_loss: 0.19499510526657104\n",
      "training: 52 batch 535 batch_loss: 0.1946471929550171\n",
      "training: 52 batch 536 batch_loss: 0.19695547223091125\n",
      "training: 52 batch 537 batch_loss: 0.19494622945785522\n",
      "training: 52 batch 538 batch_loss: 0.19614645838737488\n",
      "training: 52 batch 539 batch_loss: 0.19301298260688782\n",
      "training: 52 batch 540 batch_loss: 0.1936987042427063\n",
      "training: 52 batch 541 batch_loss: 0.1945345103740692\n",
      "training: 52 batch 542 batch_loss: 0.19438976049423218\n",
      "training: 52 batch 543 batch_loss: 0.19919833540916443\n",
      "training: 52 batch 544 batch_loss: 0.19702941179275513\n",
      "training: 52 batch 545 batch_loss: 0.19705405831336975\n",
      "training: 52 batch 546 batch_loss: 0.19977617263793945\n",
      "training: 52 batch 547 batch_loss: 0.19168493151664734\n",
      "training: 52 batch 548 batch_loss: 0.19436746835708618\n",
      "training: 52 batch 549 batch_loss: 0.19597342610359192\n",
      "training: 52 batch 550 batch_loss: 0.19426774978637695\n",
      "training: 52 batch 551 batch_loss: 0.19803205132484436\n",
      "training: 52 batch 552 batch_loss: 0.20128431916236877\n",
      "training: 52 batch 553 batch_loss: 0.19548216462135315\n",
      "training: 52 batch 554 batch_loss: 0.19322824478149414\n",
      "training: 52 batch 555 batch_loss: 0.19788885116577148\n",
      "training: 52 batch 556 batch_loss: 0.19875574111938477\n",
      "training: 52 batch 557 batch_loss: 0.1955641508102417\n",
      "training: 52 batch 558 batch_loss: 0.19424012303352356\n",
      "training: 52 batch 559 batch_loss: 0.19263702630996704\n",
      "training: 52 batch 560 batch_loss: 0.19858723878860474\n",
      "training: 52 batch 561 batch_loss: 0.19731691479682922\n",
      "training: 52 batch 562 batch_loss: 0.1987174153327942\n",
      "training: 52 batch 563 batch_loss: 0.19459664821624756\n",
      "training: 52 batch 564 batch_loss: 0.20081046223640442\n",
      "training: 52 batch 565 batch_loss: 0.19469743967056274\n",
      "training: 52 batch 566 batch_loss: 0.1975630223751068\n",
      "training: 52 batch 567 batch_loss: 0.19542598724365234\n",
      "training: 52 batch 568 batch_loss: 0.20024564862251282\n",
      "training: 52 batch 569 batch_loss: 0.19548219442367554\n",
      "training: 52 batch 570 batch_loss: 0.19818562269210815\n",
      "training: 52 batch 571 batch_loss: 0.19541704654693604\n",
      "training: 52 batch 572 batch_loss: 0.19375073909759521\n",
      "training: 52 batch 573 batch_loss: 0.19919919967651367\n",
      "training: 52 batch 574 batch_loss: 0.19621542096138\n",
      "training: 52 batch 575 batch_loss: 0.19701817631721497\n",
      "training: 52 batch 576 batch_loss: 0.20802944898605347\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 52, Hit Ratio:0.03260569033921511 | Precision:0.04810773616435663 | Recall:0.06380759989981553 | NDCG:0.06204338591648152\n",
      "*Best Performance* \n",
      "Epoch: 52, Hit Ratio:0.03260569033921511 | Precision:0.04810773616435663 | Recall:0.06380759989981553 | MDCG:0.06204338591648152\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 53 batch 0 batch_loss: 0.19755196571350098\n",
      "training: 53 batch 1 batch_loss: 0.19636639952659607\n",
      "training: 53 batch 2 batch_loss: 0.19518229365348816\n",
      "training: 53 batch 3 batch_loss: 0.1951887607574463\n",
      "training: 53 batch 4 batch_loss: 0.1922588348388672\n",
      "training: 53 batch 5 batch_loss: 0.19562417268753052\n",
      "training: 53 batch 6 batch_loss: 0.19475054740905762\n",
      "training: 53 batch 7 batch_loss: 0.19586747884750366\n",
      "training: 53 batch 8 batch_loss: 0.19938859343528748\n",
      "training: 53 batch 9 batch_loss: 0.19437721371650696\n",
      "training: 53 batch 10 batch_loss: 0.19622302055358887\n",
      "training: 53 batch 11 batch_loss: 0.1962718963623047\n",
      "training: 53 batch 12 batch_loss: 0.1932165026664734\n",
      "training: 53 batch 13 batch_loss: 0.18989074230194092\n",
      "training: 53 batch 14 batch_loss: 0.19172823429107666\n",
      "training: 53 batch 15 batch_loss: 0.19079804420471191\n",
      "training: 53 batch 16 batch_loss: 0.1956254243850708\n",
      "training: 53 batch 17 batch_loss: 0.19059395790100098\n",
      "training: 53 batch 18 batch_loss: 0.199045330286026\n",
      "training: 53 batch 19 batch_loss: 0.19441497325897217\n",
      "training: 53 batch 20 batch_loss: 0.19060537219047546\n",
      "training: 53 batch 21 batch_loss: 0.19472718238830566\n",
      "training: 53 batch 22 batch_loss: 0.1905432939529419\n",
      "training: 53 batch 23 batch_loss: 0.19535213708877563\n",
      "training: 53 batch 24 batch_loss: 0.19927161931991577\n",
      "training: 53 batch 25 batch_loss: 0.19439542293548584\n",
      "training: 53 batch 26 batch_loss: 0.1956433653831482\n",
      "training: 53 batch 27 batch_loss: 0.19153428077697754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 53 batch 28 batch_loss: 0.1962711215019226\n",
      "training: 53 batch 29 batch_loss: 0.19541946053504944\n",
      "training: 53 batch 30 batch_loss: 0.19711697101593018\n",
      "training: 53 batch 31 batch_loss: 0.19515565037727356\n",
      "training: 53 batch 32 batch_loss: 0.1912786066532135\n",
      "training: 53 batch 33 batch_loss: 0.19642341136932373\n",
      "training: 53 batch 34 batch_loss: 0.19558307528495789\n",
      "training: 53 batch 35 batch_loss: 0.19400233030319214\n",
      "training: 53 batch 36 batch_loss: 0.19545352458953857\n",
      "training: 53 batch 37 batch_loss: 0.19638168811798096\n",
      "training: 53 batch 38 batch_loss: 0.1967523694038391\n",
      "training: 53 batch 39 batch_loss: 0.19309523701667786\n",
      "training: 53 batch 40 batch_loss: 0.19600611925125122\n",
      "training: 53 batch 41 batch_loss: 0.19389775395393372\n",
      "training: 53 batch 42 batch_loss: 0.19138363003730774\n",
      "training: 53 batch 43 batch_loss: 0.195276141166687\n",
      "training: 53 batch 44 batch_loss: 0.197762131690979\n",
      "training: 53 batch 45 batch_loss: 0.1917257010936737\n",
      "training: 53 batch 46 batch_loss: 0.1954878270626068\n",
      "training: 53 batch 47 batch_loss: 0.19678348302841187\n",
      "training: 53 batch 48 batch_loss: 0.19480803608894348\n",
      "training: 53 batch 49 batch_loss: 0.19461104273796082\n",
      "training: 53 batch 50 batch_loss: 0.1971707046031952\n",
      "training: 53 batch 51 batch_loss: 0.19409316778182983\n",
      "training: 53 batch 52 batch_loss: 0.19498279690742493\n",
      "training: 53 batch 53 batch_loss: 0.19360500574111938\n",
      "training: 53 batch 54 batch_loss: 0.19505521655082703\n",
      "training: 53 batch 55 batch_loss: 0.19868633151054382\n",
      "training: 53 batch 56 batch_loss: 0.19432294368743896\n",
      "training: 53 batch 57 batch_loss: 0.19419729709625244\n",
      "training: 53 batch 58 batch_loss: 0.19829320907592773\n",
      "training: 53 batch 59 batch_loss: 0.19420531392097473\n",
      "training: 53 batch 60 batch_loss: 0.19126123189926147\n",
      "training: 53 batch 61 batch_loss: 0.19024235010147095\n",
      "training: 53 batch 62 batch_loss: 0.1934015154838562\n",
      "training: 53 batch 63 batch_loss: 0.18918773531913757\n",
      "training: 53 batch 64 batch_loss: 0.19532030820846558\n",
      "training: 53 batch 65 batch_loss: 0.1966353952884674\n",
      "training: 53 batch 66 batch_loss: 0.19841212034225464\n",
      "training: 53 batch 67 batch_loss: 0.19157066941261292\n",
      "training: 53 batch 68 batch_loss: 0.19115394353866577\n",
      "training: 53 batch 69 batch_loss: 0.193708598613739\n",
      "training: 53 batch 70 batch_loss: 0.19347262382507324\n",
      "training: 53 batch 71 batch_loss: 0.19140708446502686\n",
      "training: 53 batch 72 batch_loss: 0.190916508436203\n",
      "training: 53 batch 73 batch_loss: 0.19139671325683594\n",
      "training: 53 batch 74 batch_loss: 0.18655964732170105\n",
      "training: 53 batch 75 batch_loss: 0.19395849108695984\n",
      "training: 53 batch 76 batch_loss: 0.19233953952789307\n",
      "training: 53 batch 77 batch_loss: 0.1945633590221405\n",
      "training: 53 batch 78 batch_loss: 0.1948380172252655\n",
      "training: 53 batch 79 batch_loss: 0.19603577256202698\n",
      "training: 53 batch 80 batch_loss: 0.19823536276817322\n",
      "training: 53 batch 81 batch_loss: 0.19335037469863892\n",
      "training: 53 batch 82 batch_loss: 0.1919882595539093\n",
      "training: 53 batch 83 batch_loss: 0.19873040914535522\n",
      "training: 53 batch 84 batch_loss: 0.19671520590782166\n",
      "training: 53 batch 85 batch_loss: 0.195767343044281\n",
      "training: 53 batch 86 batch_loss: 0.19431176781654358\n",
      "training: 53 batch 87 batch_loss: 0.19490718841552734\n",
      "training: 53 batch 88 batch_loss: 0.19413593411445618\n",
      "training: 53 batch 89 batch_loss: 0.1943078339099884\n",
      "training: 53 batch 90 batch_loss: 0.198097825050354\n",
      "training: 53 batch 91 batch_loss: 0.19458812475204468\n",
      "training: 53 batch 92 batch_loss: 0.19256669282913208\n",
      "training: 53 batch 93 batch_loss: 0.19428902864456177\n",
      "training: 53 batch 94 batch_loss: 0.1934424638748169\n",
      "training: 53 batch 95 batch_loss: 0.19014284014701843\n",
      "training: 53 batch 96 batch_loss: 0.19591230154037476\n",
      "training: 53 batch 97 batch_loss: 0.18995675444602966\n",
      "training: 53 batch 98 batch_loss: 0.1955573558807373\n",
      "training: 53 batch 99 batch_loss: 0.1966792643070221\n",
      "training: 53 batch 100 batch_loss: 0.19689473509788513\n",
      "training: 53 batch 101 batch_loss: 0.197055846452713\n",
      "training: 53 batch 102 batch_loss: 0.1950167715549469\n",
      "training: 53 batch 103 batch_loss: 0.1976507306098938\n",
      "training: 53 batch 104 batch_loss: 0.19503405690193176\n",
      "training: 53 batch 105 batch_loss: 0.19595825672149658\n",
      "training: 53 batch 106 batch_loss: 0.1966080367565155\n",
      "training: 53 batch 107 batch_loss: 0.1991451382637024\n",
      "training: 53 batch 108 batch_loss: 0.19540145993232727\n",
      "training: 53 batch 109 batch_loss: 0.19278660416603088\n",
      "training: 53 batch 110 batch_loss: 0.19697734713554382\n",
      "training: 53 batch 111 batch_loss: 0.1974543333053589\n",
      "training: 53 batch 112 batch_loss: 0.1975269317626953\n",
      "training: 53 batch 113 batch_loss: 0.19144034385681152\n",
      "training: 53 batch 114 batch_loss: 0.19445326924324036\n",
      "training: 53 batch 115 batch_loss: 0.19495165348052979\n",
      "training: 53 batch 116 batch_loss: 0.19658860564231873\n",
      "training: 53 batch 117 batch_loss: 0.19741883873939514\n",
      "training: 53 batch 118 batch_loss: 0.19626301527023315\n",
      "training: 53 batch 119 batch_loss: 0.19806689023971558\n",
      "training: 53 batch 120 batch_loss: 0.19241085648536682\n",
      "training: 53 batch 121 batch_loss: 0.19019058346748352\n",
      "training: 53 batch 122 batch_loss: 0.19840538501739502\n",
      "training: 53 batch 123 batch_loss: 0.1955852508544922\n",
      "training: 53 batch 124 batch_loss: 0.19966048002243042\n",
      "training: 53 batch 125 batch_loss: 0.1977308690547943\n",
      "training: 53 batch 126 batch_loss: 0.19490495324134827\n",
      "training: 53 batch 127 batch_loss: 0.19218429923057556\n",
      "training: 53 batch 128 batch_loss: 0.19737029075622559\n",
      "training: 53 batch 129 batch_loss: 0.19626972079277039\n",
      "training: 53 batch 130 batch_loss: 0.19813993573188782\n",
      "training: 53 batch 131 batch_loss: 0.19419190287590027\n",
      "training: 53 batch 132 batch_loss: 0.1928846836090088\n",
      "training: 53 batch 133 batch_loss: 0.19730627536773682\n",
      "training: 53 batch 134 batch_loss: 0.19569909572601318\n",
      "training: 53 batch 135 batch_loss: 0.19527655839920044\n",
      "training: 53 batch 136 batch_loss: 0.19103863835334778\n",
      "training: 53 batch 137 batch_loss: 0.19650313258171082\n",
      "training: 53 batch 138 batch_loss: 0.19156554341316223\n",
      "training: 53 batch 139 batch_loss: 0.19910478591918945\n",
      "training: 53 batch 140 batch_loss: 0.1983489692211151\n",
      "training: 53 batch 141 batch_loss: 0.1928897500038147\n",
      "training: 53 batch 142 batch_loss: 0.19535350799560547\n",
      "training: 53 batch 143 batch_loss: 0.1908164918422699\n",
      "training: 53 batch 144 batch_loss: 0.19672352075576782\n",
      "training: 53 batch 145 batch_loss: 0.19707179069519043\n",
      "training: 53 batch 146 batch_loss: 0.19443279504776\n",
      "training: 53 batch 147 batch_loss: 0.20232826471328735\n",
      "training: 53 batch 148 batch_loss: 0.1945468783378601\n",
      "training: 53 batch 149 batch_loss: 0.19356802105903625\n",
      "training: 53 batch 150 batch_loss: 0.197801411151886\n",
      "training: 53 batch 151 batch_loss: 0.19503909349441528\n",
      "training: 53 batch 152 batch_loss: 0.19933563470840454\n",
      "training: 53 batch 153 batch_loss: 0.19211524724960327\n",
      "training: 53 batch 154 batch_loss: 0.19828885793685913\n",
      "training: 53 batch 155 batch_loss: 0.1954764723777771\n",
      "training: 53 batch 156 batch_loss: 0.1954823136329651\n",
      "training: 53 batch 157 batch_loss: 0.19419589638710022\n",
      "training: 53 batch 158 batch_loss: 0.19819238781929016\n",
      "training: 53 batch 159 batch_loss: 0.1965804100036621\n",
      "training: 53 batch 160 batch_loss: 0.19356852769851685\n",
      "training: 53 batch 161 batch_loss: 0.19664347171783447\n",
      "training: 53 batch 162 batch_loss: 0.19531852006912231\n",
      "training: 53 batch 163 batch_loss: 0.19516634941101074\n",
      "training: 53 batch 164 batch_loss: 0.19680440425872803\n",
      "training: 53 batch 165 batch_loss: 0.19379830360412598\n",
      "training: 53 batch 166 batch_loss: 0.19259575009346008\n",
      "training: 53 batch 167 batch_loss: 0.19882047176361084\n",
      "training: 53 batch 168 batch_loss: 0.19571593403816223\n",
      "training: 53 batch 169 batch_loss: 0.1954624354839325\n",
      "training: 53 batch 170 batch_loss: 0.1959407925605774\n",
      "training: 53 batch 171 batch_loss: 0.19917067885398865\n",
      "training: 53 batch 172 batch_loss: 0.19919657707214355\n",
      "training: 53 batch 173 batch_loss: 0.19842717051506042\n",
      "training: 53 batch 174 batch_loss: 0.1956186294555664\n",
      "training: 53 batch 175 batch_loss: 0.19515854120254517\n",
      "training: 53 batch 176 batch_loss: 0.19745919108390808\n",
      "training: 53 batch 177 batch_loss: 0.19575965404510498\n",
      "training: 53 batch 178 batch_loss: 0.19537416100502014\n",
      "training: 53 batch 179 batch_loss: 0.19498109817504883\n",
      "training: 53 batch 180 batch_loss: 0.19409900903701782\n",
      "training: 53 batch 181 batch_loss: 0.19605088233947754\n",
      "training: 53 batch 182 batch_loss: 0.19619569182395935\n",
      "training: 53 batch 183 batch_loss: 0.19515496492385864\n",
      "training: 53 batch 184 batch_loss: 0.1956082284450531\n",
      "training: 53 batch 185 batch_loss: 0.19268861413002014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 53 batch 186 batch_loss: 0.19828733801841736\n",
      "training: 53 batch 187 batch_loss: 0.1966169774532318\n",
      "training: 53 batch 188 batch_loss: 0.19882464408874512\n",
      "training: 53 batch 189 batch_loss: 0.19543567299842834\n",
      "training: 53 batch 190 batch_loss: 0.19617325067520142\n",
      "training: 53 batch 191 batch_loss: 0.19401192665100098\n",
      "training: 53 batch 192 batch_loss: 0.19543740153312683\n",
      "training: 53 batch 193 batch_loss: 0.1976023018360138\n",
      "training: 53 batch 194 batch_loss: 0.19385725259780884\n",
      "training: 53 batch 195 batch_loss: 0.1957302689552307\n",
      "training: 53 batch 196 batch_loss: 0.19325041770935059\n",
      "training: 53 batch 197 batch_loss: 0.19424745440483093\n",
      "training: 53 batch 198 batch_loss: 0.19791138172149658\n",
      "training: 53 batch 199 batch_loss: 0.1936441957950592\n",
      "training: 53 batch 200 batch_loss: 0.19779855012893677\n",
      "training: 53 batch 201 batch_loss: 0.19336381554603577\n",
      "training: 53 batch 202 batch_loss: 0.19731640815734863\n",
      "training: 53 batch 203 batch_loss: 0.19429808855056763\n",
      "training: 53 batch 204 batch_loss: 0.19191673398017883\n",
      "training: 53 batch 205 batch_loss: 0.19386160373687744\n",
      "training: 53 batch 206 batch_loss: 0.19612565636634827\n",
      "training: 53 batch 207 batch_loss: 0.19376343488693237\n",
      "training: 53 batch 208 batch_loss: 0.20246094465255737\n",
      "training: 53 batch 209 batch_loss: 0.19795295596122742\n",
      "training: 53 batch 210 batch_loss: 0.19691553711891174\n",
      "training: 53 batch 211 batch_loss: 0.19325220584869385\n",
      "training: 53 batch 212 batch_loss: 0.19686970114707947\n",
      "training: 53 batch 213 batch_loss: 0.1909267008304596\n",
      "training: 53 batch 214 batch_loss: 0.19377923011779785\n",
      "training: 53 batch 215 batch_loss: 0.19957107305526733\n",
      "training: 53 batch 216 batch_loss: 0.19399219751358032\n",
      "training: 53 batch 217 batch_loss: 0.19693535566329956\n",
      "training: 53 batch 218 batch_loss: 0.19695976376533508\n",
      "training: 53 batch 219 batch_loss: 0.19942119717597961\n",
      "training: 53 batch 220 batch_loss: 0.1937694549560547\n",
      "training: 53 batch 221 batch_loss: 0.19301879405975342\n",
      "training: 53 batch 222 batch_loss: 0.19461223483085632\n",
      "training: 53 batch 223 batch_loss: 0.19329845905303955\n",
      "training: 53 batch 224 batch_loss: 0.19746693968772888\n",
      "training: 53 batch 225 batch_loss: 0.19674864411354065\n",
      "training: 53 batch 226 batch_loss: 0.1967392861843109\n",
      "training: 53 batch 227 batch_loss: 0.19422659277915955\n",
      "training: 53 batch 228 batch_loss: 0.1963908076286316\n",
      "training: 53 batch 229 batch_loss: 0.19745704531669617\n",
      "training: 53 batch 230 batch_loss: 0.19550341367721558\n",
      "training: 53 batch 231 batch_loss: 0.1940462589263916\n",
      "training: 53 batch 232 batch_loss: 0.19450926780700684\n",
      "training: 53 batch 233 batch_loss: 0.1968574821949005\n",
      "training: 53 batch 234 batch_loss: 0.1953696310520172\n",
      "training: 53 batch 235 batch_loss: 0.1964014172554016\n",
      "training: 53 batch 236 batch_loss: 0.19723039865493774\n",
      "training: 53 batch 237 batch_loss: 0.19644033908843994\n",
      "training: 53 batch 238 batch_loss: 0.19529545307159424\n",
      "training: 53 batch 239 batch_loss: 0.1955888271331787\n",
      "training: 53 batch 240 batch_loss: 0.19962817430496216\n",
      "training: 53 batch 241 batch_loss: 0.19476428627967834\n",
      "training: 53 batch 242 batch_loss: 0.1973266303539276\n",
      "training: 53 batch 243 batch_loss: 0.1994762122631073\n",
      "training: 53 batch 244 batch_loss: 0.1972973346710205\n",
      "training: 53 batch 245 batch_loss: 0.19728082418441772\n",
      "training: 53 batch 246 batch_loss: 0.1964048147201538\n",
      "training: 53 batch 247 batch_loss: 0.19856509566307068\n",
      "training: 53 batch 248 batch_loss: 0.19532239437103271\n",
      "training: 53 batch 249 batch_loss: 0.1958221197128296\n",
      "training: 53 batch 250 batch_loss: 0.19313305616378784\n",
      "training: 53 batch 251 batch_loss: 0.19313427805900574\n",
      "training: 53 batch 252 batch_loss: 0.19597205519676208\n",
      "training: 53 batch 253 batch_loss: 0.1957165002822876\n",
      "training: 53 batch 254 batch_loss: 0.19428083300590515\n",
      "training: 53 batch 255 batch_loss: 0.20073232054710388\n",
      "training: 53 batch 256 batch_loss: 0.19699707627296448\n",
      "training: 53 batch 257 batch_loss: 0.2019290328025818\n",
      "training: 53 batch 258 batch_loss: 0.1944904923439026\n",
      "training: 53 batch 259 batch_loss: 0.19827035069465637\n",
      "training: 53 batch 260 batch_loss: 0.195468008518219\n",
      "training: 53 batch 261 batch_loss: 0.19516253471374512\n",
      "training: 53 batch 262 batch_loss: 0.19958683848381042\n",
      "training: 53 batch 263 batch_loss: 0.19362205266952515\n",
      "training: 53 batch 264 batch_loss: 0.19407886266708374\n",
      "training: 53 batch 265 batch_loss: 0.19478127360343933\n",
      "training: 53 batch 266 batch_loss: 0.19578546285629272\n",
      "training: 53 batch 267 batch_loss: 0.19619327783584595\n",
      "training: 53 batch 268 batch_loss: 0.1991482675075531\n",
      "training: 53 batch 269 batch_loss: 0.1973804235458374\n",
      "training: 53 batch 270 batch_loss: 0.19269472360610962\n",
      "training: 53 batch 271 batch_loss: 0.19861739873886108\n",
      "training: 53 batch 272 batch_loss: 0.19479304552078247\n",
      "training: 53 batch 273 batch_loss: 0.19513526558876038\n",
      "training: 53 batch 274 batch_loss: 0.19513124227523804\n",
      "training: 53 batch 275 batch_loss: 0.20145303010940552\n",
      "training: 53 batch 276 batch_loss: 0.19733113050460815\n",
      "training: 53 batch 277 batch_loss: 0.19604545831680298\n",
      "training: 53 batch 278 batch_loss: 0.19678789377212524\n",
      "training: 53 batch 279 batch_loss: 0.1929066777229309\n",
      "training: 53 batch 280 batch_loss: 0.19793900847434998\n",
      "training: 53 batch 281 batch_loss: 0.19623473286628723\n",
      "training: 53 batch 282 batch_loss: 0.1978434920310974\n",
      "training: 53 batch 283 batch_loss: 0.1928306221961975\n",
      "training: 53 batch 284 batch_loss: 0.20024341344833374\n",
      "training: 53 batch 285 batch_loss: 0.19871771335601807\n",
      "training: 53 batch 286 batch_loss: 0.19360223412513733\n",
      "training: 53 batch 287 batch_loss: 0.1963253617286682\n",
      "training: 53 batch 288 batch_loss: 0.19440999627113342\n",
      "training: 53 batch 289 batch_loss: 0.19975978136062622\n",
      "training: 53 batch 290 batch_loss: 0.19788599014282227\n",
      "training: 53 batch 291 batch_loss: 0.19525453448295593\n",
      "training: 53 batch 292 batch_loss: 0.1961677372455597\n",
      "training: 53 batch 293 batch_loss: 0.19271200895309448\n",
      "training: 53 batch 294 batch_loss: 0.19577080011367798\n",
      "training: 53 batch 295 batch_loss: 0.1976274847984314\n",
      "training: 53 batch 296 batch_loss: 0.20005229115486145\n",
      "training: 53 batch 297 batch_loss: 0.19732579588890076\n",
      "training: 53 batch 298 batch_loss: 0.19708341360092163\n",
      "training: 53 batch 299 batch_loss: 0.19534236192703247\n",
      "training: 53 batch 300 batch_loss: 0.19748130440711975\n",
      "training: 53 batch 301 batch_loss: 0.1985393464565277\n",
      "training: 53 batch 302 batch_loss: 0.19578179717063904\n",
      "training: 53 batch 303 batch_loss: 0.19340384006500244\n",
      "training: 53 batch 304 batch_loss: 0.19531917572021484\n",
      "training: 53 batch 305 batch_loss: 0.1946294605731964\n",
      "training: 53 batch 306 batch_loss: 0.19779261946678162\n",
      "training: 53 batch 307 batch_loss: 0.19634154438972473\n",
      "training: 53 batch 308 batch_loss: 0.19670692086219788\n",
      "training: 53 batch 309 batch_loss: 0.19709140062332153\n",
      "training: 53 batch 310 batch_loss: 0.19904804229736328\n",
      "training: 53 batch 311 batch_loss: 0.1980615258216858\n",
      "training: 53 batch 312 batch_loss: 0.19866865873336792\n",
      "training: 53 batch 313 batch_loss: 0.19480928778648376\n",
      "training: 53 batch 314 batch_loss: 0.2003144919872284\n",
      "training: 53 batch 315 batch_loss: 0.19309377670288086\n",
      "training: 53 batch 316 batch_loss: 0.19565680623054504\n",
      "training: 53 batch 317 batch_loss: 0.19337108731269836\n",
      "training: 53 batch 318 batch_loss: 0.19640880823135376\n",
      "training: 53 batch 319 batch_loss: 0.19957822561264038\n",
      "training: 53 batch 320 batch_loss: 0.1972011923789978\n",
      "training: 53 batch 321 batch_loss: 0.19433414936065674\n",
      "training: 53 batch 322 batch_loss: 0.19388681650161743\n",
      "training: 53 batch 323 batch_loss: 0.19857266545295715\n",
      "training: 53 batch 324 batch_loss: 0.19142746925354004\n",
      "training: 53 batch 325 batch_loss: 0.19934266805648804\n",
      "training: 53 batch 326 batch_loss: 0.19436869025230408\n",
      "training: 53 batch 327 batch_loss: 0.19859638810157776\n",
      "training: 53 batch 328 batch_loss: 0.19980263710021973\n",
      "training: 53 batch 329 batch_loss: 0.19555258750915527\n",
      "training: 53 batch 330 batch_loss: 0.19516971707344055\n",
      "training: 53 batch 331 batch_loss: 0.19626277685165405\n",
      "training: 53 batch 332 batch_loss: 0.19789788126945496\n",
      "training: 53 batch 333 batch_loss: 0.19960349798202515\n",
      "training: 53 batch 334 batch_loss: 0.19432178139686584\n",
      "training: 53 batch 335 batch_loss: 0.19869357347488403\n",
      "training: 53 batch 336 batch_loss: 0.19491985440254211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 53 batch 337 batch_loss: 0.19351205229759216\n",
      "training: 53 batch 338 batch_loss: 0.20028090476989746\n",
      "training: 53 batch 339 batch_loss: 0.1974126398563385\n",
      "training: 53 batch 340 batch_loss: 0.1943644881248474\n",
      "training: 53 batch 341 batch_loss: 0.1984424591064453\n",
      "training: 53 batch 342 batch_loss: 0.1966865062713623\n",
      "training: 53 batch 343 batch_loss: 0.19596505165100098\n",
      "training: 53 batch 344 batch_loss: 0.19582882523536682\n",
      "training: 53 batch 345 batch_loss: 0.19855019450187683\n",
      "training: 53 batch 346 batch_loss: 0.19280973076820374\n",
      "training: 53 batch 347 batch_loss: 0.19644922018051147\n",
      "training: 53 batch 348 batch_loss: 0.19944080710411072\n",
      "training: 53 batch 349 batch_loss: 0.19668257236480713\n",
      "training: 53 batch 350 batch_loss: 0.20010140538215637\n",
      "training: 53 batch 351 batch_loss: 0.19708356261253357\n",
      "training: 53 batch 352 batch_loss: 0.19945690035820007\n",
      "training: 53 batch 353 batch_loss: 0.1970786452293396\n",
      "training: 53 batch 354 batch_loss: 0.19679197669029236\n",
      "training: 53 batch 355 batch_loss: 0.19537293910980225\n",
      "training: 53 batch 356 batch_loss: 0.19424453377723694\n",
      "training: 53 batch 357 batch_loss: 0.19526243209838867\n",
      "training: 53 batch 358 batch_loss: 0.19577348232269287\n",
      "training: 53 batch 359 batch_loss: 0.19531431794166565\n",
      "training: 53 batch 360 batch_loss: 0.19546175003051758\n",
      "training: 53 batch 361 batch_loss: 0.1908143162727356\n",
      "training: 53 batch 362 batch_loss: 0.193983793258667\n",
      "training: 53 batch 363 batch_loss: 0.19174683094024658\n",
      "training: 53 batch 364 batch_loss: 0.1953040361404419\n",
      "training: 53 batch 365 batch_loss: 0.19955578446388245\n",
      "training: 53 batch 366 batch_loss: 0.20090585947036743\n",
      "training: 53 batch 367 batch_loss: 0.19588637351989746\n",
      "training: 53 batch 368 batch_loss: 0.19801050424575806\n",
      "training: 53 batch 369 batch_loss: 0.19625964760780334\n",
      "training: 53 batch 370 batch_loss: 0.19588148593902588\n",
      "training: 53 batch 371 batch_loss: 0.1985936462879181\n",
      "training: 53 batch 372 batch_loss: 0.19704177975654602\n",
      "training: 53 batch 373 batch_loss: 0.19903779029846191\n",
      "training: 53 batch 374 batch_loss: 0.1943686604499817\n",
      "training: 53 batch 375 batch_loss: 0.1966191828250885\n",
      "training: 53 batch 376 batch_loss: 0.19793641567230225\n",
      "training: 53 batch 377 batch_loss: 0.1964694857597351\n",
      "training: 53 batch 378 batch_loss: 0.19764766097068787\n",
      "training: 53 batch 379 batch_loss: 0.1944851279258728\n",
      "training: 53 batch 380 batch_loss: 0.20008611679077148\n",
      "training: 53 batch 381 batch_loss: 0.19610929489135742\n",
      "training: 53 batch 382 batch_loss: 0.1961294412612915\n",
      "training: 53 batch 383 batch_loss: 0.19443711638450623\n",
      "training: 53 batch 384 batch_loss: 0.19666272401809692\n",
      "training: 53 batch 385 batch_loss: 0.19932889938354492\n",
      "training: 53 batch 386 batch_loss: 0.19499796628952026\n",
      "training: 53 batch 387 batch_loss: 0.20282351970672607\n",
      "training: 53 batch 388 batch_loss: 0.19429251551628113\n",
      "training: 53 batch 389 batch_loss: 0.20016443729400635\n",
      "training: 53 batch 390 batch_loss: 0.19555658102035522\n",
      "training: 53 batch 391 batch_loss: 0.1976349949836731\n",
      "training: 53 batch 392 batch_loss: 0.1952800452709198\n",
      "training: 53 batch 393 batch_loss: 0.1937769651412964\n",
      "training: 53 batch 394 batch_loss: 0.19790667295455933\n",
      "training: 53 batch 395 batch_loss: 0.197523295879364\n",
      "training: 53 batch 396 batch_loss: 0.1947789192199707\n",
      "training: 53 batch 397 batch_loss: 0.19159677624702454\n",
      "training: 53 batch 398 batch_loss: 0.19344180822372437\n",
      "training: 53 batch 399 batch_loss: 0.1985740065574646\n",
      "training: 53 batch 400 batch_loss: 0.19652274250984192\n",
      "training: 53 batch 401 batch_loss: 0.2006130814552307\n",
      "training: 53 batch 402 batch_loss: 0.19833964109420776\n",
      "training: 53 batch 403 batch_loss: 0.19316941499710083\n",
      "training: 53 batch 404 batch_loss: 0.19410336017608643\n",
      "training: 53 batch 405 batch_loss: 0.19796085357666016\n",
      "training: 53 batch 406 batch_loss: 0.19696170091629028\n",
      "training: 53 batch 407 batch_loss: 0.1974373459815979\n",
      "training: 53 batch 408 batch_loss: 0.19188562035560608\n",
      "training: 53 batch 409 batch_loss: 0.19642889499664307\n",
      "training: 53 batch 410 batch_loss: 0.19541853666305542\n",
      "training: 53 batch 411 batch_loss: 0.19489958882331848\n",
      "training: 53 batch 412 batch_loss: 0.19457662105560303\n",
      "training: 53 batch 413 batch_loss: 0.1981801688671112\n",
      "training: 53 batch 414 batch_loss: 0.19829308986663818\n",
      "training: 53 batch 415 batch_loss: 0.19529816508293152\n",
      "training: 53 batch 416 batch_loss: 0.19627240300178528\n",
      "training: 53 batch 417 batch_loss: 0.19911441206932068\n",
      "training: 53 batch 418 batch_loss: 0.2004924714565277\n",
      "training: 53 batch 419 batch_loss: 0.19942030310630798\n",
      "training: 53 batch 420 batch_loss: 0.19500580430030823\n",
      "training: 53 batch 421 batch_loss: 0.19896423816680908\n",
      "training: 53 batch 422 batch_loss: 0.1965320110321045\n",
      "training: 53 batch 423 batch_loss: 0.2011982798576355\n",
      "training: 53 batch 424 batch_loss: 0.1984763741493225\n",
      "training: 53 batch 425 batch_loss: 0.19832155108451843\n",
      "training: 53 batch 426 batch_loss: 0.1909678876399994\n",
      "training: 53 batch 427 batch_loss: 0.19877150654792786\n",
      "training: 53 batch 428 batch_loss: 0.19829419255256653\n",
      "training: 53 batch 429 batch_loss: 0.19855302572250366\n",
      "training: 53 batch 430 batch_loss: 0.1969982087612152\n",
      "training: 53 batch 431 batch_loss: 0.1911846101284027\n",
      "training: 53 batch 432 batch_loss: 0.20153549313545227\n",
      "training: 53 batch 433 batch_loss: 0.19541150331497192\n",
      "training: 53 batch 434 batch_loss: 0.19507426023483276\n",
      "training: 53 batch 435 batch_loss: 0.19553253054618835\n",
      "training: 53 batch 436 batch_loss: 0.19926226139068604\n",
      "training: 53 batch 437 batch_loss: 0.19899308681488037\n",
      "training: 53 batch 438 batch_loss: 0.19696295261383057\n",
      "training: 53 batch 439 batch_loss: 0.19545358419418335\n",
      "training: 53 batch 440 batch_loss: 0.19285383820533752\n",
      "training: 53 batch 441 batch_loss: 0.1965007781982422\n",
      "training: 53 batch 442 batch_loss: 0.19443130493164062\n",
      "training: 53 batch 443 batch_loss: 0.19467827677726746\n",
      "training: 53 batch 444 batch_loss: 0.19413617253303528\n",
      "training: 53 batch 445 batch_loss: 0.1973074972629547\n",
      "training: 53 batch 446 batch_loss: 0.1976676881313324\n",
      "training: 53 batch 447 batch_loss: 0.19584670662879944\n",
      "training: 53 batch 448 batch_loss: 0.19810116291046143\n",
      "training: 53 batch 449 batch_loss: 0.19768458604812622\n",
      "training: 53 batch 450 batch_loss: 0.19738352298736572\n",
      "training: 53 batch 451 batch_loss: 0.19421935081481934\n",
      "training: 53 batch 452 batch_loss: 0.1954118311405182\n",
      "training: 53 batch 453 batch_loss: 0.1947709619998932\n",
      "training: 53 batch 454 batch_loss: 0.2038092017173767\n",
      "training: 53 batch 455 batch_loss: 0.19588541984558105\n",
      "training: 53 batch 456 batch_loss: 0.19599655270576477\n",
      "training: 53 batch 457 batch_loss: 0.19804924726486206\n",
      "training: 53 batch 458 batch_loss: 0.1980721652507782\n",
      "training: 53 batch 459 batch_loss: 0.19860410690307617\n",
      "training: 53 batch 460 batch_loss: 0.19877800345420837\n",
      "training: 53 batch 461 batch_loss: 0.19879239797592163\n",
      "training: 53 batch 462 batch_loss: 0.19701462984085083\n",
      "training: 53 batch 463 batch_loss: 0.19761186838150024\n",
      "training: 53 batch 464 batch_loss: 0.19715222716331482\n",
      "training: 53 batch 465 batch_loss: 0.19893208146095276\n",
      "training: 53 batch 466 batch_loss: 0.19719508290290833\n",
      "training: 53 batch 467 batch_loss: 0.2003747820854187\n",
      "training: 53 batch 468 batch_loss: 0.19667917490005493\n",
      "training: 53 batch 469 batch_loss: 0.1979338824748993\n",
      "training: 53 batch 470 batch_loss: 0.19625872373580933\n",
      "training: 53 batch 471 batch_loss: 0.1981525421142578\n",
      "training: 53 batch 472 batch_loss: 0.20058518648147583\n",
      "training: 53 batch 473 batch_loss: 0.19827806949615479\n",
      "training: 53 batch 474 batch_loss: 0.19617816805839539\n",
      "training: 53 batch 475 batch_loss: 0.1957419514656067\n",
      "training: 53 batch 476 batch_loss: 0.1961376667022705\n",
      "training: 53 batch 477 batch_loss: 0.1983417272567749\n",
      "training: 53 batch 478 batch_loss: 0.19614499807357788\n",
      "training: 53 batch 479 batch_loss: 0.19917815923690796\n",
      "training: 53 batch 480 batch_loss: 0.19770723581314087\n",
      "training: 53 batch 481 batch_loss: 0.19956523180007935\n",
      "training: 53 batch 482 batch_loss: 0.19484609365463257\n",
      "training: 53 batch 483 batch_loss: 0.19944533705711365\n",
      "training: 53 batch 484 batch_loss: 0.1931193470954895\n",
      "training: 53 batch 485 batch_loss: 0.19763338565826416\n",
      "training: 53 batch 486 batch_loss: 0.1967768669128418\n",
      "training: 53 batch 487 batch_loss: 0.19213268160820007\n",
      "training: 53 batch 488 batch_loss: 0.2026768922805786\n",
      "training: 53 batch 489 batch_loss: 0.1950986385345459\n",
      "training: 53 batch 490 batch_loss: 0.19880270957946777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 53 batch 491 batch_loss: 0.20317813754081726\n",
      "training: 53 batch 492 batch_loss: 0.199135422706604\n",
      "training: 53 batch 493 batch_loss: 0.19531971216201782\n",
      "training: 53 batch 494 batch_loss: 0.19177672266960144\n",
      "training: 53 batch 495 batch_loss: 0.1940060555934906\n",
      "training: 53 batch 496 batch_loss: 0.1953420341014862\n",
      "training: 53 batch 497 batch_loss: 0.19822344183921814\n",
      "training: 53 batch 498 batch_loss: 0.19217151403427124\n",
      "training: 53 batch 499 batch_loss: 0.19651421904563904\n",
      "training: 53 batch 500 batch_loss: 0.19671380519866943\n",
      "training: 53 batch 501 batch_loss: 0.19772911071777344\n",
      "training: 53 batch 502 batch_loss: 0.19723862409591675\n",
      "training: 53 batch 503 batch_loss: 0.19583618640899658\n",
      "training: 53 batch 504 batch_loss: 0.19772779941558838\n",
      "training: 53 batch 505 batch_loss: 0.19836091995239258\n",
      "training: 53 batch 506 batch_loss: 0.19315636157989502\n",
      "training: 53 batch 507 batch_loss: 0.19267719984054565\n",
      "training: 53 batch 508 batch_loss: 0.19969221949577332\n",
      "training: 53 batch 509 batch_loss: 0.19447797536849976\n",
      "training: 53 batch 510 batch_loss: 0.1974104642868042\n",
      "training: 53 batch 511 batch_loss: 0.1938556432723999\n",
      "training: 53 batch 512 batch_loss: 0.19509685039520264\n",
      "training: 53 batch 513 batch_loss: 0.19366729259490967\n",
      "training: 53 batch 514 batch_loss: 0.19738560914993286\n",
      "training: 53 batch 515 batch_loss: 0.2019958794116974\n",
      "training: 53 batch 516 batch_loss: 0.19532781839370728\n",
      "training: 53 batch 517 batch_loss: 0.19943824410438538\n",
      "training: 53 batch 518 batch_loss: 0.19889557361602783\n",
      "training: 53 batch 519 batch_loss: 0.1976994276046753\n",
      "training: 53 batch 520 batch_loss: 0.19616875052452087\n",
      "training: 53 batch 521 batch_loss: 0.1973879039287567\n",
      "training: 53 batch 522 batch_loss: 0.19756925106048584\n",
      "training: 53 batch 523 batch_loss: 0.19820544123649597\n",
      "training: 53 batch 524 batch_loss: 0.19680583477020264\n",
      "training: 53 batch 525 batch_loss: 0.20278415083885193\n",
      "training: 53 batch 526 batch_loss: 0.19900941848754883\n",
      "training: 53 batch 527 batch_loss: 0.198722243309021\n",
      "training: 53 batch 528 batch_loss: 0.1977817416191101\n",
      "training: 53 batch 529 batch_loss: 0.19926077127456665\n",
      "training: 53 batch 530 batch_loss: 0.19805029034614563\n",
      "training: 53 batch 531 batch_loss: 0.19735115766525269\n",
      "training: 53 batch 532 batch_loss: 0.19894331693649292\n",
      "training: 53 batch 533 batch_loss: 0.1969836950302124\n",
      "training: 53 batch 534 batch_loss: 0.19556128978729248\n",
      "training: 53 batch 535 batch_loss: 0.2001841962337494\n",
      "training: 53 batch 536 batch_loss: 0.19772735238075256\n",
      "training: 53 batch 537 batch_loss: 0.1976415514945984\n",
      "training: 53 batch 538 batch_loss: 0.19883179664611816\n",
      "training: 53 batch 539 batch_loss: 0.20040389895439148\n",
      "training: 53 batch 540 batch_loss: 0.19684988260269165\n",
      "training: 53 batch 541 batch_loss: 0.19664433598518372\n",
      "training: 53 batch 542 batch_loss: 0.19378146529197693\n",
      "training: 53 batch 543 batch_loss: 0.19711846113204956\n",
      "training: 53 batch 544 batch_loss: 0.1911681592464447\n",
      "training: 53 batch 545 batch_loss: 0.19183510541915894\n",
      "training: 53 batch 546 batch_loss: 0.19764861464500427\n",
      "training: 53 batch 547 batch_loss: 0.1969790756702423\n",
      "training: 53 batch 548 batch_loss: 0.19276869297027588\n",
      "training: 53 batch 549 batch_loss: 0.19707396626472473\n",
      "training: 53 batch 550 batch_loss: 0.1971002221107483\n",
      "training: 53 batch 551 batch_loss: 0.19441479444503784\n",
      "training: 53 batch 552 batch_loss: 0.1950390338897705\n",
      "training: 53 batch 553 batch_loss: 0.19762232899665833\n",
      "training: 53 batch 554 batch_loss: 0.20174217224121094\n",
      "training: 53 batch 555 batch_loss: 0.1996012032032013\n",
      "training: 53 batch 556 batch_loss: 0.19685456156730652\n",
      "training: 53 batch 557 batch_loss: 0.1971355378627777\n",
      "training: 53 batch 558 batch_loss: 0.19966551661491394\n",
      "training: 53 batch 559 batch_loss: 0.1975070834159851\n",
      "training: 53 batch 560 batch_loss: 0.19467046856880188\n",
      "training: 53 batch 561 batch_loss: 0.19574546813964844\n",
      "training: 53 batch 562 batch_loss: 0.20103687047958374\n",
      "training: 53 batch 563 batch_loss: 0.19838079810142517\n",
      "training: 53 batch 564 batch_loss: 0.193029522895813\n",
      "training: 53 batch 565 batch_loss: 0.2005799412727356\n",
      "training: 53 batch 566 batch_loss: 0.20178377628326416\n",
      "training: 53 batch 567 batch_loss: 0.19674837589263916\n",
      "training: 53 batch 568 batch_loss: 0.196123868227005\n",
      "training: 53 batch 569 batch_loss: 0.1983751654624939\n",
      "training: 53 batch 570 batch_loss: 0.19686266779899597\n",
      "training: 53 batch 571 batch_loss: 0.19723793864250183\n",
      "training: 53 batch 572 batch_loss: 0.19349926710128784\n",
      "training: 53 batch 573 batch_loss: 0.1982417106628418\n",
      "training: 53 batch 574 batch_loss: 0.19561833143234253\n",
      "training: 53 batch 575 batch_loss: 0.1952688992023468\n",
      "training: 53 batch 576 batch_loss: 0.1947786509990692\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 53, Hit Ratio:0.032685638905637374 | Precision:0.04822569546839674 | Recall:0.0635072955837164 | NDCG:0.06223539720269276\n",
      "*Best Performance* \n",
      "Epoch: 53, Hit Ratio:0.032685638905637374 | Precision:0.04822569546839674 | Recall:0.0635072955837164 | MDCG:0.06223539720269276\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 54 batch 0 batch_loss: 0.197405606508255\n",
      "training: 54 batch 1 batch_loss: 0.19426801800727844\n",
      "training: 54 batch 2 batch_loss: 0.19459182024002075\n",
      "training: 54 batch 3 batch_loss: 0.19471046328544617\n",
      "training: 54 batch 4 batch_loss: 0.1973082423210144\n",
      "training: 54 batch 5 batch_loss: 0.19366514682769775\n",
      "training: 54 batch 6 batch_loss: 0.19234219193458557\n",
      "training: 54 batch 7 batch_loss: 0.1940881609916687\n",
      "training: 54 batch 8 batch_loss: 0.19522714614868164\n",
      "training: 54 batch 9 batch_loss: 0.19803458452224731\n",
      "training: 54 batch 10 batch_loss: 0.19640937447547913\n",
      "training: 54 batch 11 batch_loss: 0.1928153932094574\n",
      "training: 54 batch 12 batch_loss: 0.1967034637928009\n",
      "training: 54 batch 13 batch_loss: 0.2014428973197937\n",
      "training: 54 batch 14 batch_loss: 0.19850236177444458\n",
      "training: 54 batch 15 batch_loss: 0.19586652517318726\n",
      "training: 54 batch 16 batch_loss: 0.19503656029701233\n",
      "training: 54 batch 17 batch_loss: 0.19766056537628174\n",
      "training: 54 batch 18 batch_loss: 0.19723767042160034\n",
      "training: 54 batch 19 batch_loss: 0.1961994469165802\n",
      "training: 54 batch 20 batch_loss: 0.197128027677536\n",
      "training: 54 batch 21 batch_loss: 0.1959691345691681\n",
      "training: 54 batch 22 batch_loss: 0.19759982824325562\n",
      "training: 54 batch 23 batch_loss: 0.1949945092201233\n",
      "training: 54 batch 24 batch_loss: 0.1957516074180603\n",
      "training: 54 batch 25 batch_loss: 0.19834116101264954\n",
      "training: 54 batch 26 batch_loss: 0.19878169894218445\n",
      "training: 54 batch 27 batch_loss: 0.19455987215042114\n",
      "training: 54 batch 28 batch_loss: 0.1968766450881958\n",
      "training: 54 batch 29 batch_loss: 0.19502460956573486\n",
      "training: 54 batch 30 batch_loss: 0.19864776730537415\n",
      "training: 54 batch 31 batch_loss: 0.19481658935546875\n",
      "training: 54 batch 32 batch_loss: 0.19691628217697144\n",
      "training: 54 batch 33 batch_loss: 0.19356125593185425\n",
      "training: 54 batch 34 batch_loss: 0.19524991512298584\n",
      "training: 54 batch 35 batch_loss: 0.19607004523277283\n",
      "training: 54 batch 36 batch_loss: 0.19577151536941528\n",
      "training: 54 batch 37 batch_loss: 0.19671839475631714\n",
      "training: 54 batch 38 batch_loss: 0.19791695475578308\n",
      "training: 54 batch 39 batch_loss: 0.19744420051574707\n",
      "training: 54 batch 40 batch_loss: 0.19590404629707336\n",
      "training: 54 batch 41 batch_loss: 0.19128715991973877\n",
      "training: 54 batch 42 batch_loss: 0.19395464658737183\n",
      "training: 54 batch 43 batch_loss: 0.19035109877586365\n",
      "training: 54 batch 44 batch_loss: 0.1998041570186615\n",
      "training: 54 batch 45 batch_loss: 0.1978137493133545\n",
      "training: 54 batch 46 batch_loss: 0.19273629784584045\n",
      "training: 54 batch 47 batch_loss: 0.1954888105392456\n",
      "training: 54 batch 48 batch_loss: 0.19589895009994507\n",
      "training: 54 batch 49 batch_loss: 0.2013540267944336\n",
      "training: 54 batch 50 batch_loss: 0.19827789068222046\n",
      "training: 54 batch 51 batch_loss: 0.20007318258285522\n",
      "training: 54 batch 52 batch_loss: 0.19301411509513855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 54 batch 53 batch_loss: 0.19732972979545593\n",
      "training: 54 batch 54 batch_loss: 0.19586873054504395\n",
      "training: 54 batch 55 batch_loss: 0.19506758451461792\n",
      "training: 54 batch 56 batch_loss: 0.1970709264278412\n",
      "training: 54 batch 57 batch_loss: 0.1919425129890442\n",
      "training: 54 batch 58 batch_loss: 0.19724702835083008\n",
      "training: 54 batch 59 batch_loss: 0.19458463788032532\n",
      "training: 54 batch 60 batch_loss: 0.19504129886627197\n",
      "training: 54 batch 61 batch_loss: 0.19414332509040833\n",
      "training: 54 batch 62 batch_loss: 0.1975472867488861\n",
      "training: 54 batch 63 batch_loss: 0.19450458884239197\n",
      "training: 54 batch 64 batch_loss: 0.19484657049179077\n",
      "training: 54 batch 65 batch_loss: 0.19669067859649658\n",
      "training: 54 batch 66 batch_loss: 0.19689705967903137\n",
      "training: 54 batch 67 batch_loss: 0.19827136397361755\n",
      "training: 54 batch 68 batch_loss: 0.1946592628955841\n",
      "training: 54 batch 69 batch_loss: 0.19821396470069885\n",
      "training: 54 batch 70 batch_loss: 0.19452890753746033\n",
      "training: 54 batch 71 batch_loss: 0.1949208676815033\n",
      "training: 54 batch 72 batch_loss: 0.19537299871444702\n",
      "training: 54 batch 73 batch_loss: 0.19673064351081848\n",
      "training: 54 batch 74 batch_loss: 0.19196301698684692\n",
      "training: 54 batch 75 batch_loss: 0.1948876976966858\n",
      "training: 54 batch 76 batch_loss: 0.1957099735736847\n",
      "training: 54 batch 77 batch_loss: 0.1933838129043579\n",
      "training: 54 batch 78 batch_loss: 0.1953180432319641\n",
      "training: 54 batch 79 batch_loss: 0.1989506483078003\n",
      "training: 54 batch 80 batch_loss: 0.1966402232646942\n",
      "training: 54 batch 81 batch_loss: 0.19311633706092834\n",
      "training: 54 batch 82 batch_loss: 0.20030680298805237\n",
      "training: 54 batch 83 batch_loss: 0.1990518867969513\n",
      "training: 54 batch 84 batch_loss: 0.20016399025917053\n",
      "training: 54 batch 85 batch_loss: 0.19756469130516052\n",
      "training: 54 batch 86 batch_loss: 0.19572752714157104\n",
      "training: 54 batch 87 batch_loss: 0.19237136840820312\n",
      "training: 54 batch 88 batch_loss: 0.19259697198867798\n",
      "training: 54 batch 89 batch_loss: 0.1907023787498474\n",
      "training: 54 batch 90 batch_loss: 0.20055773854255676\n",
      "training: 54 batch 91 batch_loss: 0.19828319549560547\n",
      "training: 54 batch 92 batch_loss: 0.20204395055770874\n",
      "training: 54 batch 93 batch_loss: 0.1967969834804535\n",
      "training: 54 batch 94 batch_loss: 0.1969895362854004\n",
      "training: 54 batch 95 batch_loss: 0.1956382393836975\n",
      "training: 54 batch 96 batch_loss: 0.1943678855895996\n",
      "training: 54 batch 97 batch_loss: 0.19718453288078308\n",
      "training: 54 batch 98 batch_loss: 0.19655641913414001\n",
      "training: 54 batch 99 batch_loss: 0.19535237550735474\n",
      "training: 54 batch 100 batch_loss: 0.19391566514968872\n",
      "training: 54 batch 101 batch_loss: 0.1980431079864502\n",
      "training: 54 batch 102 batch_loss: 0.19329243898391724\n",
      "training: 54 batch 103 batch_loss: 0.1947225034236908\n",
      "training: 54 batch 104 batch_loss: 0.19488525390625\n",
      "training: 54 batch 105 batch_loss: 0.19250881671905518\n",
      "training: 54 batch 106 batch_loss: 0.2018883228302002\n",
      "training: 54 batch 107 batch_loss: 0.20075902342796326\n",
      "training: 54 batch 108 batch_loss: 0.20357689261436462\n",
      "training: 54 batch 109 batch_loss: 0.1968669891357422\n",
      "training: 54 batch 110 batch_loss: 0.19447943568229675\n",
      "training: 54 batch 111 batch_loss: 0.19367951154708862\n",
      "training: 54 batch 112 batch_loss: 0.19614335894584656\n",
      "training: 54 batch 113 batch_loss: 0.19767504930496216\n",
      "training: 54 batch 114 batch_loss: 0.19484806060791016\n",
      "training: 54 batch 115 batch_loss: 0.19733697175979614\n",
      "training: 54 batch 116 batch_loss: 0.1940440535545349\n",
      "training: 54 batch 117 batch_loss: 0.1960294544696808\n",
      "training: 54 batch 118 batch_loss: 0.19846662878990173\n",
      "training: 54 batch 119 batch_loss: 0.19664114713668823\n",
      "training: 54 batch 120 batch_loss: 0.1987699270248413\n",
      "training: 54 batch 121 batch_loss: 0.19339784979820251\n",
      "training: 54 batch 122 batch_loss: 0.19634538888931274\n",
      "training: 54 batch 123 batch_loss: 0.20145255327224731\n",
      "training: 54 batch 124 batch_loss: 0.19736212491989136\n",
      "training: 54 batch 125 batch_loss: 0.19512781500816345\n",
      "training: 54 batch 126 batch_loss: 0.19690048694610596\n",
      "training: 54 batch 127 batch_loss: 0.19721829891204834\n",
      "training: 54 batch 128 batch_loss: 0.19514954090118408\n",
      "training: 54 batch 129 batch_loss: 0.19764602184295654\n",
      "training: 54 batch 130 batch_loss: 0.19597899913787842\n",
      "training: 54 batch 131 batch_loss: 0.19523251056671143\n",
      "training: 54 batch 132 batch_loss: 0.20196989178657532\n",
      "training: 54 batch 133 batch_loss: 0.19598734378814697\n",
      "training: 54 batch 134 batch_loss: 0.19953346252441406\n",
      "training: 54 batch 135 batch_loss: 0.20067429542541504\n",
      "training: 54 batch 136 batch_loss: 0.19745996594429016\n",
      "training: 54 batch 137 batch_loss: 0.19502946734428406\n",
      "training: 54 batch 138 batch_loss: 0.19359716773033142\n",
      "training: 54 batch 139 batch_loss: 0.19775575399398804\n",
      "training: 54 batch 140 batch_loss: 0.19500505924224854\n",
      "training: 54 batch 141 batch_loss: 0.20116493105888367\n",
      "training: 54 batch 142 batch_loss: 0.19862303137779236\n",
      "training: 54 batch 143 batch_loss: 0.19649511575698853\n",
      "training: 54 batch 144 batch_loss: 0.19387173652648926\n",
      "training: 54 batch 145 batch_loss: 0.19360575079917908\n",
      "training: 54 batch 146 batch_loss: 0.19915702939033508\n",
      "training: 54 batch 147 batch_loss: 0.19539344310760498\n",
      "training: 54 batch 148 batch_loss: 0.19703760743141174\n",
      "training: 54 batch 149 batch_loss: 0.19434258341789246\n",
      "training: 54 batch 150 batch_loss: 0.2000112533569336\n",
      "training: 54 batch 151 batch_loss: 0.19390016794204712\n",
      "training: 54 batch 152 batch_loss: 0.19487962126731873\n",
      "training: 54 batch 153 batch_loss: 0.19366219639778137\n",
      "training: 54 batch 154 batch_loss: 0.19860056042671204\n",
      "training: 54 batch 155 batch_loss: 0.19623631238937378\n",
      "training: 54 batch 156 batch_loss: 0.1943669617176056\n",
      "training: 54 batch 157 batch_loss: 0.19611325860023499\n",
      "training: 54 batch 158 batch_loss: 0.1954387128353119\n",
      "training: 54 batch 159 batch_loss: 0.19369006156921387\n",
      "training: 54 batch 160 batch_loss: 0.1989968717098236\n",
      "training: 54 batch 161 batch_loss: 0.19330108165740967\n",
      "training: 54 batch 162 batch_loss: 0.19638437032699585\n",
      "training: 54 batch 163 batch_loss: 0.19486606121063232\n",
      "training: 54 batch 164 batch_loss: 0.19777342677116394\n",
      "training: 54 batch 165 batch_loss: 0.19764089584350586\n",
      "training: 54 batch 166 batch_loss: 0.1952933669090271\n",
      "training: 54 batch 167 batch_loss: 0.19751471281051636\n",
      "training: 54 batch 168 batch_loss: 0.20019900798797607\n",
      "training: 54 batch 169 batch_loss: 0.19615024328231812\n",
      "training: 54 batch 170 batch_loss: 0.19688212871551514\n",
      "training: 54 batch 171 batch_loss: 0.19809862971305847\n",
      "training: 54 batch 172 batch_loss: 0.1904197633266449\n",
      "training: 54 batch 173 batch_loss: 0.20103096961975098\n",
      "training: 54 batch 174 batch_loss: 0.20109868049621582\n",
      "training: 54 batch 175 batch_loss: 0.19583368301391602\n",
      "training: 54 batch 176 batch_loss: 0.19353216886520386\n",
      "training: 54 batch 177 batch_loss: 0.1983025074005127\n",
      "training: 54 batch 178 batch_loss: 0.19731420278549194\n",
      "training: 54 batch 179 batch_loss: 0.19632506370544434\n",
      "training: 54 batch 180 batch_loss: 0.1912032961845398\n",
      "training: 54 batch 181 batch_loss: 0.19322651624679565\n",
      "training: 54 batch 182 batch_loss: 0.19985592365264893\n",
      "training: 54 batch 183 batch_loss: 0.20053896307945251\n",
      "training: 54 batch 184 batch_loss: 0.19781389832496643\n",
      "training: 54 batch 185 batch_loss: 0.19409358501434326\n",
      "training: 54 batch 186 batch_loss: 0.19850194454193115\n",
      "training: 54 batch 187 batch_loss: 0.1978471279144287\n",
      "training: 54 batch 188 batch_loss: 0.18960797786712646\n",
      "training: 54 batch 189 batch_loss: 0.19257166981697083\n",
      "training: 54 batch 190 batch_loss: 0.19757547974586487\n",
      "training: 54 batch 191 batch_loss: 0.19716420769691467\n",
      "training: 54 batch 192 batch_loss: 0.1980072259902954\n",
      "training: 54 batch 193 batch_loss: 0.19694668054580688\n",
      "training: 54 batch 194 batch_loss: 0.19860854744911194\n",
      "training: 54 batch 195 batch_loss: 0.19776523113250732\n",
      "training: 54 batch 196 batch_loss: 0.19894689321517944\n",
      "training: 54 batch 197 batch_loss: 0.199565589427948\n",
      "training: 54 batch 198 batch_loss: 0.19976994395256042\n",
      "training: 54 batch 199 batch_loss: 0.1987246870994568\n",
      "training: 54 batch 200 batch_loss: 0.1982276737689972\n",
      "training: 54 batch 201 batch_loss: 0.19320622086524963\n",
      "training: 54 batch 202 batch_loss: 0.19726631045341492\n",
      "training: 54 batch 203 batch_loss: 0.19361081719398499\n",
      "training: 54 batch 204 batch_loss: 0.1962152123451233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 54 batch 205 batch_loss: 0.19363456964492798\n",
      "training: 54 batch 206 batch_loss: 0.19526582956314087\n",
      "training: 54 batch 207 batch_loss: 0.19844436645507812\n",
      "training: 54 batch 208 batch_loss: 0.19922244548797607\n",
      "training: 54 batch 209 batch_loss: 0.19745400547981262\n",
      "training: 54 batch 210 batch_loss: 0.19575873017311096\n",
      "training: 54 batch 211 batch_loss: 0.19535166025161743\n",
      "training: 54 batch 212 batch_loss: 0.19632601737976074\n",
      "training: 54 batch 213 batch_loss: 0.19316285848617554\n",
      "training: 54 batch 214 batch_loss: 0.19904354214668274\n",
      "training: 54 batch 215 batch_loss: 0.19293978810310364\n",
      "training: 54 batch 216 batch_loss: 0.1960512399673462\n",
      "training: 54 batch 217 batch_loss: 0.19884121417999268\n",
      "training: 54 batch 218 batch_loss: 0.2006385624408722\n",
      "training: 54 batch 219 batch_loss: 0.19729366898536682\n",
      "training: 54 batch 220 batch_loss: 0.19758987426757812\n",
      "training: 54 batch 221 batch_loss: 0.19195616245269775\n",
      "training: 54 batch 222 batch_loss: 0.19878673553466797\n",
      "training: 54 batch 223 batch_loss: 0.19871869683265686\n",
      "training: 54 batch 224 batch_loss: 0.19684472680091858\n",
      "training: 54 batch 225 batch_loss: 0.19913089275360107\n",
      "training: 54 batch 226 batch_loss: 0.19758468866348267\n",
      "training: 54 batch 227 batch_loss: 0.19833940267562866\n",
      "training: 54 batch 228 batch_loss: 0.1993076503276825\n",
      "training: 54 batch 229 batch_loss: 0.19696462154388428\n",
      "training: 54 batch 230 batch_loss: 0.19611093401908875\n",
      "training: 54 batch 231 batch_loss: 0.19531559944152832\n",
      "training: 54 batch 232 batch_loss: 0.1981378197669983\n",
      "training: 54 batch 233 batch_loss: 0.19821733236312866\n",
      "training: 54 batch 234 batch_loss: 0.1993207335472107\n",
      "training: 54 batch 235 batch_loss: 0.19512122869491577\n",
      "training: 54 batch 236 batch_loss: 0.19618910551071167\n",
      "training: 54 batch 237 batch_loss: 0.19715344905853271\n",
      "training: 54 batch 238 batch_loss: 0.19718730449676514\n",
      "training: 54 batch 239 batch_loss: 0.1934664249420166\n",
      "training: 54 batch 240 batch_loss: 0.19770318269729614\n",
      "training: 54 batch 241 batch_loss: 0.1975061595439911\n",
      "training: 54 batch 242 batch_loss: 0.1978963017463684\n",
      "training: 54 batch 243 batch_loss: 0.19687238335609436\n",
      "training: 54 batch 244 batch_loss: 0.19788798689842224\n",
      "training: 54 batch 245 batch_loss: 0.19798460602760315\n",
      "training: 54 batch 246 batch_loss: 0.19282782077789307\n",
      "training: 54 batch 247 batch_loss: 0.1959449052810669\n",
      "training: 54 batch 248 batch_loss: 0.19739601016044617\n",
      "training: 54 batch 249 batch_loss: 0.1979186236858368\n",
      "training: 54 batch 250 batch_loss: 0.19871309399604797\n",
      "training: 54 batch 251 batch_loss: 0.19694268703460693\n",
      "training: 54 batch 252 batch_loss: 0.19832554459571838\n",
      "training: 54 batch 253 batch_loss: 0.19850614666938782\n",
      "training: 54 batch 254 batch_loss: 0.19813024997711182\n",
      "training: 54 batch 255 batch_loss: 0.1965990662574768\n",
      "training: 54 batch 256 batch_loss: 0.1969379484653473\n",
      "training: 54 batch 257 batch_loss: 0.1982119381427765\n",
      "training: 54 batch 258 batch_loss: 0.19496941566467285\n",
      "training: 54 batch 259 batch_loss: 0.1986948847770691\n",
      "training: 54 batch 260 batch_loss: 0.19848206639289856\n",
      "training: 54 batch 261 batch_loss: 0.1999841034412384\n",
      "training: 54 batch 262 batch_loss: 0.19633522629737854\n",
      "training: 54 batch 263 batch_loss: 0.1957312524318695\n",
      "training: 54 batch 264 batch_loss: 0.19791510701179504\n",
      "training: 54 batch 265 batch_loss: 0.19791635870933533\n",
      "training: 54 batch 266 batch_loss: 0.19667834043502808\n",
      "training: 54 batch 267 batch_loss: 0.1967998445034027\n",
      "training: 54 batch 268 batch_loss: 0.19793882966041565\n",
      "training: 54 batch 269 batch_loss: 0.1979236602783203\n",
      "training: 54 batch 270 batch_loss: 0.20201867818832397\n",
      "training: 54 batch 271 batch_loss: 0.19357186555862427\n",
      "training: 54 batch 272 batch_loss: 0.19690823554992676\n",
      "training: 54 batch 273 batch_loss: 0.19558680057525635\n",
      "training: 54 batch 274 batch_loss: 0.2015930712223053\n",
      "training: 54 batch 275 batch_loss: 0.1951429843902588\n",
      "training: 54 batch 276 batch_loss: 0.19628965854644775\n",
      "training: 54 batch 277 batch_loss: 0.1961073875427246\n",
      "training: 54 batch 278 batch_loss: 0.20076385140419006\n",
      "training: 54 batch 279 batch_loss: 0.19877243041992188\n",
      "training: 54 batch 280 batch_loss: 0.19816642999649048\n",
      "training: 54 batch 281 batch_loss: 0.1955045759677887\n",
      "training: 54 batch 282 batch_loss: 0.19892114400863647\n",
      "training: 54 batch 283 batch_loss: 0.19952917098999023\n",
      "training: 54 batch 284 batch_loss: 0.19584921002388\n",
      "training: 54 batch 285 batch_loss: 0.19658243656158447\n",
      "training: 54 batch 286 batch_loss: 0.2014826536178589\n",
      "training: 54 batch 287 batch_loss: 0.19520854949951172\n",
      "training: 54 batch 288 batch_loss: 0.19637608528137207\n",
      "training: 54 batch 289 batch_loss: 0.19687318801879883\n",
      "training: 54 batch 290 batch_loss: 0.19634300470352173\n",
      "training: 54 batch 291 batch_loss: 0.19824469089508057\n",
      "training: 54 batch 292 batch_loss: 0.1994321048259735\n",
      "training: 54 batch 293 batch_loss: 0.20021256804466248\n",
      "training: 54 batch 294 batch_loss: 0.19800013303756714\n",
      "training: 54 batch 295 batch_loss: 0.19671833515167236\n",
      "training: 54 batch 296 batch_loss: 0.19752588868141174\n",
      "training: 54 batch 297 batch_loss: 0.19568341970443726\n",
      "training: 54 batch 298 batch_loss: 0.1991984248161316\n",
      "training: 54 batch 299 batch_loss: 0.19282156229019165\n",
      "training: 54 batch 300 batch_loss: 0.2027243971824646\n",
      "training: 54 batch 301 batch_loss: 0.19932866096496582\n",
      "training: 54 batch 302 batch_loss: 0.19671016931533813\n",
      "training: 54 batch 303 batch_loss: 0.1967732012271881\n",
      "training: 54 batch 304 batch_loss: 0.19581973552703857\n",
      "training: 54 batch 305 batch_loss: 0.19861897826194763\n",
      "training: 54 batch 306 batch_loss: 0.19554197788238525\n",
      "training: 54 batch 307 batch_loss: 0.19956111907958984\n",
      "training: 54 batch 308 batch_loss: 0.19795355200767517\n",
      "training: 54 batch 309 batch_loss: 0.20160162448883057\n",
      "training: 54 batch 310 batch_loss: 0.1965903639793396\n",
      "training: 54 batch 311 batch_loss: 0.19702774286270142\n",
      "training: 54 batch 312 batch_loss: 0.19678986072540283\n",
      "training: 54 batch 313 batch_loss: 0.19435030221939087\n",
      "training: 54 batch 314 batch_loss: 0.19977450370788574\n",
      "training: 54 batch 315 batch_loss: 0.20203328132629395\n",
      "training: 54 batch 316 batch_loss: 0.1989925503730774\n",
      "training: 54 batch 317 batch_loss: 0.1948002576828003\n",
      "training: 54 batch 318 batch_loss: 0.19680243730545044\n",
      "training: 54 batch 319 batch_loss: 0.19879400730133057\n",
      "training: 54 batch 320 batch_loss: 0.19619151949882507\n",
      "training: 54 batch 321 batch_loss: 0.19653287529945374\n",
      "training: 54 batch 322 batch_loss: 0.19669148325920105\n",
      "training: 54 batch 323 batch_loss: 0.19618487358093262\n",
      "training: 54 batch 324 batch_loss: 0.20184746384620667\n",
      "training: 54 batch 325 batch_loss: 0.1966530680656433\n",
      "training: 54 batch 326 batch_loss: 0.1964532732963562\n",
      "training: 54 batch 327 batch_loss: 0.19597095251083374\n",
      "training: 54 batch 328 batch_loss: 0.18980485200881958\n",
      "training: 54 batch 329 batch_loss: 0.19490313529968262\n",
      "training: 54 batch 330 batch_loss: 0.19775673747062683\n",
      "training: 54 batch 331 batch_loss: 0.20195841789245605\n",
      "training: 54 batch 332 batch_loss: 0.19939973950386047\n",
      "training: 54 batch 333 batch_loss: 0.19567036628723145\n",
      "training: 54 batch 334 batch_loss: 0.1970919966697693\n",
      "training: 54 batch 335 batch_loss: 0.1958838701248169\n",
      "training: 54 batch 336 batch_loss: 0.20231622457504272\n",
      "training: 54 batch 337 batch_loss: 0.1990630328655243\n",
      "training: 54 batch 338 batch_loss: 0.19781342148780823\n",
      "training: 54 batch 339 batch_loss: 0.1939612329006195\n",
      "training: 54 batch 340 batch_loss: 0.19460827112197876\n",
      "training: 54 batch 341 batch_loss: 0.19861549139022827\n",
      "training: 54 batch 342 batch_loss: 0.19797170162200928\n",
      "training: 54 batch 343 batch_loss: 0.19921010732650757\n",
      "training: 54 batch 344 batch_loss: 0.19248709082603455\n",
      "training: 54 batch 345 batch_loss: 0.1984596848487854\n",
      "training: 54 batch 346 batch_loss: 0.19845184683799744\n",
      "training: 54 batch 347 batch_loss: 0.19840583205223083\n",
      "training: 54 batch 348 batch_loss: 0.19796887040138245\n",
      "training: 54 batch 349 batch_loss: 0.1942567229270935\n",
      "training: 54 batch 350 batch_loss: 0.19590914249420166\n",
      "training: 54 batch 351 batch_loss: 0.19712147116661072\n",
      "training: 54 batch 352 batch_loss: 0.20389559864997864\n",
      "training: 54 batch 353 batch_loss: 0.1984788179397583\n",
      "training: 54 batch 354 batch_loss: 0.19392535090446472\n",
      "training: 54 batch 355 batch_loss: 0.19669681787490845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 54 batch 356 batch_loss: 0.19535744190216064\n",
      "training: 54 batch 357 batch_loss: 0.19305378198623657\n",
      "training: 54 batch 358 batch_loss: 0.1994231641292572\n",
      "training: 54 batch 359 batch_loss: 0.19559350609779358\n",
      "training: 54 batch 360 batch_loss: 0.19770264625549316\n",
      "training: 54 batch 361 batch_loss: 0.1979973018169403\n",
      "training: 54 batch 362 batch_loss: 0.19714045524597168\n",
      "training: 54 batch 363 batch_loss: 0.197902113199234\n",
      "training: 54 batch 364 batch_loss: 0.2006496787071228\n",
      "training: 54 batch 365 batch_loss: 0.19990015029907227\n",
      "training: 54 batch 366 batch_loss: 0.19940480589866638\n",
      "training: 54 batch 367 batch_loss: 0.19627529382705688\n",
      "training: 54 batch 368 batch_loss: 0.19677427411079407\n",
      "training: 54 batch 369 batch_loss: 0.19258606433868408\n",
      "training: 54 batch 370 batch_loss: 0.1982540488243103\n",
      "training: 54 batch 371 batch_loss: 0.1931113302707672\n",
      "training: 54 batch 372 batch_loss: 0.19889307022094727\n",
      "training: 54 batch 373 batch_loss: 0.20307543873786926\n",
      "training: 54 batch 374 batch_loss: 0.1963634490966797\n",
      "training: 54 batch 375 batch_loss: 0.20303359627723694\n",
      "training: 54 batch 376 batch_loss: 0.19729280471801758\n",
      "training: 54 batch 377 batch_loss: 0.2022310495376587\n",
      "training: 54 batch 378 batch_loss: 0.19926226139068604\n",
      "training: 54 batch 379 batch_loss: 0.19673705101013184\n",
      "training: 54 batch 380 batch_loss: 0.20000219345092773\n",
      "training: 54 batch 381 batch_loss: 0.19689351320266724\n",
      "training: 54 batch 382 batch_loss: 0.19974586367607117\n",
      "training: 54 batch 383 batch_loss: 0.19627347588539124\n",
      "training: 54 batch 384 batch_loss: 0.2005237340927124\n",
      "training: 54 batch 385 batch_loss: 0.1965886950492859\n",
      "training: 54 batch 386 batch_loss: 0.19955408573150635\n",
      "training: 54 batch 387 batch_loss: 0.19895723462104797\n",
      "training: 54 batch 388 batch_loss: 0.1995713710784912\n",
      "training: 54 batch 389 batch_loss: 0.19874995946884155\n",
      "training: 54 batch 390 batch_loss: 0.1947646141052246\n",
      "training: 54 batch 391 batch_loss: 0.20310145616531372\n",
      "training: 54 batch 392 batch_loss: 0.19752591848373413\n",
      "training: 54 batch 393 batch_loss: 0.20141193270683289\n",
      "training: 54 batch 394 batch_loss: 0.19753268361091614\n",
      "training: 54 batch 395 batch_loss: 0.19458550214767456\n",
      "training: 54 batch 396 batch_loss: 0.19968727231025696\n",
      "training: 54 batch 397 batch_loss: 0.19811323285102844\n",
      "training: 54 batch 398 batch_loss: 0.19659775495529175\n",
      "training: 54 batch 399 batch_loss: 0.1989680528640747\n",
      "training: 54 batch 400 batch_loss: 0.19948241114616394\n",
      "training: 54 batch 401 batch_loss: 0.19980403780937195\n",
      "training: 54 batch 402 batch_loss: 0.19637364149093628\n",
      "training: 54 batch 403 batch_loss: 0.1967904269695282\n",
      "training: 54 batch 404 batch_loss: 0.19777321815490723\n",
      "training: 54 batch 405 batch_loss: 0.19664248824119568\n",
      "training: 54 batch 406 batch_loss: 0.1995067596435547\n",
      "training: 54 batch 407 batch_loss: 0.19824182987213135\n",
      "training: 54 batch 408 batch_loss: 0.19757425785064697\n",
      "training: 54 batch 409 batch_loss: 0.19703015685081482\n",
      "training: 54 batch 410 batch_loss: 0.2001105546951294\n",
      "training: 54 batch 411 batch_loss: 0.1988379955291748\n",
      "training: 54 batch 412 batch_loss: 0.19835609197616577\n",
      "training: 54 batch 413 batch_loss: 0.2000509798526764\n",
      "training: 54 batch 414 batch_loss: 0.20058533549308777\n",
      "training: 54 batch 415 batch_loss: 0.19434010982513428\n",
      "training: 54 batch 416 batch_loss: 0.19768565893173218\n",
      "training: 54 batch 417 batch_loss: 0.19828754663467407\n",
      "training: 54 batch 418 batch_loss: 0.19945943355560303\n",
      "training: 54 batch 419 batch_loss: 0.1983441710472107\n",
      "training: 54 batch 420 batch_loss: 0.1977100372314453\n",
      "training: 54 batch 421 batch_loss: 0.19981172680854797\n",
      "training: 54 batch 422 batch_loss: 0.19827893376350403\n",
      "training: 54 batch 423 batch_loss: 0.19689562916755676\n",
      "training: 54 batch 424 batch_loss: 0.1985289752483368\n",
      "training: 54 batch 425 batch_loss: 0.19664451479911804\n",
      "training: 54 batch 426 batch_loss: 0.19648197293281555\n",
      "training: 54 batch 427 batch_loss: 0.19312456250190735\n",
      "training: 54 batch 428 batch_loss: 0.197179913520813\n",
      "training: 54 batch 429 batch_loss: 0.1948438584804535\n",
      "training: 54 batch 430 batch_loss: 0.19585222005844116\n",
      "training: 54 batch 431 batch_loss: 0.19820261001586914\n",
      "training: 54 batch 432 batch_loss: 0.1987234652042389\n",
      "training: 54 batch 433 batch_loss: 0.1950620412826538\n",
      "training: 54 batch 434 batch_loss: 0.1961405873298645\n",
      "training: 54 batch 435 batch_loss: 0.2024013102054596\n",
      "training: 54 batch 436 batch_loss: 0.1957949697971344\n",
      "training: 54 batch 437 batch_loss: 0.19513699412345886\n",
      "training: 54 batch 438 batch_loss: 0.19679006934165955\n",
      "training: 54 batch 439 batch_loss: 0.1975538432598114\n",
      "training: 54 batch 440 batch_loss: 0.20320093631744385\n",
      "training: 54 batch 441 batch_loss: 0.1976633369922638\n",
      "training: 54 batch 442 batch_loss: 0.19528254866600037\n",
      "training: 54 batch 443 batch_loss: 0.19664978981018066\n",
      "training: 54 batch 444 batch_loss: 0.1972581148147583\n",
      "training: 54 batch 445 batch_loss: 0.20062339305877686\n",
      "training: 54 batch 446 batch_loss: 0.19849270582199097\n",
      "training: 54 batch 447 batch_loss: 0.196712464094162\n",
      "training: 54 batch 448 batch_loss: 0.19736087322235107\n",
      "training: 54 batch 449 batch_loss: 0.20236247777938843\n",
      "training: 54 batch 450 batch_loss: 0.19652828574180603\n",
      "training: 54 batch 451 batch_loss: 0.19910061359405518\n",
      "training: 54 batch 452 batch_loss: 0.19707530736923218\n",
      "training: 54 batch 453 batch_loss: 0.197136789560318\n",
      "training: 54 batch 454 batch_loss: 0.20300418138504028\n",
      "training: 54 batch 455 batch_loss: 0.19775360822677612\n",
      "training: 54 batch 456 batch_loss: 0.1964777410030365\n",
      "training: 54 batch 457 batch_loss: 0.20014938712120056\n",
      "training: 54 batch 458 batch_loss: 0.20179179310798645\n",
      "training: 54 batch 459 batch_loss: 0.1933804750442505\n",
      "training: 54 batch 460 batch_loss: 0.19788655638694763\n",
      "training: 54 batch 461 batch_loss: 0.1972043216228485\n",
      "training: 54 batch 462 batch_loss: 0.19530987739562988\n",
      "training: 54 batch 463 batch_loss: 0.19736218452453613\n",
      "training: 54 batch 464 batch_loss: 0.196620374917984\n",
      "training: 54 batch 465 batch_loss: 0.1980016529560089\n",
      "training: 54 batch 466 batch_loss: 0.1972447633743286\n",
      "training: 54 batch 467 batch_loss: 0.19711440801620483\n",
      "training: 54 batch 468 batch_loss: 0.1996709406375885\n",
      "training: 54 batch 469 batch_loss: 0.19512879848480225\n",
      "training: 54 batch 470 batch_loss: 0.1998087763786316\n",
      "training: 54 batch 471 batch_loss: 0.1963055431842804\n",
      "training: 54 batch 472 batch_loss: 0.19989439845085144\n",
      "training: 54 batch 473 batch_loss: 0.19954225420951843\n",
      "training: 54 batch 474 batch_loss: 0.19553908705711365\n",
      "training: 54 batch 475 batch_loss: 0.20047062635421753\n",
      "training: 54 batch 476 batch_loss: 0.20228296518325806\n",
      "training: 54 batch 477 batch_loss: 0.20046114921569824\n",
      "training: 54 batch 478 batch_loss: 0.1970486342906952\n",
      "training: 54 batch 479 batch_loss: 0.19623491168022156\n",
      "training: 54 batch 480 batch_loss: 0.1978510022163391\n",
      "training: 54 batch 481 batch_loss: 0.20189589262008667\n",
      "training: 54 batch 482 batch_loss: 0.19888851046562195\n",
      "training: 54 batch 483 batch_loss: 0.19534781575202942\n",
      "training: 54 batch 484 batch_loss: 0.20089039206504822\n",
      "training: 54 batch 485 batch_loss: 0.19775772094726562\n",
      "training: 54 batch 486 batch_loss: 0.19520214200019836\n",
      "training: 54 batch 487 batch_loss: 0.1936739981174469\n",
      "training: 54 batch 488 batch_loss: 0.19929933547973633\n",
      "training: 54 batch 489 batch_loss: 0.200206458568573\n",
      "training: 54 batch 490 batch_loss: 0.19602975249290466\n",
      "training: 54 batch 491 batch_loss: 0.19938266277313232\n",
      "training: 54 batch 492 batch_loss: 0.1953357458114624\n",
      "training: 54 batch 493 batch_loss: 0.20152276754379272\n",
      "training: 54 batch 494 batch_loss: 0.19616597890853882\n",
      "training: 54 batch 495 batch_loss: 0.196746826171875\n",
      "training: 54 batch 496 batch_loss: 0.19921961426734924\n",
      "training: 54 batch 497 batch_loss: 0.20056086778640747\n",
      "training: 54 batch 498 batch_loss: 0.20130598545074463\n",
      "training: 54 batch 499 batch_loss: 0.19912582635879517\n",
      "training: 54 batch 500 batch_loss: 0.19903501868247986\n",
      "training: 54 batch 501 batch_loss: 0.198701411485672\n",
      "training: 54 batch 502 batch_loss: 0.19563937187194824\n",
      "training: 54 batch 503 batch_loss: 0.19981637597084045\n",
      "training: 54 batch 504 batch_loss: 0.19781044125556946\n",
      "training: 54 batch 505 batch_loss: 0.19894444942474365\n",
      "training: 54 batch 506 batch_loss: 0.19516891241073608\n",
      "training: 54 batch 507 batch_loss: 0.1993195116519928\n",
      "training: 54 batch 508 batch_loss: 0.19902828335762024\n",
      "training: 54 batch 509 batch_loss: 0.193661630153656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 54 batch 510 batch_loss: 0.1959061324596405\n",
      "training: 54 batch 511 batch_loss: 0.1962946355342865\n",
      "training: 54 batch 512 batch_loss: 0.19819921255111694\n",
      "training: 54 batch 513 batch_loss: 0.2008587121963501\n",
      "training: 54 batch 514 batch_loss: 0.19682812690734863\n",
      "training: 54 batch 515 batch_loss: 0.19957366585731506\n",
      "training: 54 batch 516 batch_loss: 0.2008434534072876\n",
      "training: 54 batch 517 batch_loss: 0.19917118549346924\n",
      "training: 54 batch 518 batch_loss: 0.19924190640449524\n",
      "training: 54 batch 519 batch_loss: 0.20011001825332642\n",
      "training: 54 batch 520 batch_loss: 0.20250415802001953\n",
      "training: 54 batch 521 batch_loss: 0.1994636058807373\n",
      "training: 54 batch 522 batch_loss: 0.2028246521949768\n",
      "training: 54 batch 523 batch_loss: 0.19670099020004272\n",
      "training: 54 batch 524 batch_loss: 0.19962432980537415\n",
      "training: 54 batch 525 batch_loss: 0.19711536169052124\n",
      "training: 54 batch 526 batch_loss: 0.19722694158554077\n",
      "training: 54 batch 527 batch_loss: 0.19855794310569763\n",
      "training: 54 batch 528 batch_loss: 0.20348477363586426\n",
      "training: 54 batch 529 batch_loss: 0.19805127382278442\n",
      "training: 54 batch 530 batch_loss: 0.19531598687171936\n",
      "training: 54 batch 531 batch_loss: 0.19711890816688538\n",
      "training: 54 batch 532 batch_loss: 0.196562260389328\n",
      "training: 54 batch 533 batch_loss: 0.19750282168388367\n",
      "training: 54 batch 534 batch_loss: 0.19665712118148804\n",
      "training: 54 batch 535 batch_loss: 0.19960585236549377\n",
      "training: 54 batch 536 batch_loss: 0.20186111330986023\n",
      "training: 54 batch 537 batch_loss: 0.20390045642852783\n",
      "training: 54 batch 538 batch_loss: 0.19793683290481567\n",
      "training: 54 batch 539 batch_loss: 0.19931739568710327\n",
      "training: 54 batch 540 batch_loss: 0.1978282928466797\n",
      "training: 54 batch 541 batch_loss: 0.19737696647644043\n",
      "training: 54 batch 542 batch_loss: 0.19482558965682983\n",
      "training: 54 batch 543 batch_loss: 0.19960570335388184\n",
      "training: 54 batch 544 batch_loss: 0.20010647177696228\n",
      "training: 54 batch 545 batch_loss: 0.2005366086959839\n",
      "training: 54 batch 546 batch_loss: 0.19833481311798096\n",
      "training: 54 batch 547 batch_loss: 0.19651147723197937\n",
      "training: 54 batch 548 batch_loss: 0.20081767439842224\n",
      "training: 54 batch 549 batch_loss: 0.19763308763504028\n",
      "training: 54 batch 550 batch_loss: 0.19986677169799805\n",
      "training: 54 batch 551 batch_loss: 0.19245591759681702\n",
      "training: 54 batch 552 batch_loss: 0.19869866967201233\n",
      "training: 54 batch 553 batch_loss: 0.1979200541973114\n",
      "training: 54 batch 554 batch_loss: 0.1990973949432373\n",
      "training: 54 batch 555 batch_loss: 0.20122137665748596\n",
      "training: 54 batch 556 batch_loss: 0.19581159949302673\n",
      "training: 54 batch 557 batch_loss: 0.2005346417427063\n",
      "training: 54 batch 558 batch_loss: 0.2012946903705597\n",
      "training: 54 batch 559 batch_loss: 0.19804459810256958\n",
      "training: 54 batch 560 batch_loss: 0.20252835750579834\n",
      "training: 54 batch 561 batch_loss: 0.1971573829650879\n",
      "training: 54 batch 562 batch_loss: 0.1967657208442688\n",
      "training: 54 batch 563 batch_loss: 0.19985657930374146\n",
      "training: 54 batch 564 batch_loss: 0.20201334357261658\n",
      "training: 54 batch 565 batch_loss: 0.20051658153533936\n",
      "training: 54 batch 566 batch_loss: 0.19880157709121704\n",
      "training: 54 batch 567 batch_loss: 0.19409316778182983\n",
      "training: 54 batch 568 batch_loss: 0.20303022861480713\n",
      "training: 54 batch 569 batch_loss: 0.1940585970878601\n",
      "training: 54 batch 570 batch_loss: 0.19904187321662903\n",
      "training: 54 batch 571 batch_loss: 0.20077326893806458\n",
      "training: 54 batch 572 batch_loss: 0.20322781801223755\n",
      "training: 54 batch 573 batch_loss: 0.19618478417396545\n",
      "training: 54 batch 574 batch_loss: 0.1984395682811737\n",
      "training: 54 batch 575 batch_loss: 0.19527840614318848\n",
      "training: 54 batch 576 batch_loss: 0.19375672936439514\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 54, Hit Ratio:0.032625677480820674 | Precision:0.048137225990366656 | Recall:0.0641845413822689 | NDCG:0.062477078647273074\n",
      "*Best Performance* \n",
      "Epoch: 53, Hit Ratio:0.032685638905637374 | Precision:0.04822569546839674 | Recall:0.0635072955837164 | MDCG:0.06223539720269276\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 55 batch 0 batch_loss: 0.19526365399360657\n",
      "training: 55 batch 1 batch_loss: 0.19618207216262817\n",
      "training: 55 batch 2 batch_loss: 0.19606375694274902\n",
      "training: 55 batch 3 batch_loss: 0.1938614845275879\n",
      "training: 55 batch 4 batch_loss: 0.19230642914772034\n",
      "training: 55 batch 5 batch_loss: 0.20460349321365356\n",
      "training: 55 batch 6 batch_loss: 0.19401034712791443\n",
      "training: 55 batch 7 batch_loss: 0.19532164931297302\n",
      "training: 55 batch 8 batch_loss: 0.19338178634643555\n",
      "training: 55 batch 9 batch_loss: 0.1987065076828003\n",
      "training: 55 batch 10 batch_loss: 0.19704413414001465\n",
      "training: 55 batch 11 batch_loss: 0.19684147834777832\n",
      "training: 55 batch 12 batch_loss: 0.20106101036071777\n",
      "training: 55 batch 13 batch_loss: 0.19732153415679932\n",
      "training: 55 batch 14 batch_loss: 0.19572484493255615\n",
      "training: 55 batch 15 batch_loss: 0.19540813565254211\n",
      "training: 55 batch 16 batch_loss: 0.20124012231826782\n",
      "training: 55 batch 17 batch_loss: 0.19702059030532837\n",
      "training: 55 batch 18 batch_loss: 0.19620874524116516\n",
      "training: 55 batch 19 batch_loss: 0.1997029185295105\n",
      "training: 55 batch 20 batch_loss: 0.19878384470939636\n",
      "training: 55 batch 21 batch_loss: 0.19998592138290405\n",
      "training: 55 batch 22 batch_loss: 0.19529801607131958\n",
      "training: 55 batch 23 batch_loss: 0.19540268182754517\n",
      "training: 55 batch 24 batch_loss: 0.19750520586967468\n",
      "training: 55 batch 25 batch_loss: 0.19767260551452637\n",
      "training: 55 batch 26 batch_loss: 0.19900521636009216\n",
      "training: 55 batch 27 batch_loss: 0.19771748781204224\n",
      "training: 55 batch 28 batch_loss: 0.19594433903694153\n",
      "training: 55 batch 29 batch_loss: 0.19762596487998962\n",
      "training: 55 batch 30 batch_loss: 0.20045596361160278\n",
      "training: 55 batch 31 batch_loss: 0.19342195987701416\n",
      "training: 55 batch 32 batch_loss: 0.1957174837589264\n",
      "training: 55 batch 33 batch_loss: 0.19617915153503418\n",
      "training: 55 batch 34 batch_loss: 0.19572046399116516\n",
      "training: 55 batch 35 batch_loss: 0.19884806871414185\n",
      "training: 55 batch 36 batch_loss: 0.1956174075603485\n",
      "training: 55 batch 37 batch_loss: 0.19451066851615906\n",
      "training: 55 batch 38 batch_loss: 0.19640740752220154\n",
      "training: 55 batch 39 batch_loss: 0.19776511192321777\n",
      "training: 55 batch 40 batch_loss: 0.19696298241615295\n",
      "training: 55 batch 41 batch_loss: 0.19717511534690857\n",
      "training: 55 batch 42 batch_loss: 0.1979539394378662\n",
      "training: 55 batch 43 batch_loss: 0.19772228598594666\n",
      "training: 55 batch 44 batch_loss: 0.1961205005645752\n",
      "training: 55 batch 45 batch_loss: 0.19822129607200623\n",
      "training: 55 batch 46 batch_loss: 0.19382408261299133\n",
      "training: 55 batch 47 batch_loss: 0.19565245509147644\n",
      "training: 55 batch 48 batch_loss: 0.20276153087615967\n",
      "training: 55 batch 49 batch_loss: 0.192542165517807\n",
      "training: 55 batch 50 batch_loss: 0.19720953702926636\n",
      "training: 55 batch 51 batch_loss: 0.19733113050460815\n",
      "training: 55 batch 52 batch_loss: 0.20009508728981018\n",
      "training: 55 batch 53 batch_loss: 0.1946280598640442\n",
      "training: 55 batch 54 batch_loss: 0.19832274317741394\n",
      "training: 55 batch 55 batch_loss: 0.19567114114761353\n",
      "training: 55 batch 56 batch_loss: 0.19495883584022522\n",
      "training: 55 batch 57 batch_loss: 0.19795799255371094\n",
      "training: 55 batch 58 batch_loss: 0.20054784417152405\n",
      "training: 55 batch 59 batch_loss: 0.19854938983917236\n",
      "training: 55 batch 60 batch_loss: 0.1972855031490326\n",
      "training: 55 batch 61 batch_loss: 0.19324320554733276\n",
      "training: 55 batch 62 batch_loss: 0.19004374742507935\n",
      "training: 55 batch 63 batch_loss: 0.19591450691223145\n",
      "training: 55 batch 64 batch_loss: 0.19655612111091614\n",
      "training: 55 batch 65 batch_loss: 0.19652089476585388\n",
      "training: 55 batch 66 batch_loss: 0.19536441564559937\n",
      "training: 55 batch 67 batch_loss: 0.19912075996398926\n",
      "training: 55 batch 68 batch_loss: 0.1946391463279724\n",
      "training: 55 batch 69 batch_loss: 0.20146626234054565\n",
      "training: 55 batch 70 batch_loss: 0.19711104035377502\n",
      "training: 55 batch 71 batch_loss: 0.19379103183746338\n",
      "training: 55 batch 72 batch_loss: 0.19518622756004333\n",
      "training: 55 batch 73 batch_loss: 0.19900277256965637\n",
      "training: 55 batch 74 batch_loss: 0.19791021943092346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 55 batch 75 batch_loss: 0.19541794061660767\n",
      "training: 55 batch 76 batch_loss: 0.19872283935546875\n",
      "training: 55 batch 77 batch_loss: 0.19924893975257874\n",
      "training: 55 batch 78 batch_loss: 0.19825232028961182\n",
      "training: 55 batch 79 batch_loss: 0.19485437870025635\n",
      "training: 55 batch 80 batch_loss: 0.19397547841072083\n",
      "training: 55 batch 81 batch_loss: 0.2014695107936859\n",
      "training: 55 batch 82 batch_loss: 0.1992872953414917\n",
      "training: 55 batch 83 batch_loss: 0.19738644361495972\n",
      "training: 55 batch 84 batch_loss: 0.1934950351715088\n",
      "training: 55 batch 85 batch_loss: 0.1928880512714386\n",
      "training: 55 batch 86 batch_loss: 0.19633039832115173\n",
      "training: 55 batch 87 batch_loss: 0.19792884588241577\n",
      "training: 55 batch 88 batch_loss: 0.1917077898979187\n",
      "training: 55 batch 89 batch_loss: 0.19832885265350342\n",
      "training: 55 batch 90 batch_loss: 0.19806355237960815\n",
      "training: 55 batch 91 batch_loss: 0.197249174118042\n",
      "training: 55 batch 92 batch_loss: 0.19847583770751953\n",
      "training: 55 batch 93 batch_loss: 0.19684147834777832\n",
      "training: 55 batch 94 batch_loss: 0.1995660960674286\n",
      "training: 55 batch 95 batch_loss: 0.19674831628799438\n",
      "training: 55 batch 96 batch_loss: 0.19580408930778503\n",
      "training: 55 batch 97 batch_loss: 0.20058590173721313\n",
      "training: 55 batch 98 batch_loss: 0.19756215810775757\n",
      "training: 55 batch 99 batch_loss: 0.1991746723651886\n",
      "training: 55 batch 100 batch_loss: 0.19697868824005127\n",
      "training: 55 batch 101 batch_loss: 0.19524312019348145\n",
      "training: 55 batch 102 batch_loss: 0.1996203064918518\n",
      "training: 55 batch 103 batch_loss: 0.1995687484741211\n",
      "training: 55 batch 104 batch_loss: 0.20196682214736938\n",
      "training: 55 batch 105 batch_loss: 0.19726470112800598\n",
      "training: 55 batch 106 batch_loss: 0.19945916533470154\n",
      "training: 55 batch 107 batch_loss: 0.200850248336792\n",
      "training: 55 batch 108 batch_loss: 0.19540324807167053\n",
      "training: 55 batch 109 batch_loss: 0.1973104476928711\n",
      "training: 55 batch 110 batch_loss: 0.19607499241828918\n",
      "training: 55 batch 111 batch_loss: 0.19749808311462402\n",
      "training: 55 batch 112 batch_loss: 0.19910156726837158\n",
      "training: 55 batch 113 batch_loss: 0.19607898592948914\n",
      "training: 55 batch 114 batch_loss: 0.1966748833656311\n",
      "training: 55 batch 115 batch_loss: 0.19221815466880798\n",
      "training: 55 batch 116 batch_loss: 0.1959517002105713\n",
      "training: 55 batch 117 batch_loss: 0.19582995772361755\n",
      "training: 55 batch 118 batch_loss: 0.1981613039970398\n",
      "training: 55 batch 119 batch_loss: 0.1977052092552185\n",
      "training: 55 batch 120 batch_loss: 0.19224727153778076\n",
      "training: 55 batch 121 batch_loss: 0.19626417756080627\n",
      "training: 55 batch 122 batch_loss: 0.196700781583786\n",
      "training: 55 batch 123 batch_loss: 0.19785457849502563\n",
      "training: 55 batch 124 batch_loss: 0.19976922869682312\n",
      "training: 55 batch 125 batch_loss: 0.197860985994339\n",
      "training: 55 batch 126 batch_loss: 0.19442516565322876\n",
      "training: 55 batch 127 batch_loss: 0.20156508684158325\n",
      "training: 55 batch 128 batch_loss: 0.19685357809066772\n",
      "training: 55 batch 129 batch_loss: 0.19781547784805298\n",
      "training: 55 batch 130 batch_loss: 0.19485828280448914\n",
      "training: 55 batch 131 batch_loss: 0.19683271646499634\n",
      "training: 55 batch 132 batch_loss: 0.2009485363960266\n",
      "training: 55 batch 133 batch_loss: 0.19824102520942688\n",
      "training: 55 batch 134 batch_loss: 0.1980592906475067\n",
      "training: 55 batch 135 batch_loss: 0.1959802210330963\n",
      "training: 55 batch 136 batch_loss: 0.19595986604690552\n",
      "training: 55 batch 137 batch_loss: 0.19768530130386353\n",
      "training: 55 batch 138 batch_loss: 0.19666457176208496\n",
      "training: 55 batch 139 batch_loss: 0.19911110401153564\n",
      "training: 55 batch 140 batch_loss: 0.20170307159423828\n",
      "training: 55 batch 141 batch_loss: 0.19955021142959595\n",
      "training: 55 batch 142 batch_loss: 0.20115169882774353\n",
      "training: 55 batch 143 batch_loss: 0.20273667573928833\n",
      "training: 55 batch 144 batch_loss: 0.20014959573745728\n",
      "training: 55 batch 145 batch_loss: 0.20045876502990723\n",
      "training: 55 batch 146 batch_loss: 0.19672444462776184\n",
      "training: 55 batch 147 batch_loss: 0.1996949315071106\n",
      "training: 55 batch 148 batch_loss: 0.20026057958602905\n",
      "training: 55 batch 149 batch_loss: 0.19706344604492188\n",
      "training: 55 batch 150 batch_loss: 0.19958657026290894\n",
      "training: 55 batch 151 batch_loss: 0.19628214836120605\n",
      "training: 55 batch 152 batch_loss: 0.19765904545783997\n",
      "training: 55 batch 153 batch_loss: 0.19777178764343262\n",
      "training: 55 batch 154 batch_loss: 0.19493257999420166\n",
      "training: 55 batch 155 batch_loss: 0.19774970412254333\n",
      "training: 55 batch 156 batch_loss: 0.1973244547843933\n",
      "training: 55 batch 157 batch_loss: 0.1969493329524994\n",
      "training: 55 batch 158 batch_loss: 0.19728046655654907\n",
      "training: 55 batch 159 batch_loss: 0.19808220863342285\n",
      "training: 55 batch 160 batch_loss: 0.19578441977500916\n",
      "training: 55 batch 161 batch_loss: 0.19863861799240112\n",
      "training: 55 batch 162 batch_loss: 0.19945645332336426\n",
      "training: 55 batch 163 batch_loss: 0.1975651979446411\n",
      "training: 55 batch 164 batch_loss: 0.1973113715648651\n",
      "training: 55 batch 165 batch_loss: 0.19776257872581482\n",
      "training: 55 batch 166 batch_loss: 0.1955055296421051\n",
      "training: 55 batch 167 batch_loss: 0.19965362548828125\n",
      "training: 55 batch 168 batch_loss: 0.19709858298301697\n",
      "training: 55 batch 169 batch_loss: 0.19659441709518433\n",
      "training: 55 batch 170 batch_loss: 0.2001679539680481\n",
      "training: 55 batch 171 batch_loss: 0.201327383518219\n",
      "training: 55 batch 172 batch_loss: 0.19579154253005981\n",
      "training: 55 batch 173 batch_loss: 0.1952769160270691\n",
      "training: 55 batch 174 batch_loss: 0.19639569520950317\n",
      "training: 55 batch 175 batch_loss: 0.19950556755065918\n",
      "training: 55 batch 176 batch_loss: 0.1979241967201233\n",
      "training: 55 batch 177 batch_loss: 0.1998370885848999\n",
      "training: 55 batch 178 batch_loss: 0.19425144791603088\n",
      "training: 55 batch 179 batch_loss: 0.19683986902236938\n",
      "training: 55 batch 180 batch_loss: 0.19605475664138794\n",
      "training: 55 batch 181 batch_loss: 0.19867658615112305\n",
      "training: 55 batch 182 batch_loss: 0.2016332447528839\n",
      "training: 55 batch 183 batch_loss: 0.19818556308746338\n",
      "training: 55 batch 184 batch_loss: 0.20036858320236206\n",
      "training: 55 batch 185 batch_loss: 0.19789832830429077\n",
      "training: 55 batch 186 batch_loss: 0.19881367683410645\n",
      "training: 55 batch 187 batch_loss: 0.1987684965133667\n",
      "training: 55 batch 188 batch_loss: 0.19990047812461853\n",
      "training: 55 batch 189 batch_loss: 0.19611835479736328\n",
      "training: 55 batch 190 batch_loss: 0.197697252035141\n",
      "training: 55 batch 191 batch_loss: 0.1956627368927002\n",
      "training: 55 batch 192 batch_loss: 0.19975820183753967\n",
      "training: 55 batch 193 batch_loss: 0.20056045055389404\n",
      "training: 55 batch 194 batch_loss: 0.19747740030288696\n",
      "training: 55 batch 195 batch_loss: 0.19557958841323853\n",
      "training: 55 batch 196 batch_loss: 0.1968701183795929\n",
      "training: 55 batch 197 batch_loss: 0.20231389999389648\n",
      "training: 55 batch 198 batch_loss: 0.19602137804031372\n",
      "training: 55 batch 199 batch_loss: 0.19067752361297607\n",
      "training: 55 batch 200 batch_loss: 0.19881629943847656\n",
      "training: 55 batch 201 batch_loss: 0.19772756099700928\n",
      "training: 55 batch 202 batch_loss: 0.20260584354400635\n",
      "training: 55 batch 203 batch_loss: 0.1981622576713562\n",
      "training: 55 batch 204 batch_loss: 0.2007617950439453\n",
      "training: 55 batch 205 batch_loss: 0.19463172554969788\n",
      "training: 55 batch 206 batch_loss: 0.20215344429016113\n",
      "training: 55 batch 207 batch_loss: 0.20063692331314087\n",
      "training: 55 batch 208 batch_loss: 0.1948413848876953\n",
      "training: 55 batch 209 batch_loss: 0.19765692949295044\n",
      "training: 55 batch 210 batch_loss: 0.19342851638793945\n",
      "training: 55 batch 211 batch_loss: 0.19803783297538757\n",
      "training: 55 batch 212 batch_loss: 0.19745779037475586\n",
      "training: 55 batch 213 batch_loss: 0.19865474104881287\n",
      "training: 55 batch 214 batch_loss: 0.2021697461605072\n",
      "training: 55 batch 215 batch_loss: 0.19624453783035278\n",
      "training: 55 batch 216 batch_loss: 0.2049117088317871\n",
      "training: 55 batch 217 batch_loss: 0.199485182762146\n",
      "training: 55 batch 218 batch_loss: 0.20419448614120483\n",
      "training: 55 batch 219 batch_loss: 0.19305327534675598\n",
      "training: 55 batch 220 batch_loss: 0.20086777210235596\n",
      "training: 55 batch 221 batch_loss: 0.19857993721961975\n",
      "training: 55 batch 222 batch_loss: 0.19703853130340576\n",
      "training: 55 batch 223 batch_loss: 0.19699394702911377\n",
      "training: 55 batch 224 batch_loss: 0.1967066526412964\n",
      "training: 55 batch 225 batch_loss: 0.20045724511146545\n",
      "training: 55 batch 226 batch_loss: 0.19676554203033447\n",
      "training: 55 batch 227 batch_loss: 0.19807860255241394\n",
      "training: 55 batch 228 batch_loss: 0.19917333126068115\n",
      "training: 55 batch 229 batch_loss: 0.1971203088760376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 55 batch 230 batch_loss: 0.19945460557937622\n",
      "training: 55 batch 231 batch_loss: 0.1994781792163849\n",
      "training: 55 batch 232 batch_loss: 0.1986660361289978\n",
      "training: 55 batch 233 batch_loss: 0.19826644659042358\n",
      "training: 55 batch 234 batch_loss: 0.19609802961349487\n",
      "training: 55 batch 235 batch_loss: 0.19802090525627136\n",
      "training: 55 batch 236 batch_loss: 0.19560664892196655\n",
      "training: 55 batch 237 batch_loss: 0.1999475657939911\n",
      "training: 55 batch 238 batch_loss: 0.1995009481906891\n",
      "training: 55 batch 239 batch_loss: 0.19592878222465515\n",
      "training: 55 batch 240 batch_loss: 0.199150949716568\n",
      "training: 55 batch 241 batch_loss: 0.19556176662445068\n",
      "training: 55 batch 242 batch_loss: 0.19861245155334473\n",
      "training: 55 batch 243 batch_loss: 0.198386549949646\n",
      "training: 55 batch 244 batch_loss: 0.1977657675743103\n",
      "training: 55 batch 245 batch_loss: 0.19673150777816772\n",
      "training: 55 batch 246 batch_loss: 0.1958167552947998\n",
      "training: 55 batch 247 batch_loss: 0.19730499386787415\n",
      "training: 55 batch 248 batch_loss: 0.19867971539497375\n",
      "training: 55 batch 249 batch_loss: 0.20166826248168945\n",
      "training: 55 batch 250 batch_loss: 0.19614699482917786\n",
      "training: 55 batch 251 batch_loss: 0.20284229516983032\n",
      "training: 55 batch 252 batch_loss: 0.19987934827804565\n",
      "training: 55 batch 253 batch_loss: 0.19968301057815552\n",
      "training: 55 batch 254 batch_loss: 0.19974005222320557\n",
      "training: 55 batch 255 batch_loss: 0.19934669137001038\n",
      "training: 55 batch 256 batch_loss: 0.19496822357177734\n",
      "training: 55 batch 257 batch_loss: 0.20070770382881165\n",
      "training: 55 batch 258 batch_loss: 0.19938260316848755\n",
      "training: 55 batch 259 batch_loss: 0.2002822458744049\n",
      "training: 55 batch 260 batch_loss: 0.20010966062545776\n",
      "training: 55 batch 261 batch_loss: 0.19857388734817505\n",
      "training: 55 batch 262 batch_loss: 0.20078030228614807\n",
      "training: 55 batch 263 batch_loss: 0.19735077023506165\n",
      "training: 55 batch 264 batch_loss: 0.19687065482139587\n",
      "training: 55 batch 265 batch_loss: 0.19555041193962097\n",
      "training: 55 batch 266 batch_loss: 0.19650280475616455\n",
      "training: 55 batch 267 batch_loss: 0.19570863246917725\n",
      "training: 55 batch 268 batch_loss: 0.19410628080368042\n",
      "training: 55 batch 269 batch_loss: 0.19761523604393005\n",
      "training: 55 batch 270 batch_loss: 0.1986250877380371\n",
      "training: 55 batch 271 batch_loss: 0.19789111614227295\n",
      "training: 55 batch 272 batch_loss: 0.20125705003738403\n",
      "training: 55 batch 273 batch_loss: 0.19762861728668213\n",
      "training: 55 batch 274 batch_loss: 0.19766554236412048\n",
      "training: 55 batch 275 batch_loss: 0.1966990828514099\n",
      "training: 55 batch 276 batch_loss: 0.19863680005073547\n",
      "training: 55 batch 277 batch_loss: 0.19642522931098938\n",
      "training: 55 batch 278 batch_loss: 0.19444626569747925\n",
      "training: 55 batch 279 batch_loss: 0.19855818152427673\n",
      "training: 55 batch 280 batch_loss: 0.200210303068161\n",
      "training: 55 batch 281 batch_loss: 0.20089435577392578\n",
      "training: 55 batch 282 batch_loss: 0.19745013117790222\n",
      "training: 55 batch 283 batch_loss: 0.20213866233825684\n",
      "training: 55 batch 284 batch_loss: 0.19784042239189148\n",
      "training: 55 batch 285 batch_loss: 0.19763630628585815\n",
      "training: 55 batch 286 batch_loss: 0.19923722743988037\n",
      "training: 55 batch 287 batch_loss: 0.197119802236557\n",
      "training: 55 batch 288 batch_loss: 0.1965717077255249\n",
      "training: 55 batch 289 batch_loss: 0.20335441827774048\n",
      "training: 55 batch 290 batch_loss: 0.19950559735298157\n",
      "training: 55 batch 291 batch_loss: 0.19664156436920166\n",
      "training: 55 batch 292 batch_loss: 0.1969534456729889\n",
      "training: 55 batch 293 batch_loss: 0.19679555296897888\n",
      "training: 55 batch 294 batch_loss: 0.19566616415977478\n",
      "training: 55 batch 295 batch_loss: 0.19811028242111206\n",
      "training: 55 batch 296 batch_loss: 0.19671732187271118\n",
      "training: 55 batch 297 batch_loss: 0.19715887308120728\n",
      "training: 55 batch 298 batch_loss: 0.20179960131645203\n",
      "training: 55 batch 299 batch_loss: 0.19995930790901184\n",
      "training: 55 batch 300 batch_loss: 0.20182529091835022\n",
      "training: 55 batch 301 batch_loss: 0.19579452276229858\n",
      "training: 55 batch 302 batch_loss: 0.20226672291755676\n",
      "training: 55 batch 303 batch_loss: 0.19986581802368164\n",
      "training: 55 batch 304 batch_loss: 0.197246253490448\n",
      "training: 55 batch 305 batch_loss: 0.19936341047286987\n",
      "training: 55 batch 306 batch_loss: 0.19784992933273315\n",
      "training: 55 batch 307 batch_loss: 0.19700664281845093\n",
      "training: 55 batch 308 batch_loss: 0.19948434829711914\n",
      "training: 55 batch 309 batch_loss: 0.19804999232292175\n",
      "training: 55 batch 310 batch_loss: 0.1940540373325348\n",
      "training: 55 batch 311 batch_loss: 0.20158296823501587\n",
      "training: 55 batch 312 batch_loss: 0.1978740096092224\n",
      "training: 55 batch 313 batch_loss: 0.20413243770599365\n",
      "training: 55 batch 314 batch_loss: 0.1968894600868225\n",
      "training: 55 batch 315 batch_loss: 0.19555294513702393\n",
      "training: 55 batch 316 batch_loss: 0.19760137796401978\n",
      "training: 55 batch 317 batch_loss: 0.19786933064460754\n",
      "training: 55 batch 318 batch_loss: 0.19992253184318542\n",
      "training: 55 batch 319 batch_loss: 0.20244896411895752\n",
      "training: 55 batch 320 batch_loss: 0.2005353569984436\n",
      "training: 55 batch 321 batch_loss: 0.19784331321716309\n",
      "training: 55 batch 322 batch_loss: 0.1981741487979889\n",
      "training: 55 batch 323 batch_loss: 0.19911783933639526\n",
      "training: 55 batch 324 batch_loss: 0.19915050268173218\n",
      "training: 55 batch 325 batch_loss: 0.20238038897514343\n",
      "training: 55 batch 326 batch_loss: 0.20135724544525146\n",
      "training: 55 batch 327 batch_loss: 0.19883590936660767\n",
      "training: 55 batch 328 batch_loss: 0.19936805963516235\n",
      "training: 55 batch 329 batch_loss: 0.19900387525558472\n",
      "training: 55 batch 330 batch_loss: 0.19755268096923828\n",
      "training: 55 batch 331 batch_loss: 0.19710659980773926\n",
      "training: 55 batch 332 batch_loss: 0.19319391250610352\n",
      "training: 55 batch 333 batch_loss: 0.19399350881576538\n",
      "training: 55 batch 334 batch_loss: 0.19669270515441895\n",
      "training: 55 batch 335 batch_loss: 0.19635459780693054\n",
      "training: 55 batch 336 batch_loss: 0.1995249092578888\n",
      "training: 55 batch 337 batch_loss: 0.20165076851844788\n",
      "training: 55 batch 338 batch_loss: 0.19703644514083862\n",
      "training: 55 batch 339 batch_loss: 0.19784539937973022\n",
      "training: 55 batch 340 batch_loss: 0.19904333353042603\n",
      "training: 55 batch 341 batch_loss: 0.19576188921928406\n",
      "training: 55 batch 342 batch_loss: 0.20027989149093628\n",
      "training: 55 batch 343 batch_loss: 0.19497570395469666\n",
      "training: 55 batch 344 batch_loss: 0.1994096040725708\n",
      "training: 55 batch 345 batch_loss: 0.19783169031143188\n",
      "training: 55 batch 346 batch_loss: 0.20003581047058105\n",
      "training: 55 batch 347 batch_loss: 0.1962573230266571\n",
      "training: 55 batch 348 batch_loss: 0.1976833939552307\n",
      "training: 55 batch 349 batch_loss: 0.19863510131835938\n",
      "training: 55 batch 350 batch_loss: 0.19903051853179932\n",
      "training: 55 batch 351 batch_loss: 0.1959328055381775\n",
      "training: 55 batch 352 batch_loss: 0.19533956050872803\n",
      "training: 55 batch 353 batch_loss: 0.20096558332443237\n",
      "training: 55 batch 354 batch_loss: 0.1988772451877594\n",
      "training: 55 batch 355 batch_loss: 0.19938084483146667\n",
      "training: 55 batch 356 batch_loss: 0.20069870352745056\n",
      "training: 55 batch 357 batch_loss: 0.20118850469589233\n",
      "training: 55 batch 358 batch_loss: 0.19793471693992615\n",
      "training: 55 batch 359 batch_loss: 0.1968126893043518\n",
      "training: 55 batch 360 batch_loss: 0.199199378490448\n",
      "training: 55 batch 361 batch_loss: 0.19790440797805786\n",
      "training: 55 batch 362 batch_loss: 0.20245680212974548\n",
      "training: 55 batch 363 batch_loss: 0.20035740733146667\n",
      "training: 55 batch 364 batch_loss: 0.19955462217330933\n",
      "training: 55 batch 365 batch_loss: 0.19619151949882507\n",
      "training: 55 batch 366 batch_loss: 0.20153337717056274\n",
      "training: 55 batch 367 batch_loss: 0.19778618216514587\n",
      "training: 55 batch 368 batch_loss: 0.20074352622032166\n",
      "training: 55 batch 369 batch_loss: 0.19624492526054382\n",
      "training: 55 batch 370 batch_loss: 0.1993516981601715\n",
      "training: 55 batch 371 batch_loss: 0.19487372040748596\n",
      "training: 55 batch 372 batch_loss: 0.1969810128211975\n",
      "training: 55 batch 373 batch_loss: 0.20170092582702637\n",
      "training: 55 batch 374 batch_loss: 0.19867369532585144\n",
      "training: 55 batch 375 batch_loss: 0.19549515843391418\n",
      "training: 55 batch 376 batch_loss: 0.19589349627494812\n",
      "training: 55 batch 377 batch_loss: 0.19920778274536133\n",
      "training: 55 batch 378 batch_loss: 0.2035684585571289\n",
      "training: 55 batch 379 batch_loss: 0.1929311454296112\n",
      "training: 55 batch 380 batch_loss: 0.20016157627105713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 55 batch 381 batch_loss: 0.1987253725528717\n",
      "training: 55 batch 382 batch_loss: 0.1963239312171936\n",
      "training: 55 batch 383 batch_loss: 0.203080952167511\n",
      "training: 55 batch 384 batch_loss: 0.2006414234638214\n",
      "training: 55 batch 385 batch_loss: 0.19962966442108154\n",
      "training: 55 batch 386 batch_loss: 0.19765320420265198\n",
      "training: 55 batch 387 batch_loss: 0.19882220029830933\n",
      "training: 55 batch 388 batch_loss: 0.20179980993270874\n",
      "training: 55 batch 389 batch_loss: 0.19593098759651184\n",
      "training: 55 batch 390 batch_loss: 0.19517266750335693\n",
      "training: 55 batch 391 batch_loss: 0.2022036612033844\n",
      "training: 55 batch 392 batch_loss: 0.19718140363693237\n",
      "training: 55 batch 393 batch_loss: 0.19564980268478394\n",
      "training: 55 batch 394 batch_loss: 0.19909405708312988\n",
      "training: 55 batch 395 batch_loss: 0.194570392370224\n",
      "training: 55 batch 396 batch_loss: 0.19557416439056396\n",
      "training: 55 batch 397 batch_loss: 0.19779127836227417\n",
      "training: 55 batch 398 batch_loss: 0.19836148619651794\n",
      "training: 55 batch 399 batch_loss: 0.1994609534740448\n",
      "training: 55 batch 400 batch_loss: 0.20052462816238403\n",
      "training: 55 batch 401 batch_loss: 0.2009325921535492\n",
      "training: 55 batch 402 batch_loss: 0.1974073350429535\n",
      "training: 55 batch 403 batch_loss: 0.19771188497543335\n",
      "training: 55 batch 404 batch_loss: 0.20125538110733032\n",
      "training: 55 batch 405 batch_loss: 0.19862300157546997\n",
      "training: 55 batch 406 batch_loss: 0.19830530881881714\n",
      "training: 55 batch 407 batch_loss: 0.20090854167938232\n",
      "training: 55 batch 408 batch_loss: 0.19899135828018188\n",
      "training: 55 batch 409 batch_loss: 0.19654259085655212\n",
      "training: 55 batch 410 batch_loss: 0.19596850872039795\n",
      "training: 55 batch 411 batch_loss: 0.19827920198440552\n",
      "training: 55 batch 412 batch_loss: 0.20178967714309692\n",
      "training: 55 batch 413 batch_loss: 0.19716060161590576\n",
      "training: 55 batch 414 batch_loss: 0.19435563683509827\n",
      "training: 55 batch 415 batch_loss: 0.1998388171195984\n",
      "training: 55 batch 416 batch_loss: 0.19974234700202942\n",
      "training: 55 batch 417 batch_loss: 0.19960254430770874\n",
      "training: 55 batch 418 batch_loss: 0.19924253225326538\n",
      "training: 55 batch 419 batch_loss: 0.20019325613975525\n",
      "training: 55 batch 420 batch_loss: 0.19943615794181824\n",
      "training: 55 batch 421 batch_loss: 0.19616729021072388\n",
      "training: 55 batch 422 batch_loss: 0.20384341478347778\n",
      "training: 55 batch 423 batch_loss: 0.199286550283432\n",
      "training: 55 batch 424 batch_loss: 0.19598925113677979\n",
      "training: 55 batch 425 batch_loss: 0.2010047435760498\n",
      "training: 55 batch 426 batch_loss: 0.2043895125389099\n",
      "training: 55 batch 427 batch_loss: 0.20155981183052063\n",
      "training: 55 batch 428 batch_loss: 0.19821107387542725\n",
      "training: 55 batch 429 batch_loss: 0.193640798330307\n",
      "training: 55 batch 430 batch_loss: 0.19891983270645142\n",
      "training: 55 batch 431 batch_loss: 0.20008864998817444\n",
      "training: 55 batch 432 batch_loss: 0.20083671808242798\n",
      "training: 55 batch 433 batch_loss: 0.1992264688014984\n",
      "training: 55 batch 434 batch_loss: 0.19919180870056152\n",
      "training: 55 batch 435 batch_loss: 0.2001461386680603\n",
      "training: 55 batch 436 batch_loss: 0.20042556524276733\n",
      "training: 55 batch 437 batch_loss: 0.1979377567768097\n",
      "training: 55 batch 438 batch_loss: 0.19817355275154114\n",
      "training: 55 batch 439 batch_loss: 0.19728577136993408\n",
      "training: 55 batch 440 batch_loss: 0.19752168655395508\n",
      "training: 55 batch 441 batch_loss: 0.20083975791931152\n",
      "training: 55 batch 442 batch_loss: 0.20001673698425293\n",
      "training: 55 batch 443 batch_loss: 0.20044934749603271\n",
      "training: 55 batch 444 batch_loss: 0.19834721088409424\n",
      "training: 55 batch 445 batch_loss: 0.20482370257377625\n",
      "training: 55 batch 446 batch_loss: 0.20087778568267822\n",
      "training: 55 batch 447 batch_loss: 0.1968846321105957\n",
      "training: 55 batch 448 batch_loss: 0.19861391186714172\n",
      "training: 55 batch 449 batch_loss: 0.19854244589805603\n",
      "training: 55 batch 450 batch_loss: 0.19503432512283325\n",
      "training: 55 batch 451 batch_loss: 0.19420278072357178\n",
      "training: 55 batch 452 batch_loss: 0.19846028089523315\n",
      "training: 55 batch 453 batch_loss: 0.20357856154441833\n",
      "training: 55 batch 454 batch_loss: 0.20190387964248657\n",
      "training: 55 batch 455 batch_loss: 0.19699710607528687\n",
      "training: 55 batch 456 batch_loss: 0.20394060015678406\n",
      "training: 55 batch 457 batch_loss: 0.20038238167762756\n",
      "training: 55 batch 458 batch_loss: 0.2029876708984375\n",
      "training: 55 batch 459 batch_loss: 0.2005474865436554\n",
      "training: 55 batch 460 batch_loss: 0.20031440258026123\n",
      "training: 55 batch 461 batch_loss: 0.1986144781112671\n",
      "training: 55 batch 462 batch_loss: 0.1952509582042694\n",
      "training: 55 batch 463 batch_loss: 0.20264777541160583\n",
      "training: 55 batch 464 batch_loss: 0.1980563998222351\n",
      "training: 55 batch 465 batch_loss: 0.1978759765625\n",
      "training: 55 batch 466 batch_loss: 0.20029330253601074\n",
      "training: 55 batch 467 batch_loss: 0.19643524289131165\n",
      "training: 55 batch 468 batch_loss: 0.1997160017490387\n",
      "training: 55 batch 469 batch_loss: 0.19910401105880737\n",
      "training: 55 batch 470 batch_loss: 0.20057883858680725\n",
      "training: 55 batch 471 batch_loss: 0.19520330429077148\n",
      "training: 55 batch 472 batch_loss: 0.1987181007862091\n",
      "training: 55 batch 473 batch_loss: 0.20146572589874268\n",
      "training: 55 batch 474 batch_loss: 0.19908684492111206\n",
      "training: 55 batch 475 batch_loss: 0.19776684045791626\n",
      "training: 55 batch 476 batch_loss: 0.1976274847984314\n",
      "training: 55 batch 477 batch_loss: 0.1991596519947052\n",
      "training: 55 batch 478 batch_loss: 0.20065176486968994\n",
      "training: 55 batch 479 batch_loss: 0.19752037525177002\n",
      "training: 55 batch 480 batch_loss: 0.19710507988929749\n",
      "training: 55 batch 481 batch_loss: 0.20104074478149414\n",
      "training: 55 batch 482 batch_loss: 0.19772598147392273\n",
      "training: 55 batch 483 batch_loss: 0.20087674260139465\n",
      "training: 55 batch 484 batch_loss: 0.19592303037643433\n",
      "training: 55 batch 485 batch_loss: 0.19922056794166565\n",
      "training: 55 batch 486 batch_loss: 0.19835075736045837\n",
      "training: 55 batch 487 batch_loss: 0.1994842290878296\n",
      "training: 55 batch 488 batch_loss: 0.19547352194786072\n",
      "training: 55 batch 489 batch_loss: 0.2015039324760437\n",
      "training: 55 batch 490 batch_loss: 0.19864428043365479\n",
      "training: 55 batch 491 batch_loss: 0.19704586267471313\n",
      "training: 55 batch 492 batch_loss: 0.19740575551986694\n",
      "training: 55 batch 493 batch_loss: 0.19892054796218872\n",
      "training: 55 batch 494 batch_loss: 0.19858017563819885\n",
      "training: 55 batch 495 batch_loss: 0.19909656047821045\n",
      "training: 55 batch 496 batch_loss: 0.20383310317993164\n",
      "training: 55 batch 497 batch_loss: 0.2029108703136444\n",
      "training: 55 batch 498 batch_loss: 0.20343422889709473\n",
      "training: 55 batch 499 batch_loss: 0.20443052053451538\n",
      "training: 55 batch 500 batch_loss: 0.20540601015090942\n",
      "training: 55 batch 501 batch_loss: 0.20117419958114624\n",
      "training: 55 batch 502 batch_loss: 0.19988465309143066\n",
      "training: 55 batch 503 batch_loss: 0.20138975977897644\n",
      "training: 55 batch 504 batch_loss: 0.19861090183258057\n",
      "training: 55 batch 505 batch_loss: 0.2003595530986786\n",
      "training: 55 batch 506 batch_loss: 0.19672346115112305\n",
      "training: 55 batch 507 batch_loss: 0.19778046011924744\n",
      "training: 55 batch 508 batch_loss: 0.20161694288253784\n",
      "training: 55 batch 509 batch_loss: 0.19500544667243958\n",
      "training: 55 batch 510 batch_loss: 0.19690567255020142\n",
      "training: 55 batch 511 batch_loss: 0.20265138149261475\n",
      "training: 55 batch 512 batch_loss: 0.19592875242233276\n",
      "training: 55 batch 513 batch_loss: 0.1966160237789154\n",
      "training: 55 batch 514 batch_loss: 0.1993442177772522\n",
      "training: 55 batch 515 batch_loss: 0.1971302032470703\n",
      "training: 55 batch 516 batch_loss: 0.19773992896080017\n",
      "training: 55 batch 517 batch_loss: 0.20232704281806946\n",
      "training: 55 batch 518 batch_loss: 0.20079302787780762\n",
      "training: 55 batch 519 batch_loss: 0.20125609636306763\n",
      "training: 55 batch 520 batch_loss: 0.19775918126106262\n",
      "training: 55 batch 521 batch_loss: 0.20180317759513855\n",
      "training: 55 batch 522 batch_loss: 0.20081853866577148\n",
      "training: 55 batch 523 batch_loss: 0.19631299376487732\n",
      "training: 55 batch 524 batch_loss: 0.19913533329963684\n",
      "training: 55 batch 525 batch_loss: 0.19825026392936707\n",
      "training: 55 batch 526 batch_loss: 0.1993418037891388\n",
      "training: 55 batch 527 batch_loss: 0.19638696312904358\n",
      "training: 55 batch 528 batch_loss: 0.19761013984680176\n",
      "training: 55 batch 529 batch_loss: 0.20211899280548096\n",
      "training: 55 batch 530 batch_loss: 0.19941014051437378\n",
      "training: 55 batch 531 batch_loss: 0.1976434886455536\n",
      "training: 55 batch 532 batch_loss: 0.2017683982849121\n",
      "training: 55 batch 533 batch_loss: 0.1992807388305664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 55 batch 534 batch_loss: 0.19831597805023193\n",
      "training: 55 batch 535 batch_loss: 0.19983989000320435\n",
      "training: 55 batch 536 batch_loss: 0.1996285319328308\n",
      "training: 55 batch 537 batch_loss: 0.1967696249485016\n",
      "training: 55 batch 538 batch_loss: 0.19825327396392822\n",
      "training: 55 batch 539 batch_loss: 0.19892409443855286\n",
      "training: 55 batch 540 batch_loss: 0.2023240327835083\n",
      "training: 55 batch 541 batch_loss: 0.199243426322937\n",
      "training: 55 batch 542 batch_loss: 0.20016098022460938\n",
      "training: 55 batch 543 batch_loss: 0.19854220747947693\n",
      "training: 55 batch 544 batch_loss: 0.19922536611557007\n",
      "training: 55 batch 545 batch_loss: 0.19875866174697876\n",
      "training: 55 batch 546 batch_loss: 0.20200678706169128\n",
      "training: 55 batch 547 batch_loss: 0.20160028338432312\n",
      "training: 55 batch 548 batch_loss: 0.20102819800376892\n",
      "training: 55 batch 549 batch_loss: 0.19997581839561462\n",
      "training: 55 batch 550 batch_loss: 0.19763585925102234\n",
      "training: 55 batch 551 batch_loss: 0.19902566075325012\n",
      "training: 55 batch 552 batch_loss: 0.2028816044330597\n",
      "training: 55 batch 553 batch_loss: 0.1994956135749817\n",
      "training: 55 batch 554 batch_loss: 0.19993165135383606\n",
      "training: 55 batch 555 batch_loss: 0.19816091656684875\n",
      "training: 55 batch 556 batch_loss: 0.19939777255058289\n",
      "training: 55 batch 557 batch_loss: 0.19789376854896545\n",
      "training: 55 batch 558 batch_loss: 0.20158147811889648\n",
      "training: 55 batch 559 batch_loss: 0.19405341148376465\n",
      "training: 55 batch 560 batch_loss: 0.1978646218776703\n",
      "training: 55 batch 561 batch_loss: 0.19585129618644714\n",
      "training: 55 batch 562 batch_loss: 0.19986045360565186\n",
      "training: 55 batch 563 batch_loss: 0.20119619369506836\n",
      "training: 55 batch 564 batch_loss: 0.19808831810951233\n",
      "training: 55 batch 565 batch_loss: 0.203721284866333\n",
      "training: 55 batch 566 batch_loss: 0.19575315713882446\n",
      "training: 55 batch 567 batch_loss: 0.20066097378730774\n",
      "training: 55 batch 568 batch_loss: 0.19938382506370544\n",
      "training: 55 batch 569 batch_loss: 0.20505690574645996\n",
      "training: 55 batch 570 batch_loss: 0.19669458270072937\n",
      "training: 55 batch 571 batch_loss: 0.20162585377693176\n",
      "training: 55 batch 572 batch_loss: 0.2028864026069641\n",
      "training: 55 batch 573 batch_loss: 0.19949620962142944\n",
      "training: 55 batch 574 batch_loss: 0.2008817493915558\n",
      "training: 55 batch 575 batch_loss: 0.20302969217300415\n",
      "training: 55 batch 576 batch_loss: 0.19937852025032043\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 55, Hit Ratio:0.03267564533483459 | Precision:0.04821095055539172 | Recall:0.06356292421897579 | NDCG:0.062450760619138894\n",
      "*Best Performance* \n",
      "Epoch: 53, Hit Ratio:0.032685638905637374 | Precision:0.04822569546839674 | Recall:0.0635072955837164 | MDCG:0.06223539720269276\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 56 batch 0 batch_loss: 0.19971662759780884\n",
      "training: 56 batch 1 batch_loss: 0.2002989649772644\n",
      "training: 56 batch 2 batch_loss: 0.1977963149547577\n",
      "training: 56 batch 3 batch_loss: 0.20127135515213013\n",
      "training: 56 batch 4 batch_loss: 0.1989060640335083\n",
      "training: 56 batch 5 batch_loss: 0.19804814457893372\n",
      "training: 56 batch 6 batch_loss: 0.19536948204040527\n",
      "training: 56 batch 7 batch_loss: 0.19867417216300964\n",
      "training: 56 batch 8 batch_loss: 0.19951391220092773\n",
      "training: 56 batch 9 batch_loss: 0.1995992660522461\n",
      "training: 56 batch 10 batch_loss: 0.19864767789840698\n",
      "training: 56 batch 11 batch_loss: 0.1950974464416504\n",
      "training: 56 batch 12 batch_loss: 0.19817081093788147\n",
      "training: 56 batch 13 batch_loss: 0.19669592380523682\n",
      "training: 56 batch 14 batch_loss: 0.19929158687591553\n",
      "training: 56 batch 15 batch_loss: 0.194337397813797\n",
      "training: 56 batch 16 batch_loss: 0.19997000694274902\n",
      "training: 56 batch 17 batch_loss: 0.19472652673721313\n",
      "training: 56 batch 18 batch_loss: 0.19780391454696655\n",
      "training: 56 batch 19 batch_loss: 0.20145076513290405\n",
      "training: 56 batch 20 batch_loss: 0.1974058747291565\n",
      "training: 56 batch 21 batch_loss: 0.19736143946647644\n",
      "training: 56 batch 22 batch_loss: 0.19487327337265015\n",
      "training: 56 batch 23 batch_loss: 0.1976454257965088\n",
      "training: 56 batch 24 batch_loss: 0.19699779152870178\n",
      "training: 56 batch 25 batch_loss: 0.1991966962814331\n",
      "training: 56 batch 26 batch_loss: 0.19793108105659485\n",
      "training: 56 batch 27 batch_loss: 0.19733113050460815\n",
      "training: 56 batch 28 batch_loss: 0.19927626848220825\n",
      "training: 56 batch 29 batch_loss: 0.1959410309791565\n",
      "training: 56 batch 30 batch_loss: 0.1971566379070282\n",
      "training: 56 batch 31 batch_loss: 0.19743698835372925\n",
      "training: 56 batch 32 batch_loss: 0.19914549589157104\n",
      "training: 56 batch 33 batch_loss: 0.19802379608154297\n",
      "training: 56 batch 34 batch_loss: 0.19580233097076416\n",
      "training: 56 batch 35 batch_loss: 0.19863641262054443\n",
      "training: 56 batch 36 batch_loss: 0.19802287220954895\n",
      "training: 56 batch 37 batch_loss: 0.19836711883544922\n",
      "training: 56 batch 38 batch_loss: 0.19483047723770142\n",
      "training: 56 batch 39 batch_loss: 0.19861653447151184\n",
      "training: 56 batch 40 batch_loss: 0.19655340909957886\n",
      "training: 56 batch 41 batch_loss: 0.1945692002773285\n",
      "training: 56 batch 42 batch_loss: 0.197845458984375\n",
      "training: 56 batch 43 batch_loss: 0.19917351007461548\n",
      "training: 56 batch 44 batch_loss: 0.19781330227851868\n",
      "training: 56 batch 45 batch_loss: 0.19865745306015015\n",
      "training: 56 batch 46 batch_loss: 0.1986933946609497\n",
      "training: 56 batch 47 batch_loss: 0.19725516438484192\n",
      "training: 56 batch 48 batch_loss: 0.20015662908554077\n",
      "training: 56 batch 49 batch_loss: 0.19368872046470642\n",
      "training: 56 batch 50 batch_loss: 0.1981409192085266\n",
      "training: 56 batch 51 batch_loss: 0.19584789872169495\n",
      "training: 56 batch 52 batch_loss: 0.19422325491905212\n",
      "training: 56 batch 53 batch_loss: 0.1949838399887085\n",
      "training: 56 batch 54 batch_loss: 0.19769582152366638\n",
      "training: 56 batch 55 batch_loss: 0.19989126920700073\n",
      "training: 56 batch 56 batch_loss: 0.19843822717666626\n",
      "training: 56 batch 57 batch_loss: 0.20122340321540833\n",
      "training: 56 batch 58 batch_loss: 0.19947859644889832\n",
      "training: 56 batch 59 batch_loss: 0.19630512595176697\n",
      "training: 56 batch 60 batch_loss: 0.200411856174469\n",
      "training: 56 batch 61 batch_loss: 0.19446316361427307\n",
      "training: 56 batch 62 batch_loss: 0.20176947116851807\n",
      "training: 56 batch 63 batch_loss: 0.1995493769645691\n",
      "training: 56 batch 64 batch_loss: 0.1953946352005005\n",
      "training: 56 batch 65 batch_loss: 0.20274090766906738\n",
      "training: 56 batch 66 batch_loss: 0.1968647837638855\n",
      "training: 56 batch 67 batch_loss: 0.19531992077827454\n",
      "training: 56 batch 68 batch_loss: 0.19523382186889648\n",
      "training: 56 batch 69 batch_loss: 0.1925777792930603\n",
      "training: 56 batch 70 batch_loss: 0.198921799659729\n",
      "training: 56 batch 71 batch_loss: 0.19667965173721313\n",
      "training: 56 batch 72 batch_loss: 0.19497135281562805\n",
      "training: 56 batch 73 batch_loss: 0.2046668529510498\n",
      "training: 56 batch 74 batch_loss: 0.20062100887298584\n",
      "training: 56 batch 75 batch_loss: 0.2026793360710144\n",
      "training: 56 batch 76 batch_loss: 0.19828689098358154\n",
      "training: 56 batch 77 batch_loss: 0.20217382907867432\n",
      "training: 56 batch 78 batch_loss: 0.19912299513816833\n",
      "training: 56 batch 79 batch_loss: 0.20255258679389954\n",
      "training: 56 batch 80 batch_loss: 0.1954735815525055\n",
      "training: 56 batch 81 batch_loss: 0.19858473539352417\n",
      "training: 56 batch 82 batch_loss: 0.19953930377960205\n",
      "training: 56 batch 83 batch_loss: 0.19772690534591675\n",
      "training: 56 batch 84 batch_loss: 0.20121079683303833\n",
      "training: 56 batch 85 batch_loss: 0.19963517785072327\n",
      "training: 56 batch 86 batch_loss: 0.19977986812591553\n",
      "training: 56 batch 87 batch_loss: 0.19659483432769775\n",
      "training: 56 batch 88 batch_loss: 0.1970367431640625\n",
      "training: 56 batch 89 batch_loss: 0.19954347610473633\n",
      "training: 56 batch 90 batch_loss: 0.1959351897239685\n",
      "training: 56 batch 91 batch_loss: 0.1972116231918335\n",
      "training: 56 batch 92 batch_loss: 0.20000728964805603\n",
      "training: 56 batch 93 batch_loss: 0.20012110471725464\n",
      "training: 56 batch 94 batch_loss: 0.1977020502090454\n",
      "training: 56 batch 95 batch_loss: 0.20126667618751526\n",
      "training: 56 batch 96 batch_loss: 0.1985509991645813\n",
      "training: 56 batch 97 batch_loss: 0.197115957736969\n",
      "training: 56 batch 98 batch_loss: 0.19811561703681946\n",
      "training: 56 batch 99 batch_loss: 0.19696804881095886\n",
      "training: 56 batch 100 batch_loss: 0.19973424077033997\n",
      "training: 56 batch 101 batch_loss: 0.19911795854568481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 56 batch 102 batch_loss: 0.1985451579093933\n",
      "training: 56 batch 103 batch_loss: 0.19994986057281494\n",
      "training: 56 batch 104 batch_loss: 0.19796273112297058\n",
      "training: 56 batch 105 batch_loss: 0.1997281014919281\n",
      "training: 56 batch 106 batch_loss: 0.1983838677406311\n",
      "training: 56 batch 107 batch_loss: 0.19996753334999084\n",
      "training: 56 batch 108 batch_loss: 0.20054560899734497\n",
      "training: 56 batch 109 batch_loss: 0.20076218247413635\n",
      "training: 56 batch 110 batch_loss: 0.1972084939479828\n",
      "training: 56 batch 111 batch_loss: 0.19954070448875427\n",
      "training: 56 batch 112 batch_loss: 0.1974043846130371\n",
      "training: 56 batch 113 batch_loss: 0.1968330442905426\n",
      "training: 56 batch 114 batch_loss: 0.20310300588607788\n",
      "training: 56 batch 115 batch_loss: 0.1983298361301422\n",
      "training: 56 batch 116 batch_loss: 0.20082542300224304\n",
      "training: 56 batch 117 batch_loss: 0.2043340504169464\n",
      "training: 56 batch 118 batch_loss: 0.1970176100730896\n",
      "training: 56 batch 119 batch_loss: 0.19544711709022522\n",
      "training: 56 batch 120 batch_loss: 0.1992868185043335\n",
      "training: 56 batch 121 batch_loss: 0.1980208456516266\n",
      "training: 56 batch 122 batch_loss: 0.19800356030464172\n",
      "training: 56 batch 123 batch_loss: 0.1974857747554779\n",
      "training: 56 batch 124 batch_loss: 0.19656336307525635\n",
      "training: 56 batch 125 batch_loss: 0.19683849811553955\n",
      "training: 56 batch 126 batch_loss: 0.20017409324645996\n",
      "training: 56 batch 127 batch_loss: 0.20007678866386414\n",
      "training: 56 batch 128 batch_loss: 0.2005934715270996\n",
      "training: 56 batch 129 batch_loss: 0.19908276200294495\n",
      "training: 56 batch 130 batch_loss: 0.1984509825706482\n",
      "training: 56 batch 131 batch_loss: 0.19907718896865845\n",
      "training: 56 batch 132 batch_loss: 0.19670438766479492\n",
      "training: 56 batch 133 batch_loss: 0.20193105936050415\n",
      "training: 56 batch 134 batch_loss: 0.19750523567199707\n",
      "training: 56 batch 135 batch_loss: 0.19684720039367676\n",
      "training: 56 batch 136 batch_loss: 0.19715076684951782\n",
      "training: 56 batch 137 batch_loss: 0.19544243812561035\n",
      "training: 56 batch 138 batch_loss: 0.1978539228439331\n",
      "training: 56 batch 139 batch_loss: 0.19769340753555298\n",
      "training: 56 batch 140 batch_loss: 0.20050764083862305\n",
      "training: 56 batch 141 batch_loss: 0.19804063439369202\n",
      "training: 56 batch 142 batch_loss: 0.19810175895690918\n",
      "training: 56 batch 143 batch_loss: 0.1994871199131012\n",
      "training: 56 batch 144 batch_loss: 0.20015758275985718\n",
      "training: 56 batch 145 batch_loss: 0.20075884461402893\n",
      "training: 56 batch 146 batch_loss: 0.19442322850227356\n",
      "training: 56 batch 147 batch_loss: 0.19771850109100342\n",
      "training: 56 batch 148 batch_loss: 0.1980150043964386\n",
      "training: 56 batch 149 batch_loss: 0.199981689453125\n",
      "training: 56 batch 150 batch_loss: 0.199177086353302\n",
      "training: 56 batch 151 batch_loss: 0.19663557410240173\n",
      "training: 56 batch 152 batch_loss: 0.20167851448059082\n",
      "training: 56 batch 153 batch_loss: 0.19613507390022278\n",
      "training: 56 batch 154 batch_loss: 0.19511446356773376\n",
      "training: 56 batch 155 batch_loss: 0.20037618279457092\n",
      "training: 56 batch 156 batch_loss: 0.2007797360420227\n",
      "training: 56 batch 157 batch_loss: 0.19694018363952637\n",
      "training: 56 batch 158 batch_loss: 0.1993001401424408\n",
      "training: 56 batch 159 batch_loss: 0.200737863779068\n",
      "training: 56 batch 160 batch_loss: 0.1944107711315155\n",
      "training: 56 batch 161 batch_loss: 0.1986246109008789\n",
      "training: 56 batch 162 batch_loss: 0.19830191135406494\n",
      "training: 56 batch 163 batch_loss: 0.20007073879241943\n",
      "training: 56 batch 164 batch_loss: 0.19903415441513062\n",
      "training: 56 batch 165 batch_loss: 0.19821763038635254\n",
      "training: 56 batch 166 batch_loss: 0.20293065905570984\n",
      "training: 56 batch 167 batch_loss: 0.20187842845916748\n",
      "training: 56 batch 168 batch_loss: 0.1929912567138672\n",
      "training: 56 batch 169 batch_loss: 0.19872832298278809\n",
      "training: 56 batch 170 batch_loss: 0.20162451267242432\n",
      "training: 56 batch 171 batch_loss: 0.20206621289253235\n",
      "training: 56 batch 172 batch_loss: 0.19854772090911865\n",
      "training: 56 batch 173 batch_loss: 0.19924750924110413\n",
      "training: 56 batch 174 batch_loss: 0.199895441532135\n",
      "training: 56 batch 175 batch_loss: 0.20159268379211426\n",
      "training: 56 batch 176 batch_loss: 0.20547407865524292\n",
      "training: 56 batch 177 batch_loss: 0.2008763551712036\n",
      "training: 56 batch 178 batch_loss: 0.19872596859931946\n",
      "training: 56 batch 179 batch_loss: 0.19813188910484314\n",
      "training: 56 batch 180 batch_loss: 0.2020118236541748\n",
      "training: 56 batch 181 batch_loss: 0.19706010818481445\n",
      "training: 56 batch 182 batch_loss: 0.19903147220611572\n",
      "training: 56 batch 183 batch_loss: 0.1999090015888214\n",
      "training: 56 batch 184 batch_loss: 0.2000180184841156\n",
      "training: 56 batch 185 batch_loss: 0.19928669929504395\n",
      "training: 56 batch 186 batch_loss: 0.20123142004013062\n",
      "training: 56 batch 187 batch_loss: 0.19745025038719177\n",
      "training: 56 batch 188 batch_loss: 0.20128902792930603\n",
      "training: 56 batch 189 batch_loss: 0.19874703884124756\n",
      "training: 56 batch 190 batch_loss: 0.19962409138679504\n",
      "training: 56 batch 191 batch_loss: 0.19836771488189697\n",
      "training: 56 batch 192 batch_loss: 0.1997019648551941\n",
      "training: 56 batch 193 batch_loss: 0.19806122779846191\n",
      "training: 56 batch 194 batch_loss: 0.19471770524978638\n",
      "training: 56 batch 195 batch_loss: 0.20119857788085938\n",
      "training: 56 batch 196 batch_loss: 0.19641393423080444\n",
      "training: 56 batch 197 batch_loss: 0.1960652470588684\n",
      "training: 56 batch 198 batch_loss: 0.2010902464389801\n",
      "training: 56 batch 199 batch_loss: 0.19551730155944824\n",
      "training: 56 batch 200 batch_loss: 0.198106050491333\n",
      "training: 56 batch 201 batch_loss: 0.19837462902069092\n",
      "training: 56 batch 202 batch_loss: 0.1971600353717804\n",
      "training: 56 batch 203 batch_loss: 0.20064115524291992\n",
      "training: 56 batch 204 batch_loss: 0.198845773935318\n",
      "training: 56 batch 205 batch_loss: 0.20076343417167664\n",
      "training: 56 batch 206 batch_loss: 0.1987197995185852\n",
      "training: 56 batch 207 batch_loss: 0.1996760368347168\n",
      "training: 56 batch 208 batch_loss: 0.1967976689338684\n",
      "training: 56 batch 209 batch_loss: 0.19683340191841125\n",
      "training: 56 batch 210 batch_loss: 0.19997096061706543\n",
      "training: 56 batch 211 batch_loss: 0.20003944635391235\n",
      "training: 56 batch 212 batch_loss: 0.20224416255950928\n",
      "training: 56 batch 213 batch_loss: 0.19843366742134094\n",
      "training: 56 batch 214 batch_loss: 0.20062625408172607\n",
      "training: 56 batch 215 batch_loss: 0.19725534319877625\n",
      "training: 56 batch 216 batch_loss: 0.199708491563797\n",
      "training: 56 batch 217 batch_loss: 0.19726210832595825\n",
      "training: 56 batch 218 batch_loss: 0.19642609357833862\n",
      "training: 56 batch 219 batch_loss: 0.20272964239120483\n",
      "training: 56 batch 220 batch_loss: 0.198630690574646\n",
      "training: 56 batch 221 batch_loss: 0.19750583171844482\n",
      "training: 56 batch 222 batch_loss: 0.20286229252815247\n",
      "training: 56 batch 223 batch_loss: 0.19969648122787476\n",
      "training: 56 batch 224 batch_loss: 0.1984839141368866\n",
      "training: 56 batch 225 batch_loss: 0.19675278663635254\n",
      "training: 56 batch 226 batch_loss: 0.19701802730560303\n",
      "training: 56 batch 227 batch_loss: 0.19449609518051147\n",
      "training: 56 batch 228 batch_loss: 0.19622346758842468\n",
      "training: 56 batch 229 batch_loss: 0.20172256231307983\n",
      "training: 56 batch 230 batch_loss: 0.20052459836006165\n",
      "training: 56 batch 231 batch_loss: 0.20271655917167664\n",
      "training: 56 batch 232 batch_loss: 0.19989657402038574\n",
      "training: 56 batch 233 batch_loss: 0.1986398696899414\n",
      "training: 56 batch 234 batch_loss: 0.20205089449882507\n",
      "training: 56 batch 235 batch_loss: 0.2027646005153656\n",
      "training: 56 batch 236 batch_loss: 0.2018989622592926\n",
      "training: 56 batch 237 batch_loss: 0.20062988996505737\n",
      "training: 56 batch 238 batch_loss: 0.19509336352348328\n",
      "training: 56 batch 239 batch_loss: 0.20125815272331238\n",
      "training: 56 batch 240 batch_loss: 0.19792866706848145\n",
      "training: 56 batch 241 batch_loss: 0.19638395309448242\n",
      "training: 56 batch 242 batch_loss: 0.1991499364376068\n",
      "training: 56 batch 243 batch_loss: 0.198172926902771\n",
      "training: 56 batch 244 batch_loss: 0.20223408937454224\n",
      "training: 56 batch 245 batch_loss: 0.19732323288917542\n",
      "training: 56 batch 246 batch_loss: 0.19538119435310364\n",
      "training: 56 batch 247 batch_loss: 0.20064887404441833\n",
      "training: 56 batch 248 batch_loss: 0.1978275179862976\n",
      "training: 56 batch 249 batch_loss: 0.19835063815116882\n",
      "training: 56 batch 250 batch_loss: 0.2024211883544922\n",
      "training: 56 batch 251 batch_loss: 0.20302525162696838\n",
      "training: 56 batch 252 batch_loss: 0.19608992338180542\n",
      "training: 56 batch 253 batch_loss: 0.19715279340744019\n",
      "training: 56 batch 254 batch_loss: 0.19751203060150146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 56 batch 255 batch_loss: 0.1980821192264557\n",
      "training: 56 batch 256 batch_loss: 0.19952449202537537\n",
      "training: 56 batch 257 batch_loss: 0.19972386956214905\n",
      "training: 56 batch 258 batch_loss: 0.20100075006484985\n",
      "training: 56 batch 259 batch_loss: 0.19585785269737244\n",
      "training: 56 batch 260 batch_loss: 0.20016741752624512\n",
      "training: 56 batch 261 batch_loss: 0.19921058416366577\n",
      "training: 56 batch 262 batch_loss: 0.19987791776657104\n",
      "training: 56 batch 263 batch_loss: 0.20025593042373657\n",
      "training: 56 batch 264 batch_loss: 0.19454020261764526\n",
      "training: 56 batch 265 batch_loss: 0.19908326864242554\n",
      "training: 56 batch 266 batch_loss: 0.19517958164215088\n",
      "training: 56 batch 267 batch_loss: 0.19829851388931274\n",
      "training: 56 batch 268 batch_loss: 0.1978047490119934\n",
      "training: 56 batch 269 batch_loss: 0.20110145211219788\n",
      "training: 56 batch 270 batch_loss: 0.2034805417060852\n",
      "training: 56 batch 271 batch_loss: 0.2019558548927307\n",
      "training: 56 batch 272 batch_loss: 0.2001723349094391\n",
      "training: 56 batch 273 batch_loss: 0.20542728900909424\n",
      "training: 56 batch 274 batch_loss: 0.19650089740753174\n",
      "training: 56 batch 275 batch_loss: 0.19732451438903809\n",
      "training: 56 batch 276 batch_loss: 0.1966952681541443\n",
      "training: 56 batch 277 batch_loss: 0.1994025707244873\n",
      "training: 56 batch 278 batch_loss: 0.1990431249141693\n",
      "training: 56 batch 279 batch_loss: 0.19904056191444397\n",
      "training: 56 batch 280 batch_loss: 0.19613447785377502\n",
      "training: 56 batch 281 batch_loss: 0.1984943151473999\n",
      "training: 56 batch 282 batch_loss: 0.19434094429016113\n",
      "training: 56 batch 283 batch_loss: 0.19421768188476562\n",
      "training: 56 batch 284 batch_loss: 0.20071056485176086\n",
      "training: 56 batch 285 batch_loss: 0.19822418689727783\n",
      "training: 56 batch 286 batch_loss: 0.19795489311218262\n",
      "training: 56 batch 287 batch_loss: 0.20102864503860474\n",
      "training: 56 batch 288 batch_loss: 0.20010793209075928\n",
      "training: 56 batch 289 batch_loss: 0.20289009809494019\n",
      "training: 56 batch 290 batch_loss: 0.19954466819763184\n",
      "training: 56 batch 291 batch_loss: 0.2025679051876068\n",
      "training: 56 batch 292 batch_loss: 0.20235782861709595\n",
      "training: 56 batch 293 batch_loss: 0.19999772310256958\n",
      "training: 56 batch 294 batch_loss: 0.19950923323631287\n",
      "training: 56 batch 295 batch_loss: 0.20192134380340576\n",
      "training: 56 batch 296 batch_loss: 0.20317789912223816\n",
      "training: 56 batch 297 batch_loss: 0.19808369874954224\n",
      "training: 56 batch 298 batch_loss: 0.20042529702186584\n",
      "training: 56 batch 299 batch_loss: 0.20088410377502441\n",
      "training: 56 batch 300 batch_loss: 0.2013329267501831\n",
      "training: 56 batch 301 batch_loss: 0.20011818408966064\n",
      "training: 56 batch 302 batch_loss: 0.19995343685150146\n",
      "training: 56 batch 303 batch_loss: 0.199030339717865\n",
      "training: 56 batch 304 batch_loss: 0.19529283046722412\n",
      "training: 56 batch 305 batch_loss: 0.1985841691493988\n",
      "training: 56 batch 306 batch_loss: 0.19519126415252686\n",
      "training: 56 batch 307 batch_loss: 0.19881129264831543\n",
      "training: 56 batch 308 batch_loss: 0.20191752910614014\n",
      "training: 56 batch 309 batch_loss: 0.20373091101646423\n",
      "training: 56 batch 310 batch_loss: 0.20078861713409424\n",
      "training: 56 batch 311 batch_loss: 0.20241695642471313\n",
      "training: 56 batch 312 batch_loss: 0.20142582058906555\n",
      "training: 56 batch 313 batch_loss: 0.20289793610572815\n",
      "training: 56 batch 314 batch_loss: 0.20248743891716003\n",
      "training: 56 batch 315 batch_loss: 0.19642269611358643\n",
      "training: 56 batch 316 batch_loss: 0.20031321048736572\n",
      "training: 56 batch 317 batch_loss: 0.1999531388282776\n",
      "training: 56 batch 318 batch_loss: 0.20035675168037415\n",
      "training: 56 batch 319 batch_loss: 0.20183095335960388\n",
      "training: 56 batch 320 batch_loss: 0.20046034455299377\n",
      "training: 56 batch 321 batch_loss: 0.20120060443878174\n",
      "training: 56 batch 322 batch_loss: 0.2007889747619629\n",
      "training: 56 batch 323 batch_loss: 0.20020315051078796\n",
      "training: 56 batch 324 batch_loss: 0.2027164101600647\n",
      "training: 56 batch 325 batch_loss: 0.19558295607566833\n",
      "training: 56 batch 326 batch_loss: 0.19823667407035828\n",
      "training: 56 batch 327 batch_loss: 0.20067036151885986\n",
      "training: 56 batch 328 batch_loss: 0.2012200653553009\n",
      "training: 56 batch 329 batch_loss: 0.19809305667877197\n",
      "training: 56 batch 330 batch_loss: 0.20024511218070984\n",
      "training: 56 batch 331 batch_loss: 0.1939278542995453\n",
      "training: 56 batch 332 batch_loss: 0.19629496335983276\n",
      "training: 56 batch 333 batch_loss: 0.20001494884490967\n",
      "training: 56 batch 334 batch_loss: 0.1983015537261963\n",
      "training: 56 batch 335 batch_loss: 0.20491841435432434\n",
      "training: 56 batch 336 batch_loss: 0.19994336366653442\n",
      "training: 56 batch 337 batch_loss: 0.20110249519348145\n",
      "training: 56 batch 338 batch_loss: 0.19914069771766663\n",
      "training: 56 batch 339 batch_loss: 0.19871902465820312\n",
      "training: 56 batch 340 batch_loss: 0.2025536298751831\n",
      "training: 56 batch 341 batch_loss: 0.2000194489955902\n",
      "training: 56 batch 342 batch_loss: 0.19982486963272095\n",
      "training: 56 batch 343 batch_loss: 0.2068326473236084\n",
      "training: 56 batch 344 batch_loss: 0.20233860611915588\n",
      "training: 56 batch 345 batch_loss: 0.20315682888031006\n",
      "training: 56 batch 346 batch_loss: 0.20110881328582764\n",
      "training: 56 batch 347 batch_loss: 0.19747760891914368\n",
      "training: 56 batch 348 batch_loss: 0.19793426990509033\n",
      "training: 56 batch 349 batch_loss: 0.20138731598854065\n",
      "training: 56 batch 350 batch_loss: 0.20047685503959656\n",
      "training: 56 batch 351 batch_loss: 0.19916093349456787\n",
      "training: 56 batch 352 batch_loss: 0.19767045974731445\n",
      "training: 56 batch 353 batch_loss: 0.20144692063331604\n",
      "training: 56 batch 354 batch_loss: 0.19876235723495483\n",
      "training: 56 batch 355 batch_loss: 0.20042875409126282\n",
      "training: 56 batch 356 batch_loss: 0.19958451390266418\n",
      "training: 56 batch 357 batch_loss: 0.201497882604599\n",
      "training: 56 batch 358 batch_loss: 0.19588974118232727\n",
      "training: 56 batch 359 batch_loss: 0.19840338826179504\n",
      "training: 56 batch 360 batch_loss: 0.20478332042694092\n",
      "training: 56 batch 361 batch_loss: 0.2002682089805603\n",
      "training: 56 batch 362 batch_loss: 0.19984546303749084\n",
      "training: 56 batch 363 batch_loss: 0.20070230960845947\n",
      "training: 56 batch 364 batch_loss: 0.20079392194747925\n",
      "training: 56 batch 365 batch_loss: 0.19846224784851074\n",
      "training: 56 batch 366 batch_loss: 0.1992054581642151\n",
      "training: 56 batch 367 batch_loss: 0.19864913821220398\n",
      "training: 56 batch 368 batch_loss: 0.20047229528427124\n",
      "training: 56 batch 369 batch_loss: 0.19966018199920654\n",
      "training: 56 batch 370 batch_loss: 0.19945770502090454\n",
      "training: 56 batch 371 batch_loss: 0.2013787031173706\n",
      "training: 56 batch 372 batch_loss: 0.20040538907051086\n",
      "training: 56 batch 373 batch_loss: 0.19851315021514893\n",
      "training: 56 batch 374 batch_loss: 0.19597694277763367\n",
      "training: 56 batch 375 batch_loss: 0.19944176077842712\n",
      "training: 56 batch 376 batch_loss: 0.19693922996520996\n",
      "training: 56 batch 377 batch_loss: 0.20056194067001343\n",
      "training: 56 batch 378 batch_loss: 0.19848892092704773\n",
      "training: 56 batch 379 batch_loss: 0.2017863392829895\n",
      "training: 56 batch 380 batch_loss: 0.1999916434288025\n",
      "training: 56 batch 381 batch_loss: 0.19489699602127075\n",
      "training: 56 batch 382 batch_loss: 0.20277228951454163\n",
      "training: 56 batch 383 batch_loss: 0.19927677512168884\n",
      "training: 56 batch 384 batch_loss: 0.19904100894927979\n",
      "training: 56 batch 385 batch_loss: 0.20566457509994507\n",
      "training: 56 batch 386 batch_loss: 0.2007155418395996\n",
      "training: 56 batch 387 batch_loss: 0.20154523849487305\n",
      "training: 56 batch 388 batch_loss: 0.20458731055259705\n",
      "training: 56 batch 389 batch_loss: 0.19889789819717407\n",
      "training: 56 batch 390 batch_loss: 0.20218902826309204\n",
      "training: 56 batch 391 batch_loss: 0.19479206204414368\n",
      "training: 56 batch 392 batch_loss: 0.20216193795204163\n",
      "training: 56 batch 393 batch_loss: 0.1991550624370575\n",
      "training: 56 batch 394 batch_loss: 0.201747864484787\n",
      "training: 56 batch 395 batch_loss: 0.2005380392074585\n",
      "training: 56 batch 396 batch_loss: 0.19737064838409424\n",
      "training: 56 batch 397 batch_loss: 0.20424321293830872\n",
      "training: 56 batch 398 batch_loss: 0.19908791780471802\n",
      "training: 56 batch 399 batch_loss: 0.1993669867515564\n",
      "training: 56 batch 400 batch_loss: 0.19671154022216797\n",
      "training: 56 batch 401 batch_loss: 0.19860130548477173\n",
      "training: 56 batch 402 batch_loss: 0.19567042589187622\n",
      "training: 56 batch 403 batch_loss: 0.2022005319595337\n",
      "training: 56 batch 404 batch_loss: 0.2004094123840332\n",
      "training: 56 batch 405 batch_loss: 0.20171737670898438\n",
      "training: 56 batch 406 batch_loss: 0.19976606965065002\n",
      "training: 56 batch 407 batch_loss: 0.19814884662628174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 56 batch 408 batch_loss: 0.2022230625152588\n",
      "training: 56 batch 409 batch_loss: 0.20606783032417297\n",
      "training: 56 batch 410 batch_loss: 0.19855976104736328\n",
      "training: 56 batch 411 batch_loss: 0.20229828357696533\n",
      "training: 56 batch 412 batch_loss: 0.19835945963859558\n",
      "training: 56 batch 413 batch_loss: 0.19863474369049072\n",
      "training: 56 batch 414 batch_loss: 0.2022097110748291\n",
      "training: 56 batch 415 batch_loss: 0.20082774758338928\n",
      "training: 56 batch 416 batch_loss: 0.19867989420890808\n",
      "training: 56 batch 417 batch_loss: 0.20174741744995117\n",
      "training: 56 batch 418 batch_loss: 0.1971634030342102\n",
      "training: 56 batch 419 batch_loss: 0.20167061686515808\n",
      "training: 56 batch 420 batch_loss: 0.19956815242767334\n",
      "training: 56 batch 421 batch_loss: 0.2017822563648224\n",
      "training: 56 batch 422 batch_loss: 0.20304226875305176\n",
      "training: 56 batch 423 batch_loss: 0.20412439107894897\n",
      "training: 56 batch 424 batch_loss: 0.2031521499156952\n",
      "training: 56 batch 425 batch_loss: 0.2008519470691681\n",
      "training: 56 batch 426 batch_loss: 0.20193630456924438\n",
      "training: 56 batch 427 batch_loss: 0.1985965073108673\n",
      "training: 56 batch 428 batch_loss: 0.19900882244110107\n",
      "training: 56 batch 429 batch_loss: 0.20185396075248718\n",
      "training: 56 batch 430 batch_loss: 0.19997599720954895\n",
      "training: 56 batch 431 batch_loss: 0.20207905769348145\n",
      "training: 56 batch 432 batch_loss: 0.19774770736694336\n",
      "training: 56 batch 433 batch_loss: 0.2018508017063141\n",
      "training: 56 batch 434 batch_loss: 0.20024067163467407\n",
      "training: 56 batch 435 batch_loss: 0.20116299390792847\n",
      "training: 56 batch 436 batch_loss: 0.20180046558380127\n",
      "training: 56 batch 437 batch_loss: 0.20069023966789246\n",
      "training: 56 batch 438 batch_loss: 0.20154359936714172\n",
      "training: 56 batch 439 batch_loss: 0.19998720288276672\n",
      "training: 56 batch 440 batch_loss: 0.20249015092849731\n",
      "training: 56 batch 441 batch_loss: 0.19814369082450867\n",
      "training: 56 batch 442 batch_loss: 0.20096397399902344\n",
      "training: 56 batch 443 batch_loss: 0.2025269865989685\n",
      "training: 56 batch 444 batch_loss: 0.19992926716804504\n",
      "training: 56 batch 445 batch_loss: 0.19748622179031372\n",
      "training: 56 batch 446 batch_loss: 0.20065563917160034\n",
      "training: 56 batch 447 batch_loss: 0.20305073261260986\n",
      "training: 56 batch 448 batch_loss: 0.19991084933280945\n",
      "training: 56 batch 449 batch_loss: 0.19995355606079102\n",
      "training: 56 batch 450 batch_loss: 0.19662347435951233\n",
      "training: 56 batch 451 batch_loss: 0.20028558373451233\n",
      "training: 56 batch 452 batch_loss: 0.2019672989845276\n",
      "training: 56 batch 453 batch_loss: 0.19734546542167664\n",
      "training: 56 batch 454 batch_loss: 0.1989491879940033\n",
      "training: 56 batch 455 batch_loss: 0.20279473066329956\n",
      "training: 56 batch 456 batch_loss: 0.20052725076675415\n",
      "training: 56 batch 457 batch_loss: 0.19980120658874512\n",
      "training: 56 batch 458 batch_loss: 0.19849157333374023\n",
      "training: 56 batch 459 batch_loss: 0.19665032625198364\n",
      "training: 56 batch 460 batch_loss: 0.2015615701675415\n",
      "training: 56 batch 461 batch_loss: 0.19716578722000122\n",
      "training: 56 batch 462 batch_loss: 0.20098313689231873\n",
      "training: 56 batch 463 batch_loss: 0.19997438788414001\n",
      "training: 56 batch 464 batch_loss: 0.19932344555854797\n",
      "training: 56 batch 465 batch_loss: 0.20087721943855286\n",
      "training: 56 batch 466 batch_loss: 0.20119020342826843\n",
      "training: 56 batch 467 batch_loss: 0.19498282670974731\n",
      "training: 56 batch 468 batch_loss: 0.20427188277244568\n",
      "training: 56 batch 469 batch_loss: 0.2031412124633789\n",
      "training: 56 batch 470 batch_loss: 0.20010554790496826\n",
      "training: 56 batch 471 batch_loss: 0.1986420452594757\n",
      "training: 56 batch 472 batch_loss: 0.20251771807670593\n",
      "training: 56 batch 473 batch_loss: 0.20044434070587158\n",
      "training: 56 batch 474 batch_loss: 0.20215022563934326\n",
      "training: 56 batch 475 batch_loss: 0.19930344820022583\n",
      "training: 56 batch 476 batch_loss: 0.20078271627426147\n",
      "training: 56 batch 477 batch_loss: 0.20121732354164124\n",
      "training: 56 batch 478 batch_loss: 0.20449697971343994\n",
      "training: 56 batch 479 batch_loss: 0.19888561964035034\n",
      "training: 56 batch 480 batch_loss: 0.1957019567489624\n",
      "training: 56 batch 481 batch_loss: 0.20182734727859497\n",
      "training: 56 batch 482 batch_loss: 0.2001311182975769\n",
      "training: 56 batch 483 batch_loss: 0.20183396339416504\n",
      "training: 56 batch 484 batch_loss: 0.19985148310661316\n",
      "training: 56 batch 485 batch_loss: 0.1991266906261444\n",
      "training: 56 batch 486 batch_loss: 0.20268946886062622\n",
      "training: 56 batch 487 batch_loss: 0.20164161920547485\n",
      "training: 56 batch 488 batch_loss: 0.19824117422103882\n",
      "training: 56 batch 489 batch_loss: 0.2002396583557129\n",
      "training: 56 batch 490 batch_loss: 0.20121440291404724\n",
      "training: 56 batch 491 batch_loss: 0.20134681463241577\n",
      "training: 56 batch 492 batch_loss: 0.19906923174858093\n",
      "training: 56 batch 493 batch_loss: 0.20328617095947266\n",
      "training: 56 batch 494 batch_loss: 0.19534868001937866\n",
      "training: 56 batch 495 batch_loss: 0.20358902215957642\n",
      "training: 56 batch 496 batch_loss: 0.20050492882728577\n",
      "training: 56 batch 497 batch_loss: 0.20397642254829407\n",
      "training: 56 batch 498 batch_loss: 0.2005278468132019\n",
      "training: 56 batch 499 batch_loss: 0.1985144019126892\n",
      "training: 56 batch 500 batch_loss: 0.20339322090148926\n",
      "training: 56 batch 501 batch_loss: 0.20255154371261597\n",
      "training: 56 batch 502 batch_loss: 0.19890186190605164\n",
      "training: 56 batch 503 batch_loss: 0.20370572805404663\n",
      "training: 56 batch 504 batch_loss: 0.2009294033050537\n",
      "training: 56 batch 505 batch_loss: 0.20094871520996094\n",
      "training: 56 batch 506 batch_loss: 0.19983172416687012\n",
      "training: 56 batch 507 batch_loss: 0.19978398084640503\n",
      "training: 56 batch 508 batch_loss: 0.20089244842529297\n",
      "training: 56 batch 509 batch_loss: 0.2002112865447998\n",
      "training: 56 batch 510 batch_loss: 0.20408287644386292\n",
      "training: 56 batch 511 batch_loss: 0.20029869675636292\n",
      "training: 56 batch 512 batch_loss: 0.20115286111831665\n",
      "training: 56 batch 513 batch_loss: 0.19957539439201355\n",
      "training: 56 batch 514 batch_loss: 0.20119303464889526\n",
      "training: 56 batch 515 batch_loss: 0.19780918955802917\n",
      "training: 56 batch 516 batch_loss: 0.20074433088302612\n",
      "training: 56 batch 517 batch_loss: 0.19804605841636658\n",
      "training: 56 batch 518 batch_loss: 0.2006065845489502\n",
      "training: 56 batch 519 batch_loss: 0.19959428906440735\n",
      "training: 56 batch 520 batch_loss: 0.19937461614608765\n",
      "training: 56 batch 521 batch_loss: 0.2037506103515625\n",
      "training: 56 batch 522 batch_loss: 0.1963600218296051\n",
      "training: 56 batch 523 batch_loss: 0.19880077242851257\n",
      "training: 56 batch 524 batch_loss: 0.2013738453388214\n",
      "training: 56 batch 525 batch_loss: 0.19973602890968323\n",
      "training: 56 batch 526 batch_loss: 0.20216140151023865\n",
      "training: 56 batch 527 batch_loss: 0.19785714149475098\n",
      "training: 56 batch 528 batch_loss: 0.19797861576080322\n",
      "training: 56 batch 529 batch_loss: 0.20550590753555298\n",
      "training: 56 batch 530 batch_loss: 0.19985416531562805\n",
      "training: 56 batch 531 batch_loss: 0.19941097497940063\n",
      "training: 56 batch 532 batch_loss: 0.20133042335510254\n",
      "training: 56 batch 533 batch_loss: 0.19933104515075684\n",
      "training: 56 batch 534 batch_loss: 0.19898056983947754\n",
      "training: 56 batch 535 batch_loss: 0.2014930248260498\n",
      "training: 56 batch 536 batch_loss: 0.20164206624031067\n",
      "training: 56 batch 537 batch_loss: 0.20308703184127808\n",
      "training: 56 batch 538 batch_loss: 0.201264888048172\n",
      "training: 56 batch 539 batch_loss: 0.202793151140213\n",
      "training: 56 batch 540 batch_loss: 0.20696383714675903\n",
      "training: 56 batch 541 batch_loss: 0.20283278822898865\n",
      "training: 56 batch 542 batch_loss: 0.20034223794937134\n",
      "training: 56 batch 543 batch_loss: 0.20291388034820557\n",
      "training: 56 batch 544 batch_loss: 0.20294520258903503\n",
      "training: 56 batch 545 batch_loss: 0.19822555780410767\n",
      "training: 56 batch 546 batch_loss: 0.19888556003570557\n",
      "training: 56 batch 547 batch_loss: 0.20068562030792236\n",
      "training: 56 batch 548 batch_loss: 0.19734293222427368\n",
      "training: 56 batch 549 batch_loss: 0.19962766766548157\n",
      "training: 56 batch 550 batch_loss: 0.20191806554794312\n",
      "training: 56 batch 551 batch_loss: 0.1962539553642273\n",
      "training: 56 batch 552 batch_loss: 0.2031194269657135\n",
      "training: 56 batch 553 batch_loss: 0.19988137483596802\n",
      "training: 56 batch 554 batch_loss: 0.19678711891174316\n",
      "training: 56 batch 555 batch_loss: 0.2009454369544983\n",
      "training: 56 batch 556 batch_loss: 0.20082896947860718\n",
      "training: 56 batch 557 batch_loss: 0.20012131333351135\n",
      "training: 56 batch 558 batch_loss: 0.20024332404136658\n",
      "training: 56 batch 559 batch_loss: 0.2002633512020111\n",
      "training: 56 batch 560 batch_loss: 0.20158451795578003\n",
      "training: 56 batch 561 batch_loss: 0.20043694972991943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 56 batch 562 batch_loss: 0.203276127576828\n",
      "training: 56 batch 563 batch_loss: 0.19967564940452576\n",
      "training: 56 batch 564 batch_loss: 0.19881102442741394\n",
      "training: 56 batch 565 batch_loss: 0.20169860124588013\n",
      "training: 56 batch 566 batch_loss: 0.198418527841568\n",
      "training: 56 batch 567 batch_loss: 0.19952988624572754\n",
      "training: 56 batch 568 batch_loss: 0.19936531782150269\n",
      "training: 56 batch 569 batch_loss: 0.20134007930755615\n",
      "training: 56 batch 570 batch_loss: 0.2024347186088562\n",
      "training: 56 batch 571 batch_loss: 0.20298737287521362\n",
      "training: 56 batch 572 batch_loss: 0.20233136415481567\n",
      "training: 56 batch 573 batch_loss: 0.20029407739639282\n",
      "training: 56 batch 574 batch_loss: 0.1962622106075287\n",
      "training: 56 batch 575 batch_loss: 0.20210596919059753\n",
      "training: 56 batch 576 batch_loss: 0.19662892818450928\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 56, Hit Ratio:0.032922153414636585 | Precision:0.04857465840951539 | Recall:0.06427363564028983 | NDCG:0.06308737979754037\n",
      "*Best Performance* \n",
      "Epoch: 56, Hit Ratio:0.032922153414636585 | Precision:0.04857465840951539 | Recall:0.06427363564028983 | MDCG:0.06308737979754037\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 57 batch 0 batch_loss: 0.1998855471611023\n",
      "training: 57 batch 1 batch_loss: 0.19958660006523132\n",
      "training: 57 batch 2 batch_loss: 0.1956975758075714\n",
      "training: 57 batch 3 batch_loss: 0.19628649950027466\n",
      "training: 57 batch 4 batch_loss: 0.19805273413658142\n",
      "training: 57 batch 5 batch_loss: 0.19680139422416687\n",
      "training: 57 batch 6 batch_loss: 0.19853097200393677\n",
      "training: 57 batch 7 batch_loss: 0.2009378969669342\n",
      "training: 57 batch 8 batch_loss: 0.20285564661026\n",
      "training: 57 batch 9 batch_loss: 0.1940428614616394\n",
      "training: 57 batch 10 batch_loss: 0.1992044448852539\n",
      "training: 57 batch 11 batch_loss: 0.19974073767662048\n",
      "training: 57 batch 12 batch_loss: 0.19768154621124268\n",
      "training: 57 batch 13 batch_loss: 0.19946131110191345\n",
      "training: 57 batch 14 batch_loss: 0.2021084725856781\n",
      "training: 57 batch 15 batch_loss: 0.20351630449295044\n",
      "training: 57 batch 16 batch_loss: 0.20377057790756226\n",
      "training: 57 batch 17 batch_loss: 0.20137733221054077\n",
      "training: 57 batch 18 batch_loss: 0.1989651918411255\n",
      "training: 57 batch 19 batch_loss: 0.19906973838806152\n",
      "training: 57 batch 20 batch_loss: 0.20277005434036255\n",
      "training: 57 batch 21 batch_loss: 0.19678521156311035\n",
      "training: 57 batch 22 batch_loss: 0.20073413848876953\n",
      "training: 57 batch 23 batch_loss: 0.2027246356010437\n",
      "training: 57 batch 24 batch_loss: 0.19875016808509827\n",
      "training: 57 batch 25 batch_loss: 0.19932159781455994\n",
      "training: 57 batch 26 batch_loss: 0.20019549131393433\n",
      "training: 57 batch 27 batch_loss: 0.1988261640071869\n",
      "training: 57 batch 28 batch_loss: 0.20182344317436218\n",
      "training: 57 batch 29 batch_loss: 0.1974520981311798\n",
      "training: 57 batch 30 batch_loss: 0.20000803470611572\n",
      "training: 57 batch 31 batch_loss: 0.20112711191177368\n",
      "training: 57 batch 32 batch_loss: 0.1981351375579834\n",
      "training: 57 batch 33 batch_loss: 0.19666358828544617\n",
      "training: 57 batch 34 batch_loss: 0.19772720336914062\n",
      "training: 57 batch 35 batch_loss: 0.20053714513778687\n",
      "training: 57 batch 36 batch_loss: 0.20048582553863525\n",
      "training: 57 batch 37 batch_loss: 0.2010578215122223\n",
      "training: 57 batch 38 batch_loss: 0.19593024253845215\n",
      "training: 57 batch 39 batch_loss: 0.19736021757125854\n",
      "training: 57 batch 40 batch_loss: 0.19661468267440796\n",
      "training: 57 batch 41 batch_loss: 0.1979062855243683\n",
      "training: 57 batch 42 batch_loss: 0.1973048448562622\n",
      "training: 57 batch 43 batch_loss: 0.19969171285629272\n",
      "training: 57 batch 44 batch_loss: 0.19500732421875\n",
      "training: 57 batch 45 batch_loss: 0.1984984576702118\n",
      "training: 57 batch 46 batch_loss: 0.19831013679504395\n",
      "training: 57 batch 47 batch_loss: 0.20067188143730164\n",
      "training: 57 batch 48 batch_loss: 0.19914665818214417\n",
      "training: 57 batch 49 batch_loss: 0.2008899748325348\n",
      "training: 57 batch 50 batch_loss: 0.2005697786808014\n",
      "training: 57 batch 51 batch_loss: 0.19884181022644043\n",
      "training: 57 batch 52 batch_loss: 0.1992846131324768\n",
      "training: 57 batch 53 batch_loss: 0.20232918858528137\n",
      "training: 57 batch 54 batch_loss: 0.20129823684692383\n",
      "training: 57 batch 55 batch_loss: 0.2005797028541565\n",
      "training: 57 batch 56 batch_loss: 0.1972959041595459\n",
      "training: 57 batch 57 batch_loss: 0.19524866342544556\n",
      "training: 57 batch 58 batch_loss: 0.2038423717021942\n",
      "training: 57 batch 59 batch_loss: 0.20202761888504028\n",
      "training: 57 batch 60 batch_loss: 0.20267748832702637\n",
      "training: 57 batch 61 batch_loss: 0.19950085878372192\n",
      "training: 57 batch 62 batch_loss: 0.20187145471572876\n",
      "training: 57 batch 63 batch_loss: 0.19788500666618347\n",
      "training: 57 batch 64 batch_loss: 0.19641461968421936\n",
      "training: 57 batch 65 batch_loss: 0.19424289464950562\n",
      "training: 57 batch 66 batch_loss: 0.2009139060974121\n",
      "training: 57 batch 67 batch_loss: 0.1987171173095703\n",
      "training: 57 batch 68 batch_loss: 0.20166397094726562\n",
      "training: 57 batch 69 batch_loss: 0.20308202505111694\n",
      "training: 57 batch 70 batch_loss: 0.20264533162117004\n",
      "training: 57 batch 71 batch_loss: 0.19889873266220093\n",
      "training: 57 batch 72 batch_loss: 0.20237961411476135\n",
      "training: 57 batch 73 batch_loss: 0.20017695426940918\n",
      "training: 57 batch 74 batch_loss: 0.19992601871490479\n",
      "training: 57 batch 75 batch_loss: 0.19734686613082886\n",
      "training: 57 batch 76 batch_loss: 0.2024359405040741\n",
      "training: 57 batch 77 batch_loss: 0.2035100758075714\n",
      "training: 57 batch 78 batch_loss: 0.1999037265777588\n",
      "training: 57 batch 79 batch_loss: 0.20189034938812256\n",
      "training: 57 batch 80 batch_loss: 0.20249491930007935\n",
      "training: 57 batch 81 batch_loss: 0.20114019513130188\n",
      "training: 57 batch 82 batch_loss: 0.19626948237419128\n",
      "training: 57 batch 83 batch_loss: 0.1990049183368683\n",
      "training: 57 batch 84 batch_loss: 0.19540971517562866\n",
      "training: 57 batch 85 batch_loss: 0.19968557357788086\n",
      "training: 57 batch 86 batch_loss: 0.20230594277381897\n",
      "training: 57 batch 87 batch_loss: 0.20086878538131714\n",
      "training: 57 batch 88 batch_loss: 0.20027878880500793\n",
      "training: 57 batch 89 batch_loss: 0.1998213529586792\n",
      "training: 57 batch 90 batch_loss: 0.1986536979675293\n",
      "training: 57 batch 91 batch_loss: 0.2006678283214569\n",
      "training: 57 batch 92 batch_loss: 0.19934958219528198\n",
      "training: 57 batch 93 batch_loss: 0.19871452450752258\n",
      "training: 57 batch 94 batch_loss: 0.19848787784576416\n",
      "training: 57 batch 95 batch_loss: 0.19799727201461792\n",
      "training: 57 batch 96 batch_loss: 0.20193472504615784\n",
      "training: 57 batch 97 batch_loss: 0.20093005895614624\n",
      "training: 57 batch 98 batch_loss: 0.19974276423454285\n",
      "training: 57 batch 99 batch_loss: 0.19850704073905945\n",
      "training: 57 batch 100 batch_loss: 0.20150801539421082\n",
      "training: 57 batch 101 batch_loss: 0.20411795377731323\n",
      "training: 57 batch 102 batch_loss: 0.1996302604675293\n",
      "training: 57 batch 103 batch_loss: 0.19400560855865479\n",
      "training: 57 batch 104 batch_loss: 0.19989913702011108\n",
      "training: 57 batch 105 batch_loss: 0.20003676414489746\n",
      "training: 57 batch 106 batch_loss: 0.1999678909778595\n",
      "training: 57 batch 107 batch_loss: 0.20022815465927124\n",
      "training: 57 batch 108 batch_loss: 0.19822299480438232\n",
      "training: 57 batch 109 batch_loss: 0.19779041409492493\n",
      "training: 57 batch 110 batch_loss: 0.20351198315620422\n",
      "training: 57 batch 111 batch_loss: 0.19924557209014893\n",
      "training: 57 batch 112 batch_loss: 0.2025352120399475\n",
      "training: 57 batch 113 batch_loss: 0.20057305693626404\n",
      "training: 57 batch 114 batch_loss: 0.1987392008304596\n",
      "training: 57 batch 115 batch_loss: 0.19982010126113892\n",
      "training: 57 batch 116 batch_loss: 0.19532671570777893\n",
      "training: 57 batch 117 batch_loss: 0.20011547207832336\n",
      "training: 57 batch 118 batch_loss: 0.20184510946273804\n",
      "training: 57 batch 119 batch_loss: 0.197720468044281\n",
      "training: 57 batch 120 batch_loss: 0.20121535658836365\n",
      "training: 57 batch 121 batch_loss: 0.2027415633201599\n",
      "training: 57 batch 122 batch_loss: 0.2037672996520996\n",
      "training: 57 batch 123 batch_loss: 0.1952679455280304\n",
      "training: 57 batch 124 batch_loss: 0.2021312713623047\n",
      "training: 57 batch 125 batch_loss: 0.20068442821502686\n",
      "training: 57 batch 126 batch_loss: 0.20109552145004272\n",
      "training: 57 batch 127 batch_loss: 0.20200729370117188\n",
      "training: 57 batch 128 batch_loss: 0.19829729199409485\n",
      "training: 57 batch 129 batch_loss: 0.1990918517112732\n",
      "training: 57 batch 130 batch_loss: 0.19904330372810364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 57 batch 131 batch_loss: 0.20185047388076782\n",
      "training: 57 batch 132 batch_loss: 0.19514182209968567\n",
      "training: 57 batch 133 batch_loss: 0.20009514689445496\n",
      "training: 57 batch 134 batch_loss: 0.20209810137748718\n",
      "training: 57 batch 135 batch_loss: 0.1974266767501831\n",
      "training: 57 batch 136 batch_loss: 0.2006162703037262\n",
      "training: 57 batch 137 batch_loss: 0.19991779327392578\n",
      "training: 57 batch 138 batch_loss: 0.1996437907218933\n",
      "training: 57 batch 139 batch_loss: 0.2003811001777649\n",
      "training: 57 batch 140 batch_loss: 0.19867169857025146\n",
      "training: 57 batch 141 batch_loss: 0.19988232851028442\n",
      "training: 57 batch 142 batch_loss: 0.197792649269104\n",
      "training: 57 batch 143 batch_loss: 0.1999225616455078\n",
      "training: 57 batch 144 batch_loss: 0.20285555720329285\n",
      "training: 57 batch 145 batch_loss: 0.19880816340446472\n",
      "training: 57 batch 146 batch_loss: 0.2013809084892273\n",
      "training: 57 batch 147 batch_loss: 0.19722694158554077\n",
      "training: 57 batch 148 batch_loss: 0.19862592220306396\n",
      "training: 57 batch 149 batch_loss: 0.1975492537021637\n",
      "training: 57 batch 150 batch_loss: 0.19542035460472107\n",
      "training: 57 batch 151 batch_loss: 0.20158007740974426\n",
      "training: 57 batch 152 batch_loss: 0.2038588523864746\n",
      "training: 57 batch 153 batch_loss: 0.19786685705184937\n",
      "training: 57 batch 154 batch_loss: 0.20090356469154358\n",
      "training: 57 batch 155 batch_loss: 0.19973674416542053\n",
      "training: 57 batch 156 batch_loss: 0.20085418224334717\n",
      "training: 57 batch 157 batch_loss: 0.2043575644493103\n",
      "training: 57 batch 158 batch_loss: 0.19878467917442322\n",
      "training: 57 batch 159 batch_loss: 0.19909992814064026\n",
      "training: 57 batch 160 batch_loss: 0.2024897336959839\n",
      "training: 57 batch 161 batch_loss: 0.2012278437614441\n",
      "training: 57 batch 162 batch_loss: 0.2001079022884369\n",
      "training: 57 batch 163 batch_loss: 0.201925128698349\n",
      "training: 57 batch 164 batch_loss: 0.2002951204776764\n",
      "training: 57 batch 165 batch_loss: 0.20025527477264404\n",
      "training: 57 batch 166 batch_loss: 0.20225152373313904\n",
      "training: 57 batch 167 batch_loss: 0.19894379377365112\n",
      "training: 57 batch 168 batch_loss: 0.20234233140945435\n",
      "training: 57 batch 169 batch_loss: 0.20073363184928894\n",
      "training: 57 batch 170 batch_loss: 0.2020062804222107\n",
      "training: 57 batch 171 batch_loss: 0.19782733917236328\n",
      "training: 57 batch 172 batch_loss: 0.2030104696750641\n",
      "training: 57 batch 173 batch_loss: 0.20011702179908752\n",
      "training: 57 batch 174 batch_loss: 0.1989494264125824\n",
      "training: 57 batch 175 batch_loss: 0.19888457655906677\n",
      "training: 57 batch 176 batch_loss: 0.20354899764060974\n",
      "training: 57 batch 177 batch_loss: 0.20149967074394226\n",
      "training: 57 batch 178 batch_loss: 0.20167499780654907\n",
      "training: 57 batch 179 batch_loss: 0.19798341393470764\n",
      "training: 57 batch 180 batch_loss: 0.19989550113677979\n",
      "training: 57 batch 181 batch_loss: 0.1974835991859436\n",
      "training: 57 batch 182 batch_loss: 0.19728145003318787\n",
      "training: 57 batch 183 batch_loss: 0.19644057750701904\n",
      "training: 57 batch 184 batch_loss: 0.19826021790504456\n",
      "training: 57 batch 185 batch_loss: 0.19983232021331787\n",
      "training: 57 batch 186 batch_loss: 0.2047393023967743\n",
      "training: 57 batch 187 batch_loss: 0.20140159130096436\n",
      "training: 57 batch 188 batch_loss: 0.1958991289138794\n",
      "training: 57 batch 189 batch_loss: 0.20372703671455383\n",
      "training: 57 batch 190 batch_loss: 0.1961870789527893\n",
      "training: 57 batch 191 batch_loss: 0.20263031125068665\n",
      "training: 57 batch 192 batch_loss: 0.20132067799568176\n",
      "training: 57 batch 193 batch_loss: 0.20169132947921753\n",
      "training: 57 batch 194 batch_loss: 0.2019040584564209\n",
      "training: 57 batch 195 batch_loss: 0.2022155523300171\n",
      "training: 57 batch 196 batch_loss: 0.19983792304992676\n",
      "training: 57 batch 197 batch_loss: 0.19712573289871216\n",
      "training: 57 batch 198 batch_loss: 0.19915693998336792\n",
      "training: 57 batch 199 batch_loss: 0.19695335626602173\n",
      "training: 57 batch 200 batch_loss: 0.1996796727180481\n",
      "training: 57 batch 201 batch_loss: 0.20092985033988953\n",
      "training: 57 batch 202 batch_loss: 0.198600172996521\n",
      "training: 57 batch 203 batch_loss: 0.19841650128364563\n",
      "training: 57 batch 204 batch_loss: 0.19925272464752197\n",
      "training: 57 batch 205 batch_loss: 0.19923028349876404\n",
      "training: 57 batch 206 batch_loss: 0.19816216826438904\n",
      "training: 57 batch 207 batch_loss: 0.20249193906784058\n",
      "training: 57 batch 208 batch_loss: 0.20102331042289734\n",
      "training: 57 batch 209 batch_loss: 0.19923833012580872\n",
      "training: 57 batch 210 batch_loss: 0.1934041976928711\n",
      "training: 57 batch 211 batch_loss: 0.20295733213424683\n",
      "training: 57 batch 212 batch_loss: 0.19807913899421692\n",
      "training: 57 batch 213 batch_loss: 0.20092135667800903\n",
      "training: 57 batch 214 batch_loss: 0.20194894075393677\n",
      "training: 57 batch 215 batch_loss: 0.20068100094795227\n",
      "training: 57 batch 216 batch_loss: 0.20112645626068115\n",
      "training: 57 batch 217 batch_loss: 0.19746515154838562\n",
      "training: 57 batch 218 batch_loss: 0.20444107055664062\n",
      "training: 57 batch 219 batch_loss: 0.2000933289527893\n",
      "training: 57 batch 220 batch_loss: 0.199385404586792\n",
      "training: 57 batch 221 batch_loss: 0.20037123560905457\n",
      "training: 57 batch 222 batch_loss: 0.19758641719818115\n",
      "training: 57 batch 223 batch_loss: 0.2033158242702484\n",
      "training: 57 batch 224 batch_loss: 0.20022910833358765\n",
      "training: 57 batch 225 batch_loss: 0.2005462944507599\n",
      "training: 57 batch 226 batch_loss: 0.19912025332450867\n",
      "training: 57 batch 227 batch_loss: 0.20403018593788147\n",
      "training: 57 batch 228 batch_loss: 0.19579142332077026\n",
      "training: 57 batch 229 batch_loss: 0.20296365022659302\n",
      "training: 57 batch 230 batch_loss: 0.19818353652954102\n",
      "training: 57 batch 231 batch_loss: 0.19685250520706177\n",
      "training: 57 batch 232 batch_loss: 0.20281124114990234\n",
      "training: 57 batch 233 batch_loss: 0.19967031478881836\n",
      "training: 57 batch 234 batch_loss: 0.19841450452804565\n",
      "training: 57 batch 235 batch_loss: 0.1974741816520691\n",
      "training: 57 batch 236 batch_loss: 0.2011469602584839\n",
      "training: 57 batch 237 batch_loss: 0.19933602213859558\n",
      "training: 57 batch 238 batch_loss: 0.19593346118927002\n",
      "training: 57 batch 239 batch_loss: 0.20163971185684204\n",
      "training: 57 batch 240 batch_loss: 0.20232418179512024\n",
      "training: 57 batch 241 batch_loss: 0.19978713989257812\n",
      "training: 57 batch 242 batch_loss: 0.19762936234474182\n",
      "training: 57 batch 243 batch_loss: 0.19694513082504272\n",
      "training: 57 batch 244 batch_loss: 0.2016901671886444\n",
      "training: 57 batch 245 batch_loss: 0.2000618577003479\n",
      "training: 57 batch 246 batch_loss: 0.20160654187202454\n",
      "training: 57 batch 247 batch_loss: 0.1986817717552185\n",
      "training: 57 batch 248 batch_loss: 0.19777390360832214\n",
      "training: 57 batch 249 batch_loss: 0.20038443803787231\n",
      "training: 57 batch 250 batch_loss: 0.19878935813903809\n",
      "training: 57 batch 251 batch_loss: 0.19980096817016602\n",
      "training: 57 batch 252 batch_loss: 0.20317509770393372\n",
      "training: 57 batch 253 batch_loss: 0.19936427474021912\n",
      "training: 57 batch 254 batch_loss: 0.19955888390541077\n",
      "training: 57 batch 255 batch_loss: 0.19956356287002563\n",
      "training: 57 batch 256 batch_loss: 0.20326471328735352\n",
      "training: 57 batch 257 batch_loss: 0.19927531480789185\n",
      "training: 57 batch 258 batch_loss: 0.19740527868270874\n",
      "training: 57 batch 259 batch_loss: 0.20271512866020203\n",
      "training: 57 batch 260 batch_loss: 0.20095664262771606\n",
      "training: 57 batch 261 batch_loss: 0.19946807622909546\n",
      "training: 57 batch 262 batch_loss: 0.19809052348136902\n",
      "training: 57 batch 263 batch_loss: 0.2013218104839325\n",
      "training: 57 batch 264 batch_loss: 0.1998760998249054\n",
      "training: 57 batch 265 batch_loss: 0.2022547423839569\n",
      "training: 57 batch 266 batch_loss: 0.1999598741531372\n",
      "training: 57 batch 267 batch_loss: 0.20137804746627808\n",
      "training: 57 batch 268 batch_loss: 0.20126497745513916\n",
      "training: 57 batch 269 batch_loss: 0.20359894633293152\n",
      "training: 57 batch 270 batch_loss: 0.1974668800830841\n",
      "training: 57 batch 271 batch_loss: 0.20197933912277222\n",
      "training: 57 batch 272 batch_loss: 0.19990748167037964\n",
      "training: 57 batch 273 batch_loss: 0.19641730189323425\n",
      "training: 57 batch 274 batch_loss: 0.1986803412437439\n",
      "training: 57 batch 275 batch_loss: 0.20016103982925415\n",
      "training: 57 batch 276 batch_loss: 0.1973744034767151\n",
      "training: 57 batch 277 batch_loss: 0.20077866315841675\n",
      "training: 57 batch 278 batch_loss: 0.20318907499313354\n",
      "training: 57 batch 279 batch_loss: 0.2022721767425537\n",
      "training: 57 batch 280 batch_loss: 0.1982218623161316\n",
      "training: 57 batch 281 batch_loss: 0.20010876655578613\n",
      "training: 57 batch 282 batch_loss: 0.20458242297172546\n",
      "training: 57 batch 283 batch_loss: 0.20063841342926025\n",
      "training: 57 batch 284 batch_loss: 0.20203858613967896\n",
      "training: 57 batch 285 batch_loss: 0.20085150003433228\n",
      "training: 57 batch 286 batch_loss: 0.2015019953250885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 57 batch 287 batch_loss: 0.1999962329864502\n",
      "training: 57 batch 288 batch_loss: 0.20015385746955872\n",
      "training: 57 batch 289 batch_loss: 0.198357492685318\n",
      "training: 57 batch 290 batch_loss: 0.1995542049407959\n",
      "training: 57 batch 291 batch_loss: 0.20169281959533691\n",
      "training: 57 batch 292 batch_loss: 0.20005780458450317\n",
      "training: 57 batch 293 batch_loss: 0.20291653275489807\n",
      "training: 57 batch 294 batch_loss: 0.2004988193511963\n",
      "training: 57 batch 295 batch_loss: 0.2035233974456787\n",
      "training: 57 batch 296 batch_loss: 0.1990450620651245\n",
      "training: 57 batch 297 batch_loss: 0.19921576976776123\n",
      "training: 57 batch 298 batch_loss: 0.19811785221099854\n",
      "training: 57 batch 299 batch_loss: 0.19921565055847168\n",
      "training: 57 batch 300 batch_loss: 0.19913998246192932\n",
      "training: 57 batch 301 batch_loss: 0.19893798232078552\n",
      "training: 57 batch 302 batch_loss: 0.20344328880310059\n",
      "training: 57 batch 303 batch_loss: 0.20347949862480164\n",
      "training: 57 batch 304 batch_loss: 0.19971156120300293\n",
      "training: 57 batch 305 batch_loss: 0.20448607206344604\n",
      "training: 57 batch 306 batch_loss: 0.20430272817611694\n",
      "training: 57 batch 307 batch_loss: 0.20074769854545593\n",
      "training: 57 batch 308 batch_loss: 0.20160433650016785\n",
      "training: 57 batch 309 batch_loss: 0.20207208395004272\n",
      "training: 57 batch 310 batch_loss: 0.20175248384475708\n",
      "training: 57 batch 311 batch_loss: 0.2002694010734558\n",
      "training: 57 batch 312 batch_loss: 0.19857057929039001\n",
      "training: 57 batch 313 batch_loss: 0.19833159446716309\n",
      "training: 57 batch 314 batch_loss: 0.19915980100631714\n",
      "training: 57 batch 315 batch_loss: 0.1994333565235138\n",
      "training: 57 batch 316 batch_loss: 0.20043355226516724\n",
      "training: 57 batch 317 batch_loss: 0.20078885555267334\n",
      "training: 57 batch 318 batch_loss: 0.2009877860546112\n",
      "training: 57 batch 319 batch_loss: 0.1996627151966095\n",
      "training: 57 batch 320 batch_loss: 0.2020430564880371\n",
      "training: 57 batch 321 batch_loss: 0.19743281602859497\n",
      "training: 57 batch 322 batch_loss: 0.2054671049118042\n",
      "training: 57 batch 323 batch_loss: 0.1995248794555664\n",
      "training: 57 batch 324 batch_loss: 0.2023354470729828\n",
      "training: 57 batch 325 batch_loss: 0.19949430227279663\n",
      "training: 57 batch 326 batch_loss: 0.20338234305381775\n",
      "training: 57 batch 327 batch_loss: 0.20124858617782593\n",
      "training: 57 batch 328 batch_loss: 0.2015153169631958\n",
      "training: 57 batch 329 batch_loss: 0.19928672909736633\n",
      "training: 57 batch 330 batch_loss: 0.19739773869514465\n",
      "training: 57 batch 331 batch_loss: 0.19672051072120667\n",
      "training: 57 batch 332 batch_loss: 0.19914337992668152\n",
      "training: 57 batch 333 batch_loss: 0.20124167203903198\n",
      "training: 57 batch 334 batch_loss: 0.20098748803138733\n",
      "training: 57 batch 335 batch_loss: 0.19952994585037231\n",
      "training: 57 batch 336 batch_loss: 0.20175504684448242\n",
      "training: 57 batch 337 batch_loss: 0.19862458109855652\n",
      "training: 57 batch 338 batch_loss: 0.1991453468799591\n",
      "training: 57 batch 339 batch_loss: 0.20229566097259521\n",
      "training: 57 batch 340 batch_loss: 0.2012214958667755\n",
      "training: 57 batch 341 batch_loss: 0.2028254270553589\n",
      "training: 57 batch 342 batch_loss: 0.20140939950942993\n",
      "training: 57 batch 343 batch_loss: 0.20116278529167175\n",
      "training: 57 batch 344 batch_loss: 0.20002532005310059\n",
      "training: 57 batch 345 batch_loss: 0.19741719961166382\n",
      "training: 57 batch 346 batch_loss: 0.19827410578727722\n",
      "training: 57 batch 347 batch_loss: 0.20143365859985352\n",
      "training: 57 batch 348 batch_loss: 0.2042023241519928\n",
      "training: 57 batch 349 batch_loss: 0.1993175446987152\n",
      "training: 57 batch 350 batch_loss: 0.1989688277244568\n",
      "training: 57 batch 351 batch_loss: 0.1977478563785553\n",
      "training: 57 batch 352 batch_loss: 0.20468851923942566\n",
      "training: 57 batch 353 batch_loss: 0.20422106981277466\n",
      "training: 57 batch 354 batch_loss: 0.20237350463867188\n",
      "training: 57 batch 355 batch_loss: 0.20101457834243774\n",
      "training: 57 batch 356 batch_loss: 0.19841617345809937\n",
      "training: 57 batch 357 batch_loss: 0.20382291078567505\n",
      "training: 57 batch 358 batch_loss: 0.2027454674243927\n",
      "training: 57 batch 359 batch_loss: 0.20121467113494873\n",
      "training: 57 batch 360 batch_loss: 0.20144182443618774\n",
      "training: 57 batch 361 batch_loss: 0.20331060886383057\n",
      "training: 57 batch 362 batch_loss: 0.19973638653755188\n",
      "training: 57 batch 363 batch_loss: 0.20090138912200928\n",
      "training: 57 batch 364 batch_loss: 0.19436585903167725\n",
      "training: 57 batch 365 batch_loss: 0.20257997512817383\n",
      "training: 57 batch 366 batch_loss: 0.20216244459152222\n",
      "training: 57 batch 367 batch_loss: 0.19975656270980835\n",
      "training: 57 batch 368 batch_loss: 0.19782912731170654\n",
      "training: 57 batch 369 batch_loss: 0.20014655590057373\n",
      "training: 57 batch 370 batch_loss: 0.20539569854736328\n",
      "training: 57 batch 371 batch_loss: 0.1980767846107483\n",
      "training: 57 batch 372 batch_loss: 0.20010808110237122\n",
      "training: 57 batch 373 batch_loss: 0.2006990909576416\n",
      "training: 57 batch 374 batch_loss: 0.20133724808692932\n",
      "training: 57 batch 375 batch_loss: 0.20041948556900024\n",
      "training: 57 batch 376 batch_loss: 0.19957801699638367\n",
      "training: 57 batch 377 batch_loss: 0.20309096574783325\n",
      "training: 57 batch 378 batch_loss: 0.2011880874633789\n",
      "training: 57 batch 379 batch_loss: 0.2008783221244812\n",
      "training: 57 batch 380 batch_loss: 0.2025178074836731\n",
      "training: 57 batch 381 batch_loss: 0.19922378659248352\n",
      "training: 57 batch 382 batch_loss: 0.20206183195114136\n",
      "training: 57 batch 383 batch_loss: 0.2027873694896698\n",
      "training: 57 batch 384 batch_loss: 0.2006458044052124\n",
      "training: 57 batch 385 batch_loss: 0.1962258219718933\n",
      "training: 57 batch 386 batch_loss: 0.19804471731185913\n",
      "training: 57 batch 387 batch_loss: 0.20090025663375854\n",
      "training: 57 batch 388 batch_loss: 0.203020840883255\n",
      "training: 57 batch 389 batch_loss: 0.19647327065467834\n",
      "training: 57 batch 390 batch_loss: 0.1977461874485016\n",
      "training: 57 batch 391 batch_loss: 0.2014128863811493\n",
      "training: 57 batch 392 batch_loss: 0.2054728865623474\n",
      "training: 57 batch 393 batch_loss: 0.20239081978797913\n",
      "training: 57 batch 394 batch_loss: 0.19856882095336914\n",
      "training: 57 batch 395 batch_loss: 0.20345711708068848\n",
      "training: 57 batch 396 batch_loss: 0.19976457953453064\n",
      "training: 57 batch 397 batch_loss: 0.2000666856765747\n",
      "training: 57 batch 398 batch_loss: 0.19761145114898682\n",
      "training: 57 batch 399 batch_loss: 0.20508897304534912\n",
      "training: 57 batch 400 batch_loss: 0.197358638048172\n",
      "training: 57 batch 401 batch_loss: 0.19897258281707764\n",
      "training: 57 batch 402 batch_loss: 0.2014102041721344\n",
      "training: 57 batch 403 batch_loss: 0.20298033952713013\n",
      "training: 57 batch 404 batch_loss: 0.20704466104507446\n",
      "training: 57 batch 405 batch_loss: 0.20011204481124878\n",
      "training: 57 batch 406 batch_loss: 0.20138013362884521\n",
      "training: 57 batch 407 batch_loss: 0.19806092977523804\n",
      "training: 57 batch 408 batch_loss: 0.20611459016799927\n",
      "training: 57 batch 409 batch_loss: 0.2038843035697937\n",
      "training: 57 batch 410 batch_loss: 0.1995651125907898\n",
      "training: 57 batch 411 batch_loss: 0.20026791095733643\n",
      "training: 57 batch 412 batch_loss: 0.19819840788841248\n",
      "training: 57 batch 413 batch_loss: 0.19691556692123413\n",
      "training: 57 batch 414 batch_loss: 0.20559093356132507\n",
      "training: 57 batch 415 batch_loss: 0.20347696542739868\n",
      "training: 57 batch 416 batch_loss: 0.20199400186538696\n",
      "training: 57 batch 417 batch_loss: 0.20349380373954773\n",
      "training: 57 batch 418 batch_loss: 0.1954750418663025\n",
      "training: 57 batch 419 batch_loss: 0.19930872321128845\n",
      "training: 57 batch 420 batch_loss: 0.19814077019691467\n",
      "training: 57 batch 421 batch_loss: 0.2039632797241211\n",
      "training: 57 batch 422 batch_loss: 0.20043295621871948\n",
      "training: 57 batch 423 batch_loss: 0.20131522417068481\n",
      "training: 57 batch 424 batch_loss: 0.19961172342300415\n",
      "training: 57 batch 425 batch_loss: 0.20106178522109985\n",
      "training: 57 batch 426 batch_loss: 0.20126760005950928\n",
      "training: 57 batch 427 batch_loss: 0.20024102926254272\n",
      "training: 57 batch 428 batch_loss: 0.2027284801006317\n",
      "training: 57 batch 429 batch_loss: 0.2012176513671875\n",
      "training: 57 batch 430 batch_loss: 0.20058876276016235\n",
      "training: 57 batch 431 batch_loss: 0.19867825508117676\n",
      "training: 57 batch 432 batch_loss: 0.2041667103767395\n",
      "training: 57 batch 433 batch_loss: 0.19931456446647644\n",
      "training: 57 batch 434 batch_loss: 0.20000052452087402\n",
      "training: 57 batch 435 batch_loss: 0.20010730624198914\n",
      "training: 57 batch 436 batch_loss: 0.20378628373146057\n",
      "training: 57 batch 437 batch_loss: 0.20071449875831604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 57 batch 438 batch_loss: 0.20465195178985596\n",
      "training: 57 batch 439 batch_loss: 0.2029012143611908\n",
      "training: 57 batch 440 batch_loss: 0.2044154405593872\n",
      "training: 57 batch 441 batch_loss: 0.2025602161884308\n",
      "training: 57 batch 442 batch_loss: 0.20318877696990967\n",
      "training: 57 batch 443 batch_loss: 0.2002507746219635\n",
      "training: 57 batch 444 batch_loss: 0.1977231502532959\n",
      "training: 57 batch 445 batch_loss: 0.20088547468185425\n",
      "training: 57 batch 446 batch_loss: 0.20060402154922485\n",
      "training: 57 batch 447 batch_loss: 0.20012301206588745\n",
      "training: 57 batch 448 batch_loss: 0.20568883419036865\n",
      "training: 57 batch 449 batch_loss: 0.20283189415931702\n",
      "training: 57 batch 450 batch_loss: 0.20220303535461426\n",
      "training: 57 batch 451 batch_loss: 0.19945219159126282\n",
      "training: 57 batch 452 batch_loss: 0.20209449529647827\n",
      "training: 57 batch 453 batch_loss: 0.19822946190834045\n",
      "training: 57 batch 454 batch_loss: 0.20131748914718628\n",
      "training: 57 batch 455 batch_loss: 0.20056551694869995\n",
      "training: 57 batch 456 batch_loss: 0.19805413484573364\n",
      "training: 57 batch 457 batch_loss: 0.1994735300540924\n",
      "training: 57 batch 458 batch_loss: 0.20352047681808472\n",
      "training: 57 batch 459 batch_loss: 0.20516526699066162\n",
      "training: 57 batch 460 batch_loss: 0.20101380348205566\n",
      "training: 57 batch 461 batch_loss: 0.20291996002197266\n",
      "training: 57 batch 462 batch_loss: 0.2072249948978424\n",
      "training: 57 batch 463 batch_loss: 0.2005481719970703\n",
      "training: 57 batch 464 batch_loss: 0.19914668798446655\n",
      "training: 57 batch 465 batch_loss: 0.19857224822044373\n",
      "training: 57 batch 466 batch_loss: 0.20024874806404114\n",
      "training: 57 batch 467 batch_loss: 0.20211368799209595\n",
      "training: 57 batch 468 batch_loss: 0.20613574981689453\n",
      "training: 57 batch 469 batch_loss: 0.20027989149093628\n",
      "training: 57 batch 470 batch_loss: 0.2006276547908783\n",
      "training: 57 batch 471 batch_loss: 0.1994912028312683\n",
      "training: 57 batch 472 batch_loss: 0.20647773146629333\n",
      "training: 57 batch 473 batch_loss: 0.20242971181869507\n",
      "training: 57 batch 474 batch_loss: 0.2037610411643982\n",
      "training: 57 batch 475 batch_loss: 0.20105257630348206\n",
      "training: 57 batch 476 batch_loss: 0.19998687505722046\n",
      "training: 57 batch 477 batch_loss: 0.2028121054172516\n",
      "training: 57 batch 478 batch_loss: 0.2058192491531372\n",
      "training: 57 batch 479 batch_loss: 0.2060692012310028\n",
      "training: 57 batch 480 batch_loss: 0.20164334774017334\n",
      "training: 57 batch 481 batch_loss: 0.20120054483413696\n",
      "training: 57 batch 482 batch_loss: 0.20264357328414917\n",
      "training: 57 batch 483 batch_loss: 0.19696557521820068\n",
      "training: 57 batch 484 batch_loss: 0.20591223239898682\n",
      "training: 57 batch 485 batch_loss: 0.200364351272583\n",
      "training: 57 batch 486 batch_loss: 0.1981450319290161\n",
      "training: 57 batch 487 batch_loss: 0.20173406600952148\n",
      "training: 57 batch 488 batch_loss: 0.19906580448150635\n",
      "training: 57 batch 489 batch_loss: 0.20354676246643066\n",
      "training: 57 batch 490 batch_loss: 0.20046532154083252\n",
      "training: 57 batch 491 batch_loss: 0.1971890926361084\n",
      "training: 57 batch 492 batch_loss: 0.201244056224823\n",
      "training: 57 batch 493 batch_loss: 0.20094969868659973\n",
      "training: 57 batch 494 batch_loss: 0.1960979402065277\n",
      "training: 57 batch 495 batch_loss: 0.1996883749961853\n",
      "training: 57 batch 496 batch_loss: 0.20197051763534546\n",
      "training: 57 batch 497 batch_loss: 0.19913071393966675\n",
      "training: 57 batch 498 batch_loss: 0.19765719771385193\n",
      "training: 57 batch 499 batch_loss: 0.20084768533706665\n",
      "training: 57 batch 500 batch_loss: 0.20247390866279602\n",
      "training: 57 batch 501 batch_loss: 0.20096096396446228\n",
      "training: 57 batch 502 batch_loss: 0.1994258165359497\n",
      "training: 57 batch 503 batch_loss: 0.2043592929840088\n",
      "training: 57 batch 504 batch_loss: 0.2004099190235138\n",
      "training: 57 batch 505 batch_loss: 0.2020423710346222\n",
      "training: 57 batch 506 batch_loss: 0.2042163610458374\n",
      "training: 57 batch 507 batch_loss: 0.2088916301727295\n",
      "training: 57 batch 508 batch_loss: 0.20129883289337158\n",
      "training: 57 batch 509 batch_loss: 0.19737136363983154\n",
      "training: 57 batch 510 batch_loss: 0.20504266023635864\n",
      "training: 57 batch 511 batch_loss: 0.19897443056106567\n",
      "training: 57 batch 512 batch_loss: 0.20407623052597046\n",
      "training: 57 batch 513 batch_loss: 0.19913122057914734\n",
      "training: 57 batch 514 batch_loss: 0.20258504152297974\n",
      "training: 57 batch 515 batch_loss: 0.20294243097305298\n",
      "training: 57 batch 516 batch_loss: 0.19653040170669556\n",
      "training: 57 batch 517 batch_loss: 0.20116159319877625\n",
      "training: 57 batch 518 batch_loss: 0.20507097244262695\n",
      "training: 57 batch 519 batch_loss: 0.20353856682777405\n",
      "training: 57 batch 520 batch_loss: 0.20229804515838623\n",
      "training: 57 batch 521 batch_loss: 0.20463648438453674\n",
      "training: 57 batch 522 batch_loss: 0.20396915078163147\n",
      "training: 57 batch 523 batch_loss: 0.20017698407173157\n",
      "training: 57 batch 524 batch_loss: 0.20072606205940247\n",
      "training: 57 batch 525 batch_loss: 0.20155104994773865\n",
      "training: 57 batch 526 batch_loss: 0.2040272057056427\n",
      "training: 57 batch 527 batch_loss: 0.19998908042907715\n",
      "training: 57 batch 528 batch_loss: 0.20381465554237366\n",
      "training: 57 batch 529 batch_loss: 0.19998833537101746\n",
      "training: 57 batch 530 batch_loss: 0.2043512761592865\n",
      "training: 57 batch 531 batch_loss: 0.19904297590255737\n",
      "training: 57 batch 532 batch_loss: 0.2024783492088318\n",
      "training: 57 batch 533 batch_loss: 0.19963324069976807\n",
      "training: 57 batch 534 batch_loss: 0.20040783286094666\n",
      "training: 57 batch 535 batch_loss: 0.19999879598617554\n",
      "training: 57 batch 536 batch_loss: 0.20495614409446716\n",
      "training: 57 batch 537 batch_loss: 0.20026743412017822\n",
      "training: 57 batch 538 batch_loss: 0.20066866278648376\n",
      "training: 57 batch 539 batch_loss: 0.2008855938911438\n",
      "training: 57 batch 540 batch_loss: 0.19783934950828552\n",
      "training: 57 batch 541 batch_loss: 0.20426008105278015\n",
      "training: 57 batch 542 batch_loss: 0.20485329627990723\n",
      "training: 57 batch 543 batch_loss: 0.20154529809951782\n",
      "training: 57 batch 544 batch_loss: 0.20081371068954468\n",
      "training: 57 batch 545 batch_loss: 0.2023494839668274\n",
      "training: 57 batch 546 batch_loss: 0.2060984969139099\n",
      "training: 57 batch 547 batch_loss: 0.20222848653793335\n",
      "training: 57 batch 548 batch_loss: 0.19823461771011353\n",
      "training: 57 batch 549 batch_loss: 0.2032821774482727\n",
      "training: 57 batch 550 batch_loss: 0.2035980224609375\n",
      "training: 57 batch 551 batch_loss: 0.20213305950164795\n",
      "training: 57 batch 552 batch_loss: 0.20271462202072144\n",
      "training: 57 batch 553 batch_loss: 0.2021929919719696\n",
      "training: 57 batch 554 batch_loss: 0.20330092310905457\n",
      "training: 57 batch 555 batch_loss: 0.20333969593048096\n",
      "training: 57 batch 556 batch_loss: 0.19962897896766663\n",
      "training: 57 batch 557 batch_loss: 0.2048007845878601\n",
      "training: 57 batch 558 batch_loss: 0.20243331789970398\n",
      "training: 57 batch 559 batch_loss: 0.20220041275024414\n",
      "training: 57 batch 560 batch_loss: 0.19778454303741455\n",
      "training: 57 batch 561 batch_loss: 0.202750563621521\n",
      "training: 57 batch 562 batch_loss: 0.20426464080810547\n",
      "training: 57 batch 563 batch_loss: 0.20543718338012695\n",
      "training: 57 batch 564 batch_loss: 0.2044236958026886\n",
      "training: 57 batch 565 batch_loss: 0.2021198868751526\n",
      "training: 57 batch 566 batch_loss: 0.20302584767341614\n",
      "training: 57 batch 567 batch_loss: 0.19893085956573486\n",
      "training: 57 batch 568 batch_loss: 0.20220571756362915\n",
      "training: 57 batch 569 batch_loss: 0.19848081469535828\n",
      "training: 57 batch 570 batch_loss: 0.20128700137138367\n",
      "training: 57 batch 571 batch_loss: 0.2057376205921173\n",
      "training: 57 batch 572 batch_loss: 0.20301896333694458\n",
      "training: 57 batch 573 batch_loss: 0.20174482464790344\n",
      "training: 57 batch 574 batch_loss: 0.2010141909122467\n",
      "training: 57 batch 575 batch_loss: 0.20499742031097412\n",
      "training: 57 batch 576 batch_loss: 0.2120530605316162\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 57, Hit Ratio:0.03294214055624215 | Precision:0.04860414823552541 | Recall:0.06415975949289045 | NDCG:0.06280432318836056\n",
      "*Best Performance* \n",
      "Epoch: 56, Hit Ratio:0.032922153414636585 | Precision:0.04857465840951539 | Recall:0.06427363564028983 | MDCG:0.06308737979754037\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 58 batch 0 batch_loss: 0.20305630564689636\n",
      "training: 58 batch 1 batch_loss: 0.20345711708068848\n",
      "training: 58 batch 2 batch_loss: 0.19677108526229858\n",
      "training: 58 batch 3 batch_loss: 0.19937828183174133\n",
      "training: 58 batch 4 batch_loss: 0.19988954067230225\n",
      "training: 58 batch 5 batch_loss: 0.1979057788848877\n",
      "training: 58 batch 6 batch_loss: 0.19544482231140137\n",
      "training: 58 batch 7 batch_loss: 0.19681814312934875\n",
      "training: 58 batch 8 batch_loss: 0.20020058751106262\n",
      "training: 58 batch 9 batch_loss: 0.2020922303199768\n",
      "training: 58 batch 10 batch_loss: 0.19907841086387634\n",
      "training: 58 batch 11 batch_loss: 0.20065149664878845\n",
      "training: 58 batch 12 batch_loss: 0.20099151134490967\n",
      "training: 58 batch 13 batch_loss: 0.1991792917251587\n",
      "training: 58 batch 14 batch_loss: 0.1990060806274414\n",
      "training: 58 batch 15 batch_loss: 0.19866213202476501\n",
      "training: 58 batch 16 batch_loss: 0.19897550344467163\n",
      "training: 58 batch 17 batch_loss: 0.20047882199287415\n",
      "training: 58 batch 18 batch_loss: 0.19994980096817017\n",
      "training: 58 batch 19 batch_loss: 0.20321625471115112\n",
      "training: 58 batch 20 batch_loss: 0.20250818133354187\n",
      "training: 58 batch 21 batch_loss: 0.20554903149604797\n",
      "training: 58 batch 22 batch_loss: 0.20061475038528442\n",
      "training: 58 batch 23 batch_loss: 0.1972734034061432\n",
      "training: 58 batch 24 batch_loss: 0.20342639088630676\n",
      "training: 58 batch 25 batch_loss: 0.202892005443573\n",
      "training: 58 batch 26 batch_loss: 0.20105212926864624\n",
      "training: 58 batch 27 batch_loss: 0.20362138748168945\n",
      "training: 58 batch 28 batch_loss: 0.19783064723014832\n",
      "training: 58 batch 29 batch_loss: 0.20016279816627502\n",
      "training: 58 batch 30 batch_loss: 0.20113280415534973\n",
      "training: 58 batch 31 batch_loss: 0.20165416598320007\n",
      "training: 58 batch 32 batch_loss: 0.2002790868282318\n",
      "training: 58 batch 33 batch_loss: 0.1967376172542572\n",
      "training: 58 batch 34 batch_loss: 0.20221567153930664\n",
      "training: 58 batch 35 batch_loss: 0.2024640440940857\n",
      "training: 58 batch 36 batch_loss: 0.20206978917121887\n",
      "training: 58 batch 37 batch_loss: 0.2016819417476654\n",
      "training: 58 batch 38 batch_loss: 0.20109078288078308\n",
      "training: 58 batch 39 batch_loss: 0.20410054922103882\n",
      "training: 58 batch 40 batch_loss: 0.20040646195411682\n",
      "training: 58 batch 41 batch_loss: 0.20041167736053467\n",
      "training: 58 batch 42 batch_loss: 0.1981019377708435\n",
      "training: 58 batch 43 batch_loss: 0.1988183856010437\n",
      "training: 58 batch 44 batch_loss: 0.19856765866279602\n",
      "training: 58 batch 45 batch_loss: 0.2008242905139923\n",
      "training: 58 batch 46 batch_loss: 0.20308080315589905\n",
      "training: 58 batch 47 batch_loss: 0.19857245683670044\n",
      "training: 58 batch 48 batch_loss: 0.19734415411949158\n",
      "training: 58 batch 49 batch_loss: 0.19791871309280396\n",
      "training: 58 batch 50 batch_loss: 0.2041703760623932\n",
      "training: 58 batch 51 batch_loss: 0.19694331288337708\n",
      "training: 58 batch 52 batch_loss: 0.20066088438034058\n",
      "training: 58 batch 53 batch_loss: 0.20588833093643188\n",
      "training: 58 batch 54 batch_loss: 0.20523390173912048\n",
      "training: 58 batch 55 batch_loss: 0.20022153854370117\n",
      "training: 58 batch 56 batch_loss: 0.20133909583091736\n",
      "training: 58 batch 57 batch_loss: 0.1990492045879364\n",
      "training: 58 batch 58 batch_loss: 0.19970470666885376\n",
      "training: 58 batch 59 batch_loss: 0.20306065678596497\n",
      "training: 58 batch 60 batch_loss: 0.19893157482147217\n",
      "training: 58 batch 61 batch_loss: 0.20007359981536865\n",
      "training: 58 batch 62 batch_loss: 0.20118805766105652\n",
      "training: 58 batch 63 batch_loss: 0.19798901677131653\n",
      "training: 58 batch 64 batch_loss: 0.19743937253952026\n",
      "training: 58 batch 65 batch_loss: 0.20450270175933838\n",
      "training: 58 batch 66 batch_loss: 0.2018723487854004\n",
      "training: 58 batch 67 batch_loss: 0.20079073309898376\n",
      "training: 58 batch 68 batch_loss: 0.20490863919258118\n",
      "training: 58 batch 69 batch_loss: 0.20154041051864624\n",
      "training: 58 batch 70 batch_loss: 0.1965254843235016\n",
      "training: 58 batch 71 batch_loss: 0.19905617833137512\n",
      "training: 58 batch 72 batch_loss: 0.19983452558517456\n",
      "training: 58 batch 73 batch_loss: 0.20182397961616516\n",
      "training: 58 batch 74 batch_loss: 0.20392760634422302\n",
      "training: 58 batch 75 batch_loss: 0.19904133677482605\n",
      "training: 58 batch 76 batch_loss: 0.1967143714427948\n",
      "training: 58 batch 77 batch_loss: 0.2000615894794464\n",
      "training: 58 batch 78 batch_loss: 0.200810045003891\n",
      "training: 58 batch 79 batch_loss: 0.2013702094554901\n",
      "training: 58 batch 80 batch_loss: 0.19986984133720398\n",
      "training: 58 batch 81 batch_loss: 0.20053449273109436\n",
      "training: 58 batch 82 batch_loss: 0.20000019669532776\n",
      "training: 58 batch 83 batch_loss: 0.197612464427948\n",
      "training: 58 batch 84 batch_loss: 0.19766736030578613\n",
      "training: 58 batch 85 batch_loss: 0.20235377550125122\n",
      "training: 58 batch 86 batch_loss: 0.19865036010742188\n",
      "training: 58 batch 87 batch_loss: 0.20226013660430908\n",
      "training: 58 batch 88 batch_loss: 0.20052474737167358\n",
      "training: 58 batch 89 batch_loss: 0.19569745659828186\n",
      "training: 58 batch 90 batch_loss: 0.19617900252342224\n",
      "training: 58 batch 91 batch_loss: 0.2021632194519043\n",
      "training: 58 batch 92 batch_loss: 0.2004041075706482\n",
      "training: 58 batch 93 batch_loss: 0.1987234354019165\n",
      "training: 58 batch 94 batch_loss: 0.2003445327281952\n",
      "training: 58 batch 95 batch_loss: 0.2027161717414856\n",
      "training: 58 batch 96 batch_loss: 0.19942012429237366\n",
      "training: 58 batch 97 batch_loss: 0.19972500205039978\n",
      "training: 58 batch 98 batch_loss: 0.20012810826301575\n",
      "training: 58 batch 99 batch_loss: 0.19849532842636108\n",
      "training: 58 batch 100 batch_loss: 0.20105531811714172\n",
      "training: 58 batch 101 batch_loss: 0.2070944607257843\n",
      "training: 58 batch 102 batch_loss: 0.20523583889007568\n",
      "training: 58 batch 103 batch_loss: 0.1985119879245758\n",
      "training: 58 batch 104 batch_loss: 0.1985148787498474\n",
      "training: 58 batch 105 batch_loss: 0.19869950413703918\n",
      "training: 58 batch 106 batch_loss: 0.19476354122161865\n",
      "training: 58 batch 107 batch_loss: 0.20046871900558472\n",
      "training: 58 batch 108 batch_loss: 0.20247429609298706\n",
      "training: 58 batch 109 batch_loss: 0.2030022144317627\n",
      "training: 58 batch 110 batch_loss: 0.19754719734191895\n",
      "training: 58 batch 111 batch_loss: 0.20038717985153198\n",
      "training: 58 batch 112 batch_loss: 0.19875434041023254\n",
      "training: 58 batch 113 batch_loss: 0.19950181245803833\n",
      "training: 58 batch 114 batch_loss: 0.20275390148162842\n",
      "training: 58 batch 115 batch_loss: 0.1995968520641327\n",
      "training: 58 batch 116 batch_loss: 0.20282799005508423\n",
      "training: 58 batch 117 batch_loss: 0.20211145281791687\n",
      "training: 58 batch 118 batch_loss: 0.2006772756576538\n",
      "training: 58 batch 119 batch_loss: 0.2040601670742035\n",
      "training: 58 batch 120 batch_loss: 0.1978459358215332\n",
      "training: 58 batch 121 batch_loss: 0.20075157284736633\n",
      "training: 58 batch 122 batch_loss: 0.20223969221115112\n",
      "training: 58 batch 123 batch_loss: 0.20065906643867493\n",
      "training: 58 batch 124 batch_loss: 0.1997009515762329\n",
      "training: 58 batch 125 batch_loss: 0.1982017159461975\n",
      "training: 58 batch 126 batch_loss: 0.19943970441818237\n",
      "training: 58 batch 127 batch_loss: 0.20144420862197876\n",
      "training: 58 batch 128 batch_loss: 0.201324462890625\n",
      "training: 58 batch 129 batch_loss: 0.204535573720932\n",
      "training: 58 batch 130 batch_loss: 0.2016792595386505\n",
      "training: 58 batch 131 batch_loss: 0.19768178462982178\n",
      "training: 58 batch 132 batch_loss: 0.19742143154144287\n",
      "training: 58 batch 133 batch_loss: 0.20501628518104553\n",
      "training: 58 batch 134 batch_loss: 0.2005663812160492\n",
      "training: 58 batch 135 batch_loss: 0.2018190324306488\n",
      "training: 58 batch 136 batch_loss: 0.20061975717544556\n",
      "training: 58 batch 137 batch_loss: 0.20003855228424072\n",
      "training: 58 batch 138 batch_loss: 0.20205157995224\n",
      "training: 58 batch 139 batch_loss: 0.19994232058525085\n",
      "training: 58 batch 140 batch_loss: 0.20074790716171265\n",
      "training: 58 batch 141 batch_loss: 0.19793495535850525\n",
      "training: 58 batch 142 batch_loss: 0.20213022828102112\n",
      "training: 58 batch 143 batch_loss: 0.2012033462524414\n",
      "training: 58 batch 144 batch_loss: 0.2040589451789856\n",
      "training: 58 batch 145 batch_loss: 0.20077675580978394\n",
      "training: 58 batch 146 batch_loss: 0.1986733078956604\n",
      "training: 58 batch 147 batch_loss: 0.1994011402130127\n",
      "training: 58 batch 148 batch_loss: 0.20077401399612427\n",
      "training: 58 batch 149 batch_loss: 0.20314744114875793\n",
      "training: 58 batch 150 batch_loss: 0.19998854398727417\n",
      "training: 58 batch 151 batch_loss: 0.20146510004997253\n",
      "training: 58 batch 152 batch_loss: 0.20016980171203613\n",
      "training: 58 batch 153 batch_loss: 0.19964653253555298\n",
      "training: 58 batch 154 batch_loss: 0.19955813884735107\n",
      "training: 58 batch 155 batch_loss: 0.19781869649887085\n",
      "training: 58 batch 156 batch_loss: 0.19882279634475708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 58 batch 157 batch_loss: 0.20122608542442322\n",
      "training: 58 batch 158 batch_loss: 0.20332494378089905\n",
      "training: 58 batch 159 batch_loss: 0.20066893100738525\n",
      "training: 58 batch 160 batch_loss: 0.20131567120552063\n",
      "training: 58 batch 161 batch_loss: 0.19915413856506348\n",
      "training: 58 batch 162 batch_loss: 0.20038571953773499\n",
      "training: 58 batch 163 batch_loss: 0.205161452293396\n",
      "training: 58 batch 164 batch_loss: 0.1977357566356659\n",
      "training: 58 batch 165 batch_loss: 0.19949749112129211\n",
      "training: 58 batch 166 batch_loss: 0.1987164318561554\n",
      "training: 58 batch 167 batch_loss: 0.20234566926956177\n",
      "training: 58 batch 168 batch_loss: 0.20125573873519897\n",
      "training: 58 batch 169 batch_loss: 0.20165252685546875\n",
      "training: 58 batch 170 batch_loss: 0.20028150081634521\n",
      "training: 58 batch 171 batch_loss: 0.2014053463935852\n",
      "training: 58 batch 172 batch_loss: 0.1982216238975525\n",
      "training: 58 batch 173 batch_loss: 0.20088434219360352\n",
      "training: 58 batch 174 batch_loss: 0.1988913118839264\n",
      "training: 58 batch 175 batch_loss: 0.20159614086151123\n",
      "training: 58 batch 176 batch_loss: 0.20439493656158447\n",
      "training: 58 batch 177 batch_loss: 0.2011961042881012\n",
      "training: 58 batch 178 batch_loss: 0.20303881168365479\n",
      "training: 58 batch 179 batch_loss: 0.20133262872695923\n",
      "training: 58 batch 180 batch_loss: 0.2036873698234558\n",
      "training: 58 batch 181 batch_loss: 0.19351714849472046\n",
      "training: 58 batch 182 batch_loss: 0.20027375221252441\n",
      "training: 58 batch 183 batch_loss: 0.19943362474441528\n",
      "training: 58 batch 184 batch_loss: 0.20088613033294678\n",
      "training: 58 batch 185 batch_loss: 0.1994166076183319\n",
      "training: 58 batch 186 batch_loss: 0.1993318796157837\n",
      "training: 58 batch 187 batch_loss: 0.20238706469535828\n",
      "training: 58 batch 188 batch_loss: 0.20205062627792358\n",
      "training: 58 batch 189 batch_loss: 0.20576995611190796\n",
      "training: 58 batch 190 batch_loss: 0.2023390531539917\n",
      "training: 58 batch 191 batch_loss: 0.2005932629108429\n",
      "training: 58 batch 192 batch_loss: 0.20277011394500732\n",
      "training: 58 batch 193 batch_loss: 0.2013818323612213\n",
      "training: 58 batch 194 batch_loss: 0.20370733737945557\n",
      "training: 58 batch 195 batch_loss: 0.20355603098869324\n",
      "training: 58 batch 196 batch_loss: 0.19867360591888428\n",
      "training: 58 batch 197 batch_loss: 0.20117777585983276\n",
      "training: 58 batch 198 batch_loss: 0.20065364241600037\n",
      "training: 58 batch 199 batch_loss: 0.20170816779136658\n",
      "training: 58 batch 200 batch_loss: 0.2003258764743805\n",
      "training: 58 batch 201 batch_loss: 0.20088455080986023\n",
      "training: 58 batch 202 batch_loss: 0.19726264476776123\n",
      "training: 58 batch 203 batch_loss: 0.20256149768829346\n",
      "training: 58 batch 204 batch_loss: 0.19852951169013977\n",
      "training: 58 batch 205 batch_loss: 0.1985306739807129\n",
      "training: 58 batch 206 batch_loss: 0.2037644386291504\n",
      "training: 58 batch 207 batch_loss: 0.19765344262123108\n",
      "training: 58 batch 208 batch_loss: 0.19742199778556824\n",
      "training: 58 batch 209 batch_loss: 0.20344150066375732\n",
      "training: 58 batch 210 batch_loss: 0.20126399397850037\n",
      "training: 58 batch 211 batch_loss: 0.20004704594612122\n",
      "training: 58 batch 212 batch_loss: 0.2014595866203308\n",
      "training: 58 batch 213 batch_loss: 0.1999620795249939\n",
      "training: 58 batch 214 batch_loss: 0.20097115635871887\n",
      "training: 58 batch 215 batch_loss: 0.19868367910385132\n",
      "training: 58 batch 216 batch_loss: 0.2033839225769043\n",
      "training: 58 batch 217 batch_loss: 0.20168688893318176\n",
      "training: 58 batch 218 batch_loss: 0.1968807578086853\n",
      "training: 58 batch 219 batch_loss: 0.20023411512374878\n",
      "training: 58 batch 220 batch_loss: 0.20032891631126404\n",
      "training: 58 batch 221 batch_loss: 0.1989332139492035\n",
      "training: 58 batch 222 batch_loss: 0.1994178295135498\n",
      "training: 58 batch 223 batch_loss: 0.2028159499168396\n",
      "training: 58 batch 224 batch_loss: 0.2014772891998291\n",
      "training: 58 batch 225 batch_loss: 0.19866502285003662\n",
      "training: 58 batch 226 batch_loss: 0.20093688368797302\n",
      "training: 58 batch 227 batch_loss: 0.2027495801448822\n",
      "training: 58 batch 228 batch_loss: 0.20248883962631226\n",
      "training: 58 batch 229 batch_loss: 0.20309168100357056\n",
      "training: 58 batch 230 batch_loss: 0.20170482993125916\n",
      "training: 58 batch 231 batch_loss: 0.2074016034603119\n",
      "training: 58 batch 232 batch_loss: 0.20355850458145142\n",
      "training: 58 batch 233 batch_loss: 0.2010018229484558\n",
      "training: 58 batch 234 batch_loss: 0.20416486263275146\n",
      "training: 58 batch 235 batch_loss: 0.20094940066337585\n",
      "training: 58 batch 236 batch_loss: 0.20496821403503418\n",
      "training: 58 batch 237 batch_loss: 0.2045736312866211\n",
      "training: 58 batch 238 batch_loss: 0.19938531517982483\n",
      "training: 58 batch 239 batch_loss: 0.20294234156608582\n",
      "training: 58 batch 240 batch_loss: 0.20340490341186523\n",
      "training: 58 batch 241 batch_loss: 0.19854950904846191\n",
      "training: 58 batch 242 batch_loss: 0.19832199811935425\n",
      "training: 58 batch 243 batch_loss: 0.2027205526828766\n",
      "training: 58 batch 244 batch_loss: 0.2027217149734497\n",
      "training: 58 batch 245 batch_loss: 0.20059549808502197\n",
      "training: 58 batch 246 batch_loss: 0.20371073484420776\n",
      "training: 58 batch 247 batch_loss: 0.20033594965934753\n",
      "training: 58 batch 248 batch_loss: 0.2052740454673767\n",
      "training: 58 batch 249 batch_loss: 0.20033875107765198\n",
      "training: 58 batch 250 batch_loss: 0.19800898432731628\n",
      "training: 58 batch 251 batch_loss: 0.19710978865623474\n",
      "training: 58 batch 252 batch_loss: 0.20018219947814941\n",
      "training: 58 batch 253 batch_loss: 0.19904905557632446\n",
      "training: 58 batch 254 batch_loss: 0.20195543766021729\n",
      "training: 58 batch 255 batch_loss: 0.20050159096717834\n",
      "training: 58 batch 256 batch_loss: 0.19985994696617126\n",
      "training: 58 batch 257 batch_loss: 0.20312422513961792\n",
      "training: 58 batch 258 batch_loss: 0.1979508101940155\n",
      "training: 58 batch 259 batch_loss: 0.20479518175125122\n",
      "training: 58 batch 260 batch_loss: 0.20452383160591125\n",
      "training: 58 batch 261 batch_loss: 0.20240220427513123\n",
      "training: 58 batch 262 batch_loss: 0.20071938633918762\n",
      "training: 58 batch 263 batch_loss: 0.201846182346344\n",
      "training: 58 batch 264 batch_loss: 0.20094990730285645\n",
      "training: 58 batch 265 batch_loss: 0.20159852504730225\n",
      "training: 58 batch 266 batch_loss: 0.2005859911441803\n",
      "training: 58 batch 267 batch_loss: 0.20345348119735718\n",
      "training: 58 batch 268 batch_loss: 0.19873124361038208\n",
      "training: 58 batch 269 batch_loss: 0.20010286569595337\n",
      "training: 58 batch 270 batch_loss: 0.20027554035186768\n",
      "training: 58 batch 271 batch_loss: 0.20434880256652832\n",
      "training: 58 batch 272 batch_loss: 0.20607712864875793\n",
      "training: 58 batch 273 batch_loss: 0.20125365257263184\n",
      "training: 58 batch 274 batch_loss: 0.20313557982444763\n",
      "training: 58 batch 275 batch_loss: 0.20005545020103455\n",
      "training: 58 batch 276 batch_loss: 0.20297983288764954\n",
      "training: 58 batch 277 batch_loss: 0.20375072956085205\n",
      "training: 58 batch 278 batch_loss: 0.20408004522323608\n",
      "training: 58 batch 279 batch_loss: 0.20075690746307373\n",
      "training: 58 batch 280 batch_loss: 0.19712713360786438\n",
      "training: 58 batch 281 batch_loss: 0.20108237862586975\n",
      "training: 58 batch 282 batch_loss: 0.2006632685661316\n",
      "training: 58 batch 283 batch_loss: 0.2028965950012207\n",
      "training: 58 batch 284 batch_loss: 0.20442387461662292\n",
      "training: 58 batch 285 batch_loss: 0.20391374826431274\n",
      "training: 58 batch 286 batch_loss: 0.19898778200149536\n",
      "training: 58 batch 287 batch_loss: 0.20625784993171692\n",
      "training: 58 batch 288 batch_loss: 0.20042431354522705\n",
      "training: 58 batch 289 batch_loss: 0.20535483956336975\n",
      "training: 58 batch 290 batch_loss: 0.204454243183136\n",
      "training: 58 batch 291 batch_loss: 0.20237255096435547\n",
      "training: 58 batch 292 batch_loss: 0.2006189227104187\n",
      "training: 58 batch 293 batch_loss: 0.20363616943359375\n",
      "training: 58 batch 294 batch_loss: 0.19835567474365234\n",
      "training: 58 batch 295 batch_loss: 0.20161676406860352\n",
      "training: 58 batch 296 batch_loss: 0.19835659861564636\n",
      "training: 58 batch 297 batch_loss: 0.2012685239315033\n",
      "training: 58 batch 298 batch_loss: 0.20221459865570068\n",
      "training: 58 batch 299 batch_loss: 0.19937852025032043\n",
      "training: 58 batch 300 batch_loss: 0.20311716198921204\n",
      "training: 58 batch 301 batch_loss: 0.20097503066062927\n",
      "training: 58 batch 302 batch_loss: 0.19991010427474976\n",
      "training: 58 batch 303 batch_loss: 0.20513254404067993\n",
      "training: 58 batch 304 batch_loss: 0.2031489908695221\n",
      "training: 58 batch 305 batch_loss: 0.20292222499847412\n",
      "training: 58 batch 306 batch_loss: 0.20228785276412964\n",
      "training: 58 batch 307 batch_loss: 0.2024899125099182\n",
      "training: 58 batch 308 batch_loss: 0.20107337832450867\n",
      "training: 58 batch 309 batch_loss: 0.20242053270339966\n",
      "training: 58 batch 310 batch_loss: 0.20365676283836365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 58 batch 311 batch_loss: 0.2022920548915863\n",
      "training: 58 batch 312 batch_loss: 0.19983986020088196\n",
      "training: 58 batch 313 batch_loss: 0.20449495315551758\n",
      "training: 58 batch 314 batch_loss: 0.20098161697387695\n",
      "training: 58 batch 315 batch_loss: 0.2018330991268158\n",
      "training: 58 batch 316 batch_loss: 0.19963747262954712\n",
      "training: 58 batch 317 batch_loss: 0.19996902346611023\n",
      "training: 58 batch 318 batch_loss: 0.20612770318984985\n",
      "training: 58 batch 319 batch_loss: 0.20339995622634888\n",
      "training: 58 batch 320 batch_loss: 0.2040085792541504\n",
      "training: 58 batch 321 batch_loss: 0.20245829224586487\n",
      "training: 58 batch 322 batch_loss: 0.197696715593338\n",
      "training: 58 batch 323 batch_loss: 0.2038896083831787\n",
      "training: 58 batch 324 batch_loss: 0.198980450630188\n",
      "training: 58 batch 325 batch_loss: 0.20620843768119812\n",
      "training: 58 batch 326 batch_loss: 0.20190289616584778\n",
      "training: 58 batch 327 batch_loss: 0.2004801630973816\n",
      "training: 58 batch 328 batch_loss: 0.20515164732933044\n",
      "training: 58 batch 329 batch_loss: 0.20120763778686523\n",
      "training: 58 batch 330 batch_loss: 0.20093226432800293\n",
      "training: 58 batch 331 batch_loss: 0.20160862803459167\n",
      "training: 58 batch 332 batch_loss: 0.2030249834060669\n",
      "training: 58 batch 333 batch_loss: 0.20101869106292725\n",
      "training: 58 batch 334 batch_loss: 0.20300671458244324\n",
      "training: 58 batch 335 batch_loss: 0.20487889647483826\n",
      "training: 58 batch 336 batch_loss: 0.19387021660804749\n",
      "training: 58 batch 337 batch_loss: 0.20128348469734192\n",
      "training: 58 batch 338 batch_loss: 0.20042791962623596\n",
      "training: 58 batch 339 batch_loss: 0.20352452993392944\n",
      "training: 58 batch 340 batch_loss: 0.20180150866508484\n",
      "training: 58 batch 341 batch_loss: 0.20387911796569824\n",
      "training: 58 batch 342 batch_loss: 0.20538443326950073\n",
      "training: 58 batch 343 batch_loss: 0.1996738314628601\n",
      "training: 58 batch 344 batch_loss: 0.19990882277488708\n",
      "training: 58 batch 345 batch_loss: 0.1964712142944336\n",
      "training: 58 batch 346 batch_loss: 0.20365333557128906\n",
      "training: 58 batch 347 batch_loss: 0.20459404587745667\n",
      "training: 58 batch 348 batch_loss: 0.20196056365966797\n",
      "training: 58 batch 349 batch_loss: 0.20269596576690674\n",
      "training: 58 batch 350 batch_loss: 0.2035180628299713\n",
      "training: 58 batch 351 batch_loss: 0.19850558042526245\n",
      "training: 58 batch 352 batch_loss: 0.20446354150772095\n",
      "training: 58 batch 353 batch_loss: 0.20437875390052795\n",
      "training: 58 batch 354 batch_loss: 0.19982340931892395\n",
      "training: 58 batch 355 batch_loss: 0.20770400762557983\n",
      "training: 58 batch 356 batch_loss: 0.20337620377540588\n",
      "training: 58 batch 357 batch_loss: 0.20419567823410034\n",
      "training: 58 batch 358 batch_loss: 0.1985812783241272\n",
      "training: 58 batch 359 batch_loss: 0.1995256245136261\n",
      "training: 58 batch 360 batch_loss: 0.20005404949188232\n",
      "training: 58 batch 361 batch_loss: 0.20322781801223755\n",
      "training: 58 batch 362 batch_loss: 0.19910961389541626\n",
      "training: 58 batch 363 batch_loss: 0.19896093010902405\n",
      "training: 58 batch 364 batch_loss: 0.20262759923934937\n",
      "training: 58 batch 365 batch_loss: 0.20451372861862183\n",
      "training: 58 batch 366 batch_loss: 0.20431408286094666\n",
      "training: 58 batch 367 batch_loss: 0.20507851243019104\n",
      "training: 58 batch 368 batch_loss: 0.19731274247169495\n",
      "training: 58 batch 369 batch_loss: 0.1996113657951355\n",
      "training: 58 batch 370 batch_loss: 0.20129749178886414\n",
      "training: 58 batch 371 batch_loss: 0.20106685161590576\n",
      "training: 58 batch 372 batch_loss: 0.2054716944694519\n",
      "training: 58 batch 373 batch_loss: 0.20455074310302734\n",
      "training: 58 batch 374 batch_loss: 0.2061161994934082\n",
      "training: 58 batch 375 batch_loss: 0.2005000114440918\n",
      "training: 58 batch 376 batch_loss: 0.19658634066581726\n",
      "training: 58 batch 377 batch_loss: 0.2019473910331726\n",
      "training: 58 batch 378 batch_loss: 0.20430347323417664\n",
      "training: 58 batch 379 batch_loss: 0.2051885724067688\n",
      "training: 58 batch 380 batch_loss: 0.20099800825119019\n",
      "training: 58 batch 381 batch_loss: 0.2043866217136383\n",
      "training: 58 batch 382 batch_loss: 0.20225512981414795\n",
      "training: 58 batch 383 batch_loss: 0.20181336998939514\n",
      "training: 58 batch 384 batch_loss: 0.20230033993721008\n",
      "training: 58 batch 385 batch_loss: 0.20124602317810059\n",
      "training: 58 batch 386 batch_loss: 0.19779056310653687\n",
      "training: 58 batch 387 batch_loss: 0.2038026750087738\n",
      "training: 58 batch 388 batch_loss: 0.20518481731414795\n",
      "training: 58 batch 389 batch_loss: 0.20109272003173828\n",
      "training: 58 batch 390 batch_loss: 0.201196551322937\n",
      "training: 58 batch 391 batch_loss: 0.19996288418769836\n",
      "training: 58 batch 392 batch_loss: 0.20411795377731323\n",
      "training: 58 batch 393 batch_loss: 0.2047918736934662\n",
      "training: 58 batch 394 batch_loss: 0.19771674275398254\n",
      "training: 58 batch 395 batch_loss: 0.2033759355545044\n",
      "training: 58 batch 396 batch_loss: 0.20217585563659668\n",
      "training: 58 batch 397 batch_loss: 0.20054450631141663\n",
      "training: 58 batch 398 batch_loss: 0.20321297645568848\n",
      "training: 58 batch 399 batch_loss: 0.19817271828651428\n",
      "training: 58 batch 400 batch_loss: 0.20499831438064575\n",
      "training: 58 batch 401 batch_loss: 0.20112234354019165\n",
      "training: 58 batch 402 batch_loss: 0.2034178078174591\n",
      "training: 58 batch 403 batch_loss: 0.20262762904167175\n",
      "training: 58 batch 404 batch_loss: 0.20447075366973877\n",
      "training: 58 batch 405 batch_loss: 0.19835707545280457\n",
      "training: 58 batch 406 batch_loss: 0.19872567057609558\n",
      "training: 58 batch 407 batch_loss: 0.19842302799224854\n",
      "training: 58 batch 408 batch_loss: 0.20368951559066772\n",
      "training: 58 batch 409 batch_loss: 0.20221996307373047\n",
      "training: 58 batch 410 batch_loss: 0.20011025667190552\n",
      "training: 58 batch 411 batch_loss: 0.20153877139091492\n",
      "training: 58 batch 412 batch_loss: 0.19893893599510193\n",
      "training: 58 batch 413 batch_loss: 0.20337191224098206\n",
      "training: 58 batch 414 batch_loss: 0.2022809386253357\n",
      "training: 58 batch 415 batch_loss: 0.19979554414749146\n",
      "training: 58 batch 416 batch_loss: 0.20115572214126587\n",
      "training: 58 batch 417 batch_loss: 0.2020050585269928\n",
      "training: 58 batch 418 batch_loss: 0.2029920220375061\n",
      "training: 58 batch 419 batch_loss: 0.19985812902450562\n",
      "training: 58 batch 420 batch_loss: 0.2043129801750183\n",
      "training: 58 batch 421 batch_loss: 0.19840729236602783\n",
      "training: 58 batch 422 batch_loss: 0.2029818892478943\n",
      "training: 58 batch 423 batch_loss: 0.2027750015258789\n",
      "training: 58 batch 424 batch_loss: 0.20321640372276306\n",
      "training: 58 batch 425 batch_loss: 0.20436355471611023\n",
      "training: 58 batch 426 batch_loss: 0.1981373131275177\n",
      "training: 58 batch 427 batch_loss: 0.2000296413898468\n",
      "training: 58 batch 428 batch_loss: 0.20089074969291687\n",
      "training: 58 batch 429 batch_loss: 0.20285376906394958\n",
      "training: 58 batch 430 batch_loss: 0.20453262329101562\n",
      "training: 58 batch 431 batch_loss: 0.1987244188785553\n",
      "training: 58 batch 432 batch_loss: 0.20137852430343628\n",
      "training: 58 batch 433 batch_loss: 0.20487046241760254\n",
      "training: 58 batch 434 batch_loss: 0.20146256685256958\n",
      "training: 58 batch 435 batch_loss: 0.2034701406955719\n",
      "training: 58 batch 436 batch_loss: 0.20446878671646118\n",
      "training: 58 batch 437 batch_loss: 0.2056487798690796\n",
      "training: 58 batch 438 batch_loss: 0.20090174674987793\n",
      "training: 58 batch 439 batch_loss: 0.20502972602844238\n",
      "training: 58 batch 440 batch_loss: 0.20482218265533447\n",
      "training: 58 batch 441 batch_loss: 0.2032422423362732\n",
      "training: 58 batch 442 batch_loss: 0.2049759328365326\n",
      "training: 58 batch 443 batch_loss: 0.19922810792922974\n",
      "training: 58 batch 444 batch_loss: 0.20744869112968445\n",
      "training: 58 batch 445 batch_loss: 0.19915437698364258\n",
      "training: 58 batch 446 batch_loss: 0.19912084937095642\n",
      "training: 58 batch 447 batch_loss: 0.20207664370536804\n",
      "training: 58 batch 448 batch_loss: 0.20333445072174072\n",
      "training: 58 batch 449 batch_loss: 0.20672178268432617\n",
      "training: 58 batch 450 batch_loss: 0.2022349238395691\n",
      "training: 58 batch 451 batch_loss: 0.20370513200759888\n",
      "training: 58 batch 452 batch_loss: 0.2032746970653534\n",
      "training: 58 batch 453 batch_loss: 0.20234960317611694\n",
      "training: 58 batch 454 batch_loss: 0.2023625671863556\n",
      "training: 58 batch 455 batch_loss: 0.20370984077453613\n",
      "training: 58 batch 456 batch_loss: 0.2000720202922821\n",
      "training: 58 batch 457 batch_loss: 0.2056179940700531\n",
      "training: 58 batch 458 batch_loss: 0.2061736285686493\n",
      "training: 58 batch 459 batch_loss: 0.19999787211418152\n",
      "training: 58 batch 460 batch_loss: 0.20171308517456055\n",
      "training: 58 batch 461 batch_loss: 0.2003766894340515\n",
      "training: 58 batch 462 batch_loss: 0.20477768778800964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 58 batch 463 batch_loss: 0.19834628701210022\n",
      "training: 58 batch 464 batch_loss: 0.20377105474472046\n",
      "training: 58 batch 465 batch_loss: 0.20285052061080933\n",
      "training: 58 batch 466 batch_loss: 0.20677915215492249\n",
      "training: 58 batch 467 batch_loss: 0.20343142747879028\n",
      "training: 58 batch 468 batch_loss: 0.20502439141273499\n",
      "training: 58 batch 469 batch_loss: 0.20561489462852478\n",
      "training: 58 batch 470 batch_loss: 0.20130425691604614\n",
      "training: 58 batch 471 batch_loss: 0.206565260887146\n",
      "training: 58 batch 472 batch_loss: 0.20392340421676636\n",
      "training: 58 batch 473 batch_loss: 0.2008681297302246\n",
      "training: 58 batch 474 batch_loss: 0.20345818996429443\n",
      "training: 58 batch 475 batch_loss: 0.2024317979812622\n",
      "training: 58 batch 476 batch_loss: 0.20066171884536743\n",
      "training: 58 batch 477 batch_loss: 0.20049667358398438\n",
      "training: 58 batch 478 batch_loss: 0.20321780443191528\n",
      "training: 58 batch 479 batch_loss: 0.20118173956871033\n",
      "training: 58 batch 480 batch_loss: 0.20049354434013367\n",
      "training: 58 batch 481 batch_loss: 0.20152178406715393\n",
      "training: 58 batch 482 batch_loss: 0.20444762706756592\n",
      "training: 58 batch 483 batch_loss: 0.20022782683372498\n",
      "training: 58 batch 484 batch_loss: 0.20520150661468506\n",
      "training: 58 batch 485 batch_loss: 0.20512107014656067\n",
      "training: 58 batch 486 batch_loss: 0.20920294523239136\n",
      "training: 58 batch 487 batch_loss: 0.2032293975353241\n",
      "training: 58 batch 488 batch_loss: 0.2012440264225006\n",
      "training: 58 batch 489 batch_loss: 0.20023617148399353\n",
      "training: 58 batch 490 batch_loss: 0.2030537724494934\n",
      "training: 58 batch 491 batch_loss: 0.20365822315216064\n",
      "training: 58 batch 492 batch_loss: 0.20261499285697937\n",
      "training: 58 batch 493 batch_loss: 0.20385533571243286\n",
      "training: 58 batch 494 batch_loss: 0.20263630151748657\n",
      "training: 58 batch 495 batch_loss: 0.2040189802646637\n",
      "training: 58 batch 496 batch_loss: 0.20174115896224976\n",
      "training: 58 batch 497 batch_loss: 0.19655221700668335\n",
      "training: 58 batch 498 batch_loss: 0.20323941111564636\n",
      "training: 58 batch 499 batch_loss: 0.2018117904663086\n",
      "training: 58 batch 500 batch_loss: 0.20274540781974792\n",
      "training: 58 batch 501 batch_loss: 0.20434385538101196\n",
      "training: 58 batch 502 batch_loss: 0.20056527853012085\n",
      "training: 58 batch 503 batch_loss: 0.2034240961074829\n",
      "training: 58 batch 504 batch_loss: 0.20210528373718262\n",
      "training: 58 batch 505 batch_loss: 0.2020266056060791\n",
      "training: 58 batch 506 batch_loss: 0.2051381766796112\n",
      "training: 58 batch 507 batch_loss: 0.20111948251724243\n",
      "training: 58 batch 508 batch_loss: 0.2020435929298401\n",
      "training: 58 batch 509 batch_loss: 0.2020782232284546\n",
      "training: 58 batch 510 batch_loss: 0.20413246750831604\n",
      "training: 58 batch 511 batch_loss: 0.2011089324951172\n",
      "training: 58 batch 512 batch_loss: 0.20137420296669006\n",
      "training: 58 batch 513 batch_loss: 0.20301777124404907\n",
      "training: 58 batch 514 batch_loss: 0.19988957047462463\n",
      "training: 58 batch 515 batch_loss: 0.20372682809829712\n",
      "training: 58 batch 516 batch_loss: 0.20036175847053528\n",
      "training: 58 batch 517 batch_loss: 0.2017698585987091\n",
      "training: 58 batch 518 batch_loss: 0.20322489738464355\n",
      "training: 58 batch 519 batch_loss: 0.20270228385925293\n",
      "training: 58 batch 520 batch_loss: 0.1997111439704895\n",
      "training: 58 batch 521 batch_loss: 0.20481222867965698\n",
      "training: 58 batch 522 batch_loss: 0.20434969663619995\n",
      "training: 58 batch 523 batch_loss: 0.20229527354240417\n",
      "training: 58 batch 524 batch_loss: 0.20241019129753113\n",
      "training: 58 batch 525 batch_loss: 0.20343026518821716\n",
      "training: 58 batch 526 batch_loss: 0.2030235230922699\n",
      "training: 58 batch 527 batch_loss: 0.19993099570274353\n",
      "training: 58 batch 528 batch_loss: 0.20647916197776794\n",
      "training: 58 batch 529 batch_loss: 0.20445355772972107\n",
      "training: 58 batch 530 batch_loss: 0.1998491883277893\n",
      "training: 58 batch 531 batch_loss: 0.2052401304244995\n",
      "training: 58 batch 532 batch_loss: 0.19887495040893555\n",
      "training: 58 batch 533 batch_loss: 0.20029008388519287\n",
      "training: 58 batch 534 batch_loss: 0.2070455551147461\n",
      "training: 58 batch 535 batch_loss: 0.20444899797439575\n",
      "training: 58 batch 536 batch_loss: 0.20309990644454956\n",
      "training: 58 batch 537 batch_loss: 0.20261424779891968\n",
      "training: 58 batch 538 batch_loss: 0.20602649450302124\n",
      "training: 58 batch 539 batch_loss: 0.19732347130775452\n",
      "training: 58 batch 540 batch_loss: 0.19860628247261047\n",
      "training: 58 batch 541 batch_loss: 0.20604977011680603\n",
      "training: 58 batch 542 batch_loss: 0.20511770248413086\n",
      "training: 58 batch 543 batch_loss: 0.2029086947441101\n",
      "training: 58 batch 544 batch_loss: 0.203718900680542\n",
      "training: 58 batch 545 batch_loss: 0.20258557796478271\n",
      "training: 58 batch 546 batch_loss: 0.20377585291862488\n",
      "training: 58 batch 547 batch_loss: 0.20375779271125793\n",
      "training: 58 batch 548 batch_loss: 0.20467302203178406\n",
      "training: 58 batch 549 batch_loss: 0.20137673616409302\n",
      "training: 58 batch 550 batch_loss: 0.20309913158416748\n",
      "training: 58 batch 551 batch_loss: 0.20171231031417847\n",
      "training: 58 batch 552 batch_loss: 0.20101937651634216\n",
      "training: 58 batch 553 batch_loss: 0.1964741349220276\n",
      "training: 58 batch 554 batch_loss: 0.20194122195243835\n",
      "training: 58 batch 555 batch_loss: 0.20620658993721008\n",
      "training: 58 batch 556 batch_loss: 0.19623303413391113\n",
      "training: 58 batch 557 batch_loss: 0.20285671949386597\n",
      "training: 58 batch 558 batch_loss: 0.20714068412780762\n",
      "training: 58 batch 559 batch_loss: 0.20051896572113037\n",
      "training: 58 batch 560 batch_loss: 0.20420032739639282\n",
      "training: 58 batch 561 batch_loss: 0.20416045188903809\n",
      "training: 58 batch 562 batch_loss: 0.20725178718566895\n",
      "training: 58 batch 563 batch_loss: 0.20435214042663574\n",
      "training: 58 batch 564 batch_loss: 0.2002629041671753\n",
      "training: 58 batch 565 batch_loss: 0.2042805552482605\n",
      "training: 58 batch 566 batch_loss: 0.203957200050354\n",
      "training: 58 batch 567 batch_loss: 0.20270034670829773\n",
      "training: 58 batch 568 batch_loss: 0.20082420110702515\n",
      "training: 58 batch 569 batch_loss: 0.20144647359848022\n",
      "training: 58 batch 570 batch_loss: 0.2086276412010193\n",
      "training: 58 batch 571 batch_loss: 0.2028581202030182\n",
      "training: 58 batch 572 batch_loss: 0.2022218108177185\n",
      "training: 58 batch 573 batch_loss: 0.20384222269058228\n",
      "training: 58 batch 574 batch_loss: 0.20351719856262207\n",
      "training: 58 batch 575 batch_loss: 0.20483681559562683\n",
      "training: 58 batch 576 batch_loss: 0.20338785648345947\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 58, Hit Ratio:0.03280556175527077 | Precision:0.04840263442445689 | Recall:0.06423183021273221 | NDCG:0.06268311255360136\n",
      "*Best Performance* \n",
      "Epoch: 56, Hit Ratio:0.032922153414636585 | Precision:0.04857465840951539 | Recall:0.06427363564028983 | MDCG:0.06308737979754037\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 59 batch 0 batch_loss: 0.2007969319820404\n",
      "training: 59 batch 1 batch_loss: 0.20104649662971497\n",
      "training: 59 batch 2 batch_loss: 0.2010411024093628\n",
      "training: 59 batch 3 batch_loss: 0.19593960046768188\n",
      "training: 59 batch 4 batch_loss: 0.19869980216026306\n",
      "training: 59 batch 5 batch_loss: 0.20076367259025574\n",
      "training: 59 batch 6 batch_loss: 0.201291024684906\n",
      "training: 59 batch 7 batch_loss: 0.20314258337020874\n",
      "training: 59 batch 8 batch_loss: 0.2029452919960022\n",
      "training: 59 batch 9 batch_loss: 0.19786235690116882\n",
      "training: 59 batch 10 batch_loss: 0.20086950063705444\n",
      "training: 59 batch 11 batch_loss: 0.20326954126358032\n",
      "training: 59 batch 12 batch_loss: 0.20171988010406494\n",
      "training: 59 batch 13 batch_loss: 0.20042681694030762\n",
      "training: 59 batch 14 batch_loss: 0.19702911376953125\n",
      "training: 59 batch 15 batch_loss: 0.1984427571296692\n",
      "training: 59 batch 16 batch_loss: 0.2023734450340271\n",
      "training: 59 batch 17 batch_loss: 0.20060396194458008\n",
      "training: 59 batch 18 batch_loss: 0.20509418845176697\n",
      "training: 59 batch 19 batch_loss: 0.20334625244140625\n",
      "training: 59 batch 20 batch_loss: 0.20203575491905212\n",
      "training: 59 batch 21 batch_loss: 0.20020681619644165\n",
      "training: 59 batch 22 batch_loss: 0.19889095425605774\n",
      "training: 59 batch 23 batch_loss: 0.20047923922538757\n",
      "training: 59 batch 24 batch_loss: 0.20414799451828003\n",
      "training: 59 batch 25 batch_loss: 0.20349037647247314\n",
      "training: 59 batch 26 batch_loss: 0.19893485307693481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 59 batch 27 batch_loss: 0.19721606373786926\n",
      "training: 59 batch 28 batch_loss: 0.2044457197189331\n",
      "training: 59 batch 29 batch_loss: 0.20277908444404602\n",
      "training: 59 batch 30 batch_loss: 0.20160126686096191\n",
      "training: 59 batch 31 batch_loss: 0.19947084784507751\n",
      "training: 59 batch 32 batch_loss: 0.1974705457687378\n",
      "training: 59 batch 33 batch_loss: 0.201825350522995\n",
      "training: 59 batch 34 batch_loss: 0.2040102779865265\n",
      "training: 59 batch 35 batch_loss: 0.19876715540885925\n",
      "training: 59 batch 36 batch_loss: 0.20105105638504028\n",
      "training: 59 batch 37 batch_loss: 0.20626050233840942\n",
      "training: 59 batch 38 batch_loss: 0.2041475772857666\n",
      "training: 59 batch 39 batch_loss: 0.20350322127342224\n",
      "training: 59 batch 40 batch_loss: 0.19963514804840088\n",
      "training: 59 batch 41 batch_loss: 0.2002583146095276\n",
      "training: 59 batch 42 batch_loss: 0.2003522515296936\n",
      "training: 59 batch 43 batch_loss: 0.20135033130645752\n",
      "training: 59 batch 44 batch_loss: 0.20424869656562805\n",
      "training: 59 batch 45 batch_loss: 0.2044411599636078\n",
      "training: 59 batch 46 batch_loss: 0.20175904035568237\n",
      "training: 59 batch 47 batch_loss: 0.19842493534088135\n",
      "training: 59 batch 48 batch_loss: 0.20486414432525635\n",
      "training: 59 batch 49 batch_loss: 0.19720908999443054\n",
      "training: 59 batch 50 batch_loss: 0.19824683666229248\n",
      "training: 59 batch 51 batch_loss: 0.2032812237739563\n",
      "training: 59 batch 52 batch_loss: 0.19702666997909546\n",
      "training: 59 batch 53 batch_loss: 0.20422786474227905\n",
      "training: 59 batch 54 batch_loss: 0.20077192783355713\n",
      "training: 59 batch 55 batch_loss: 0.20564883947372437\n",
      "training: 59 batch 56 batch_loss: 0.2018052637577057\n",
      "training: 59 batch 57 batch_loss: 0.2030174434185028\n",
      "training: 59 batch 58 batch_loss: 0.20125144720077515\n",
      "training: 59 batch 59 batch_loss: 0.1941872537136078\n",
      "training: 59 batch 60 batch_loss: 0.20059657096862793\n",
      "training: 59 batch 61 batch_loss: 0.1977916657924652\n",
      "training: 59 batch 62 batch_loss: 0.19917914271354675\n",
      "training: 59 batch 63 batch_loss: 0.20304903388023376\n",
      "training: 59 batch 64 batch_loss: 0.19960051774978638\n",
      "training: 59 batch 65 batch_loss: 0.19995421171188354\n",
      "training: 59 batch 66 batch_loss: 0.19977152347564697\n",
      "training: 59 batch 67 batch_loss: 0.20015031099319458\n",
      "training: 59 batch 68 batch_loss: 0.19984683394432068\n",
      "training: 59 batch 69 batch_loss: 0.20334139466285706\n",
      "training: 59 batch 70 batch_loss: 0.2054816484451294\n",
      "training: 59 batch 71 batch_loss: 0.20352357625961304\n",
      "training: 59 batch 72 batch_loss: 0.20170065760612488\n",
      "training: 59 batch 73 batch_loss: 0.20262330770492554\n",
      "training: 59 batch 74 batch_loss: 0.20586374402046204\n",
      "training: 59 batch 75 batch_loss: 0.20091593265533447\n",
      "training: 59 batch 76 batch_loss: 0.198958158493042\n",
      "training: 59 batch 77 batch_loss: 0.2051386833190918\n",
      "training: 59 batch 78 batch_loss: 0.20415714383125305\n",
      "training: 59 batch 79 batch_loss: 0.2014583945274353\n",
      "training: 59 batch 80 batch_loss: 0.20430448651313782\n",
      "training: 59 batch 81 batch_loss: 0.203811913728714\n",
      "training: 59 batch 82 batch_loss: 0.20114296674728394\n",
      "training: 59 batch 83 batch_loss: 0.20441487431526184\n",
      "training: 59 batch 84 batch_loss: 0.19872820377349854\n",
      "training: 59 batch 85 batch_loss: 0.20229971408843994\n",
      "training: 59 batch 86 batch_loss: 0.19870984554290771\n",
      "training: 59 batch 87 batch_loss: 0.20152786374092102\n",
      "training: 59 batch 88 batch_loss: 0.19829964637756348\n",
      "training: 59 batch 89 batch_loss: 0.20246675610542297\n",
      "training: 59 batch 90 batch_loss: 0.19993045926094055\n",
      "training: 59 batch 91 batch_loss: 0.2012280821800232\n",
      "training: 59 batch 92 batch_loss: 0.20357745885849\n",
      "training: 59 batch 93 batch_loss: 0.20587792992591858\n",
      "training: 59 batch 94 batch_loss: 0.20194914937019348\n",
      "training: 59 batch 95 batch_loss: 0.20408779382705688\n",
      "training: 59 batch 96 batch_loss: 0.20212653279304504\n",
      "training: 59 batch 97 batch_loss: 0.19685474038124084\n",
      "training: 59 batch 98 batch_loss: 0.20596250891685486\n",
      "training: 59 batch 99 batch_loss: 0.201003760099411\n",
      "training: 59 batch 100 batch_loss: 0.20099365711212158\n",
      "training: 59 batch 101 batch_loss: 0.2043563425540924\n",
      "training: 59 batch 102 batch_loss: 0.20564204454421997\n",
      "training: 59 batch 103 batch_loss: 0.20479243993759155\n",
      "training: 59 batch 104 batch_loss: 0.2023594081401825\n",
      "training: 59 batch 105 batch_loss: 0.1976604461669922\n",
      "training: 59 batch 106 batch_loss: 0.20080000162124634\n",
      "training: 59 batch 107 batch_loss: 0.20402467250823975\n",
      "training: 59 batch 108 batch_loss: 0.1977832317352295\n",
      "training: 59 batch 109 batch_loss: 0.1997135877609253\n",
      "training: 59 batch 110 batch_loss: 0.20226037502288818\n",
      "training: 59 batch 111 batch_loss: 0.20563524961471558\n",
      "training: 59 batch 112 batch_loss: 0.20247024297714233\n",
      "training: 59 batch 113 batch_loss: 0.19938045740127563\n",
      "training: 59 batch 114 batch_loss: 0.1985805630683899\n",
      "training: 59 batch 115 batch_loss: 0.20141303539276123\n",
      "training: 59 batch 116 batch_loss: 0.20303672552108765\n",
      "training: 59 batch 117 batch_loss: 0.20045670866966248\n",
      "training: 59 batch 118 batch_loss: 0.2026570737361908\n",
      "training: 59 batch 119 batch_loss: 0.19882279634475708\n",
      "training: 59 batch 120 batch_loss: 0.20620712637901306\n",
      "training: 59 batch 121 batch_loss: 0.20411527156829834\n",
      "training: 59 batch 122 batch_loss: 0.20423373579978943\n",
      "training: 59 batch 123 batch_loss: 0.20377174019813538\n",
      "training: 59 batch 124 batch_loss: 0.20224601030349731\n",
      "training: 59 batch 125 batch_loss: 0.20409676432609558\n",
      "training: 59 batch 126 batch_loss: 0.20039480924606323\n",
      "training: 59 batch 127 batch_loss: 0.1994728446006775\n",
      "training: 59 batch 128 batch_loss: 0.2008129060268402\n",
      "training: 59 batch 129 batch_loss: 0.2005249261856079\n",
      "training: 59 batch 130 batch_loss: 0.2004256248474121\n",
      "training: 59 batch 131 batch_loss: 0.20435753464698792\n",
      "training: 59 batch 132 batch_loss: 0.20299172401428223\n",
      "training: 59 batch 133 batch_loss: 0.20316389203071594\n",
      "training: 59 batch 134 batch_loss: 0.20194116234779358\n",
      "training: 59 batch 135 batch_loss: 0.19957447052001953\n",
      "training: 59 batch 136 batch_loss: 0.20525842905044556\n",
      "training: 59 batch 137 batch_loss: 0.20418086647987366\n",
      "training: 59 batch 138 batch_loss: 0.20192262530326843\n",
      "training: 59 batch 139 batch_loss: 0.19950473308563232\n",
      "training: 59 batch 140 batch_loss: 0.20945313572883606\n",
      "training: 59 batch 141 batch_loss: 0.2031838297843933\n",
      "training: 59 batch 142 batch_loss: 0.2013241946697235\n",
      "training: 59 batch 143 batch_loss: 0.20310676097869873\n",
      "training: 59 batch 144 batch_loss: 0.20124861598014832\n",
      "training: 59 batch 145 batch_loss: 0.20045986771583557\n",
      "training: 59 batch 146 batch_loss: 0.19960886240005493\n",
      "training: 59 batch 147 batch_loss: 0.19872775673866272\n",
      "training: 59 batch 148 batch_loss: 0.20147669315338135\n",
      "training: 59 batch 149 batch_loss: 0.20112067461013794\n",
      "training: 59 batch 150 batch_loss: 0.20191115140914917\n",
      "training: 59 batch 151 batch_loss: 0.20281100273132324\n",
      "training: 59 batch 152 batch_loss: 0.2068306803703308\n",
      "training: 59 batch 153 batch_loss: 0.20188847184181213\n",
      "training: 59 batch 154 batch_loss: 0.20419132709503174\n",
      "training: 59 batch 155 batch_loss: 0.20512226223945618\n",
      "training: 59 batch 156 batch_loss: 0.2030290961265564\n",
      "training: 59 batch 157 batch_loss: 0.20423626899719238\n",
      "training: 59 batch 158 batch_loss: 0.19685980677604675\n",
      "training: 59 batch 159 batch_loss: 0.198989599943161\n",
      "training: 59 batch 160 batch_loss: 0.19955697655677795\n",
      "training: 59 batch 161 batch_loss: 0.2008695900440216\n",
      "training: 59 batch 162 batch_loss: 0.20093697309494019\n",
      "training: 59 batch 163 batch_loss: 0.2018272578716278\n",
      "training: 59 batch 164 batch_loss: 0.20664143562316895\n",
      "training: 59 batch 165 batch_loss: 0.20315736532211304\n",
      "training: 59 batch 166 batch_loss: 0.20507019758224487\n",
      "training: 59 batch 167 batch_loss: 0.2015117108821869\n",
      "training: 59 batch 168 batch_loss: 0.2038879692554474\n",
      "training: 59 batch 169 batch_loss: 0.20634573698043823\n",
      "training: 59 batch 170 batch_loss: 0.2022801637649536\n",
      "training: 59 batch 171 batch_loss: 0.20383617281913757\n",
      "training: 59 batch 172 batch_loss: 0.20125746726989746\n",
      "training: 59 batch 173 batch_loss: 0.20676809549331665\n",
      "training: 59 batch 174 batch_loss: 0.20210358500480652\n",
      "training: 59 batch 175 batch_loss: 0.19652646780014038\n",
      "training: 59 batch 176 batch_loss: 0.20256930589675903\n",
      "training: 59 batch 177 batch_loss: 0.20233532786369324\n",
      "training: 59 batch 178 batch_loss: 0.2015416920185089\n",
      "training: 59 batch 179 batch_loss: 0.19702386856079102\n",
      "training: 59 batch 180 batch_loss: 0.20379936695098877\n",
      "training: 59 batch 181 batch_loss: 0.203904390335083\n",
      "training: 59 batch 182 batch_loss: 0.20025408267974854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 59 batch 183 batch_loss: 0.202725350856781\n",
      "training: 59 batch 184 batch_loss: 0.20567283034324646\n",
      "training: 59 batch 185 batch_loss: 0.20379281044006348\n",
      "training: 59 batch 186 batch_loss: 0.20046669244766235\n",
      "training: 59 batch 187 batch_loss: 0.20615556836128235\n",
      "training: 59 batch 188 batch_loss: 0.2010507881641388\n",
      "training: 59 batch 189 batch_loss: 0.19744768738746643\n",
      "training: 59 batch 190 batch_loss: 0.20488005876541138\n",
      "training: 59 batch 191 batch_loss: 0.1986006498336792\n",
      "training: 59 batch 192 batch_loss: 0.1991763710975647\n",
      "training: 59 batch 193 batch_loss: 0.20046323537826538\n",
      "training: 59 batch 194 batch_loss: 0.19879969954490662\n",
      "training: 59 batch 195 batch_loss: 0.20364731550216675\n",
      "training: 59 batch 196 batch_loss: 0.20200413465499878\n",
      "training: 59 batch 197 batch_loss: 0.20430698990821838\n",
      "training: 59 batch 198 batch_loss: 0.19782954454421997\n",
      "training: 59 batch 199 batch_loss: 0.20415762066841125\n",
      "training: 59 batch 200 batch_loss: 0.2051962912082672\n",
      "training: 59 batch 201 batch_loss: 0.2056765854358673\n",
      "training: 59 batch 202 batch_loss: 0.20541954040527344\n",
      "training: 59 batch 203 batch_loss: 0.20423543453216553\n",
      "training: 59 batch 204 batch_loss: 0.2064175009727478\n",
      "training: 59 batch 205 batch_loss: 0.20101234316825867\n",
      "training: 59 batch 206 batch_loss: 0.2038799524307251\n",
      "training: 59 batch 207 batch_loss: 0.20101410150527954\n",
      "training: 59 batch 208 batch_loss: 0.20049786567687988\n",
      "training: 59 batch 209 batch_loss: 0.20330911874771118\n",
      "training: 59 batch 210 batch_loss: 0.20548278093338013\n",
      "training: 59 batch 211 batch_loss: 0.19780710339546204\n",
      "training: 59 batch 212 batch_loss: 0.19944033026695251\n",
      "training: 59 batch 213 batch_loss: 0.20216941833496094\n",
      "training: 59 batch 214 batch_loss: 0.2012597620487213\n",
      "training: 59 batch 215 batch_loss: 0.20232069492340088\n",
      "training: 59 batch 216 batch_loss: 0.20678603649139404\n",
      "training: 59 batch 217 batch_loss: 0.19878342747688293\n",
      "training: 59 batch 218 batch_loss: 0.19944384694099426\n",
      "training: 59 batch 219 batch_loss: 0.20253795385360718\n",
      "training: 59 batch 220 batch_loss: 0.20242872834205627\n",
      "training: 59 batch 221 batch_loss: 0.20295268297195435\n",
      "training: 59 batch 222 batch_loss: 0.2024816870689392\n",
      "training: 59 batch 223 batch_loss: 0.20626896619796753\n",
      "training: 59 batch 224 batch_loss: 0.20539793372154236\n",
      "training: 59 batch 225 batch_loss: 0.20262762904167175\n",
      "training: 59 batch 226 batch_loss: 0.2025211751461029\n",
      "training: 59 batch 227 batch_loss: 0.20611217617988586\n",
      "training: 59 batch 228 batch_loss: 0.20291167497634888\n",
      "training: 59 batch 229 batch_loss: 0.19886356592178345\n",
      "training: 59 batch 230 batch_loss: 0.20633447170257568\n",
      "training: 59 batch 231 batch_loss: 0.20347291231155396\n",
      "training: 59 batch 232 batch_loss: 0.20283856987953186\n",
      "training: 59 batch 233 batch_loss: 0.20242947340011597\n",
      "training: 59 batch 234 batch_loss: 0.2015933394432068\n",
      "training: 59 batch 235 batch_loss: 0.2040565013885498\n",
      "training: 59 batch 236 batch_loss: 0.20217964053153992\n",
      "training: 59 batch 237 batch_loss: 0.20469462871551514\n",
      "training: 59 batch 238 batch_loss: 0.20260214805603027\n",
      "training: 59 batch 239 batch_loss: 0.1967431902885437\n",
      "training: 59 batch 240 batch_loss: 0.20487159490585327\n",
      "training: 59 batch 241 batch_loss: 0.20490089058876038\n",
      "training: 59 batch 242 batch_loss: 0.20385289192199707\n",
      "training: 59 batch 243 batch_loss: 0.20481646060943604\n",
      "training: 59 batch 244 batch_loss: 0.2031538486480713\n",
      "training: 59 batch 245 batch_loss: 0.20188120007514954\n",
      "training: 59 batch 246 batch_loss: 0.19977229833602905\n",
      "training: 59 batch 247 batch_loss: 0.20245137810707092\n",
      "training: 59 batch 248 batch_loss: 0.19983989000320435\n",
      "training: 59 batch 249 batch_loss: 0.2004086971282959\n",
      "training: 59 batch 250 batch_loss: 0.20373335480690002\n",
      "training: 59 batch 251 batch_loss: 0.20439913868904114\n",
      "training: 59 batch 252 batch_loss: 0.20274609327316284\n",
      "training: 59 batch 253 batch_loss: 0.20608806610107422\n",
      "training: 59 batch 254 batch_loss: 0.20508909225463867\n",
      "training: 59 batch 255 batch_loss: 0.2023853361606598\n",
      "training: 59 batch 256 batch_loss: 0.20440763235092163\n",
      "training: 59 batch 257 batch_loss: 0.2045496702194214\n",
      "training: 59 batch 258 batch_loss: 0.19982463121414185\n",
      "training: 59 batch 259 batch_loss: 0.2030608057975769\n",
      "training: 59 batch 260 batch_loss: 0.20083513855934143\n",
      "training: 59 batch 261 batch_loss: 0.19963744282722473\n",
      "training: 59 batch 262 batch_loss: 0.20674273371696472\n",
      "training: 59 batch 263 batch_loss: 0.2038257122039795\n",
      "training: 59 batch 264 batch_loss: 0.203237384557724\n",
      "training: 59 batch 265 batch_loss: 0.2016705870628357\n",
      "training: 59 batch 266 batch_loss: 0.20492559671401978\n",
      "training: 59 batch 267 batch_loss: 0.20220157504081726\n",
      "training: 59 batch 268 batch_loss: 0.20518720149993896\n",
      "training: 59 batch 269 batch_loss: 0.20123228430747986\n",
      "training: 59 batch 270 batch_loss: 0.20311421155929565\n",
      "training: 59 batch 271 batch_loss: 0.2039187252521515\n",
      "training: 59 batch 272 batch_loss: 0.2039201855659485\n",
      "training: 59 batch 273 batch_loss: 0.20369747281074524\n",
      "training: 59 batch 274 batch_loss: 0.20314490795135498\n",
      "training: 59 batch 275 batch_loss: 0.2022218406200409\n",
      "training: 59 batch 276 batch_loss: 0.20215561985969543\n",
      "training: 59 batch 277 batch_loss: 0.2032572329044342\n",
      "training: 59 batch 278 batch_loss: 0.20110547542572021\n",
      "training: 59 batch 279 batch_loss: 0.20398074388504028\n",
      "training: 59 batch 280 batch_loss: 0.20121371746063232\n",
      "training: 59 batch 281 batch_loss: 0.20342138409614563\n",
      "training: 59 batch 282 batch_loss: 0.20181596279144287\n",
      "training: 59 batch 283 batch_loss: 0.2036731243133545\n",
      "training: 59 batch 284 batch_loss: 0.20227691531181335\n",
      "training: 59 batch 285 batch_loss: 0.20165732502937317\n",
      "training: 59 batch 286 batch_loss: 0.20300978422164917\n",
      "training: 59 batch 287 batch_loss: 0.20559161901474\n",
      "training: 59 batch 288 batch_loss: 0.19924020767211914\n",
      "training: 59 batch 289 batch_loss: 0.20538070797920227\n",
      "training: 59 batch 290 batch_loss: 0.20110252499580383\n",
      "training: 59 batch 291 batch_loss: 0.20268750190734863\n",
      "training: 59 batch 292 batch_loss: 0.2029789388179779\n",
      "training: 59 batch 293 batch_loss: 0.20211297273635864\n",
      "training: 59 batch 294 batch_loss: 0.20130479335784912\n",
      "training: 59 batch 295 batch_loss: 0.20341363549232483\n",
      "training: 59 batch 296 batch_loss: 0.20640742778778076\n",
      "training: 59 batch 297 batch_loss: 0.19979986548423767\n",
      "training: 59 batch 298 batch_loss: 0.20210328698158264\n",
      "training: 59 batch 299 batch_loss: 0.2045641541481018\n",
      "training: 59 batch 300 batch_loss: 0.20070257782936096\n",
      "training: 59 batch 301 batch_loss: 0.19876888394355774\n",
      "training: 59 batch 302 batch_loss: 0.19569215178489685\n",
      "training: 59 batch 303 batch_loss: 0.20125669240951538\n",
      "training: 59 batch 304 batch_loss: 0.20154571533203125\n",
      "training: 59 batch 305 batch_loss: 0.20307165384292603\n",
      "training: 59 batch 306 batch_loss: 0.20551809668540955\n",
      "training: 59 batch 307 batch_loss: 0.20687994360923767\n",
      "training: 59 batch 308 batch_loss: 0.2080942690372467\n",
      "training: 59 batch 309 batch_loss: 0.2031446397304535\n",
      "training: 59 batch 310 batch_loss: 0.2044943869113922\n",
      "training: 59 batch 311 batch_loss: 0.20270025730133057\n",
      "training: 59 batch 312 batch_loss: 0.2076593041419983\n",
      "training: 59 batch 313 batch_loss: 0.20279079675674438\n",
      "training: 59 batch 314 batch_loss: 0.2010035514831543\n",
      "training: 59 batch 315 batch_loss: 0.20172089338302612\n",
      "training: 59 batch 316 batch_loss: 0.20036393404006958\n",
      "training: 59 batch 317 batch_loss: 0.20550990104675293\n",
      "training: 59 batch 318 batch_loss: 0.2048678994178772\n",
      "training: 59 batch 319 batch_loss: 0.20564329624176025\n",
      "training: 59 batch 320 batch_loss: 0.2056271731853485\n",
      "training: 59 batch 321 batch_loss: 0.20371070504188538\n",
      "training: 59 batch 322 batch_loss: 0.20312008261680603\n",
      "training: 59 batch 323 batch_loss: 0.20373988151550293\n",
      "training: 59 batch 324 batch_loss: 0.20151805877685547\n",
      "training: 59 batch 325 batch_loss: 0.19941040873527527\n",
      "training: 59 batch 326 batch_loss: 0.20566004514694214\n",
      "training: 59 batch 327 batch_loss: 0.20521965622901917\n",
      "training: 59 batch 328 batch_loss: 0.20517155528068542\n",
      "training: 59 batch 329 batch_loss: 0.19986379146575928\n",
      "training: 59 batch 330 batch_loss: 0.20255392789840698\n",
      "training: 59 batch 331 batch_loss: 0.2045394480228424\n",
      "training: 59 batch 332 batch_loss: 0.20208078622817993\n",
      "training: 59 batch 333 batch_loss: 0.20136699080467224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 59 batch 334 batch_loss: 0.20130589604377747\n",
      "training: 59 batch 335 batch_loss: 0.2014121413230896\n",
      "training: 59 batch 336 batch_loss: 0.2082800567150116\n",
      "training: 59 batch 337 batch_loss: 0.2041621208190918\n",
      "training: 59 batch 338 batch_loss: 0.1997489631175995\n",
      "training: 59 batch 339 batch_loss: 0.20319414138793945\n",
      "training: 59 batch 340 batch_loss: 0.20304912328720093\n",
      "training: 59 batch 341 batch_loss: 0.20444363355636597\n",
      "training: 59 batch 342 batch_loss: 0.2017250955104828\n",
      "training: 59 batch 343 batch_loss: 0.1998312771320343\n",
      "training: 59 batch 344 batch_loss: 0.20191189646720886\n",
      "training: 59 batch 345 batch_loss: 0.2012265920639038\n",
      "training: 59 batch 346 batch_loss: 0.20226949453353882\n",
      "training: 59 batch 347 batch_loss: 0.20090654492378235\n",
      "training: 59 batch 348 batch_loss: 0.2050803303718567\n",
      "training: 59 batch 349 batch_loss: 0.2001645267009735\n",
      "training: 59 batch 350 batch_loss: 0.20170772075653076\n",
      "training: 59 batch 351 batch_loss: 0.2060316801071167\n",
      "training: 59 batch 352 batch_loss: 0.2041751742362976\n",
      "training: 59 batch 353 batch_loss: 0.19818717241287231\n",
      "training: 59 batch 354 batch_loss: 0.20533132553100586\n",
      "training: 59 batch 355 batch_loss: 0.19974520802497864\n",
      "training: 59 batch 356 batch_loss: 0.20330649614334106\n",
      "training: 59 batch 357 batch_loss: 0.1988528072834015\n",
      "training: 59 batch 358 batch_loss: 0.20248004794120789\n",
      "training: 59 batch 359 batch_loss: 0.20153158903121948\n",
      "training: 59 batch 360 batch_loss: 0.20092791318893433\n",
      "training: 59 batch 361 batch_loss: 0.20038318634033203\n",
      "training: 59 batch 362 batch_loss: 0.20019719004631042\n",
      "training: 59 batch 363 batch_loss: 0.2049911618232727\n",
      "training: 59 batch 364 batch_loss: 0.2012706995010376\n",
      "training: 59 batch 365 batch_loss: 0.2038397490978241\n",
      "training: 59 batch 366 batch_loss: 0.202009916305542\n",
      "training: 59 batch 367 batch_loss: 0.20154613256454468\n",
      "training: 59 batch 368 batch_loss: 0.20351237058639526\n",
      "training: 59 batch 369 batch_loss: 0.20731830596923828\n",
      "training: 59 batch 370 batch_loss: 0.2013101577758789\n",
      "training: 59 batch 371 batch_loss: 0.20497506856918335\n",
      "training: 59 batch 372 batch_loss: 0.19957414269447327\n",
      "training: 59 batch 373 batch_loss: 0.20518732070922852\n",
      "training: 59 batch 374 batch_loss: 0.20494157075881958\n",
      "training: 59 batch 375 batch_loss: 0.20505362749099731\n",
      "training: 59 batch 376 batch_loss: 0.20572498440742493\n",
      "training: 59 batch 377 batch_loss: 0.20546698570251465\n",
      "training: 59 batch 378 batch_loss: 0.19856828451156616\n",
      "training: 59 batch 379 batch_loss: 0.20646917819976807\n",
      "training: 59 batch 380 batch_loss: 0.19993656873703003\n",
      "training: 59 batch 381 batch_loss: 0.20459210872650146\n",
      "training: 59 batch 382 batch_loss: 0.20213857293128967\n",
      "training: 59 batch 383 batch_loss: 0.20234134793281555\n",
      "training: 59 batch 384 batch_loss: 0.2089659869670868\n",
      "training: 59 batch 385 batch_loss: 0.20657411217689514\n",
      "training: 59 batch 386 batch_loss: 0.20339089632034302\n",
      "training: 59 batch 387 batch_loss: 0.19666510820388794\n",
      "training: 59 batch 388 batch_loss: 0.20192277431488037\n",
      "training: 59 batch 389 batch_loss: 0.2012060284614563\n",
      "training: 59 batch 390 batch_loss: 0.20207464694976807\n",
      "training: 59 batch 391 batch_loss: 0.20319050550460815\n",
      "training: 59 batch 392 batch_loss: 0.20256546139717102\n",
      "training: 59 batch 393 batch_loss: 0.20234352350234985\n",
      "training: 59 batch 394 batch_loss: 0.19933632016181946\n",
      "training: 59 batch 395 batch_loss: 0.20169579982757568\n",
      "training: 59 batch 396 batch_loss: 0.20111194252967834\n",
      "training: 59 batch 397 batch_loss: 0.20597901940345764\n",
      "training: 59 batch 398 batch_loss: 0.20875245332717896\n",
      "training: 59 batch 399 batch_loss: 0.20420187711715698\n",
      "training: 59 batch 400 batch_loss: 0.2005157172679901\n",
      "training: 59 batch 401 batch_loss: 0.20224016904830933\n",
      "training: 59 batch 402 batch_loss: 0.19654017686843872\n",
      "training: 59 batch 403 batch_loss: 0.2051151692867279\n",
      "training: 59 batch 404 batch_loss: 0.20108669996261597\n",
      "training: 59 batch 405 batch_loss: 0.20409995317459106\n",
      "training: 59 batch 406 batch_loss: 0.20737043023109436\n",
      "training: 59 batch 407 batch_loss: 0.20930129289627075\n",
      "training: 59 batch 408 batch_loss: 0.20079705119132996\n",
      "training: 59 batch 409 batch_loss: 0.20499560236930847\n",
      "training: 59 batch 410 batch_loss: 0.2030811905860901\n",
      "training: 59 batch 411 batch_loss: 0.2018522024154663\n",
      "training: 59 batch 412 batch_loss: 0.20098364353179932\n",
      "training: 59 batch 413 batch_loss: 0.19904211163520813\n",
      "training: 59 batch 414 batch_loss: 0.20555895566940308\n",
      "training: 59 batch 415 batch_loss: 0.20556381344795227\n",
      "training: 59 batch 416 batch_loss: 0.20716506242752075\n",
      "training: 59 batch 417 batch_loss: 0.2012571394443512\n",
      "training: 59 batch 418 batch_loss: 0.2057276964187622\n",
      "training: 59 batch 419 batch_loss: 0.20287856459617615\n",
      "training: 59 batch 420 batch_loss: 0.20422181487083435\n",
      "training: 59 batch 421 batch_loss: 0.202121764421463\n",
      "training: 59 batch 422 batch_loss: 0.20314949750900269\n",
      "training: 59 batch 423 batch_loss: 0.20324644446372986\n",
      "training: 59 batch 424 batch_loss: 0.19764679670333862\n",
      "training: 59 batch 425 batch_loss: 0.20198136568069458\n",
      "training: 59 batch 426 batch_loss: 0.2027241289615631\n",
      "training: 59 batch 427 batch_loss: 0.20539888739585876\n",
      "training: 59 batch 428 batch_loss: 0.20419049263000488\n",
      "training: 59 batch 429 batch_loss: 0.2017136514186859\n",
      "training: 59 batch 430 batch_loss: 0.20559877157211304\n",
      "training: 59 batch 431 batch_loss: 0.20428526401519775\n",
      "training: 59 batch 432 batch_loss: 0.2044651210308075\n",
      "training: 59 batch 433 batch_loss: 0.2005915343761444\n",
      "training: 59 batch 434 batch_loss: 0.20518866181373596\n",
      "training: 59 batch 435 batch_loss: 0.2037685513496399\n",
      "training: 59 batch 436 batch_loss: 0.2042020559310913\n",
      "training: 59 batch 437 batch_loss: 0.20306289196014404\n",
      "training: 59 batch 438 batch_loss: 0.2026132345199585\n",
      "training: 59 batch 439 batch_loss: 0.2030935287475586\n",
      "training: 59 batch 440 batch_loss: 0.20400142669677734\n",
      "training: 59 batch 441 batch_loss: 0.19858500361442566\n",
      "training: 59 batch 442 batch_loss: 0.20271837711334229\n",
      "training: 59 batch 443 batch_loss: 0.20488092303276062\n",
      "training: 59 batch 444 batch_loss: 0.2060565948486328\n",
      "training: 59 batch 445 batch_loss: 0.19916486740112305\n",
      "training: 59 batch 446 batch_loss: 0.20232194662094116\n",
      "training: 59 batch 447 batch_loss: 0.20299983024597168\n",
      "training: 59 batch 448 batch_loss: 0.20699062943458557\n",
      "training: 59 batch 449 batch_loss: 0.20104745030403137\n",
      "training: 59 batch 450 batch_loss: 0.2046239674091339\n",
      "training: 59 batch 451 batch_loss: 0.20062756538391113\n",
      "training: 59 batch 452 batch_loss: 0.20331087708473206\n",
      "training: 59 batch 453 batch_loss: 0.20495206117630005\n",
      "training: 59 batch 454 batch_loss: 0.20246976613998413\n",
      "training: 59 batch 455 batch_loss: 0.2071472704410553\n",
      "training: 59 batch 456 batch_loss: 0.20054662227630615\n",
      "training: 59 batch 457 batch_loss: 0.20315152406692505\n",
      "training: 59 batch 458 batch_loss: 0.20150479674339294\n",
      "training: 59 batch 459 batch_loss: 0.20129704475402832\n",
      "training: 59 batch 460 batch_loss: 0.19960862398147583\n",
      "training: 59 batch 461 batch_loss: 0.20177915692329407\n",
      "training: 59 batch 462 batch_loss: 0.2011350691318512\n",
      "training: 59 batch 463 batch_loss: 0.20270925760269165\n",
      "training: 59 batch 464 batch_loss: 0.20283737778663635\n",
      "training: 59 batch 465 batch_loss: 0.20458045601844788\n",
      "training: 59 batch 466 batch_loss: 0.2032841444015503\n",
      "training: 59 batch 467 batch_loss: 0.2047121524810791\n",
      "training: 59 batch 468 batch_loss: 0.20285719633102417\n",
      "training: 59 batch 469 batch_loss: 0.2014460265636444\n",
      "training: 59 batch 470 batch_loss: 0.20029416680335999\n",
      "training: 59 batch 471 batch_loss: 0.20379501581192017\n",
      "training: 59 batch 472 batch_loss: 0.2017994523048401\n",
      "training: 59 batch 473 batch_loss: 0.20383942127227783\n",
      "training: 59 batch 474 batch_loss: 0.20361950993537903\n",
      "training: 59 batch 475 batch_loss: 0.20405125617980957\n",
      "training: 59 batch 476 batch_loss: 0.2037246823310852\n",
      "training: 59 batch 477 batch_loss: 0.20334845781326294\n",
      "training: 59 batch 478 batch_loss: 0.20331263542175293\n",
      "training: 59 batch 479 batch_loss: 0.2032020390033722\n",
      "training: 59 batch 480 batch_loss: 0.20319992303848267\n",
      "training: 59 batch 481 batch_loss: 0.2027578353881836\n",
      "training: 59 batch 482 batch_loss: 0.19990986585617065\n",
      "training: 59 batch 483 batch_loss: 0.197981059551239\n",
      "training: 59 batch 484 batch_loss: 0.203436017036438\n",
      "training: 59 batch 485 batch_loss: 0.2011488378047943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 59 batch 486 batch_loss: 0.20052355527877808\n",
      "training: 59 batch 487 batch_loss: 0.20580855011940002\n",
      "training: 59 batch 488 batch_loss: 0.20409858226776123\n",
      "training: 59 batch 489 batch_loss: 0.2034769058227539\n",
      "training: 59 batch 490 batch_loss: 0.2036151885986328\n",
      "training: 59 batch 491 batch_loss: 0.20370015501976013\n",
      "training: 59 batch 492 batch_loss: 0.20696532726287842\n",
      "training: 59 batch 493 batch_loss: 0.20117458701133728\n",
      "training: 59 batch 494 batch_loss: 0.1988632082939148\n",
      "training: 59 batch 495 batch_loss: 0.2096366286277771\n",
      "training: 59 batch 496 batch_loss: 0.20336630940437317\n",
      "training: 59 batch 497 batch_loss: 0.20101654529571533\n",
      "training: 59 batch 498 batch_loss: 0.2042800784111023\n",
      "training: 59 batch 499 batch_loss: 0.20377996563911438\n",
      "training: 59 batch 500 batch_loss: 0.20269590616226196\n",
      "training: 59 batch 501 batch_loss: 0.20328623056411743\n",
      "training: 59 batch 502 batch_loss: 0.2046891748905182\n",
      "training: 59 batch 503 batch_loss: 0.20781955122947693\n",
      "training: 59 batch 504 batch_loss: 0.20442146062850952\n",
      "training: 59 batch 505 batch_loss: 0.20422226190567017\n",
      "training: 59 batch 506 batch_loss: 0.20293939113616943\n",
      "training: 59 batch 507 batch_loss: 0.2048856019973755\n",
      "training: 59 batch 508 batch_loss: 0.2015152871608734\n",
      "training: 59 batch 509 batch_loss: 0.20417630672454834\n",
      "training: 59 batch 510 batch_loss: 0.20540648698806763\n",
      "training: 59 batch 511 batch_loss: 0.20624932646751404\n",
      "training: 59 batch 512 batch_loss: 0.20654180645942688\n",
      "training: 59 batch 513 batch_loss: 0.20110392570495605\n",
      "training: 59 batch 514 batch_loss: 0.2043987512588501\n",
      "training: 59 batch 515 batch_loss: 0.2027350664138794\n",
      "training: 59 batch 516 batch_loss: 0.20329609513282776\n",
      "training: 59 batch 517 batch_loss: 0.20902401208877563\n",
      "training: 59 batch 518 batch_loss: 0.2094595730304718\n",
      "training: 59 batch 519 batch_loss: 0.20131778717041016\n",
      "training: 59 batch 520 batch_loss: 0.2036641538143158\n",
      "training: 59 batch 521 batch_loss: 0.20635250210762024\n",
      "training: 59 batch 522 batch_loss: 0.20075583457946777\n",
      "training: 59 batch 523 batch_loss: 0.19955706596374512\n",
      "training: 59 batch 524 batch_loss: 0.20086121559143066\n",
      "training: 59 batch 525 batch_loss: 0.20404621958732605\n",
      "training: 59 batch 526 batch_loss: 0.2005692422389984\n",
      "training: 59 batch 527 batch_loss: 0.2019195854663849\n",
      "training: 59 batch 528 batch_loss: 0.20343071222305298\n",
      "training: 59 batch 529 batch_loss: 0.20665055513381958\n",
      "training: 59 batch 530 batch_loss: 0.20279541611671448\n",
      "training: 59 batch 531 batch_loss: 0.20683163404464722\n",
      "training: 59 batch 532 batch_loss: 0.20679092407226562\n",
      "training: 59 batch 533 batch_loss: 0.2035398781299591\n",
      "training: 59 batch 534 batch_loss: 0.2029266655445099\n",
      "training: 59 batch 535 batch_loss: 0.20285272598266602\n",
      "training: 59 batch 536 batch_loss: 0.2040482461452484\n",
      "training: 59 batch 537 batch_loss: 0.20082172751426697\n",
      "training: 59 batch 538 batch_loss: 0.20140516757965088\n",
      "training: 59 batch 539 batch_loss: 0.20322895050048828\n",
      "training: 59 batch 540 batch_loss: 0.1998852789402008\n",
      "training: 59 batch 541 batch_loss: 0.20579379796981812\n",
      "training: 59 batch 542 batch_loss: 0.20733025670051575\n",
      "training: 59 batch 543 batch_loss: 0.20298194885253906\n",
      "training: 59 batch 544 batch_loss: 0.20306187868118286\n",
      "training: 59 batch 545 batch_loss: 0.2067941129207611\n",
      "training: 59 batch 546 batch_loss: 0.2011057734489441\n",
      "training: 59 batch 547 batch_loss: 0.205175518989563\n",
      "training: 59 batch 548 batch_loss: 0.20755669474601746\n",
      "training: 59 batch 549 batch_loss: 0.20278334617614746\n",
      "training: 59 batch 550 batch_loss: 0.20488840341567993\n",
      "training: 59 batch 551 batch_loss: 0.20324018597602844\n",
      "training: 59 batch 552 batch_loss: 0.20464560389518738\n",
      "training: 59 batch 553 batch_loss: 0.2058592438697815\n",
      "training: 59 batch 554 batch_loss: 0.2091647982597351\n",
      "training: 59 batch 555 batch_loss: 0.20328745245933533\n",
      "training: 59 batch 556 batch_loss: 0.2045588195323944\n",
      "training: 59 batch 557 batch_loss: 0.2043580710887909\n",
      "training: 59 batch 558 batch_loss: 0.2049676775932312\n",
      "training: 59 batch 559 batch_loss: 0.20148733258247375\n",
      "training: 59 batch 560 batch_loss: 0.20756804943084717\n",
      "training: 59 batch 561 batch_loss: 0.20486795902252197\n",
      "training: 59 batch 562 batch_loss: 0.20755279064178467\n",
      "training: 59 batch 563 batch_loss: 0.2050357162952423\n",
      "training: 59 batch 564 batch_loss: 0.20529299974441528\n",
      "training: 59 batch 565 batch_loss: 0.2026888132095337\n",
      "training: 59 batch 566 batch_loss: 0.204015851020813\n",
      "training: 59 batch 567 batch_loss: 0.20456305146217346\n",
      "training: 59 batch 568 batch_loss: 0.20460647344589233\n",
      "training: 59 batch 569 batch_loss: 0.20444223284721375\n",
      "training: 59 batch 570 batch_loss: 0.20604479312896729\n",
      "training: 59 batch 571 batch_loss: 0.20026463270187378\n",
      "training: 59 batch 572 batch_loss: 0.20614886283874512\n",
      "training: 59 batch 573 batch_loss: 0.2056739330291748\n",
      "training: 59 batch 574 batch_loss: 0.20639044046401978\n",
      "training: 59 batch 575 batch_loss: 0.2067442238330841\n",
      "training: 59 batch 576 batch_loss: 0.20760148763656616\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 59, Hit Ratio:0.03241581249396222 | Precision:0.04782758281726138 | Recall:0.06384094120120251 | NDCG:0.06209253978180965\n",
      "*Best Performance* \n",
      "Epoch: 56, Hit Ratio:0.032922153414636585 | Precision:0.04857465840951539 | Recall:0.06427363564028983 | MDCG:0.06308737979754037\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 60 batch 0 batch_loss: 0.20269599556922913\n",
      "training: 60 batch 1 batch_loss: 0.2029595673084259\n",
      "training: 60 batch 2 batch_loss: 0.19773349165916443\n",
      "training: 60 batch 3 batch_loss: 0.20078688859939575\n",
      "training: 60 batch 4 batch_loss: 0.20187503099441528\n",
      "training: 60 batch 5 batch_loss: 0.20018768310546875\n",
      "training: 60 batch 6 batch_loss: 0.20349377393722534\n",
      "training: 60 batch 7 batch_loss: 0.2042599320411682\n",
      "training: 60 batch 8 batch_loss: 0.20582446455955505\n",
      "training: 60 batch 9 batch_loss: 0.19973254203796387\n",
      "training: 60 batch 10 batch_loss: 0.20068207383155823\n",
      "training: 60 batch 11 batch_loss: 0.19826608896255493\n",
      "training: 60 batch 12 batch_loss: 0.20534300804138184\n",
      "training: 60 batch 13 batch_loss: 0.19735082983970642\n",
      "training: 60 batch 14 batch_loss: 0.20346957445144653\n",
      "training: 60 batch 15 batch_loss: 0.20307853817939758\n",
      "training: 60 batch 16 batch_loss: 0.19888508319854736\n",
      "training: 60 batch 17 batch_loss: 0.20049357414245605\n",
      "training: 60 batch 18 batch_loss: 0.20402297377586365\n",
      "training: 60 batch 19 batch_loss: 0.19842547178268433\n",
      "training: 60 batch 20 batch_loss: 0.20532557368278503\n",
      "training: 60 batch 21 batch_loss: 0.2023410201072693\n",
      "training: 60 batch 22 batch_loss: 0.19993805885314941\n",
      "training: 60 batch 23 batch_loss: 0.20357194542884827\n",
      "training: 60 batch 24 batch_loss: 0.20626980066299438\n",
      "training: 60 batch 25 batch_loss: 0.2050004005432129\n",
      "training: 60 batch 26 batch_loss: 0.20162808895111084\n",
      "training: 60 batch 27 batch_loss: 0.20489096641540527\n",
      "training: 60 batch 28 batch_loss: 0.20179182291030884\n",
      "training: 60 batch 29 batch_loss: 0.20134004950523376\n",
      "training: 60 batch 30 batch_loss: 0.20682474970817566\n",
      "training: 60 batch 31 batch_loss: 0.19875788688659668\n",
      "training: 60 batch 32 batch_loss: 0.20411095023155212\n",
      "training: 60 batch 33 batch_loss: 0.20519456267356873\n",
      "training: 60 batch 34 batch_loss: 0.19868877530097961\n",
      "training: 60 batch 35 batch_loss: 0.20739293098449707\n",
      "training: 60 batch 36 batch_loss: 0.2020207643508911\n",
      "training: 60 batch 37 batch_loss: 0.2022341787815094\n",
      "training: 60 batch 38 batch_loss: 0.2043781280517578\n",
      "training: 60 batch 39 batch_loss: 0.2008042335510254\n",
      "training: 60 batch 40 batch_loss: 0.20358845591545105\n",
      "training: 60 batch 41 batch_loss: 0.19550716876983643\n",
      "training: 60 batch 42 batch_loss: 0.2021908462047577\n",
      "training: 60 batch 43 batch_loss: 0.19649255275726318\n",
      "training: 60 batch 44 batch_loss: 0.20341849327087402\n",
      "training: 60 batch 45 batch_loss: 0.20108920335769653\n",
      "training: 60 batch 46 batch_loss: 0.19988450407981873\n",
      "training: 60 batch 47 batch_loss: 0.20249760150909424\n",
      "training: 60 batch 48 batch_loss: 0.20696890354156494\n",
      "training: 60 batch 49 batch_loss: 0.2032441794872284\n",
      "training: 60 batch 50 batch_loss: 0.20212751626968384\n",
      "training: 60 batch 51 batch_loss: 0.2031363844871521\n",
      "training: 60 batch 52 batch_loss: 0.20276403427124023\n",
      "training: 60 batch 53 batch_loss: 0.19939252734184265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 60 batch 54 batch_loss: 0.20179349184036255\n",
      "training: 60 batch 55 batch_loss: 0.19847899675369263\n",
      "training: 60 batch 56 batch_loss: 0.20392557978630066\n",
      "training: 60 batch 57 batch_loss: 0.20079874992370605\n",
      "training: 60 batch 58 batch_loss: 0.2022625207901001\n",
      "training: 60 batch 59 batch_loss: 0.20280790328979492\n",
      "training: 60 batch 60 batch_loss: 0.20752936601638794\n",
      "training: 60 batch 61 batch_loss: 0.2058238685131073\n",
      "training: 60 batch 62 batch_loss: 0.20644927024841309\n",
      "training: 60 batch 63 batch_loss: 0.20035678148269653\n",
      "training: 60 batch 64 batch_loss: 0.2055891752243042\n",
      "training: 60 batch 65 batch_loss: 0.20009171962738037\n",
      "training: 60 batch 66 batch_loss: 0.20027562975883484\n",
      "training: 60 batch 67 batch_loss: 0.20421883463859558\n",
      "training: 60 batch 68 batch_loss: 0.20301219820976257\n",
      "training: 60 batch 69 batch_loss: 0.2013918161392212\n",
      "training: 60 batch 70 batch_loss: 0.2039179801940918\n",
      "training: 60 batch 71 batch_loss: 0.2043353021144867\n",
      "training: 60 batch 72 batch_loss: 0.2043721079826355\n",
      "training: 60 batch 73 batch_loss: 0.2012588083744049\n",
      "training: 60 batch 74 batch_loss: 0.20446869730949402\n",
      "training: 60 batch 75 batch_loss: 0.20168942213058472\n",
      "training: 60 batch 76 batch_loss: 0.20911905169487\n",
      "training: 60 batch 77 batch_loss: 0.20572388172149658\n",
      "training: 60 batch 78 batch_loss: 0.19522032141685486\n",
      "training: 60 batch 79 batch_loss: 0.20323771238327026\n",
      "training: 60 batch 80 batch_loss: 0.20379775762557983\n",
      "training: 60 batch 81 batch_loss: 0.206059068441391\n",
      "training: 60 batch 82 batch_loss: 0.20395982265472412\n",
      "training: 60 batch 83 batch_loss: 0.20328718423843384\n",
      "training: 60 batch 84 batch_loss: 0.2040402889251709\n",
      "training: 60 batch 85 batch_loss: 0.2038007378578186\n",
      "training: 60 batch 86 batch_loss: 0.20279353857040405\n",
      "training: 60 batch 87 batch_loss: 0.2060731053352356\n",
      "training: 60 batch 88 batch_loss: 0.20025408267974854\n",
      "training: 60 batch 89 batch_loss: 0.1968204379081726\n",
      "training: 60 batch 90 batch_loss: 0.20035377144813538\n",
      "training: 60 batch 91 batch_loss: 0.20193207263946533\n",
      "training: 60 batch 92 batch_loss: 0.20730021595954895\n",
      "training: 60 batch 93 batch_loss: 0.20248425006866455\n",
      "training: 60 batch 94 batch_loss: 0.20082125067710876\n",
      "training: 60 batch 95 batch_loss: 0.20335865020751953\n",
      "training: 60 batch 96 batch_loss: 0.20269715785980225\n",
      "training: 60 batch 97 batch_loss: 0.20226505398750305\n",
      "training: 60 batch 98 batch_loss: 0.19871243834495544\n",
      "training: 60 batch 99 batch_loss: 0.20485159754753113\n",
      "training: 60 batch 100 batch_loss: 0.1999821662902832\n",
      "training: 60 batch 101 batch_loss: 0.20233985781669617\n",
      "training: 60 batch 102 batch_loss: 0.20779293775558472\n",
      "training: 60 batch 103 batch_loss: 0.20402798056602478\n",
      "training: 60 batch 104 batch_loss: 0.20536017417907715\n",
      "training: 60 batch 105 batch_loss: 0.2038668394088745\n",
      "training: 60 batch 106 batch_loss: 0.20400798320770264\n",
      "training: 60 batch 107 batch_loss: 0.2049548327922821\n",
      "training: 60 batch 108 batch_loss: 0.20176780223846436\n",
      "training: 60 batch 109 batch_loss: 0.2026604413986206\n",
      "training: 60 batch 110 batch_loss: 0.20436108112335205\n",
      "training: 60 batch 111 batch_loss: 0.20722529292106628\n",
      "training: 60 batch 112 batch_loss: 0.20242559909820557\n",
      "training: 60 batch 113 batch_loss: 0.20189940929412842\n",
      "training: 60 batch 114 batch_loss: 0.197104811668396\n",
      "training: 60 batch 115 batch_loss: 0.2060297131538391\n",
      "training: 60 batch 116 batch_loss: 0.2080811858177185\n",
      "training: 60 batch 117 batch_loss: 0.20227143168449402\n",
      "training: 60 batch 118 batch_loss: 0.20677244663238525\n",
      "training: 60 batch 119 batch_loss: 0.20498314499855042\n",
      "training: 60 batch 120 batch_loss: 0.20170912146568298\n",
      "training: 60 batch 121 batch_loss: 0.20426416397094727\n",
      "training: 60 batch 122 batch_loss: 0.20150965452194214\n",
      "training: 60 batch 123 batch_loss: 0.20161747932434082\n",
      "training: 60 batch 124 batch_loss: 0.20335227251052856\n",
      "training: 60 batch 125 batch_loss: 0.2034425437450409\n",
      "training: 60 batch 126 batch_loss: 0.20183825492858887\n",
      "training: 60 batch 127 batch_loss: 0.20633184909820557\n",
      "training: 60 batch 128 batch_loss: 0.1993180215358734\n",
      "training: 60 batch 129 batch_loss: 0.20690876245498657\n",
      "training: 60 batch 130 batch_loss: 0.20642238855361938\n",
      "training: 60 batch 131 batch_loss: 0.20210587978363037\n",
      "training: 60 batch 132 batch_loss: 0.20005649328231812\n",
      "training: 60 batch 133 batch_loss: 0.2090005874633789\n",
      "training: 60 batch 134 batch_loss: 0.20971256494522095\n",
      "training: 60 batch 135 batch_loss: 0.20543670654296875\n",
      "training: 60 batch 136 batch_loss: 0.20413869619369507\n",
      "training: 60 batch 137 batch_loss: 0.2056427001953125\n",
      "training: 60 batch 138 batch_loss: 0.20192360877990723\n",
      "training: 60 batch 139 batch_loss: 0.2031247615814209\n",
      "training: 60 batch 140 batch_loss: 0.20483869314193726\n",
      "training: 60 batch 141 batch_loss: 0.201093852519989\n",
      "training: 60 batch 142 batch_loss: 0.20125222206115723\n",
      "training: 60 batch 143 batch_loss: 0.20595139265060425\n",
      "training: 60 batch 144 batch_loss: 0.1978001594543457\n",
      "training: 60 batch 145 batch_loss: 0.2035367488861084\n",
      "training: 60 batch 146 batch_loss: 0.20712465047836304\n",
      "training: 60 batch 147 batch_loss: 0.2009851038455963\n",
      "training: 60 batch 148 batch_loss: 0.20379233360290527\n",
      "training: 60 batch 149 batch_loss: 0.2042933702468872\n",
      "training: 60 batch 150 batch_loss: 0.199904203414917\n",
      "training: 60 batch 151 batch_loss: 0.20600679516792297\n",
      "training: 60 batch 152 batch_loss: 0.20719939470291138\n",
      "training: 60 batch 153 batch_loss: 0.20020800828933716\n",
      "training: 60 batch 154 batch_loss: 0.20130515098571777\n",
      "training: 60 batch 155 batch_loss: 0.20627135038375854\n",
      "training: 60 batch 156 batch_loss: 0.20324251055717468\n",
      "training: 60 batch 157 batch_loss: 0.19802778959274292\n",
      "training: 60 batch 158 batch_loss: 0.20254924893379211\n",
      "training: 60 batch 159 batch_loss: 0.2025701403617859\n",
      "training: 60 batch 160 batch_loss: 0.20334160327911377\n",
      "training: 60 batch 161 batch_loss: 0.20005574822425842\n",
      "training: 60 batch 162 batch_loss: 0.19936716556549072\n",
      "training: 60 batch 163 batch_loss: 0.2078574001789093\n",
      "training: 60 batch 164 batch_loss: 0.20226746797561646\n",
      "training: 60 batch 165 batch_loss: 0.20274662971496582\n",
      "training: 60 batch 166 batch_loss: 0.19935554265975952\n",
      "training: 60 batch 167 batch_loss: 0.2105778455734253\n",
      "training: 60 batch 168 batch_loss: 0.20386260747909546\n",
      "training: 60 batch 169 batch_loss: 0.20144355297088623\n",
      "training: 60 batch 170 batch_loss: 0.20236945152282715\n",
      "training: 60 batch 171 batch_loss: 0.20136302709579468\n",
      "training: 60 batch 172 batch_loss: 0.20154353976249695\n",
      "training: 60 batch 173 batch_loss: 0.2041945457458496\n",
      "training: 60 batch 174 batch_loss: 0.20211490988731384\n",
      "training: 60 batch 175 batch_loss: 0.20000526309013367\n",
      "training: 60 batch 176 batch_loss: 0.20261675119400024\n",
      "training: 60 batch 177 batch_loss: 0.20370325446128845\n",
      "training: 60 batch 178 batch_loss: 0.203563392162323\n",
      "training: 60 batch 179 batch_loss: 0.20486178994178772\n",
      "training: 60 batch 180 batch_loss: 0.20290398597717285\n",
      "training: 60 batch 181 batch_loss: 0.2039896845817566\n",
      "training: 60 batch 182 batch_loss: 0.20483577251434326\n",
      "training: 60 batch 183 batch_loss: 0.1992284655570984\n",
      "training: 60 batch 184 batch_loss: 0.20029723644256592\n",
      "training: 60 batch 185 batch_loss: 0.20516833662986755\n",
      "training: 60 batch 186 batch_loss: 0.198502779006958\n",
      "training: 60 batch 187 batch_loss: 0.2053564190864563\n",
      "training: 60 batch 188 batch_loss: 0.20523154735565186\n",
      "training: 60 batch 189 batch_loss: 0.20450133085250854\n",
      "training: 60 batch 190 batch_loss: 0.20333969593048096\n",
      "training: 60 batch 191 batch_loss: 0.2053925096988678\n",
      "training: 60 batch 192 batch_loss: 0.20381516218185425\n",
      "training: 60 batch 193 batch_loss: 0.2032722532749176\n",
      "training: 60 batch 194 batch_loss: 0.20428219437599182\n",
      "training: 60 batch 195 batch_loss: 0.2027258574962616\n",
      "training: 60 batch 196 batch_loss: 0.20606714487075806\n",
      "training: 60 batch 197 batch_loss: 0.20497676730155945\n",
      "training: 60 batch 198 batch_loss: 0.2012624442577362\n",
      "training: 60 batch 199 batch_loss: 0.19959229230880737\n",
      "training: 60 batch 200 batch_loss: 0.20255857706069946\n",
      "training: 60 batch 201 batch_loss: 0.20056381821632385\n",
      "training: 60 batch 202 batch_loss: 0.201926589012146\n",
      "training: 60 batch 203 batch_loss: 0.20459744334220886\n",
      "training: 60 batch 204 batch_loss: 0.20751813054084778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 60 batch 205 batch_loss: 0.20599377155303955\n",
      "training: 60 batch 206 batch_loss: 0.20401078462600708\n",
      "training: 60 batch 207 batch_loss: 0.20496109127998352\n",
      "training: 60 batch 208 batch_loss: 0.2047603726387024\n",
      "training: 60 batch 209 batch_loss: 0.20374378561973572\n",
      "training: 60 batch 210 batch_loss: 0.2075052261352539\n",
      "training: 60 batch 211 batch_loss: 0.20723575353622437\n",
      "training: 60 batch 212 batch_loss: 0.20470622181892395\n",
      "training: 60 batch 213 batch_loss: 0.2031393051147461\n",
      "training: 60 batch 214 batch_loss: 0.2056281864643097\n",
      "training: 60 batch 215 batch_loss: 0.20182174444198608\n",
      "training: 60 batch 216 batch_loss: 0.20374464988708496\n",
      "training: 60 batch 217 batch_loss: 0.20271193981170654\n",
      "training: 60 batch 218 batch_loss: 0.20192956924438477\n",
      "training: 60 batch 219 batch_loss: 0.20101895928382874\n",
      "training: 60 batch 220 batch_loss: 0.20576432347297668\n",
      "training: 60 batch 221 batch_loss: 0.2068728506565094\n",
      "training: 60 batch 222 batch_loss: 0.20456048846244812\n",
      "training: 60 batch 223 batch_loss: 0.20100316405296326\n",
      "training: 60 batch 224 batch_loss: 0.2075551152229309\n",
      "training: 60 batch 225 batch_loss: 0.20567891001701355\n",
      "training: 60 batch 226 batch_loss: 0.20750167965888977\n",
      "training: 60 batch 227 batch_loss: 0.2042069137096405\n",
      "training: 60 batch 228 batch_loss: 0.20413634181022644\n",
      "training: 60 batch 229 batch_loss: 0.19931063055992126\n",
      "training: 60 batch 230 batch_loss: 0.2029031217098236\n",
      "training: 60 batch 231 batch_loss: 0.20261570811271667\n",
      "training: 60 batch 232 batch_loss: 0.20576485991477966\n",
      "training: 60 batch 233 batch_loss: 0.20004189014434814\n",
      "training: 60 batch 234 batch_loss: 0.20144879817962646\n",
      "training: 60 batch 235 batch_loss: 0.20679384469985962\n",
      "training: 60 batch 236 batch_loss: 0.20113226771354675\n",
      "training: 60 batch 237 batch_loss: 0.2028856873512268\n",
      "training: 60 batch 238 batch_loss: 0.20356103777885437\n",
      "training: 60 batch 239 batch_loss: 0.2071281373500824\n",
      "training: 60 batch 240 batch_loss: 0.20313000679016113\n",
      "training: 60 batch 241 batch_loss: 0.20124468207359314\n",
      "training: 60 batch 242 batch_loss: 0.20380711555480957\n",
      "training: 60 batch 243 batch_loss: 0.1997392773628235\n",
      "training: 60 batch 244 batch_loss: 0.2066672444343567\n",
      "training: 60 batch 245 batch_loss: 0.20281758904457092\n",
      "training: 60 batch 246 batch_loss: 0.20453926920890808\n",
      "training: 60 batch 247 batch_loss: 0.20387989282608032\n",
      "training: 60 batch 248 batch_loss: 0.20014825463294983\n",
      "training: 60 batch 249 batch_loss: 0.2058085799217224\n",
      "training: 60 batch 250 batch_loss: 0.20910736918449402\n",
      "training: 60 batch 251 batch_loss: 0.20485752820968628\n",
      "training: 60 batch 252 batch_loss: 0.20627567172050476\n",
      "training: 60 batch 253 batch_loss: 0.2040058970451355\n",
      "training: 60 batch 254 batch_loss: 0.2013532817363739\n",
      "training: 60 batch 255 batch_loss: 0.20310133695602417\n",
      "training: 60 batch 256 batch_loss: 0.2061791718006134\n",
      "training: 60 batch 257 batch_loss: 0.20249062776565552\n",
      "training: 60 batch 258 batch_loss: 0.20241543650627136\n",
      "training: 60 batch 259 batch_loss: 0.20568811893463135\n",
      "training: 60 batch 260 batch_loss: 0.2022227942943573\n",
      "training: 60 batch 261 batch_loss: 0.2060050368309021\n",
      "training: 60 batch 262 batch_loss: 0.20397326350212097\n",
      "training: 60 batch 263 batch_loss: 0.20189622044563293\n",
      "training: 60 batch 264 batch_loss: 0.20642128586769104\n",
      "training: 60 batch 265 batch_loss: 0.20376801490783691\n",
      "training: 60 batch 266 batch_loss: 0.20926055312156677\n",
      "training: 60 batch 267 batch_loss: 0.20192116498947144\n",
      "training: 60 batch 268 batch_loss: 0.19503891468048096\n",
      "training: 60 batch 269 batch_loss: 0.19796782732009888\n",
      "training: 60 batch 270 batch_loss: 0.20023208856582642\n",
      "training: 60 batch 271 batch_loss: 0.20060747861862183\n",
      "training: 60 batch 272 batch_loss: 0.20686864852905273\n",
      "training: 60 batch 273 batch_loss: 0.20249620079994202\n",
      "training: 60 batch 274 batch_loss: 0.205452561378479\n",
      "training: 60 batch 275 batch_loss: 0.2010103464126587\n",
      "training: 60 batch 276 batch_loss: 0.20037972927093506\n",
      "training: 60 batch 277 batch_loss: 0.20046967267990112\n",
      "training: 60 batch 278 batch_loss: 0.20388725399971008\n",
      "training: 60 batch 279 batch_loss: 0.2000715136528015\n",
      "training: 60 batch 280 batch_loss: 0.20078879594802856\n",
      "training: 60 batch 281 batch_loss: 0.2043761909008026\n",
      "training: 60 batch 282 batch_loss: 0.20062434673309326\n",
      "training: 60 batch 283 batch_loss: 0.19864925742149353\n",
      "training: 60 batch 284 batch_loss: 0.2088567614555359\n",
      "training: 60 batch 285 batch_loss: 0.19758820533752441\n",
      "training: 60 batch 286 batch_loss: 0.20559504628181458\n",
      "training: 60 batch 287 batch_loss: 0.2025679051876068\n",
      "training: 60 batch 288 batch_loss: 0.20609772205352783\n",
      "training: 60 batch 289 batch_loss: 0.203413724899292\n",
      "training: 60 batch 290 batch_loss: 0.20219957828521729\n",
      "training: 60 batch 291 batch_loss: 0.20534032583236694\n",
      "training: 60 batch 292 batch_loss: 0.20475637912750244\n",
      "training: 60 batch 293 batch_loss: 0.2010415494441986\n",
      "training: 60 batch 294 batch_loss: 0.20306381583213806\n",
      "training: 60 batch 295 batch_loss: 0.20138689875602722\n",
      "training: 60 batch 296 batch_loss: 0.2015380859375\n",
      "training: 60 batch 297 batch_loss: 0.20416805148124695\n",
      "training: 60 batch 298 batch_loss: 0.1983439326286316\n",
      "training: 60 batch 299 batch_loss: 0.2038549780845642\n",
      "training: 60 batch 300 batch_loss: 0.20546740293502808\n",
      "training: 60 batch 301 batch_loss: 0.20484790205955505\n",
      "training: 60 batch 302 batch_loss: 0.20298150181770325\n",
      "training: 60 batch 303 batch_loss: 0.20271778106689453\n",
      "training: 60 batch 304 batch_loss: 0.2005656361579895\n",
      "training: 60 batch 305 batch_loss: 0.20484650135040283\n",
      "training: 60 batch 306 batch_loss: 0.19785088300704956\n",
      "training: 60 batch 307 batch_loss: 0.20632943511009216\n",
      "training: 60 batch 308 batch_loss: 0.2075175940990448\n",
      "training: 60 batch 309 batch_loss: 0.20421400666236877\n",
      "training: 60 batch 310 batch_loss: 0.20929700136184692\n",
      "training: 60 batch 311 batch_loss: 0.20032349228858948\n",
      "training: 60 batch 312 batch_loss: 0.20468202233314514\n",
      "training: 60 batch 313 batch_loss: 0.2039969265460968\n",
      "training: 60 batch 314 batch_loss: 0.200067400932312\n",
      "training: 60 batch 315 batch_loss: 0.2042672038078308\n",
      "training: 60 batch 316 batch_loss: 0.2064608633518219\n",
      "training: 60 batch 317 batch_loss: 0.20797017216682434\n",
      "training: 60 batch 318 batch_loss: 0.20434552431106567\n",
      "training: 60 batch 319 batch_loss: 0.20018690824508667\n",
      "training: 60 batch 320 batch_loss: 0.20708417892456055\n",
      "training: 60 batch 321 batch_loss: 0.20589002966880798\n",
      "training: 60 batch 322 batch_loss: 0.20242762565612793\n",
      "training: 60 batch 323 batch_loss: 0.2026466727256775\n",
      "training: 60 batch 324 batch_loss: 0.20292383432388306\n",
      "training: 60 batch 325 batch_loss: 0.20428916811943054\n",
      "training: 60 batch 326 batch_loss: 0.20458799600601196\n",
      "training: 60 batch 327 batch_loss: 0.20477479696273804\n",
      "training: 60 batch 328 batch_loss: 0.20389455556869507\n",
      "training: 60 batch 329 batch_loss: 0.20590350031852722\n",
      "training: 60 batch 330 batch_loss: 0.20243379473686218\n",
      "training: 60 batch 331 batch_loss: 0.20359128713607788\n",
      "training: 60 batch 332 batch_loss: 0.2059570848941803\n",
      "training: 60 batch 333 batch_loss: 0.2043340802192688\n",
      "training: 60 batch 334 batch_loss: 0.20271623134613037\n",
      "training: 60 batch 335 batch_loss: 0.20214691758155823\n",
      "training: 60 batch 336 batch_loss: 0.20294225215911865\n",
      "training: 60 batch 337 batch_loss: 0.20424002408981323\n",
      "training: 60 batch 338 batch_loss: 0.20263126492500305\n",
      "training: 60 batch 339 batch_loss: 0.2060163915157318\n",
      "training: 60 batch 340 batch_loss: 0.19965627789497375\n",
      "training: 60 batch 341 batch_loss: 0.20351725816726685\n",
      "training: 60 batch 342 batch_loss: 0.201939195394516\n",
      "training: 60 batch 343 batch_loss: 0.20416796207427979\n",
      "training: 60 batch 344 batch_loss: 0.20185455679893494\n",
      "training: 60 batch 345 batch_loss: 0.20084741711616516\n",
      "training: 60 batch 346 batch_loss: 0.20296943187713623\n",
      "training: 60 batch 347 batch_loss: 0.20212188363075256\n",
      "training: 60 batch 348 batch_loss: 0.2005443274974823\n",
      "training: 60 batch 349 batch_loss: 0.20084479451179504\n",
      "training: 60 batch 350 batch_loss: 0.2026655673980713\n",
      "training: 60 batch 351 batch_loss: 0.204652339220047\n",
      "training: 60 batch 352 batch_loss: 0.20394635200500488\n",
      "training: 60 batch 353 batch_loss: 0.20616650581359863\n",
      "training: 60 batch 354 batch_loss: 0.2058720588684082\n",
      "training: 60 batch 355 batch_loss: 0.2104017734527588\n",
      "training: 60 batch 356 batch_loss: 0.20497235655784607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 60 batch 357 batch_loss: 0.20351403951644897\n",
      "training: 60 batch 358 batch_loss: 0.20158043503761292\n",
      "training: 60 batch 359 batch_loss: 0.20497101545333862\n",
      "training: 60 batch 360 batch_loss: 0.20314976572990417\n",
      "training: 60 batch 361 batch_loss: 0.20915696024894714\n",
      "training: 60 batch 362 batch_loss: 0.202853262424469\n",
      "training: 60 batch 363 batch_loss: 0.20647892355918884\n",
      "training: 60 batch 364 batch_loss: 0.2003362774848938\n",
      "training: 60 batch 365 batch_loss: 0.20297771692276\n",
      "training: 60 batch 366 batch_loss: 0.2057284414768219\n",
      "training: 60 batch 367 batch_loss: 0.20170727372169495\n",
      "training: 60 batch 368 batch_loss: 0.20298683643341064\n",
      "training: 60 batch 369 batch_loss: 0.20419025421142578\n",
      "training: 60 batch 370 batch_loss: 0.20176374912261963\n",
      "training: 60 batch 371 batch_loss: 0.2063138484954834\n",
      "training: 60 batch 372 batch_loss: 0.2033979594707489\n",
      "training: 60 batch 373 batch_loss: 0.20810985565185547\n",
      "training: 60 batch 374 batch_loss: 0.204144686460495\n",
      "training: 60 batch 375 batch_loss: 0.20452427864074707\n",
      "training: 60 batch 376 batch_loss: 0.2036750316619873\n",
      "training: 60 batch 377 batch_loss: 0.20496058464050293\n",
      "training: 60 batch 378 batch_loss: 0.20691558718681335\n",
      "training: 60 batch 379 batch_loss: 0.20477479696273804\n",
      "training: 60 batch 380 batch_loss: 0.20607683062553406\n",
      "training: 60 batch 381 batch_loss: 0.20386356115341187\n",
      "training: 60 batch 382 batch_loss: 0.20477727055549622\n",
      "training: 60 batch 383 batch_loss: 0.20796701312065125\n",
      "training: 60 batch 384 batch_loss: 0.20264983177185059\n",
      "training: 60 batch 385 batch_loss: 0.20498108863830566\n",
      "training: 60 batch 386 batch_loss: 0.20330536365509033\n",
      "training: 60 batch 387 batch_loss: 0.2017502784729004\n",
      "training: 60 batch 388 batch_loss: 0.20768976211547852\n",
      "training: 60 batch 389 batch_loss: 0.20213079452514648\n",
      "training: 60 batch 390 batch_loss: 0.20736417174339294\n",
      "training: 60 batch 391 batch_loss: 0.2063782513141632\n",
      "training: 60 batch 392 batch_loss: 0.20091953873634338\n",
      "training: 60 batch 393 batch_loss: 0.20248571038246155\n",
      "training: 60 batch 394 batch_loss: 0.20629435777664185\n",
      "training: 60 batch 395 batch_loss: 0.20350024104118347\n",
      "training: 60 batch 396 batch_loss: 0.2035285234451294\n",
      "training: 60 batch 397 batch_loss: 0.20531517267227173\n",
      "training: 60 batch 398 batch_loss: 0.20617252588272095\n",
      "training: 60 batch 399 batch_loss: 0.204725980758667\n",
      "training: 60 batch 400 batch_loss: 0.20676279067993164\n",
      "training: 60 batch 401 batch_loss: 0.20293337106704712\n",
      "training: 60 batch 402 batch_loss: 0.2036800980567932\n",
      "training: 60 batch 403 batch_loss: 0.20177045464515686\n",
      "training: 60 batch 404 batch_loss: 0.20618349313735962\n",
      "training: 60 batch 405 batch_loss: 0.20835551619529724\n",
      "training: 60 batch 406 batch_loss: 0.20528000593185425\n",
      "training: 60 batch 407 batch_loss: 0.2080753743648529\n",
      "training: 60 batch 408 batch_loss: 0.2015843391418457\n",
      "training: 60 batch 409 batch_loss: 0.2055714726448059\n",
      "training: 60 batch 410 batch_loss: 0.2029045820236206\n",
      "training: 60 batch 411 batch_loss: 0.20460915565490723\n",
      "training: 60 batch 412 batch_loss: 0.20445600152015686\n",
      "training: 60 batch 413 batch_loss: 0.20127809047698975\n",
      "training: 60 batch 414 batch_loss: 0.20315605401992798\n",
      "training: 60 batch 415 batch_loss: 0.20479938387870789\n",
      "training: 60 batch 416 batch_loss: 0.20471125841140747\n",
      "training: 60 batch 417 batch_loss: 0.20293831825256348\n",
      "training: 60 batch 418 batch_loss: 0.20465731620788574\n",
      "training: 60 batch 419 batch_loss: 0.20520725846290588\n",
      "training: 60 batch 420 batch_loss: 0.20228329300880432\n",
      "training: 60 batch 421 batch_loss: 0.20218437910079956\n",
      "training: 60 batch 422 batch_loss: 0.20432832837104797\n",
      "training: 60 batch 423 batch_loss: 0.20371165871620178\n",
      "training: 60 batch 424 batch_loss: 0.20096024870872498\n",
      "training: 60 batch 425 batch_loss: 0.20619308948516846\n",
      "training: 60 batch 426 batch_loss: 0.20322340726852417\n",
      "training: 60 batch 427 batch_loss: 0.20408540964126587\n",
      "training: 60 batch 428 batch_loss: 0.2067427635192871\n",
      "training: 60 batch 429 batch_loss: 0.20347344875335693\n",
      "training: 60 batch 430 batch_loss: 0.2000119686126709\n",
      "training: 60 batch 431 batch_loss: 0.20369312167167664\n",
      "training: 60 batch 432 batch_loss: 0.20382148027420044\n",
      "training: 60 batch 433 batch_loss: 0.202592670917511\n",
      "training: 60 batch 434 batch_loss: 0.20467740297317505\n",
      "training: 60 batch 435 batch_loss: 0.20359432697296143\n",
      "training: 60 batch 436 batch_loss: 0.20292216539382935\n",
      "training: 60 batch 437 batch_loss: 0.20779281854629517\n",
      "training: 60 batch 438 batch_loss: 0.20671799778938293\n",
      "training: 60 batch 439 batch_loss: 0.20246946811676025\n",
      "training: 60 batch 440 batch_loss: 0.20635926723480225\n",
      "training: 60 batch 441 batch_loss: 0.20804572105407715\n",
      "training: 60 batch 442 batch_loss: 0.20512664318084717\n",
      "training: 60 batch 443 batch_loss: 0.20772784948349\n",
      "training: 60 batch 444 batch_loss: 0.2049531638622284\n",
      "training: 60 batch 445 batch_loss: 0.20637592673301697\n",
      "training: 60 batch 446 batch_loss: 0.20521721243858337\n",
      "training: 60 batch 447 batch_loss: 0.2074446678161621\n",
      "training: 60 batch 448 batch_loss: 0.2083587348461151\n",
      "training: 60 batch 449 batch_loss: 0.20591461658477783\n",
      "training: 60 batch 450 batch_loss: 0.2043118178844452\n",
      "training: 60 batch 451 batch_loss: 0.2068839967250824\n",
      "training: 60 batch 452 batch_loss: 0.208680659532547\n",
      "training: 60 batch 453 batch_loss: 0.19992193579673767\n",
      "training: 60 batch 454 batch_loss: 0.20339131355285645\n",
      "training: 60 batch 455 batch_loss: 0.20121994614601135\n",
      "training: 60 batch 456 batch_loss: 0.20695602893829346\n",
      "training: 60 batch 457 batch_loss: 0.20221835374832153\n",
      "training: 60 batch 458 batch_loss: 0.2090260088443756\n",
      "training: 60 batch 459 batch_loss: 0.20236283540725708\n",
      "training: 60 batch 460 batch_loss: 0.20737895369529724\n",
      "training: 60 batch 461 batch_loss: 0.20331791043281555\n",
      "training: 60 batch 462 batch_loss: 0.2029939889907837\n",
      "training: 60 batch 463 batch_loss: 0.20244506001472473\n",
      "training: 60 batch 464 batch_loss: 0.20277467370033264\n",
      "training: 60 batch 465 batch_loss: 0.20677858591079712\n",
      "training: 60 batch 466 batch_loss: 0.20062652230262756\n",
      "training: 60 batch 467 batch_loss: 0.20593029260635376\n",
      "training: 60 batch 468 batch_loss: 0.2046097218990326\n",
      "training: 60 batch 469 batch_loss: 0.19912442564964294\n",
      "training: 60 batch 470 batch_loss: 0.20219686627388\n",
      "training: 60 batch 471 batch_loss: 0.20675742626190186\n",
      "training: 60 batch 472 batch_loss: 0.20465996861457825\n",
      "training: 60 batch 473 batch_loss: 0.20235532522201538\n",
      "training: 60 batch 474 batch_loss: 0.20412960648536682\n",
      "training: 60 batch 475 batch_loss: 0.20459720492362976\n",
      "training: 60 batch 476 batch_loss: 0.2011064887046814\n",
      "training: 60 batch 477 batch_loss: 0.20575782656669617\n",
      "training: 60 batch 478 batch_loss: 0.20498451590538025\n",
      "training: 60 batch 479 batch_loss: 0.2038184404373169\n",
      "training: 60 batch 480 batch_loss: 0.20697182416915894\n",
      "training: 60 batch 481 batch_loss: 0.20151904225349426\n",
      "training: 60 batch 482 batch_loss: 0.20351949334144592\n",
      "training: 60 batch 483 batch_loss: 0.2014775574207306\n",
      "training: 60 batch 484 batch_loss: 0.19987094402313232\n",
      "training: 60 batch 485 batch_loss: 0.20160475373268127\n",
      "training: 60 batch 486 batch_loss: 0.20264887809753418\n",
      "training: 60 batch 487 batch_loss: 0.20367920398712158\n",
      "training: 60 batch 488 batch_loss: 0.2056768238544464\n",
      "training: 60 batch 489 batch_loss: 0.20129066705703735\n",
      "training: 60 batch 490 batch_loss: 0.20654910802841187\n",
      "training: 60 batch 491 batch_loss: 0.20200324058532715\n",
      "training: 60 batch 492 batch_loss: 0.20325616002082825\n",
      "training: 60 batch 493 batch_loss: 0.2082550823688507\n",
      "training: 60 batch 494 batch_loss: 0.20218434929847717\n",
      "training: 60 batch 495 batch_loss: 0.2026352882385254\n",
      "training: 60 batch 496 batch_loss: 0.20397043228149414\n",
      "training: 60 batch 497 batch_loss: 0.1983238160610199\n",
      "training: 60 batch 498 batch_loss: 0.20105710625648499\n",
      "training: 60 batch 499 batch_loss: 0.2099757194519043\n",
      "training: 60 batch 500 batch_loss: 0.20587211847305298\n",
      "training: 60 batch 501 batch_loss: 0.20355933904647827\n",
      "training: 60 batch 502 batch_loss: 0.2023475468158722\n",
      "training: 60 batch 503 batch_loss: 0.20312228798866272\n",
      "training: 60 batch 504 batch_loss: 0.2078186571598053\n",
      "training: 60 batch 505 batch_loss: 0.2016606330871582\n",
      "training: 60 batch 506 batch_loss: 0.20749947428703308\n",
      "training: 60 batch 507 batch_loss: 0.2041204571723938\n",
      "training: 60 batch 508 batch_loss: 0.20726147294044495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 60 batch 509 batch_loss: 0.20587411522865295\n",
      "training: 60 batch 510 batch_loss: 0.20653167366981506\n",
      "training: 60 batch 511 batch_loss: 0.20710700750350952\n",
      "training: 60 batch 512 batch_loss: 0.2004079520702362\n",
      "training: 60 batch 513 batch_loss: 0.20594504475593567\n",
      "training: 60 batch 514 batch_loss: 0.2080014944076538\n",
      "training: 60 batch 515 batch_loss: 0.2059047520160675\n",
      "training: 60 batch 516 batch_loss: 0.2067362666130066\n",
      "training: 60 batch 517 batch_loss: 0.2030494511127472\n",
      "training: 60 batch 518 batch_loss: 0.20211032032966614\n",
      "training: 60 batch 519 batch_loss: 0.20362237095832825\n",
      "training: 60 batch 520 batch_loss: 0.2023276686668396\n",
      "training: 60 batch 521 batch_loss: 0.20736467838287354\n",
      "training: 60 batch 522 batch_loss: 0.20780324935913086\n",
      "training: 60 batch 523 batch_loss: 0.2064536213874817\n",
      "training: 60 batch 524 batch_loss: 0.20449715852737427\n",
      "training: 60 batch 525 batch_loss: 0.2060125470161438\n",
      "training: 60 batch 526 batch_loss: 0.19979986548423767\n",
      "training: 60 batch 527 batch_loss: 0.20354685187339783\n",
      "training: 60 batch 528 batch_loss: 0.20961976051330566\n",
      "training: 60 batch 529 batch_loss: 0.20351651310920715\n",
      "training: 60 batch 530 batch_loss: 0.20423021912574768\n",
      "training: 60 batch 531 batch_loss: 0.20441493391990662\n",
      "training: 60 batch 532 batch_loss: 0.20484977960586548\n",
      "training: 60 batch 533 batch_loss: 0.2096450924873352\n",
      "training: 60 batch 534 batch_loss: 0.202042818069458\n",
      "training: 60 batch 535 batch_loss: 0.2056545913219452\n",
      "training: 60 batch 536 batch_loss: 0.2045128345489502\n",
      "training: 60 batch 537 batch_loss: 0.203226238489151\n",
      "training: 60 batch 538 batch_loss: 0.20184385776519775\n",
      "training: 60 batch 539 batch_loss: 0.2028224766254425\n",
      "training: 60 batch 540 batch_loss: 0.20175641775131226\n",
      "training: 60 batch 541 batch_loss: 0.20740553736686707\n",
      "training: 60 batch 542 batch_loss: 0.20538854598999023\n",
      "training: 60 batch 543 batch_loss: 0.20368310809135437\n",
      "training: 60 batch 544 batch_loss: 0.20607203245162964\n",
      "training: 60 batch 545 batch_loss: 0.20205897092819214\n",
      "training: 60 batch 546 batch_loss: 0.20608434081077576\n",
      "training: 60 batch 547 batch_loss: 0.20216262340545654\n",
      "training: 60 batch 548 batch_loss: 0.20513653755187988\n",
      "training: 60 batch 549 batch_loss: 0.20593702793121338\n",
      "training: 60 batch 550 batch_loss: 0.2056926190853119\n",
      "training: 60 batch 551 batch_loss: 0.20350360870361328\n",
      "training: 60 batch 552 batch_loss: 0.20406782627105713\n",
      "training: 60 batch 553 batch_loss: 0.2073592245578766\n",
      "training: 60 batch 554 batch_loss: 0.20361310243606567\n",
      "training: 60 batch 555 batch_loss: 0.20380359888076782\n",
      "training: 60 batch 556 batch_loss: 0.20345568656921387\n",
      "training: 60 batch 557 batch_loss: 0.20729205012321472\n",
      "training: 60 batch 558 batch_loss: 0.2046985626220703\n",
      "training: 60 batch 559 batch_loss: 0.20477283000946045\n",
      "training: 60 batch 560 batch_loss: 0.20236921310424805\n",
      "training: 60 batch 561 batch_loss: 0.20321998000144958\n",
      "training: 60 batch 562 batch_loss: 0.2043045461177826\n",
      "training: 60 batch 563 batch_loss: 0.20525124669075012\n",
      "training: 60 batch 564 batch_loss: 0.20873862504959106\n",
      "training: 60 batch 565 batch_loss: 0.21066191792488098\n",
      "training: 60 batch 566 batch_loss: 0.20486482977867126\n",
      "training: 60 batch 567 batch_loss: 0.20411720871925354\n",
      "training: 60 batch 568 batch_loss: 0.2102663815021515\n",
      "training: 60 batch 569 batch_loss: 0.20160314440727234\n",
      "training: 60 batch 570 batch_loss: 0.20883709192276\n",
      "training: 60 batch 571 batch_loss: 0.20799461007118225\n",
      "training: 60 batch 572 batch_loss: 0.2059030830860138\n",
      "training: 60 batch 573 batch_loss: 0.20523962378501892\n",
      "training: 60 batch 574 batch_loss: 0.20495393872261047\n",
      "training: 60 batch 575 batch_loss: 0.2055029571056366\n",
      "training: 60 batch 576 batch_loss: 0.21151486039161682\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 60, Hit Ratio:0.03272561318884851 | Precision:0.04828467512041679 | Recall:0.06382045976090546 | NDCG:0.06270079397868701\n",
      "*Best Performance* \n",
      "Epoch: 56, Hit Ratio:0.032922153414636585 | Precision:0.04857465840951539 | Recall:0.06427363564028983 | MDCG:0.06308737979754037\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 61 batch 0 batch_loss: 0.20707440376281738\n",
      "training: 61 batch 1 batch_loss: 0.20503801107406616\n",
      "training: 61 batch 2 batch_loss: 0.20457851886749268\n",
      "training: 61 batch 3 batch_loss: 0.20346525311470032\n",
      "training: 61 batch 4 batch_loss: 0.20566385984420776\n",
      "training: 61 batch 5 batch_loss: 0.201117604970932\n",
      "training: 61 batch 6 batch_loss: 0.20439302921295166\n",
      "training: 61 batch 7 batch_loss: 0.2026832401752472\n",
      "training: 61 batch 8 batch_loss: 0.20207929611206055\n",
      "training: 61 batch 9 batch_loss: 0.20574170351028442\n",
      "training: 61 batch 10 batch_loss: 0.20232370495796204\n",
      "training: 61 batch 11 batch_loss: 0.207625150680542\n",
      "training: 61 batch 12 batch_loss: 0.20382332801818848\n",
      "training: 61 batch 13 batch_loss: 0.19933077692985535\n",
      "training: 61 batch 14 batch_loss: 0.20370346307754517\n",
      "training: 61 batch 15 batch_loss: 0.2058890163898468\n",
      "training: 61 batch 16 batch_loss: 0.1979883909225464\n",
      "training: 61 batch 17 batch_loss: 0.2001197636127472\n",
      "training: 61 batch 18 batch_loss: 0.20385435223579407\n",
      "training: 61 batch 19 batch_loss: 0.2002353072166443\n",
      "training: 61 batch 20 batch_loss: 0.20395460724830627\n",
      "training: 61 batch 21 batch_loss: 0.20220670104026794\n",
      "training: 61 batch 22 batch_loss: 0.20413818955421448\n",
      "training: 61 batch 23 batch_loss: 0.20290333032608032\n",
      "training: 61 batch 24 batch_loss: 0.20500022172927856\n",
      "training: 61 batch 25 batch_loss: 0.2033831775188446\n",
      "training: 61 batch 26 batch_loss: 0.20216333866119385\n",
      "training: 61 batch 27 batch_loss: 0.2025766372680664\n",
      "training: 61 batch 28 batch_loss: 0.20601478219032288\n",
      "training: 61 batch 29 batch_loss: 0.20458942651748657\n",
      "training: 61 batch 30 batch_loss: 0.2082790732383728\n",
      "training: 61 batch 31 batch_loss: 0.20776966214179993\n",
      "training: 61 batch 32 batch_loss: 0.20351317524909973\n",
      "training: 61 batch 33 batch_loss: 0.20055937767028809\n",
      "training: 61 batch 34 batch_loss: 0.20521363615989685\n",
      "training: 61 batch 35 batch_loss: 0.20367035269737244\n",
      "training: 61 batch 36 batch_loss: 0.20020556449890137\n",
      "training: 61 batch 37 batch_loss: 0.2089066505432129\n",
      "training: 61 batch 38 batch_loss: 0.20420557260513306\n",
      "training: 61 batch 39 batch_loss: 0.203656405210495\n",
      "training: 61 batch 40 batch_loss: 0.20084303617477417\n",
      "training: 61 batch 41 batch_loss: 0.2000449001789093\n",
      "training: 61 batch 42 batch_loss: 0.19883224368095398\n",
      "training: 61 batch 43 batch_loss: 0.20123499631881714\n",
      "training: 61 batch 44 batch_loss: 0.20325982570648193\n",
      "training: 61 batch 45 batch_loss: 0.20086973905563354\n",
      "training: 61 batch 46 batch_loss: 0.20262548327445984\n",
      "training: 61 batch 47 batch_loss: 0.20402616262435913\n",
      "training: 61 batch 48 batch_loss: 0.20375052094459534\n",
      "training: 61 batch 49 batch_loss: 0.20188167691230774\n",
      "training: 61 batch 50 batch_loss: 0.2041492462158203\n",
      "training: 61 batch 51 batch_loss: 0.2033483386039734\n",
      "training: 61 batch 52 batch_loss: 0.20090311765670776\n",
      "training: 61 batch 53 batch_loss: 0.2056596279144287\n",
      "training: 61 batch 54 batch_loss: 0.2071199119091034\n",
      "training: 61 batch 55 batch_loss: 0.20480689406394958\n",
      "training: 61 batch 56 batch_loss: 0.20117858052253723\n",
      "training: 61 batch 57 batch_loss: 0.20029139518737793\n",
      "training: 61 batch 58 batch_loss: 0.20334279537200928\n",
      "training: 61 batch 59 batch_loss: 0.20423749089241028\n",
      "training: 61 batch 60 batch_loss: 0.20583224296569824\n",
      "training: 61 batch 61 batch_loss: 0.20188245177268982\n",
      "training: 61 batch 62 batch_loss: 0.20024633407592773\n",
      "training: 61 batch 63 batch_loss: 0.20358043909072876\n",
      "training: 61 batch 64 batch_loss: 0.2019558548927307\n",
      "training: 61 batch 65 batch_loss: 0.1999821662902832\n",
      "training: 61 batch 66 batch_loss: 0.20517930388450623\n",
      "training: 61 batch 67 batch_loss: 0.20337200164794922\n",
      "training: 61 batch 68 batch_loss: 0.2091725468635559\n",
      "training: 61 batch 69 batch_loss: 0.20453500747680664\n",
      "training: 61 batch 70 batch_loss: 0.20435699820518494\n",
      "training: 61 batch 71 batch_loss: 0.2022656798362732\n",
      "training: 61 batch 72 batch_loss: 0.205857515335083\n",
      "training: 61 batch 73 batch_loss: 0.20058107376098633\n",
      "training: 61 batch 74 batch_loss: 0.20602929592132568\n",
      "training: 61 batch 75 batch_loss: 0.2035112977027893\n",
      "training: 61 batch 76 batch_loss: 0.20521998405456543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 61 batch 77 batch_loss: 0.20740753412246704\n",
      "training: 61 batch 78 batch_loss: 0.20087125897407532\n",
      "training: 61 batch 79 batch_loss: 0.2037467658519745\n",
      "training: 61 batch 80 batch_loss: 0.2068464457988739\n",
      "training: 61 batch 81 batch_loss: 0.20309871435165405\n",
      "training: 61 batch 82 batch_loss: 0.2008349895477295\n",
      "training: 61 batch 83 batch_loss: 0.2059379518032074\n",
      "training: 61 batch 84 batch_loss: 0.20303285121917725\n",
      "training: 61 batch 85 batch_loss: 0.2041182518005371\n",
      "training: 61 batch 86 batch_loss: 0.2071746289730072\n",
      "training: 61 batch 87 batch_loss: 0.20120126008987427\n",
      "training: 61 batch 88 batch_loss: 0.20111939311027527\n",
      "training: 61 batch 89 batch_loss: 0.20208603143692017\n",
      "training: 61 batch 90 batch_loss: 0.20450377464294434\n",
      "training: 61 batch 91 batch_loss: 0.202998548746109\n",
      "training: 61 batch 92 batch_loss: 0.20127305388450623\n",
      "training: 61 batch 93 batch_loss: 0.20323124527931213\n",
      "training: 61 batch 94 batch_loss: 0.20470461249351501\n",
      "training: 61 batch 95 batch_loss: 0.20126014947891235\n",
      "training: 61 batch 96 batch_loss: 0.2058563232421875\n",
      "training: 61 batch 97 batch_loss: 0.2040567696094513\n",
      "training: 61 batch 98 batch_loss: 0.202234148979187\n",
      "training: 61 batch 99 batch_loss: 0.2071194350719452\n",
      "training: 61 batch 100 batch_loss: 0.20585063099861145\n",
      "training: 61 batch 101 batch_loss: 0.2026957869529724\n",
      "training: 61 batch 102 batch_loss: 0.20150670409202576\n",
      "training: 61 batch 103 batch_loss: 0.20063096284866333\n",
      "training: 61 batch 104 batch_loss: 0.2054293155670166\n",
      "training: 61 batch 105 batch_loss: 0.2017344832420349\n",
      "training: 61 batch 106 batch_loss: 0.2052021026611328\n",
      "training: 61 batch 107 batch_loss: 0.2000894546508789\n",
      "training: 61 batch 108 batch_loss: 0.20478558540344238\n",
      "training: 61 batch 109 batch_loss: 0.20502597093582153\n",
      "training: 61 batch 110 batch_loss: 0.20549672842025757\n",
      "training: 61 batch 111 batch_loss: 0.20132824778556824\n",
      "training: 61 batch 112 batch_loss: 0.20147231221199036\n",
      "training: 61 batch 113 batch_loss: 0.20354685187339783\n",
      "training: 61 batch 114 batch_loss: 0.20231813192367554\n",
      "training: 61 batch 115 batch_loss: 0.2042773962020874\n",
      "training: 61 batch 116 batch_loss: 0.20290187001228333\n",
      "training: 61 batch 117 batch_loss: 0.20886927843093872\n",
      "training: 61 batch 118 batch_loss: 0.2065005898475647\n",
      "training: 61 batch 119 batch_loss: 0.20266887545585632\n",
      "training: 61 batch 120 batch_loss: 0.20619016885757446\n",
      "training: 61 batch 121 batch_loss: 0.20262056589126587\n",
      "training: 61 batch 122 batch_loss: 0.20345249772071838\n",
      "training: 61 batch 123 batch_loss: 0.20465824007987976\n",
      "training: 61 batch 124 batch_loss: 0.20482027530670166\n",
      "training: 61 batch 125 batch_loss: 0.2055039405822754\n",
      "training: 61 batch 126 batch_loss: 0.20174044370651245\n",
      "training: 61 batch 127 batch_loss: 0.2044386863708496\n",
      "training: 61 batch 128 batch_loss: 0.20342129468917847\n",
      "training: 61 batch 129 batch_loss: 0.20731976628303528\n",
      "training: 61 batch 130 batch_loss: 0.20788675546646118\n",
      "training: 61 batch 131 batch_loss: 0.20618689060211182\n",
      "training: 61 batch 132 batch_loss: 0.2041575312614441\n",
      "training: 61 batch 133 batch_loss: 0.2046654224395752\n",
      "training: 61 batch 134 batch_loss: 0.20101627707481384\n",
      "training: 61 batch 135 batch_loss: 0.20200607180595398\n",
      "training: 61 batch 136 batch_loss: 0.20709070563316345\n",
      "training: 61 batch 137 batch_loss: 0.2051425278186798\n",
      "training: 61 batch 138 batch_loss: 0.20244604349136353\n",
      "training: 61 batch 139 batch_loss: 0.20546174049377441\n",
      "training: 61 batch 140 batch_loss: 0.20604634284973145\n",
      "training: 61 batch 141 batch_loss: 0.20311307907104492\n",
      "training: 61 batch 142 batch_loss: 0.2041303515434265\n",
      "training: 61 batch 143 batch_loss: 0.1997259259223938\n",
      "training: 61 batch 144 batch_loss: 0.20860373973846436\n",
      "training: 61 batch 145 batch_loss: 0.20372578501701355\n",
      "training: 61 batch 146 batch_loss: 0.2047283947467804\n",
      "training: 61 batch 147 batch_loss: 0.20275989174842834\n",
      "training: 61 batch 148 batch_loss: 0.19982409477233887\n",
      "training: 61 batch 149 batch_loss: 0.20243176817893982\n",
      "training: 61 batch 150 batch_loss: 0.20651906728744507\n",
      "training: 61 batch 151 batch_loss: 0.2051697075366974\n",
      "training: 61 batch 152 batch_loss: 0.206712007522583\n",
      "training: 61 batch 153 batch_loss: 0.19984236359596252\n",
      "training: 61 batch 154 batch_loss: 0.20395731925964355\n",
      "training: 61 batch 155 batch_loss: 0.20668309926986694\n",
      "training: 61 batch 156 batch_loss: 0.20789670944213867\n",
      "training: 61 batch 157 batch_loss: 0.20183169841766357\n",
      "training: 61 batch 158 batch_loss: 0.20773988962173462\n",
      "training: 61 batch 159 batch_loss: 0.2001662254333496\n",
      "training: 61 batch 160 batch_loss: 0.20230427384376526\n",
      "training: 61 batch 161 batch_loss: 0.20841899514198303\n",
      "training: 61 batch 162 batch_loss: 0.20274969935417175\n",
      "training: 61 batch 163 batch_loss: 0.2076932191848755\n",
      "training: 61 batch 164 batch_loss: 0.20096608996391296\n",
      "training: 61 batch 165 batch_loss: 0.2034345269203186\n",
      "training: 61 batch 166 batch_loss: 0.20369493961334229\n",
      "training: 61 batch 167 batch_loss: 0.20645776391029358\n",
      "training: 61 batch 168 batch_loss: 0.19844412803649902\n",
      "training: 61 batch 169 batch_loss: 0.20543301105499268\n",
      "training: 61 batch 170 batch_loss: 0.2107408344745636\n",
      "training: 61 batch 171 batch_loss: 0.20561733841896057\n",
      "training: 61 batch 172 batch_loss: 0.2011118233203888\n",
      "training: 61 batch 173 batch_loss: 0.20254051685333252\n",
      "training: 61 batch 174 batch_loss: 0.2032349705696106\n",
      "training: 61 batch 175 batch_loss: 0.20622283220291138\n",
      "training: 61 batch 176 batch_loss: 0.20720288157463074\n",
      "training: 61 batch 177 batch_loss: 0.20820671319961548\n",
      "training: 61 batch 178 batch_loss: 0.202693372964859\n",
      "training: 61 batch 179 batch_loss: 0.20196416974067688\n",
      "training: 61 batch 180 batch_loss: 0.20270493626594543\n",
      "training: 61 batch 181 batch_loss: 0.2033151388168335\n",
      "training: 61 batch 182 batch_loss: 0.20378664135932922\n",
      "training: 61 batch 183 batch_loss: 0.19845765829086304\n",
      "training: 61 batch 184 batch_loss: 0.20096343755722046\n",
      "training: 61 batch 185 batch_loss: 0.20136713981628418\n",
      "training: 61 batch 186 batch_loss: 0.2072782814502716\n",
      "training: 61 batch 187 batch_loss: 0.20663556456565857\n",
      "training: 61 batch 188 batch_loss: 0.20327982306480408\n",
      "training: 61 batch 189 batch_loss: 0.1998843550682068\n",
      "training: 61 batch 190 batch_loss: 0.20204854011535645\n",
      "training: 61 batch 191 batch_loss: 0.20279207825660706\n",
      "training: 61 batch 192 batch_loss: 0.19833311438560486\n",
      "training: 61 batch 193 batch_loss: 0.20324093103408813\n",
      "training: 61 batch 194 batch_loss: 0.20704993605613708\n",
      "training: 61 batch 195 batch_loss: 0.20158126950263977\n",
      "training: 61 batch 196 batch_loss: 0.2044687271118164\n",
      "training: 61 batch 197 batch_loss: 0.20360875129699707\n",
      "training: 61 batch 198 batch_loss: 0.20929181575775146\n",
      "training: 61 batch 199 batch_loss: 0.20140108466148376\n",
      "training: 61 batch 200 batch_loss: 0.20732495188713074\n",
      "training: 61 batch 201 batch_loss: 0.20261454582214355\n",
      "training: 61 batch 202 batch_loss: 0.2058994174003601\n",
      "training: 61 batch 203 batch_loss: 0.20310115814208984\n",
      "training: 61 batch 204 batch_loss: 0.20746049284934998\n",
      "training: 61 batch 205 batch_loss: 0.20153915882110596\n",
      "training: 61 batch 206 batch_loss: 0.20381703972816467\n",
      "training: 61 batch 207 batch_loss: 0.20843371748924255\n",
      "training: 61 batch 208 batch_loss: 0.20452281832695007\n",
      "training: 61 batch 209 batch_loss: 0.20556268095970154\n",
      "training: 61 batch 210 batch_loss: 0.20611903071403503\n",
      "training: 61 batch 211 batch_loss: 0.20172131061553955\n",
      "training: 61 batch 212 batch_loss: 0.19966626167297363\n",
      "training: 61 batch 213 batch_loss: 0.20199289917945862\n",
      "training: 61 batch 214 batch_loss: 0.20520156621932983\n",
      "training: 61 batch 215 batch_loss: 0.20283746719360352\n",
      "training: 61 batch 216 batch_loss: 0.20478522777557373\n",
      "training: 61 batch 217 batch_loss: 0.20735660195350647\n",
      "training: 61 batch 218 batch_loss: 0.20457109808921814\n",
      "training: 61 batch 219 batch_loss: 0.20678874850273132\n",
      "training: 61 batch 220 batch_loss: 0.2086494266986847\n",
      "training: 61 batch 221 batch_loss: 0.20412445068359375\n",
      "training: 61 batch 222 batch_loss: 0.20564433932304382\n",
      "training: 61 batch 223 batch_loss: 0.20320898294448853\n",
      "training: 61 batch 224 batch_loss: 0.20180010795593262\n",
      "training: 61 batch 225 batch_loss: 0.2036791741847992\n",
      "training: 61 batch 226 batch_loss: 0.20582181215286255\n",
      "training: 61 batch 227 batch_loss: 0.204889178276062\n",
      "training: 61 batch 228 batch_loss: 0.20313474535942078\n",
      "training: 61 batch 229 batch_loss: 0.2044694721698761\n",
      "training: 61 batch 230 batch_loss: 0.20498722791671753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 61 batch 231 batch_loss: 0.2052588164806366\n",
      "training: 61 batch 232 batch_loss: 0.2028687596321106\n",
      "training: 61 batch 233 batch_loss: 0.20243576169013977\n",
      "training: 61 batch 234 batch_loss: 0.20411944389343262\n",
      "training: 61 batch 235 batch_loss: 0.20368635654449463\n",
      "training: 61 batch 236 batch_loss: 0.20549148321151733\n",
      "training: 61 batch 237 batch_loss: 0.20141366124153137\n",
      "training: 61 batch 238 batch_loss: 0.20692571997642517\n",
      "training: 61 batch 239 batch_loss: 0.20259979367256165\n",
      "training: 61 batch 240 batch_loss: 0.20442968606948853\n",
      "training: 61 batch 241 batch_loss: 0.20300695300102234\n",
      "training: 61 batch 242 batch_loss: 0.20448905229568481\n",
      "training: 61 batch 243 batch_loss: 0.2069290280342102\n",
      "training: 61 batch 244 batch_loss: 0.20424741506576538\n",
      "training: 61 batch 245 batch_loss: 0.20404189825057983\n",
      "training: 61 batch 246 batch_loss: 0.20835036039352417\n",
      "training: 61 batch 247 batch_loss: 0.20196613669395447\n",
      "training: 61 batch 248 batch_loss: 0.20134982466697693\n",
      "training: 61 batch 249 batch_loss: 0.20008030533790588\n",
      "training: 61 batch 250 batch_loss: 0.20789504051208496\n",
      "training: 61 batch 251 batch_loss: 0.21106573939323425\n",
      "training: 61 batch 252 batch_loss: 0.2063809633255005\n",
      "training: 61 batch 253 batch_loss: 0.20826923847198486\n",
      "training: 61 batch 254 batch_loss: 0.20533224940299988\n",
      "training: 61 batch 255 batch_loss: 0.20821723341941833\n",
      "training: 61 batch 256 batch_loss: 0.20492926239967346\n",
      "training: 61 batch 257 batch_loss: 0.20964980125427246\n",
      "training: 61 batch 258 batch_loss: 0.2071290910243988\n",
      "training: 61 batch 259 batch_loss: 0.20490217208862305\n",
      "training: 61 batch 260 batch_loss: 0.20650127530097961\n",
      "training: 61 batch 261 batch_loss: 0.2030331790447235\n",
      "training: 61 batch 262 batch_loss: 0.20341312885284424\n",
      "training: 61 batch 263 batch_loss: 0.20923110842704773\n",
      "training: 61 batch 264 batch_loss: 0.20566591620445251\n",
      "training: 61 batch 265 batch_loss: 0.20333552360534668\n",
      "training: 61 batch 266 batch_loss: 0.20223602652549744\n",
      "training: 61 batch 267 batch_loss: 0.20469537377357483\n",
      "training: 61 batch 268 batch_loss: 0.198820561170578\n",
      "training: 61 batch 269 batch_loss: 0.20682504773139954\n",
      "training: 61 batch 270 batch_loss: 0.20307183265686035\n",
      "training: 61 batch 271 batch_loss: 0.20125019550323486\n",
      "training: 61 batch 272 batch_loss: 0.20271316170692444\n",
      "training: 61 batch 273 batch_loss: 0.2045319378376007\n",
      "training: 61 batch 274 batch_loss: 0.20538237690925598\n",
      "training: 61 batch 275 batch_loss: 0.21012359857559204\n",
      "training: 61 batch 276 batch_loss: 0.2039552927017212\n",
      "training: 61 batch 277 batch_loss: 0.2091289460659027\n",
      "training: 61 batch 278 batch_loss: 0.20549127459526062\n",
      "training: 61 batch 279 batch_loss: 0.2032327950000763\n",
      "training: 61 batch 280 batch_loss: 0.20304512977600098\n",
      "training: 61 batch 281 batch_loss: 0.2035754919052124\n",
      "training: 61 batch 282 batch_loss: 0.20343488454818726\n",
      "training: 61 batch 283 batch_loss: 0.20744943618774414\n",
      "training: 61 batch 284 batch_loss: 0.20509448647499084\n",
      "training: 61 batch 285 batch_loss: 0.2029666304588318\n",
      "training: 61 batch 286 batch_loss: 0.20503222942352295\n",
      "training: 61 batch 287 batch_loss: 0.1992034912109375\n",
      "training: 61 batch 288 batch_loss: 0.2031494379043579\n",
      "training: 61 batch 289 batch_loss: 0.20397263765335083\n",
      "training: 61 batch 290 batch_loss: 0.20762887597084045\n",
      "training: 61 batch 291 batch_loss: 0.20795097947120667\n",
      "training: 61 batch 292 batch_loss: 0.2035841941833496\n",
      "training: 61 batch 293 batch_loss: 0.20688721537590027\n",
      "training: 61 batch 294 batch_loss: 0.2019926905632019\n",
      "training: 61 batch 295 batch_loss: 0.20203545689582825\n",
      "training: 61 batch 296 batch_loss: 0.20519495010375977\n",
      "training: 61 batch 297 batch_loss: 0.20226126909255981\n",
      "training: 61 batch 298 batch_loss: 0.2064204216003418\n",
      "training: 61 batch 299 batch_loss: 0.20534023642539978\n",
      "training: 61 batch 300 batch_loss: 0.20495209097862244\n",
      "training: 61 batch 301 batch_loss: 0.20491579174995422\n",
      "training: 61 batch 302 batch_loss: 0.2065747082233429\n",
      "training: 61 batch 303 batch_loss: 0.20364594459533691\n",
      "training: 61 batch 304 batch_loss: 0.20616072416305542\n",
      "training: 61 batch 305 batch_loss: 0.20307683944702148\n",
      "training: 61 batch 306 batch_loss: 0.20831164717674255\n",
      "training: 61 batch 307 batch_loss: 0.20706361532211304\n",
      "training: 61 batch 308 batch_loss: 0.20485243201255798\n",
      "training: 61 batch 309 batch_loss: 0.20348310470581055\n",
      "training: 61 batch 310 batch_loss: 0.20349109172821045\n",
      "training: 61 batch 311 batch_loss: 0.20543432235717773\n",
      "training: 61 batch 312 batch_loss: 0.20128244161605835\n",
      "training: 61 batch 313 batch_loss: 0.20442214608192444\n",
      "training: 61 batch 314 batch_loss: 0.20621931552886963\n",
      "training: 61 batch 315 batch_loss: 0.2070797085762024\n",
      "training: 61 batch 316 batch_loss: 0.2055673897266388\n",
      "training: 61 batch 317 batch_loss: 0.20566365122795105\n",
      "training: 61 batch 318 batch_loss: 0.2064363956451416\n",
      "training: 61 batch 319 batch_loss: 0.20678555965423584\n",
      "training: 61 batch 320 batch_loss: 0.2040049135684967\n",
      "training: 61 batch 321 batch_loss: 0.20183393359184265\n",
      "training: 61 batch 322 batch_loss: 0.20530769228935242\n",
      "training: 61 batch 323 batch_loss: 0.20519495010375977\n",
      "training: 61 batch 324 batch_loss: 0.20382961630821228\n",
      "training: 61 batch 325 batch_loss: 0.2055734395980835\n",
      "training: 61 batch 326 batch_loss: 0.20364922285079956\n",
      "training: 61 batch 327 batch_loss: 0.20632556080818176\n",
      "training: 61 batch 328 batch_loss: 0.20600375533103943\n",
      "training: 61 batch 329 batch_loss: 0.20452487468719482\n",
      "training: 61 batch 330 batch_loss: 0.20588573813438416\n",
      "training: 61 batch 331 batch_loss: 0.20546036958694458\n",
      "training: 61 batch 332 batch_loss: 0.20495596528053284\n",
      "training: 61 batch 333 batch_loss: 0.20696896314620972\n",
      "training: 61 batch 334 batch_loss: 0.20146793127059937\n",
      "training: 61 batch 335 batch_loss: 0.20537248253822327\n",
      "training: 61 batch 336 batch_loss: 0.20109117031097412\n",
      "training: 61 batch 337 batch_loss: 0.20578104257583618\n",
      "training: 61 batch 338 batch_loss: 0.20422986149787903\n",
      "training: 61 batch 339 batch_loss: 0.20551592111587524\n",
      "training: 61 batch 340 batch_loss: 0.2093079388141632\n",
      "training: 61 batch 341 batch_loss: 0.20223093032836914\n",
      "training: 61 batch 342 batch_loss: 0.20691025257110596\n",
      "training: 61 batch 343 batch_loss: 0.20483478903770447\n",
      "training: 61 batch 344 batch_loss: 0.2013821303844452\n",
      "training: 61 batch 345 batch_loss: 0.20269635319709778\n",
      "training: 61 batch 346 batch_loss: 0.20443683862686157\n",
      "training: 61 batch 347 batch_loss: 0.20614638924598694\n",
      "training: 61 batch 348 batch_loss: 0.20560330152511597\n",
      "training: 61 batch 349 batch_loss: 0.208851158618927\n",
      "training: 61 batch 350 batch_loss: 0.203481525182724\n",
      "training: 61 batch 351 batch_loss: 0.2035849690437317\n",
      "training: 61 batch 352 batch_loss: 0.20725446939468384\n",
      "training: 61 batch 353 batch_loss: 0.2014654278755188\n",
      "training: 61 batch 354 batch_loss: 0.20273983478546143\n",
      "training: 61 batch 355 batch_loss: 0.2041436731815338\n",
      "training: 61 batch 356 batch_loss: 0.20958390831947327\n",
      "training: 61 batch 357 batch_loss: 0.1999642550945282\n",
      "training: 61 batch 358 batch_loss: 0.20414003729820251\n",
      "training: 61 batch 359 batch_loss: 0.2056313157081604\n",
      "training: 61 batch 360 batch_loss: 0.20692360401153564\n",
      "training: 61 batch 361 batch_loss: 0.20562699437141418\n",
      "training: 61 batch 362 batch_loss: 0.20574602484703064\n",
      "training: 61 batch 363 batch_loss: 0.2026107907295227\n",
      "training: 61 batch 364 batch_loss: 0.20279821753501892\n",
      "training: 61 batch 365 batch_loss: 0.20779666304588318\n",
      "training: 61 batch 366 batch_loss: 0.2013283669948578\n",
      "training: 61 batch 367 batch_loss: 0.20207756757736206\n",
      "training: 61 batch 368 batch_loss: 0.20101705193519592\n",
      "training: 61 batch 369 batch_loss: 0.2077350616455078\n",
      "training: 61 batch 370 batch_loss: 0.20588117837905884\n",
      "training: 61 batch 371 batch_loss: 0.20265057682991028\n",
      "training: 61 batch 372 batch_loss: 0.2060200572013855\n",
      "training: 61 batch 373 batch_loss: 0.20594057440757751\n",
      "training: 61 batch 374 batch_loss: 0.20491746068000793\n",
      "training: 61 batch 375 batch_loss: 0.20223501324653625\n",
      "training: 61 batch 376 batch_loss: 0.2021612823009491\n",
      "training: 61 batch 377 batch_loss: 0.20418065786361694\n",
      "training: 61 batch 378 batch_loss: 0.20706069469451904\n",
      "training: 61 batch 379 batch_loss: 0.2057930827140808\n",
      "training: 61 batch 380 batch_loss: 0.20704537630081177\n",
      "training: 61 batch 381 batch_loss: 0.2066935896873474\n",
      "training: 61 batch 382 batch_loss: 0.20571091771125793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 61 batch 383 batch_loss: 0.20257925987243652\n",
      "training: 61 batch 384 batch_loss: 0.20744189620018005\n",
      "training: 61 batch 385 batch_loss: 0.20185664296150208\n",
      "training: 61 batch 386 batch_loss: 0.20420929789543152\n",
      "training: 61 batch 387 batch_loss: 0.20793092250823975\n",
      "training: 61 batch 388 batch_loss: 0.20067647099494934\n",
      "training: 61 batch 389 batch_loss: 0.20368552207946777\n",
      "training: 61 batch 390 batch_loss: 0.20764636993408203\n",
      "training: 61 batch 391 batch_loss: 0.20546886324882507\n",
      "training: 61 batch 392 batch_loss: 0.20611536502838135\n",
      "training: 61 batch 393 batch_loss: 0.20194876194000244\n",
      "training: 61 batch 394 batch_loss: 0.20502808690071106\n",
      "training: 61 batch 395 batch_loss: 0.2077764868736267\n",
      "training: 61 batch 396 batch_loss: 0.20588159561157227\n",
      "training: 61 batch 397 batch_loss: 0.20558375120162964\n",
      "training: 61 batch 398 batch_loss: 0.20654210448265076\n",
      "training: 61 batch 399 batch_loss: 0.2122017741203308\n",
      "training: 61 batch 400 batch_loss: 0.19995248317718506\n",
      "training: 61 batch 401 batch_loss: 0.20383384823799133\n",
      "training: 61 batch 402 batch_loss: 0.2050592303276062\n",
      "training: 61 batch 403 batch_loss: 0.2019367516040802\n",
      "training: 61 batch 404 batch_loss: 0.20784401893615723\n",
      "training: 61 batch 405 batch_loss: 0.203774094581604\n",
      "training: 61 batch 406 batch_loss: 0.20788836479187012\n",
      "training: 61 batch 407 batch_loss: 0.2045363187789917\n",
      "training: 61 batch 408 batch_loss: 0.20942050218582153\n",
      "training: 61 batch 409 batch_loss: 0.205294668674469\n",
      "training: 61 batch 410 batch_loss: 0.20619118213653564\n",
      "training: 61 batch 411 batch_loss: 0.20373761653900146\n",
      "training: 61 batch 412 batch_loss: 0.2018800675868988\n",
      "training: 61 batch 413 batch_loss: 0.20535054802894592\n",
      "training: 61 batch 414 batch_loss: 0.20484068989753723\n",
      "training: 61 batch 415 batch_loss: 0.20509377121925354\n",
      "training: 61 batch 416 batch_loss: 0.20921850204467773\n",
      "training: 61 batch 417 batch_loss: 0.20484954118728638\n",
      "training: 61 batch 418 batch_loss: 0.20502421259880066\n",
      "training: 61 batch 419 batch_loss: 0.20347213745117188\n",
      "training: 61 batch 420 batch_loss: 0.2083764672279358\n",
      "training: 61 batch 421 batch_loss: 0.209640234708786\n",
      "training: 61 batch 422 batch_loss: 0.2101140022277832\n",
      "training: 61 batch 423 batch_loss: 0.21071535348892212\n",
      "training: 61 batch 424 batch_loss: 0.19986408948898315\n",
      "training: 61 batch 425 batch_loss: 0.20318782329559326\n",
      "training: 61 batch 426 batch_loss: 0.2021549940109253\n",
      "training: 61 batch 427 batch_loss: 0.20578664541244507\n",
      "training: 61 batch 428 batch_loss: 0.2041885256767273\n",
      "training: 61 batch 429 batch_loss: 0.20776411890983582\n",
      "training: 61 batch 430 batch_loss: 0.20506387948989868\n",
      "training: 61 batch 431 batch_loss: 0.2068938910961151\n",
      "training: 61 batch 432 batch_loss: 0.2066240906715393\n",
      "training: 61 batch 433 batch_loss: 0.2051677405834198\n",
      "training: 61 batch 434 batch_loss: 0.2049192190170288\n",
      "training: 61 batch 435 batch_loss: 0.20520025491714478\n",
      "training: 61 batch 436 batch_loss: 0.20565173029899597\n",
      "training: 61 batch 437 batch_loss: 0.2063176929950714\n",
      "training: 61 batch 438 batch_loss: 0.20723357796669006\n",
      "training: 61 batch 439 batch_loss: 0.2013149857521057\n",
      "training: 61 batch 440 batch_loss: 0.2102181315422058\n",
      "training: 61 batch 441 batch_loss: 0.20418968796730042\n",
      "training: 61 batch 442 batch_loss: 0.20915564894676208\n",
      "training: 61 batch 443 batch_loss: 0.2012980580329895\n",
      "training: 61 batch 444 batch_loss: 0.20183077454566956\n",
      "training: 61 batch 445 batch_loss: 0.20292681455612183\n",
      "training: 61 batch 446 batch_loss: 0.20428988337516785\n",
      "training: 61 batch 447 batch_loss: 0.20919010043144226\n",
      "training: 61 batch 448 batch_loss: 0.20599576830863953\n",
      "training: 61 batch 449 batch_loss: 0.2076275646686554\n",
      "training: 61 batch 450 batch_loss: 0.2065669298171997\n",
      "training: 61 batch 451 batch_loss: 0.20736676454544067\n",
      "training: 61 batch 452 batch_loss: 0.20995602011680603\n",
      "training: 61 batch 453 batch_loss: 0.20691514015197754\n",
      "training: 61 batch 454 batch_loss: 0.20500802993774414\n",
      "training: 61 batch 455 batch_loss: 0.20831739902496338\n",
      "training: 61 batch 456 batch_loss: 0.20232099294662476\n",
      "training: 61 batch 457 batch_loss: 0.20483124256134033\n",
      "training: 61 batch 458 batch_loss: 0.2049579620361328\n",
      "training: 61 batch 459 batch_loss: 0.20682519674301147\n",
      "training: 61 batch 460 batch_loss: 0.21018651127815247\n",
      "training: 61 batch 461 batch_loss: 0.20593595504760742\n",
      "training: 61 batch 462 batch_loss: 0.20639684796333313\n",
      "training: 61 batch 463 batch_loss: 0.20580533146858215\n",
      "training: 61 batch 464 batch_loss: 0.20856016874313354\n",
      "training: 61 batch 465 batch_loss: 0.20252937078475952\n",
      "training: 61 batch 466 batch_loss: 0.20858818292617798\n",
      "training: 61 batch 467 batch_loss: 0.20552802085876465\n",
      "training: 61 batch 468 batch_loss: 0.20632952451705933\n",
      "training: 61 batch 469 batch_loss: 0.20464691519737244\n",
      "training: 61 batch 470 batch_loss: 0.20008361339569092\n",
      "training: 61 batch 471 batch_loss: 0.20810037851333618\n",
      "training: 61 batch 472 batch_loss: 0.20518654584884644\n",
      "training: 61 batch 473 batch_loss: 0.20884141325950623\n",
      "training: 61 batch 474 batch_loss: 0.2074858546257019\n",
      "training: 61 batch 475 batch_loss: 0.20302808284759521\n",
      "training: 61 batch 476 batch_loss: 0.20250144600868225\n",
      "training: 61 batch 477 batch_loss: 0.20400136709213257\n",
      "training: 61 batch 478 batch_loss: 0.20369267463684082\n",
      "training: 61 batch 479 batch_loss: 0.21058577299118042\n",
      "training: 61 batch 480 batch_loss: 0.20479214191436768\n",
      "training: 61 batch 481 batch_loss: 0.2084994912147522\n",
      "training: 61 batch 482 batch_loss: 0.20459836721420288\n",
      "training: 61 batch 483 batch_loss: 0.1997060775756836\n",
      "training: 61 batch 484 batch_loss: 0.20203620195388794\n",
      "training: 61 batch 485 batch_loss: 0.2066713571548462\n",
      "training: 61 batch 486 batch_loss: 0.20213180780410767\n",
      "training: 61 batch 487 batch_loss: 0.2020905613899231\n",
      "training: 61 batch 488 batch_loss: 0.2031429409980774\n",
      "training: 61 batch 489 batch_loss: 0.20421746373176575\n",
      "training: 61 batch 490 batch_loss: 0.20507362484931946\n",
      "training: 61 batch 491 batch_loss: 0.20463716983795166\n",
      "training: 61 batch 492 batch_loss: 0.2060643434524536\n",
      "training: 61 batch 493 batch_loss: 0.20396265387535095\n",
      "training: 61 batch 494 batch_loss: 0.20386749505996704\n",
      "training: 61 batch 495 batch_loss: 0.2009318470954895\n",
      "training: 61 batch 496 batch_loss: 0.2053324580192566\n",
      "training: 61 batch 497 batch_loss: 0.2065308392047882\n",
      "training: 61 batch 498 batch_loss: 0.20349860191345215\n",
      "training: 61 batch 499 batch_loss: 0.20538508892059326\n",
      "training: 61 batch 500 batch_loss: 0.20817291736602783\n",
      "training: 61 batch 501 batch_loss: 0.20757201313972473\n",
      "training: 61 batch 502 batch_loss: 0.20075386762619019\n",
      "training: 61 batch 503 batch_loss: 0.2082774043083191\n",
      "training: 61 batch 504 batch_loss: 0.2096509039402008\n",
      "training: 61 batch 505 batch_loss: 0.2058049440383911\n",
      "training: 61 batch 506 batch_loss: 0.20510441064834595\n",
      "training: 61 batch 507 batch_loss: 0.20595359802246094\n",
      "training: 61 batch 508 batch_loss: 0.20645710825920105\n",
      "training: 61 batch 509 batch_loss: 0.21199488639831543\n",
      "training: 61 batch 510 batch_loss: 0.20845264196395874\n",
      "training: 61 batch 511 batch_loss: 0.2050851285457611\n",
      "training: 61 batch 512 batch_loss: 0.2104763388633728\n",
      "training: 61 batch 513 batch_loss: 0.2068883776664734\n",
      "training: 61 batch 514 batch_loss: 0.20840293169021606\n",
      "training: 61 batch 515 batch_loss: 0.20758149027824402\n",
      "training: 61 batch 516 batch_loss: 0.20478364825248718\n",
      "training: 61 batch 517 batch_loss: 0.20961076021194458\n",
      "training: 61 batch 518 batch_loss: 0.20363402366638184\n",
      "training: 61 batch 519 batch_loss: 0.20499736070632935\n",
      "training: 61 batch 520 batch_loss: 0.20676955580711365\n",
      "training: 61 batch 521 batch_loss: 0.20477676391601562\n",
      "training: 61 batch 522 batch_loss: 0.20320525765419006\n",
      "training: 61 batch 523 batch_loss: 0.20634090900421143\n",
      "training: 61 batch 524 batch_loss: 0.20540845394134521\n",
      "training: 61 batch 525 batch_loss: 0.21046078205108643\n",
      "training: 61 batch 526 batch_loss: 0.20474109053611755\n",
      "training: 61 batch 527 batch_loss: 0.20427310466766357\n",
      "training: 61 batch 528 batch_loss: 0.20401573181152344\n",
      "training: 61 batch 529 batch_loss: 0.206088125705719\n",
      "training: 61 batch 530 batch_loss: 0.20675355195999146\n",
      "training: 61 batch 531 batch_loss: 0.20665860176086426\n",
      "training: 61 batch 532 batch_loss: 0.20434316992759705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 61 batch 533 batch_loss: 0.20842593908309937\n",
      "training: 61 batch 534 batch_loss: 0.2028675675392151\n",
      "training: 61 batch 535 batch_loss: 0.20283323526382446\n",
      "training: 61 batch 536 batch_loss: 0.20649060606956482\n",
      "training: 61 batch 537 batch_loss: 0.20206820964813232\n",
      "training: 61 batch 538 batch_loss: 0.20526021718978882\n",
      "training: 61 batch 539 batch_loss: 0.20635545253753662\n",
      "training: 61 batch 540 batch_loss: 0.20675694942474365\n",
      "training: 61 batch 541 batch_loss: 0.21043622493743896\n",
      "training: 61 batch 542 batch_loss: 0.20780932903289795\n",
      "training: 61 batch 543 batch_loss: 0.20157411694526672\n",
      "training: 61 batch 544 batch_loss: 0.20258939266204834\n",
      "training: 61 batch 545 batch_loss: 0.20553898811340332\n",
      "training: 61 batch 546 batch_loss: 0.202704519033432\n",
      "training: 61 batch 547 batch_loss: 0.2082383930683136\n",
      "training: 61 batch 548 batch_loss: 0.20656847953796387\n",
      "training: 61 batch 549 batch_loss: 0.2027478814125061\n",
      "training: 61 batch 550 batch_loss: 0.20819061994552612\n",
      "training: 61 batch 551 batch_loss: 0.2070661187171936\n",
      "training: 61 batch 552 batch_loss: 0.2058747112751007\n",
      "training: 61 batch 553 batch_loss: 0.20416542887687683\n",
      "training: 61 batch 554 batch_loss: 0.20614445209503174\n",
      "training: 61 batch 555 batch_loss: 0.2061569094657898\n",
      "training: 61 batch 556 batch_loss: 0.20040655136108398\n",
      "training: 61 batch 557 batch_loss: 0.20649272203445435\n",
      "training: 61 batch 558 batch_loss: 0.2007925808429718\n",
      "training: 61 batch 559 batch_loss: 0.20597487688064575\n",
      "training: 61 batch 560 batch_loss: 0.2051105797290802\n",
      "training: 61 batch 561 batch_loss: 0.20458704233169556\n",
      "training: 61 batch 562 batch_loss: 0.2038756012916565\n",
      "training: 61 batch 563 batch_loss: 0.2006266713142395\n",
      "training: 61 batch 564 batch_loss: 0.20484405755996704\n",
      "training: 61 batch 565 batch_loss: 0.20418831706047058\n",
      "training: 61 batch 566 batch_loss: 0.20706221461296082\n",
      "training: 61 batch 567 batch_loss: 0.20473718643188477\n",
      "training: 61 batch 568 batch_loss: 0.20464158058166504\n",
      "training: 61 batch 569 batch_loss: 0.200362890958786\n",
      "training: 61 batch 570 batch_loss: 0.207845538854599\n",
      "training: 61 batch 571 batch_loss: 0.20662569999694824\n",
      "training: 61 batch 572 batch_loss: 0.20600929856300354\n",
      "training: 61 batch 573 batch_loss: 0.20309865474700928\n",
      "training: 61 batch 574 batch_loss: 0.2073938250541687\n",
      "training: 61 batch 575 batch_loss: 0.20079797506332397\n",
      "training: 61 batch 576 batch_loss: 0.20914235711097717\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 61, Hit Ratio:0.033261934821931224 | Precision:0.04907598545168584 | Recall:0.06479544314235144 | NDCG:0.06348668156373174\n",
      "*Best Performance* \n",
      "Epoch: 61, Hit Ratio:0.033261934821931224 | Precision:0.04907598545168584 | Recall:0.06479544314235144 | MDCG:0.06348668156373174\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 62 batch 0 batch_loss: 0.20597636699676514\n",
      "training: 62 batch 1 batch_loss: 0.20318475365638733\n",
      "training: 62 batch 2 batch_loss: 0.2049424946308136\n",
      "training: 62 batch 3 batch_loss: 0.20485535264015198\n",
      "training: 62 batch 4 batch_loss: 0.2024436593055725\n",
      "training: 62 batch 5 batch_loss: 0.2060520052909851\n",
      "training: 62 batch 6 batch_loss: 0.20076122879981995\n",
      "training: 62 batch 7 batch_loss: 0.20423325896263123\n",
      "training: 62 batch 8 batch_loss: 0.207447350025177\n",
      "training: 62 batch 9 batch_loss: 0.19863876700401306\n",
      "training: 62 batch 10 batch_loss: 0.2072315514087677\n",
      "training: 62 batch 11 batch_loss: 0.20459553599357605\n",
      "training: 62 batch 12 batch_loss: 0.20763951539993286\n",
      "training: 62 batch 13 batch_loss: 0.20064648985862732\n",
      "training: 62 batch 14 batch_loss: 0.2053285837173462\n",
      "training: 62 batch 15 batch_loss: 0.2003849446773529\n",
      "training: 62 batch 16 batch_loss: 0.20452985167503357\n",
      "training: 62 batch 17 batch_loss: 0.20794260501861572\n",
      "training: 62 batch 18 batch_loss: 0.20256459712982178\n",
      "training: 62 batch 19 batch_loss: 0.2061055302619934\n",
      "training: 62 batch 20 batch_loss: 0.2061578929424286\n",
      "training: 62 batch 21 batch_loss: 0.2065526247024536\n",
      "training: 62 batch 22 batch_loss: 0.20268580317497253\n",
      "training: 62 batch 23 batch_loss: 0.20578494668006897\n",
      "training: 62 batch 24 batch_loss: 0.2072974443435669\n",
      "training: 62 batch 25 batch_loss: 0.20233407616615295\n",
      "training: 62 batch 26 batch_loss: 0.204695463180542\n",
      "training: 62 batch 27 batch_loss: 0.20612871646881104\n",
      "training: 62 batch 28 batch_loss: 0.2027263343334198\n",
      "training: 62 batch 29 batch_loss: 0.19917607307434082\n",
      "training: 62 batch 30 batch_loss: 0.20366781949996948\n",
      "training: 62 batch 31 batch_loss: 0.19884949922561646\n",
      "training: 62 batch 32 batch_loss: 0.20286235213279724\n",
      "training: 62 batch 33 batch_loss: 0.20100179314613342\n",
      "training: 62 batch 34 batch_loss: 0.20429924130439758\n",
      "training: 62 batch 35 batch_loss: 0.2044835090637207\n",
      "training: 62 batch 36 batch_loss: 0.20138725638389587\n",
      "training: 62 batch 37 batch_loss: 0.20159953832626343\n",
      "training: 62 batch 38 batch_loss: 0.20240938663482666\n",
      "training: 62 batch 39 batch_loss: 0.2032896876335144\n",
      "training: 62 batch 40 batch_loss: 0.2048918604850769\n",
      "training: 62 batch 41 batch_loss: 0.2049456238746643\n",
      "training: 62 batch 42 batch_loss: 0.20648598670959473\n",
      "training: 62 batch 43 batch_loss: 0.2071990966796875\n",
      "training: 62 batch 44 batch_loss: 0.20441129803657532\n",
      "training: 62 batch 45 batch_loss: 0.20659619569778442\n",
      "training: 62 batch 46 batch_loss: 0.20613440871238708\n",
      "training: 62 batch 47 batch_loss: 0.20611190795898438\n",
      "training: 62 batch 48 batch_loss: 0.20076513290405273\n",
      "training: 62 batch 49 batch_loss: 0.2036600410938263\n",
      "training: 62 batch 50 batch_loss: 0.20147079229354858\n",
      "training: 62 batch 51 batch_loss: 0.20460155606269836\n",
      "training: 62 batch 52 batch_loss: 0.20162734389305115\n",
      "training: 62 batch 53 batch_loss: 0.20069006085395813\n",
      "training: 62 batch 54 batch_loss: 0.20556050539016724\n",
      "training: 62 batch 55 batch_loss: 0.206581711769104\n",
      "training: 62 batch 56 batch_loss: 0.2071419656276703\n",
      "training: 62 batch 57 batch_loss: 0.20425447821617126\n",
      "training: 62 batch 58 batch_loss: 0.20491573214530945\n",
      "training: 62 batch 59 batch_loss: 0.204062819480896\n",
      "training: 62 batch 60 batch_loss: 0.20465737581253052\n",
      "training: 62 batch 61 batch_loss: 0.20319342613220215\n",
      "training: 62 batch 62 batch_loss: 0.20410194993019104\n",
      "training: 62 batch 63 batch_loss: 0.19910353422164917\n",
      "training: 62 batch 64 batch_loss: 0.20219966769218445\n",
      "training: 62 batch 65 batch_loss: 0.20387452840805054\n",
      "training: 62 batch 66 batch_loss: 0.20451942086219788\n",
      "training: 62 batch 67 batch_loss: 0.20297867059707642\n",
      "training: 62 batch 68 batch_loss: 0.20430710911750793\n",
      "training: 62 batch 69 batch_loss: 0.205118328332901\n",
      "training: 62 batch 70 batch_loss: 0.20342686772346497\n",
      "training: 62 batch 71 batch_loss: 0.2049015760421753\n",
      "training: 62 batch 72 batch_loss: 0.20519152283668518\n",
      "training: 62 batch 73 batch_loss: 0.20249298214912415\n",
      "training: 62 batch 74 batch_loss: 0.20396414399147034\n",
      "training: 62 batch 75 batch_loss: 0.20465457439422607\n",
      "training: 62 batch 76 batch_loss: 0.20495808124542236\n",
      "training: 62 batch 77 batch_loss: 0.2037906050682068\n",
      "training: 62 batch 78 batch_loss: 0.20725402235984802\n",
      "training: 62 batch 79 batch_loss: 0.2053186297416687\n",
      "training: 62 batch 80 batch_loss: 0.2086414098739624\n",
      "training: 62 batch 81 batch_loss: 0.2047136425971985\n",
      "training: 62 batch 82 batch_loss: 0.2045976221561432\n",
      "training: 62 batch 83 batch_loss: 0.20256638526916504\n",
      "training: 62 batch 84 batch_loss: 0.20617735385894775\n",
      "training: 62 batch 85 batch_loss: 0.20536163449287415\n",
      "training: 62 batch 86 batch_loss: 0.2036762237548828\n",
      "training: 62 batch 87 batch_loss: 0.20516839623451233\n",
      "training: 62 batch 88 batch_loss: 0.2036917507648468\n",
      "training: 62 batch 89 batch_loss: 0.2028721570968628\n",
      "training: 62 batch 90 batch_loss: 0.2073218822479248\n",
      "training: 62 batch 91 batch_loss: 0.20577594637870789\n",
      "training: 62 batch 92 batch_loss: 0.20514151453971863\n",
      "training: 62 batch 93 batch_loss: 0.2027135193347931\n",
      "training: 62 batch 94 batch_loss: 0.20736682415008545\n",
      "training: 62 batch 95 batch_loss: 0.20495006442070007\n",
      "training: 62 batch 96 batch_loss: 0.2029692530632019\n",
      "training: 62 batch 97 batch_loss: 0.20413488149642944\n",
      "training: 62 batch 98 batch_loss: 0.20481184124946594\n",
      "training: 62 batch 99 batch_loss: 0.2096768021583557\n",
      "training: 62 batch 100 batch_loss: 0.2020266354084015\n",
      "training: 62 batch 101 batch_loss: 0.20294907689094543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 62 batch 102 batch_loss: 0.20622608065605164\n",
      "training: 62 batch 103 batch_loss: 0.2043311595916748\n",
      "training: 62 batch 104 batch_loss: 0.20558753609657288\n",
      "training: 62 batch 105 batch_loss: 0.2041967809200287\n",
      "training: 62 batch 106 batch_loss: 0.20874661207199097\n",
      "training: 62 batch 107 batch_loss: 0.20590707659721375\n",
      "training: 62 batch 108 batch_loss: 0.20924824476242065\n",
      "training: 62 batch 109 batch_loss: 0.20435047149658203\n",
      "training: 62 batch 110 batch_loss: 0.20630091428756714\n",
      "training: 62 batch 111 batch_loss: 0.20752650499343872\n",
      "training: 62 batch 112 batch_loss: 0.20635661482810974\n",
      "training: 62 batch 113 batch_loss: 0.20231643319129944\n",
      "training: 62 batch 114 batch_loss: 0.20792746543884277\n",
      "training: 62 batch 115 batch_loss: 0.20626842975616455\n",
      "training: 62 batch 116 batch_loss: 0.20659908652305603\n",
      "training: 62 batch 117 batch_loss: 0.20613113045692444\n",
      "training: 62 batch 118 batch_loss: 0.20025917887687683\n",
      "training: 62 batch 119 batch_loss: 0.20625177025794983\n",
      "training: 62 batch 120 batch_loss: 0.20431861281394958\n",
      "training: 62 batch 121 batch_loss: 0.2037942111492157\n",
      "training: 62 batch 122 batch_loss: 0.2060699760913849\n",
      "training: 62 batch 123 batch_loss: 0.20526835322380066\n",
      "training: 62 batch 124 batch_loss: 0.2097843587398529\n",
      "training: 62 batch 125 batch_loss: 0.20265299081802368\n",
      "training: 62 batch 126 batch_loss: 0.2032587230205536\n",
      "training: 62 batch 127 batch_loss: 0.2044808268547058\n",
      "training: 62 batch 128 batch_loss: 0.2036876380443573\n",
      "training: 62 batch 129 batch_loss: 0.20506522059440613\n",
      "training: 62 batch 130 batch_loss: 0.20701202750205994\n",
      "training: 62 batch 131 batch_loss: 0.20380651950836182\n",
      "training: 62 batch 132 batch_loss: 0.2055588662624359\n",
      "training: 62 batch 133 batch_loss: 0.2058517038822174\n",
      "training: 62 batch 134 batch_loss: 0.20560216903686523\n",
      "training: 62 batch 135 batch_loss: 0.20243868231773376\n",
      "training: 62 batch 136 batch_loss: 0.20670107007026672\n",
      "training: 62 batch 137 batch_loss: 0.20077890157699585\n",
      "training: 62 batch 138 batch_loss: 0.20311853289604187\n",
      "training: 62 batch 139 batch_loss: 0.20472824573516846\n",
      "training: 62 batch 140 batch_loss: 0.20098066329956055\n",
      "training: 62 batch 141 batch_loss: 0.20357927680015564\n",
      "training: 62 batch 142 batch_loss: 0.20937639474868774\n",
      "training: 62 batch 143 batch_loss: 0.20569497346878052\n",
      "training: 62 batch 144 batch_loss: 0.20705702900886536\n",
      "training: 62 batch 145 batch_loss: 0.20341026782989502\n",
      "training: 62 batch 146 batch_loss: 0.20739015936851501\n",
      "training: 62 batch 147 batch_loss: 0.2060413360595703\n",
      "training: 62 batch 148 batch_loss: 0.206248939037323\n",
      "training: 62 batch 149 batch_loss: 0.20600110292434692\n",
      "training: 62 batch 150 batch_loss: 0.20482000708580017\n",
      "training: 62 batch 151 batch_loss: 0.20742198824882507\n",
      "training: 62 batch 152 batch_loss: 0.2032383382320404\n",
      "training: 62 batch 153 batch_loss: 0.20441541075706482\n",
      "training: 62 batch 154 batch_loss: 0.20668411254882812\n",
      "training: 62 batch 155 batch_loss: 0.21023747324943542\n",
      "training: 62 batch 156 batch_loss: 0.2063596546649933\n",
      "training: 62 batch 157 batch_loss: 0.20178481936454773\n",
      "training: 62 batch 158 batch_loss: 0.20228472352027893\n",
      "training: 62 batch 159 batch_loss: 0.2052832841873169\n",
      "training: 62 batch 160 batch_loss: 0.20947265625\n",
      "training: 62 batch 161 batch_loss: 0.21035325527191162\n",
      "training: 62 batch 162 batch_loss: 0.20517069101333618\n",
      "training: 62 batch 163 batch_loss: 0.20474272966384888\n",
      "training: 62 batch 164 batch_loss: 0.20545047521591187\n",
      "training: 62 batch 165 batch_loss: 0.21125975251197815\n",
      "training: 62 batch 166 batch_loss: 0.203128844499588\n",
      "training: 62 batch 167 batch_loss: 0.20314571261405945\n",
      "training: 62 batch 168 batch_loss: 0.20168501138687134\n",
      "training: 62 batch 169 batch_loss: 0.2040807604789734\n",
      "training: 62 batch 170 batch_loss: 0.203069269657135\n",
      "training: 62 batch 171 batch_loss: 0.2078566551208496\n",
      "training: 62 batch 172 batch_loss: 0.20310750603675842\n",
      "training: 62 batch 173 batch_loss: 0.20437222719192505\n",
      "training: 62 batch 174 batch_loss: 0.2053213119506836\n",
      "training: 62 batch 175 batch_loss: 0.20010149478912354\n",
      "training: 62 batch 176 batch_loss: 0.20435473322868347\n",
      "training: 62 batch 177 batch_loss: 0.20796573162078857\n",
      "training: 62 batch 178 batch_loss: 0.20615780353546143\n",
      "training: 62 batch 179 batch_loss: 0.20502251386642456\n",
      "training: 62 batch 180 batch_loss: 0.20726636052131653\n",
      "training: 62 batch 181 batch_loss: 0.20156505703926086\n",
      "training: 62 batch 182 batch_loss: 0.20581316947937012\n",
      "training: 62 batch 183 batch_loss: 0.20627033710479736\n",
      "training: 62 batch 184 batch_loss: 0.2021636664867401\n",
      "training: 62 batch 185 batch_loss: 0.20595607161521912\n",
      "training: 62 batch 186 batch_loss: 0.20368456840515137\n",
      "training: 62 batch 187 batch_loss: 0.20677712559700012\n",
      "training: 62 batch 188 batch_loss: 0.20697155594825745\n",
      "training: 62 batch 189 batch_loss: 0.20717760920524597\n",
      "training: 62 batch 190 batch_loss: 0.20500978827476501\n",
      "training: 62 batch 191 batch_loss: 0.20384031534194946\n",
      "training: 62 batch 192 batch_loss: 0.2113465964794159\n",
      "training: 62 batch 193 batch_loss: 0.20334631204605103\n",
      "training: 62 batch 194 batch_loss: 0.20452210307121277\n",
      "training: 62 batch 195 batch_loss: 0.20604336261749268\n",
      "training: 62 batch 196 batch_loss: 0.20636126399040222\n",
      "training: 62 batch 197 batch_loss: 0.20910686254501343\n",
      "training: 62 batch 198 batch_loss: 0.2054368257522583\n",
      "training: 62 batch 199 batch_loss: 0.2049962282180786\n",
      "training: 62 batch 200 batch_loss: 0.20403516292572021\n",
      "training: 62 batch 201 batch_loss: 0.20335620641708374\n",
      "training: 62 batch 202 batch_loss: 0.2098153829574585\n",
      "training: 62 batch 203 batch_loss: 0.20132863521575928\n",
      "training: 62 batch 204 batch_loss: 0.206281840801239\n",
      "training: 62 batch 205 batch_loss: 0.20420348644256592\n",
      "training: 62 batch 206 batch_loss: 0.20876917243003845\n",
      "training: 62 batch 207 batch_loss: 0.20509588718414307\n",
      "training: 62 batch 208 batch_loss: 0.20366057753562927\n",
      "training: 62 batch 209 batch_loss: 0.20407435297966003\n",
      "training: 62 batch 210 batch_loss: 0.20583707094192505\n",
      "training: 62 batch 211 batch_loss: 0.20607757568359375\n",
      "training: 62 batch 212 batch_loss: 0.2033042311668396\n",
      "training: 62 batch 213 batch_loss: 0.2035050392150879\n",
      "training: 62 batch 214 batch_loss: 0.20526856184005737\n",
      "training: 62 batch 215 batch_loss: 0.20294499397277832\n",
      "training: 62 batch 216 batch_loss: 0.20089685916900635\n",
      "training: 62 batch 217 batch_loss: 0.2002667486667633\n",
      "training: 62 batch 218 batch_loss: 0.20395329594612122\n",
      "training: 62 batch 219 batch_loss: 0.20387446880340576\n",
      "training: 62 batch 220 batch_loss: 0.20453375577926636\n",
      "training: 62 batch 221 batch_loss: 0.20456638932228088\n",
      "training: 62 batch 222 batch_loss: 0.20559898018836975\n",
      "training: 62 batch 223 batch_loss: 0.20595133304595947\n",
      "training: 62 batch 224 batch_loss: 0.20457512140274048\n",
      "training: 62 batch 225 batch_loss: 0.20143914222717285\n",
      "training: 62 batch 226 batch_loss: 0.20655426383018494\n",
      "training: 62 batch 227 batch_loss: 0.20803368091583252\n",
      "training: 62 batch 228 batch_loss: 0.2065872848033905\n",
      "training: 62 batch 229 batch_loss: 0.21016889810562134\n",
      "training: 62 batch 230 batch_loss: 0.20648887753486633\n",
      "training: 62 batch 231 batch_loss: 0.20451080799102783\n",
      "training: 62 batch 232 batch_loss: 0.20950183272361755\n",
      "training: 62 batch 233 batch_loss: 0.20456865429878235\n",
      "training: 62 batch 234 batch_loss: 0.2030317485332489\n",
      "training: 62 batch 235 batch_loss: 0.20465680956840515\n",
      "training: 62 batch 236 batch_loss: 0.20549532771110535\n",
      "training: 62 batch 237 batch_loss: 0.2053661048412323\n",
      "training: 62 batch 238 batch_loss: 0.2041143774986267\n",
      "training: 62 batch 239 batch_loss: 0.20535385608673096\n",
      "training: 62 batch 240 batch_loss: 0.20224744081497192\n",
      "training: 62 batch 241 batch_loss: 0.20721524953842163\n",
      "training: 62 batch 242 batch_loss: 0.20461511611938477\n",
      "training: 62 batch 243 batch_loss: 0.2064915895462036\n",
      "training: 62 batch 244 batch_loss: 0.20538538694381714\n",
      "training: 62 batch 245 batch_loss: 0.2034059464931488\n",
      "training: 62 batch 246 batch_loss: 0.20447823405265808\n",
      "training: 62 batch 247 batch_loss: 0.2052030861377716\n",
      "training: 62 batch 248 batch_loss: 0.202068030834198\n",
      "training: 62 batch 249 batch_loss: 0.20643019676208496\n",
      "training: 62 batch 250 batch_loss: 0.2091541290283203\n",
      "training: 62 batch 251 batch_loss: 0.2102971374988556\n",
      "training: 62 batch 252 batch_loss: 0.21088191866874695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 62 batch 253 batch_loss: 0.20479953289031982\n",
      "training: 62 batch 254 batch_loss: 0.20553240180015564\n",
      "training: 62 batch 255 batch_loss: 0.20082080364227295\n",
      "training: 62 batch 256 batch_loss: 0.20577135682106018\n",
      "training: 62 batch 257 batch_loss: 0.2079268991947174\n",
      "training: 62 batch 258 batch_loss: 0.20746386051177979\n",
      "training: 62 batch 259 batch_loss: 0.2059800922870636\n",
      "training: 62 batch 260 batch_loss: 0.20476213097572327\n",
      "training: 62 batch 261 batch_loss: 0.20606458187103271\n",
      "training: 62 batch 262 batch_loss: 0.2088181972503662\n",
      "training: 62 batch 263 batch_loss: 0.20644935965538025\n",
      "training: 62 batch 264 batch_loss: 0.20591402053833008\n",
      "training: 62 batch 265 batch_loss: 0.2049882709980011\n",
      "training: 62 batch 266 batch_loss: 0.20297157764434814\n",
      "training: 62 batch 267 batch_loss: 0.20405876636505127\n",
      "training: 62 batch 268 batch_loss: 0.20889320969581604\n",
      "training: 62 batch 269 batch_loss: 0.2054581642150879\n",
      "training: 62 batch 270 batch_loss: 0.20574724674224854\n",
      "training: 62 batch 271 batch_loss: 0.206050306558609\n",
      "training: 62 batch 272 batch_loss: 0.20732194185256958\n",
      "training: 62 batch 273 batch_loss: 0.2086169421672821\n",
      "training: 62 batch 274 batch_loss: 0.20625147223472595\n",
      "training: 62 batch 275 batch_loss: 0.2044234573841095\n",
      "training: 62 batch 276 batch_loss: 0.20240557193756104\n",
      "training: 62 batch 277 batch_loss: 0.20692920684814453\n",
      "training: 62 batch 278 batch_loss: 0.20928388833999634\n",
      "training: 62 batch 279 batch_loss: 0.2048235833644867\n",
      "training: 62 batch 280 batch_loss: 0.20602253079414368\n",
      "training: 62 batch 281 batch_loss: 0.20840129256248474\n",
      "training: 62 batch 282 batch_loss: 0.2017805576324463\n",
      "training: 62 batch 283 batch_loss: 0.20939278602600098\n",
      "training: 62 batch 284 batch_loss: 0.21231412887573242\n",
      "training: 62 batch 285 batch_loss: 0.2022271752357483\n",
      "training: 62 batch 286 batch_loss: 0.20492270588874817\n",
      "training: 62 batch 287 batch_loss: 0.21055996417999268\n",
      "training: 62 batch 288 batch_loss: 0.2077682614326477\n",
      "training: 62 batch 289 batch_loss: 0.20114755630493164\n",
      "training: 62 batch 290 batch_loss: 0.2059963345527649\n",
      "training: 62 batch 291 batch_loss: 0.2087957262992859\n",
      "training: 62 batch 292 batch_loss: 0.20521926879882812\n",
      "training: 62 batch 293 batch_loss: 0.20857971906661987\n",
      "training: 62 batch 294 batch_loss: 0.20705613493919373\n",
      "training: 62 batch 295 batch_loss: 0.2115013599395752\n",
      "training: 62 batch 296 batch_loss: 0.20696312189102173\n",
      "training: 62 batch 297 batch_loss: 0.20507970452308655\n",
      "training: 62 batch 298 batch_loss: 0.2081698179244995\n",
      "training: 62 batch 299 batch_loss: 0.20071694254875183\n",
      "training: 62 batch 300 batch_loss: 0.203388512134552\n",
      "training: 62 batch 301 batch_loss: 0.21031105518341064\n",
      "training: 62 batch 302 batch_loss: 0.2062033712863922\n",
      "training: 62 batch 303 batch_loss: 0.20577788352966309\n",
      "training: 62 batch 304 batch_loss: 0.2069249153137207\n",
      "training: 62 batch 305 batch_loss: 0.20578867197036743\n",
      "training: 62 batch 306 batch_loss: 0.20489990711212158\n",
      "training: 62 batch 307 batch_loss: 0.20451217889785767\n",
      "training: 62 batch 308 batch_loss: 0.21046102046966553\n",
      "training: 62 batch 309 batch_loss: 0.20832687616348267\n",
      "training: 62 batch 310 batch_loss: 0.20461413264274597\n",
      "training: 62 batch 311 batch_loss: 0.20169204473495483\n",
      "training: 62 batch 312 batch_loss: 0.2071463167667389\n",
      "training: 62 batch 313 batch_loss: 0.20238390564918518\n",
      "training: 62 batch 314 batch_loss: 0.2083832323551178\n",
      "training: 62 batch 315 batch_loss: 0.20392107963562012\n",
      "training: 62 batch 316 batch_loss: 0.20465624332427979\n",
      "training: 62 batch 317 batch_loss: 0.2076515555381775\n",
      "training: 62 batch 318 batch_loss: 0.20200571417808533\n",
      "training: 62 batch 319 batch_loss: 0.20586669445037842\n",
      "training: 62 batch 320 batch_loss: 0.20531010627746582\n",
      "training: 62 batch 321 batch_loss: 0.20224261283874512\n",
      "training: 62 batch 322 batch_loss: 0.20742249488830566\n",
      "training: 62 batch 323 batch_loss: 0.20397794246673584\n",
      "training: 62 batch 324 batch_loss: 0.20612967014312744\n",
      "training: 62 batch 325 batch_loss: 0.20852899551391602\n",
      "training: 62 batch 326 batch_loss: 0.20537829399108887\n",
      "training: 62 batch 327 batch_loss: 0.20473650097846985\n",
      "training: 62 batch 328 batch_loss: 0.20489558577537537\n",
      "training: 62 batch 329 batch_loss: 0.20745325088500977\n",
      "training: 62 batch 330 batch_loss: 0.20471513271331787\n",
      "training: 62 batch 331 batch_loss: 0.20562231540679932\n",
      "training: 62 batch 332 batch_loss: 0.20444127917289734\n",
      "training: 62 batch 333 batch_loss: 0.20749589800834656\n",
      "training: 62 batch 334 batch_loss: 0.20872309803962708\n",
      "training: 62 batch 335 batch_loss: 0.21144479513168335\n",
      "training: 62 batch 336 batch_loss: 0.208072692155838\n",
      "training: 62 batch 337 batch_loss: 0.20735257863998413\n",
      "training: 62 batch 338 batch_loss: 0.20562505722045898\n",
      "training: 62 batch 339 batch_loss: 0.20520421862602234\n",
      "training: 62 batch 340 batch_loss: 0.2061765193939209\n",
      "training: 62 batch 341 batch_loss: 0.2006431221961975\n",
      "training: 62 batch 342 batch_loss: 0.20605263113975525\n",
      "training: 62 batch 343 batch_loss: 0.20347675681114197\n",
      "training: 62 batch 344 batch_loss: 0.20496362447738647\n",
      "training: 62 batch 345 batch_loss: 0.20657065510749817\n",
      "training: 62 batch 346 batch_loss: 0.20594584941864014\n",
      "training: 62 batch 347 batch_loss: 0.20430982112884521\n",
      "training: 62 batch 348 batch_loss: 0.20817574858665466\n",
      "training: 62 batch 349 batch_loss: 0.2047460675239563\n",
      "training: 62 batch 350 batch_loss: 0.20735818147659302\n",
      "training: 62 batch 351 batch_loss: 0.20474350452423096\n",
      "training: 62 batch 352 batch_loss: 0.2051478922367096\n",
      "training: 62 batch 353 batch_loss: 0.20497819781303406\n",
      "training: 62 batch 354 batch_loss: 0.20747283101081848\n",
      "training: 62 batch 355 batch_loss: 0.20554053783416748\n",
      "training: 62 batch 356 batch_loss: 0.20920351147651672\n",
      "training: 62 batch 357 batch_loss: 0.2042270302772522\n",
      "training: 62 batch 358 batch_loss: 0.205318421125412\n",
      "training: 62 batch 359 batch_loss: 0.20864316821098328\n",
      "training: 62 batch 360 batch_loss: 0.2080569565296173\n",
      "training: 62 batch 361 batch_loss: 0.20440027117729187\n",
      "training: 62 batch 362 batch_loss: 0.20738309621810913\n",
      "training: 62 batch 363 batch_loss: 0.20500227808952332\n",
      "training: 62 batch 364 batch_loss: 0.20728141069412231\n",
      "training: 62 batch 365 batch_loss: 0.20660477876663208\n",
      "training: 62 batch 366 batch_loss: 0.2044982612133026\n",
      "training: 62 batch 367 batch_loss: 0.20339369773864746\n",
      "training: 62 batch 368 batch_loss: 0.20313477516174316\n",
      "training: 62 batch 369 batch_loss: 0.20878168940544128\n",
      "training: 62 batch 370 batch_loss: 0.20733150839805603\n",
      "training: 62 batch 371 batch_loss: 0.20585733652114868\n",
      "training: 62 batch 372 batch_loss: 0.20299765467643738\n",
      "training: 62 batch 373 batch_loss: 0.20832177996635437\n",
      "training: 62 batch 374 batch_loss: 0.20773473381996155\n",
      "training: 62 batch 375 batch_loss: 0.20712801814079285\n",
      "training: 62 batch 376 batch_loss: 0.20295366644859314\n",
      "training: 62 batch 377 batch_loss: 0.20625993609428406\n",
      "training: 62 batch 378 batch_loss: 0.20697593688964844\n",
      "training: 62 batch 379 batch_loss: 0.20172211527824402\n",
      "training: 62 batch 380 batch_loss: 0.21009761095046997\n",
      "training: 62 batch 381 batch_loss: 0.20049721002578735\n",
      "training: 62 batch 382 batch_loss: 0.2045411765575409\n",
      "training: 62 batch 383 batch_loss: 0.2037981152534485\n",
      "training: 62 batch 384 batch_loss: 0.20587033033370972\n",
      "training: 62 batch 385 batch_loss: 0.20736730098724365\n",
      "training: 62 batch 386 batch_loss: 0.2018289566040039\n",
      "training: 62 batch 387 batch_loss: 0.20342159271240234\n",
      "training: 62 batch 388 batch_loss: 0.21053490042686462\n",
      "training: 62 batch 389 batch_loss: 0.20682039856910706\n",
      "training: 62 batch 390 batch_loss: 0.20949026942253113\n",
      "training: 62 batch 391 batch_loss: 0.20502030849456787\n",
      "training: 62 batch 392 batch_loss: 0.20894157886505127\n",
      "training: 62 batch 393 batch_loss: 0.20824936032295227\n",
      "training: 62 batch 394 batch_loss: 0.20874226093292236\n",
      "training: 62 batch 395 batch_loss: 0.20519745349884033\n",
      "training: 62 batch 396 batch_loss: 0.20813265442848206\n",
      "training: 62 batch 397 batch_loss: 0.20636674761772156\n",
      "training: 62 batch 398 batch_loss: 0.20799952745437622\n",
      "training: 62 batch 399 batch_loss: 0.2059173882007599\n",
      "training: 62 batch 400 batch_loss: 0.20652642846107483\n",
      "training: 62 batch 401 batch_loss: 0.20081201195716858\n",
      "training: 62 batch 402 batch_loss: 0.20452749729156494\n",
      "training: 62 batch 403 batch_loss: 0.2071961760520935\n",
      "training: 62 batch 404 batch_loss: 0.20494821667671204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 62 batch 405 batch_loss: 0.20399001240730286\n",
      "training: 62 batch 406 batch_loss: 0.20641207695007324\n",
      "training: 62 batch 407 batch_loss: 0.2046492099761963\n",
      "training: 62 batch 408 batch_loss: 0.2059408724308014\n",
      "training: 62 batch 409 batch_loss: 0.20937281847000122\n",
      "training: 62 batch 410 batch_loss: 0.20609834790229797\n",
      "training: 62 batch 411 batch_loss: 0.20378932356834412\n",
      "training: 62 batch 412 batch_loss: 0.20995080471038818\n",
      "training: 62 batch 413 batch_loss: 0.20320653915405273\n",
      "training: 62 batch 414 batch_loss: 0.20835646986961365\n",
      "training: 62 batch 415 batch_loss: 0.20585626363754272\n",
      "training: 62 batch 416 batch_loss: 0.20773419737815857\n",
      "training: 62 batch 417 batch_loss: 0.2056294083595276\n",
      "training: 62 batch 418 batch_loss: 0.202413409948349\n",
      "training: 62 batch 419 batch_loss: 0.20578831434249878\n",
      "training: 62 batch 420 batch_loss: 0.20351672172546387\n",
      "training: 62 batch 421 batch_loss: 0.20730969309806824\n",
      "training: 62 batch 422 batch_loss: 0.2057456076145172\n",
      "training: 62 batch 423 batch_loss: 0.20958399772644043\n",
      "training: 62 batch 424 batch_loss: 0.20617926120758057\n",
      "training: 62 batch 425 batch_loss: 0.20906350016593933\n",
      "training: 62 batch 426 batch_loss: 0.2041071057319641\n",
      "training: 62 batch 427 batch_loss: 0.2047458291053772\n",
      "training: 62 batch 428 batch_loss: 0.2066313922405243\n",
      "training: 62 batch 429 batch_loss: 0.2074548900127411\n",
      "training: 62 batch 430 batch_loss: 0.20543402433395386\n",
      "training: 62 batch 431 batch_loss: 0.2051270604133606\n",
      "training: 62 batch 432 batch_loss: 0.20816004276275635\n",
      "training: 62 batch 433 batch_loss: 0.20683270692825317\n",
      "training: 62 batch 434 batch_loss: 0.20288342237472534\n",
      "training: 62 batch 435 batch_loss: 0.20775973796844482\n",
      "training: 62 batch 436 batch_loss: 0.20786195993423462\n",
      "training: 62 batch 437 batch_loss: 0.20894595980644226\n",
      "training: 62 batch 438 batch_loss: 0.20341646671295166\n",
      "training: 62 batch 439 batch_loss: 0.2065017819404602\n",
      "training: 62 batch 440 batch_loss: 0.20554080605506897\n",
      "training: 62 batch 441 batch_loss: 0.20811688899993896\n",
      "training: 62 batch 442 batch_loss: 0.20496124029159546\n",
      "training: 62 batch 443 batch_loss: 0.20797425508499146\n",
      "training: 62 batch 444 batch_loss: 0.20416659116744995\n",
      "training: 62 batch 445 batch_loss: 0.20571312308311462\n",
      "training: 62 batch 446 batch_loss: 0.2068614661693573\n",
      "training: 62 batch 447 batch_loss: 0.20944637060165405\n",
      "training: 62 batch 448 batch_loss: 0.20657092332839966\n",
      "training: 62 batch 449 batch_loss: 0.2090318202972412\n",
      "training: 62 batch 450 batch_loss: 0.20899415016174316\n",
      "training: 62 batch 451 batch_loss: 0.20549342036247253\n",
      "training: 62 batch 452 batch_loss: 0.20639973878860474\n",
      "training: 62 batch 453 batch_loss: 0.20492172241210938\n",
      "training: 62 batch 454 batch_loss: 0.2081335484981537\n",
      "training: 62 batch 455 batch_loss: 0.20277342200279236\n",
      "training: 62 batch 456 batch_loss: 0.2064080536365509\n",
      "training: 62 batch 457 batch_loss: 0.20728814601898193\n",
      "training: 62 batch 458 batch_loss: 0.20674914121627808\n",
      "training: 62 batch 459 batch_loss: 0.20394659042358398\n",
      "training: 62 batch 460 batch_loss: 0.21152198314666748\n",
      "training: 62 batch 461 batch_loss: 0.2095152735710144\n",
      "training: 62 batch 462 batch_loss: 0.2079325020313263\n",
      "training: 62 batch 463 batch_loss: 0.20460498332977295\n",
      "training: 62 batch 464 batch_loss: 0.2090475857257843\n",
      "training: 62 batch 465 batch_loss: 0.20303687453269958\n",
      "training: 62 batch 466 batch_loss: 0.2071003019809723\n",
      "training: 62 batch 467 batch_loss: 0.20493939518928528\n",
      "training: 62 batch 468 batch_loss: 0.2076820731163025\n",
      "training: 62 batch 469 batch_loss: 0.2122330665588379\n",
      "training: 62 batch 470 batch_loss: 0.2078847885131836\n",
      "training: 62 batch 471 batch_loss: 0.2088145911693573\n",
      "training: 62 batch 472 batch_loss: 0.2104349136352539\n",
      "training: 62 batch 473 batch_loss: 0.20220711827278137\n",
      "training: 62 batch 474 batch_loss: 0.20375901460647583\n",
      "training: 62 batch 475 batch_loss: 0.20566663146018982\n",
      "training: 62 batch 476 batch_loss: 0.20978879928588867\n",
      "training: 62 batch 477 batch_loss: 0.2061326503753662\n",
      "training: 62 batch 478 batch_loss: 0.20752418041229248\n",
      "training: 62 batch 479 batch_loss: 0.20563557744026184\n",
      "training: 62 batch 480 batch_loss: 0.20254206657409668\n",
      "training: 62 batch 481 batch_loss: 0.2054223120212555\n",
      "training: 62 batch 482 batch_loss: 0.20695331692695618\n",
      "training: 62 batch 483 batch_loss: 0.20774558186531067\n",
      "training: 62 batch 484 batch_loss: 0.20493879914283752\n",
      "training: 62 batch 485 batch_loss: 0.2078550159931183\n",
      "training: 62 batch 486 batch_loss: 0.2056085169315338\n",
      "training: 62 batch 487 batch_loss: 0.20672529935836792\n",
      "training: 62 batch 488 batch_loss: 0.20850428938865662\n",
      "training: 62 batch 489 batch_loss: 0.20734575390815735\n",
      "training: 62 batch 490 batch_loss: 0.20584848523139954\n",
      "training: 62 batch 491 batch_loss: 0.20853054523468018\n",
      "training: 62 batch 492 batch_loss: 0.2021934986114502\n",
      "training: 62 batch 493 batch_loss: 0.2063266634941101\n",
      "training: 62 batch 494 batch_loss: 0.2039593756198883\n",
      "training: 62 batch 495 batch_loss: 0.20860588550567627\n",
      "training: 62 batch 496 batch_loss: 0.20697486400604248\n",
      "training: 62 batch 497 batch_loss: 0.20972192287445068\n",
      "training: 62 batch 498 batch_loss: 0.2093384563922882\n",
      "training: 62 batch 499 batch_loss: 0.20898592472076416\n",
      "training: 62 batch 500 batch_loss: 0.2056223750114441\n",
      "training: 62 batch 501 batch_loss: 0.20503857731819153\n",
      "training: 62 batch 502 batch_loss: 0.2036186158657074\n",
      "training: 62 batch 503 batch_loss: 0.20907720923423767\n",
      "training: 62 batch 504 batch_loss: 0.20528912544250488\n",
      "training: 62 batch 505 batch_loss: 0.20816141366958618\n",
      "training: 62 batch 506 batch_loss: 0.20414581894874573\n",
      "training: 62 batch 507 batch_loss: 0.206886887550354\n",
      "training: 62 batch 508 batch_loss: 0.2022188901901245\n",
      "training: 62 batch 509 batch_loss: 0.20980912446975708\n",
      "training: 62 batch 510 batch_loss: 0.2055237889289856\n",
      "training: 62 batch 511 batch_loss: 0.20827814936637878\n",
      "training: 62 batch 512 batch_loss: 0.2039940059185028\n",
      "training: 62 batch 513 batch_loss: 0.2061794102191925\n",
      "training: 62 batch 514 batch_loss: 0.2080841362476349\n",
      "training: 62 batch 515 batch_loss: 0.20702022314071655\n",
      "training: 62 batch 516 batch_loss: 0.2091800570487976\n",
      "training: 62 batch 517 batch_loss: 0.20961600542068481\n",
      "training: 62 batch 518 batch_loss: 0.20769453048706055\n",
      "training: 62 batch 519 batch_loss: 0.2085253894329071\n",
      "training: 62 batch 520 batch_loss: 0.20494809746742249\n",
      "training: 62 batch 521 batch_loss: 0.2080976665019989\n",
      "training: 62 batch 522 batch_loss: 0.20661038160324097\n",
      "training: 62 batch 523 batch_loss: 0.20863297581672668\n",
      "training: 62 batch 524 batch_loss: 0.2067183554172516\n",
      "training: 62 batch 525 batch_loss: 0.20517897605895996\n",
      "training: 62 batch 526 batch_loss: 0.20346033573150635\n",
      "training: 62 batch 527 batch_loss: 0.205293208360672\n",
      "training: 62 batch 528 batch_loss: 0.2067670226097107\n",
      "training: 62 batch 529 batch_loss: 0.20314925909042358\n",
      "training: 62 batch 530 batch_loss: 0.20516586303710938\n",
      "training: 62 batch 531 batch_loss: 0.20599287748336792\n",
      "training: 62 batch 532 batch_loss: 0.20446985960006714\n",
      "training: 62 batch 533 batch_loss: 0.20856666564941406\n",
      "training: 62 batch 534 batch_loss: 0.2071371078491211\n",
      "training: 62 batch 535 batch_loss: 0.2069176435470581\n",
      "training: 62 batch 536 batch_loss: 0.2100347876548767\n",
      "training: 62 batch 537 batch_loss: 0.20827451348304749\n",
      "training: 62 batch 538 batch_loss: 0.21207904815673828\n",
      "training: 62 batch 539 batch_loss: 0.20906376838684082\n",
      "training: 62 batch 540 batch_loss: 0.20884299278259277\n",
      "training: 62 batch 541 batch_loss: 0.20853865146636963\n",
      "training: 62 batch 542 batch_loss: 0.2062479853630066\n",
      "training: 62 batch 543 batch_loss: 0.2045852243900299\n",
      "training: 62 batch 544 batch_loss: 0.2068403959274292\n",
      "training: 62 batch 545 batch_loss: 0.20297521352767944\n",
      "training: 62 batch 546 batch_loss: 0.20898881554603577\n",
      "training: 62 batch 547 batch_loss: 0.20566922426223755\n",
      "training: 62 batch 548 batch_loss: 0.2058488130569458\n",
      "training: 62 batch 549 batch_loss: 0.20696237683296204\n",
      "training: 62 batch 550 batch_loss: 0.20924344658851624\n",
      "training: 62 batch 551 batch_loss: 0.20609429478645325\n",
      "training: 62 batch 552 batch_loss: 0.20124977827072144\n",
      "training: 62 batch 553 batch_loss: 0.2097075879573822\n",
      "training: 62 batch 554 batch_loss: 0.207708477973938\n",
      "training: 62 batch 555 batch_loss: 0.20651042461395264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 62 batch 556 batch_loss: 0.20647305250167847\n",
      "training: 62 batch 557 batch_loss: 0.20403480529785156\n",
      "training: 62 batch 558 batch_loss: 0.2098812460899353\n",
      "training: 62 batch 559 batch_loss: 0.20718324184417725\n",
      "training: 62 batch 560 batch_loss: 0.2085731327533722\n",
      "training: 62 batch 561 batch_loss: 0.20622631907463074\n",
      "training: 62 batch 562 batch_loss: 0.20546117424964905\n",
      "training: 62 batch 563 batch_loss: 0.20403265953063965\n",
      "training: 62 batch 564 batch_loss: 0.2054983377456665\n",
      "training: 62 batch 565 batch_loss: 0.21132832765579224\n",
      "training: 62 batch 566 batch_loss: 0.20845246315002441\n",
      "training: 62 batch 567 batch_loss: 0.20718002319335938\n",
      "training: 62 batch 568 batch_loss: 0.20487192273139954\n",
      "training: 62 batch 569 batch_loss: 0.2052634358406067\n",
      "training: 62 batch 570 batch_loss: 0.21098482608795166\n",
      "training: 62 batch 571 batch_loss: 0.20598214864730835\n",
      "training: 62 batch 572 batch_loss: 0.20586171746253967\n",
      "training: 62 batch 573 batch_loss: 0.20952162146568298\n",
      "training: 62 batch 574 batch_loss: 0.20904704928398132\n",
      "training: 62 batch 575 batch_loss: 0.20394760370254517\n",
      "training: 62 batch 576 batch_loss: 0.20195046067237854\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 62, Hit Ratio:0.03296879007838291 | Precision:0.04864346800353878 | Recall:0.06453611796031818 | NDCG:0.06308217401482145\n",
      "*Best Performance* \n",
      "Epoch: 61, Hit Ratio:0.033261934821931224 | Precision:0.04907598545168584 | Recall:0.06479544314235144 | MDCG:0.06348668156373174\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 63 batch 0 batch_loss: 0.20981153845787048\n",
      "training: 63 batch 1 batch_loss: 0.20785343647003174\n",
      "training: 63 batch 2 batch_loss: 0.2090129852294922\n",
      "training: 63 batch 3 batch_loss: 0.20353299379348755\n",
      "training: 63 batch 4 batch_loss: 0.20208272337913513\n",
      "training: 63 batch 5 batch_loss: 0.20786446332931519\n",
      "training: 63 batch 6 batch_loss: 0.20506367087364197\n",
      "training: 63 batch 7 batch_loss: 0.2034631371498108\n",
      "training: 63 batch 8 batch_loss: 0.206954687833786\n",
      "training: 63 batch 9 batch_loss: 0.20406940579414368\n",
      "training: 63 batch 10 batch_loss: 0.20116758346557617\n",
      "training: 63 batch 11 batch_loss: 0.2002429962158203\n",
      "training: 63 batch 12 batch_loss: 0.2059105634689331\n",
      "training: 63 batch 13 batch_loss: 0.2033628523349762\n",
      "training: 63 batch 14 batch_loss: 0.20491144061088562\n",
      "training: 63 batch 15 batch_loss: 0.20104438066482544\n",
      "training: 63 batch 16 batch_loss: 0.21023023128509521\n",
      "training: 63 batch 17 batch_loss: 0.20605424046516418\n",
      "training: 63 batch 18 batch_loss: 0.20635131001472473\n",
      "training: 63 batch 19 batch_loss: 0.20371896028518677\n",
      "training: 63 batch 20 batch_loss: 0.20163536071777344\n",
      "training: 63 batch 21 batch_loss: 0.21093803644180298\n",
      "training: 63 batch 22 batch_loss: 0.20402109622955322\n",
      "training: 63 batch 23 batch_loss: 0.2051805853843689\n",
      "training: 63 batch 24 batch_loss: 0.20507904887199402\n",
      "training: 63 batch 25 batch_loss: 0.2059187889099121\n",
      "training: 63 batch 26 batch_loss: 0.2033843994140625\n",
      "training: 63 batch 27 batch_loss: 0.20771026611328125\n",
      "training: 63 batch 28 batch_loss: 0.20534265041351318\n",
      "training: 63 batch 29 batch_loss: 0.20568245649337769\n",
      "training: 63 batch 30 batch_loss: 0.20564991235733032\n",
      "training: 63 batch 31 batch_loss: 0.20009103417396545\n",
      "training: 63 batch 32 batch_loss: 0.20542964339256287\n",
      "training: 63 batch 33 batch_loss: 0.20668739080429077\n",
      "training: 63 batch 34 batch_loss: 0.20664748549461365\n",
      "training: 63 batch 35 batch_loss: 0.2060510516166687\n",
      "training: 63 batch 36 batch_loss: 0.19970771670341492\n",
      "training: 63 batch 37 batch_loss: 0.2063382863998413\n",
      "training: 63 batch 38 batch_loss: 0.20644980669021606\n",
      "training: 63 batch 39 batch_loss: 0.2020793855190277\n",
      "training: 63 batch 40 batch_loss: 0.20753204822540283\n",
      "training: 63 batch 41 batch_loss: 0.20211663842201233\n",
      "training: 63 batch 42 batch_loss: 0.2027479112148285\n",
      "training: 63 batch 43 batch_loss: 0.2001073658466339\n",
      "training: 63 batch 44 batch_loss: 0.20367431640625\n",
      "training: 63 batch 45 batch_loss: 0.20773503184318542\n",
      "training: 63 batch 46 batch_loss: 0.20367366075515747\n",
      "training: 63 batch 47 batch_loss: 0.2014368772506714\n",
      "training: 63 batch 48 batch_loss: 0.20616403222084045\n",
      "training: 63 batch 49 batch_loss: 0.20653390884399414\n",
      "training: 63 batch 50 batch_loss: 0.2056952714920044\n",
      "training: 63 batch 51 batch_loss: 0.20425444841384888\n",
      "training: 63 batch 52 batch_loss: 0.20557060837745667\n",
      "training: 63 batch 53 batch_loss: 0.20837724208831787\n",
      "training: 63 batch 54 batch_loss: 0.20501413941383362\n",
      "training: 63 batch 55 batch_loss: 0.20544487237930298\n",
      "training: 63 batch 56 batch_loss: 0.1997033953666687\n",
      "training: 63 batch 57 batch_loss: 0.20677778124809265\n",
      "training: 63 batch 58 batch_loss: 0.20357754826545715\n",
      "training: 63 batch 59 batch_loss: 0.19793736934661865\n",
      "training: 63 batch 60 batch_loss: 0.20595133304595947\n",
      "training: 63 batch 61 batch_loss: 0.20778748393058777\n",
      "training: 63 batch 62 batch_loss: 0.2052646279335022\n",
      "training: 63 batch 63 batch_loss: 0.20469340682029724\n",
      "training: 63 batch 64 batch_loss: 0.20764446258544922\n",
      "training: 63 batch 65 batch_loss: 0.20266833901405334\n",
      "training: 63 batch 66 batch_loss: 0.20739006996154785\n",
      "training: 63 batch 67 batch_loss: 0.20395225286483765\n",
      "training: 63 batch 68 batch_loss: 0.20888152718544006\n",
      "training: 63 batch 69 batch_loss: 0.2028183937072754\n",
      "training: 63 batch 70 batch_loss: 0.20596548914909363\n",
      "training: 63 batch 71 batch_loss: 0.20495456457138062\n",
      "training: 63 batch 72 batch_loss: 0.2043212652206421\n",
      "training: 63 batch 73 batch_loss: 0.2046462893486023\n",
      "training: 63 batch 74 batch_loss: 0.20604655146598816\n",
      "training: 63 batch 75 batch_loss: 0.20721033215522766\n",
      "training: 63 batch 76 batch_loss: 0.2068983018398285\n",
      "training: 63 batch 77 batch_loss: 0.20580706000328064\n",
      "training: 63 batch 78 batch_loss: 0.20778322219848633\n",
      "training: 63 batch 79 batch_loss: 0.1996062994003296\n",
      "training: 63 batch 80 batch_loss: 0.2025415599346161\n",
      "training: 63 batch 81 batch_loss: 0.20611661672592163\n",
      "training: 63 batch 82 batch_loss: 0.210443913936615\n",
      "training: 63 batch 83 batch_loss: 0.19971933960914612\n",
      "training: 63 batch 84 batch_loss: 0.20573699474334717\n",
      "training: 63 batch 85 batch_loss: 0.20572581887245178\n",
      "training: 63 batch 86 batch_loss: 0.20502841472625732\n",
      "training: 63 batch 87 batch_loss: 0.20365765690803528\n",
      "training: 63 batch 88 batch_loss: 0.20247882604599\n",
      "training: 63 batch 89 batch_loss: 0.20287853479385376\n",
      "training: 63 batch 90 batch_loss: 0.2068406045436859\n",
      "training: 63 batch 91 batch_loss: 0.20558184385299683\n",
      "training: 63 batch 92 batch_loss: 0.20340991020202637\n",
      "training: 63 batch 93 batch_loss: 0.20542055368423462\n",
      "training: 63 batch 94 batch_loss: 0.20623540878295898\n",
      "training: 63 batch 95 batch_loss: 0.20970410108566284\n",
      "training: 63 batch 96 batch_loss: 0.20399349927902222\n",
      "training: 63 batch 97 batch_loss: 0.20623239874839783\n",
      "training: 63 batch 98 batch_loss: 0.2109847068786621\n",
      "training: 63 batch 99 batch_loss: 0.2076324224472046\n",
      "training: 63 batch 100 batch_loss: 0.20752206444740295\n",
      "training: 63 batch 101 batch_loss: 0.20918598771095276\n",
      "training: 63 batch 102 batch_loss: 0.20428892970085144\n",
      "training: 63 batch 103 batch_loss: 0.2031545341014862\n",
      "training: 63 batch 104 batch_loss: 0.2042127251625061\n",
      "training: 63 batch 105 batch_loss: 0.20883312821388245\n",
      "training: 63 batch 106 batch_loss: 0.2053564488887787\n",
      "training: 63 batch 107 batch_loss: 0.20764419436454773\n",
      "training: 63 batch 108 batch_loss: 0.20774328708648682\n",
      "training: 63 batch 109 batch_loss: 0.20548105239868164\n",
      "training: 63 batch 110 batch_loss: 0.20616275072097778\n",
      "training: 63 batch 111 batch_loss: 0.20525360107421875\n",
      "training: 63 batch 112 batch_loss: 0.20837876200675964\n",
      "training: 63 batch 113 batch_loss: 0.20083603262901306\n",
      "training: 63 batch 114 batch_loss: 0.2047840654850006\n",
      "training: 63 batch 115 batch_loss: 0.20248949527740479\n",
      "training: 63 batch 116 batch_loss: 0.20742225646972656\n",
      "training: 63 batch 117 batch_loss: 0.20766738057136536\n",
      "training: 63 batch 118 batch_loss: 0.2073427140712738\n",
      "training: 63 batch 119 batch_loss: 0.20252379775047302\n",
      "training: 63 batch 120 batch_loss: 0.20966216921806335\n",
      "training: 63 batch 121 batch_loss: 0.20689553022384644\n",
      "training: 63 batch 122 batch_loss: 0.20483246445655823\n",
      "training: 63 batch 123 batch_loss: 0.2094016671180725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 63 batch 124 batch_loss: 0.20565184950828552\n",
      "training: 63 batch 125 batch_loss: 0.20815449953079224\n",
      "training: 63 batch 126 batch_loss: 0.2067159116268158\n",
      "training: 63 batch 127 batch_loss: 0.20952677726745605\n",
      "training: 63 batch 128 batch_loss: 0.2050531506538391\n",
      "training: 63 batch 129 batch_loss: 0.20478808879852295\n",
      "training: 63 batch 130 batch_loss: 0.20494148135185242\n",
      "training: 63 batch 131 batch_loss: 0.20565685629844666\n",
      "training: 63 batch 132 batch_loss: 0.209494948387146\n",
      "training: 63 batch 133 batch_loss: 0.2080615758895874\n",
      "training: 63 batch 134 batch_loss: 0.19965499639511108\n",
      "training: 63 batch 135 batch_loss: 0.2017713487148285\n",
      "training: 63 batch 136 batch_loss: 0.2071262001991272\n",
      "training: 63 batch 137 batch_loss: 0.20984512567520142\n",
      "training: 63 batch 138 batch_loss: 0.20392748713493347\n",
      "training: 63 batch 139 batch_loss: 0.200519859790802\n",
      "training: 63 batch 140 batch_loss: 0.19856280088424683\n",
      "training: 63 batch 141 batch_loss: 0.20772260427474976\n",
      "training: 63 batch 142 batch_loss: 0.20364946126937866\n",
      "training: 63 batch 143 batch_loss: 0.20633363723754883\n",
      "training: 63 batch 144 batch_loss: 0.20394983887672424\n",
      "training: 63 batch 145 batch_loss: 0.2028927206993103\n",
      "training: 63 batch 146 batch_loss: 0.2033693790435791\n",
      "training: 63 batch 147 batch_loss: 0.20784059166908264\n",
      "training: 63 batch 148 batch_loss: 0.20942208170890808\n",
      "training: 63 batch 149 batch_loss: 0.2055400013923645\n",
      "training: 63 batch 150 batch_loss: 0.2020263969898224\n",
      "training: 63 batch 151 batch_loss: 0.20487439632415771\n",
      "training: 63 batch 152 batch_loss: 0.20705848932266235\n",
      "training: 63 batch 153 batch_loss: 0.20340925455093384\n",
      "training: 63 batch 154 batch_loss: 0.2066136598587036\n",
      "training: 63 batch 155 batch_loss: 0.20888328552246094\n",
      "training: 63 batch 156 batch_loss: 0.2103559374809265\n",
      "training: 63 batch 157 batch_loss: 0.20609882473945618\n",
      "training: 63 batch 158 batch_loss: 0.20703211426734924\n",
      "training: 63 batch 159 batch_loss: 0.20346957445144653\n",
      "training: 63 batch 160 batch_loss: 0.211919903755188\n",
      "training: 63 batch 161 batch_loss: 0.20458704233169556\n",
      "training: 63 batch 162 batch_loss: 0.20566686987876892\n",
      "training: 63 batch 163 batch_loss: 0.20530858635902405\n",
      "training: 63 batch 164 batch_loss: 0.2096986472606659\n",
      "training: 63 batch 165 batch_loss: 0.2058437466621399\n",
      "training: 63 batch 166 batch_loss: 0.2076166570186615\n",
      "training: 63 batch 167 batch_loss: 0.20852738618850708\n",
      "training: 63 batch 168 batch_loss: 0.20999497175216675\n",
      "training: 63 batch 169 batch_loss: 0.203357994556427\n",
      "training: 63 batch 170 batch_loss: 0.20050689578056335\n",
      "training: 63 batch 171 batch_loss: 0.20638206601142883\n",
      "training: 63 batch 172 batch_loss: 0.2040298581123352\n",
      "training: 63 batch 173 batch_loss: 0.20449918508529663\n",
      "training: 63 batch 174 batch_loss: 0.2068069875240326\n",
      "training: 63 batch 175 batch_loss: 0.20861223340034485\n",
      "training: 63 batch 176 batch_loss: 0.20800304412841797\n",
      "training: 63 batch 177 batch_loss: 0.20534250140190125\n",
      "training: 63 batch 178 batch_loss: 0.2056349515914917\n",
      "training: 63 batch 179 batch_loss: 0.2065858542919159\n",
      "training: 63 batch 180 batch_loss: 0.20681285858154297\n",
      "training: 63 batch 181 batch_loss: 0.2049247920513153\n",
      "training: 63 batch 182 batch_loss: 0.20568063855171204\n",
      "training: 63 batch 183 batch_loss: 0.20583978295326233\n",
      "training: 63 batch 184 batch_loss: 0.20640778541564941\n",
      "training: 63 batch 185 batch_loss: 0.20281419157981873\n",
      "training: 63 batch 186 batch_loss: 0.2123602330684662\n",
      "training: 63 batch 187 batch_loss: 0.2068159282207489\n",
      "training: 63 batch 188 batch_loss: 0.20488682389259338\n",
      "training: 63 batch 189 batch_loss: 0.2092680037021637\n",
      "training: 63 batch 190 batch_loss: 0.20491164922714233\n",
      "training: 63 batch 191 batch_loss: 0.2120635211467743\n",
      "training: 63 batch 192 batch_loss: 0.2054556906223297\n",
      "training: 63 batch 193 batch_loss: 0.20266759395599365\n",
      "training: 63 batch 194 batch_loss: 0.20382636785507202\n",
      "training: 63 batch 195 batch_loss: 0.20581161975860596\n",
      "training: 63 batch 196 batch_loss: 0.2071743607521057\n",
      "training: 63 batch 197 batch_loss: 0.20522552728652954\n",
      "training: 63 batch 198 batch_loss: 0.21106159687042236\n",
      "training: 63 batch 199 batch_loss: 0.20511677861213684\n",
      "training: 63 batch 200 batch_loss: 0.20770499110221863\n",
      "training: 63 batch 201 batch_loss: 0.20380687713623047\n",
      "training: 63 batch 202 batch_loss: 0.20293694734573364\n",
      "training: 63 batch 203 batch_loss: 0.20750409364700317\n",
      "training: 63 batch 204 batch_loss: 0.20572873950004578\n",
      "training: 63 batch 205 batch_loss: 0.20611411333084106\n",
      "training: 63 batch 206 batch_loss: 0.20296376943588257\n",
      "training: 63 batch 207 batch_loss: 0.2110390067100525\n",
      "training: 63 batch 208 batch_loss: 0.20567181706428528\n",
      "training: 63 batch 209 batch_loss: 0.20841047167778015\n",
      "training: 63 batch 210 batch_loss: 0.21008175611495972\n",
      "training: 63 batch 211 batch_loss: 0.2031528651714325\n",
      "training: 63 batch 212 batch_loss: 0.20705276727676392\n",
      "training: 63 batch 213 batch_loss: 0.21060538291931152\n",
      "training: 63 batch 214 batch_loss: 0.2035675048828125\n",
      "training: 63 batch 215 batch_loss: 0.2098046839237213\n",
      "training: 63 batch 216 batch_loss: 0.20475205779075623\n",
      "training: 63 batch 217 batch_loss: 0.20975077152252197\n",
      "training: 63 batch 218 batch_loss: 0.20447367429733276\n",
      "training: 63 batch 219 batch_loss: 0.20564687252044678\n",
      "training: 63 batch 220 batch_loss: 0.20542755722999573\n",
      "training: 63 batch 221 batch_loss: 0.2051635980606079\n",
      "training: 63 batch 222 batch_loss: 0.20581990480422974\n",
      "training: 63 batch 223 batch_loss: 0.20299547910690308\n",
      "training: 63 batch 224 batch_loss: 0.20749062299728394\n",
      "training: 63 batch 225 batch_loss: 0.20630264282226562\n",
      "training: 63 batch 226 batch_loss: 0.20235788822174072\n",
      "training: 63 batch 227 batch_loss: 0.20412862300872803\n",
      "training: 63 batch 228 batch_loss: 0.20517084002494812\n",
      "training: 63 batch 229 batch_loss: 0.21036338806152344\n",
      "training: 63 batch 230 batch_loss: 0.20522332191467285\n",
      "training: 63 batch 231 batch_loss: 0.20014777779579163\n",
      "training: 63 batch 232 batch_loss: 0.20725682377815247\n",
      "training: 63 batch 233 batch_loss: 0.20811134576797485\n",
      "training: 63 batch 234 batch_loss: 0.2087584137916565\n",
      "training: 63 batch 235 batch_loss: 0.20859885215759277\n",
      "training: 63 batch 236 batch_loss: 0.20829889178276062\n",
      "training: 63 batch 237 batch_loss: 0.20344278216362\n",
      "training: 63 batch 238 batch_loss: 0.20821473002433777\n",
      "training: 63 batch 239 batch_loss: 0.20390331745147705\n",
      "training: 63 batch 240 batch_loss: 0.20436343550682068\n",
      "training: 63 batch 241 batch_loss: 0.20540916919708252\n",
      "training: 63 batch 242 batch_loss: 0.20463019609451294\n",
      "training: 63 batch 243 batch_loss: 0.20667243003845215\n",
      "training: 63 batch 244 batch_loss: 0.20840179920196533\n",
      "training: 63 batch 245 batch_loss: 0.2070934772491455\n",
      "training: 63 batch 246 batch_loss: 0.20708835124969482\n",
      "training: 63 batch 247 batch_loss: 0.2073478102684021\n",
      "training: 63 batch 248 batch_loss: 0.20502033829689026\n",
      "training: 63 batch 249 batch_loss: 0.20649650692939758\n",
      "training: 63 batch 250 batch_loss: 0.20758268237113953\n",
      "training: 63 batch 251 batch_loss: 0.21300068497657776\n",
      "training: 63 batch 252 batch_loss: 0.20493125915527344\n",
      "training: 63 batch 253 batch_loss: 0.20426321029663086\n",
      "training: 63 batch 254 batch_loss: 0.20240584015846252\n",
      "training: 63 batch 255 batch_loss: 0.20724156498908997\n",
      "training: 63 batch 256 batch_loss: 0.2073214054107666\n",
      "training: 63 batch 257 batch_loss: 0.2058199644088745\n",
      "training: 63 batch 258 batch_loss: 0.20435115694999695\n",
      "training: 63 batch 259 batch_loss: 0.20497563481330872\n",
      "training: 63 batch 260 batch_loss: 0.20442882180213928\n",
      "training: 63 batch 261 batch_loss: 0.2043296992778778\n",
      "training: 63 batch 262 batch_loss: 0.20607781410217285\n",
      "training: 63 batch 263 batch_loss: 0.20400092005729675\n",
      "training: 63 batch 264 batch_loss: 0.20181706547737122\n",
      "training: 63 batch 265 batch_loss: 0.20927083492279053\n",
      "training: 63 batch 266 batch_loss: 0.20355701446533203\n",
      "training: 63 batch 267 batch_loss: 0.20739337801933289\n",
      "training: 63 batch 268 batch_loss: 0.20805925130844116\n",
      "training: 63 batch 269 batch_loss: 0.20808836817741394\n",
      "training: 63 batch 270 batch_loss: 0.2060914933681488\n",
      "training: 63 batch 271 batch_loss: 0.20706966519355774\n",
      "training: 63 batch 272 batch_loss: 0.20396602153778076\n",
      "training: 63 batch 273 batch_loss: 0.20723044872283936\n",
      "training: 63 batch 274 batch_loss: 0.2039434015750885\n",
      "training: 63 batch 275 batch_loss: 0.20660486817359924\n",
      "training: 63 batch 276 batch_loss: 0.20750290155410767\n",
      "training: 63 batch 277 batch_loss: 0.20675331354141235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 63 batch 278 batch_loss: 0.2083132266998291\n",
      "training: 63 batch 279 batch_loss: 0.21306633949279785\n",
      "training: 63 batch 280 batch_loss: 0.20735052227973938\n",
      "training: 63 batch 281 batch_loss: 0.20779001712799072\n",
      "training: 63 batch 282 batch_loss: 0.20392566919326782\n",
      "training: 63 batch 283 batch_loss: 0.20701420307159424\n",
      "training: 63 batch 284 batch_loss: 0.20470649003982544\n",
      "training: 63 batch 285 batch_loss: 0.2018284797668457\n",
      "training: 63 batch 286 batch_loss: 0.20463436841964722\n",
      "training: 63 batch 287 batch_loss: 0.20641186833381653\n",
      "training: 63 batch 288 batch_loss: 0.21142444014549255\n",
      "training: 63 batch 289 batch_loss: 0.2056199312210083\n",
      "training: 63 batch 290 batch_loss: 0.20433497428894043\n",
      "training: 63 batch 291 batch_loss: 0.20283058285713196\n",
      "training: 63 batch 292 batch_loss: 0.20518678426742554\n",
      "training: 63 batch 293 batch_loss: 0.20435228943824768\n",
      "training: 63 batch 294 batch_loss: 0.20633664727210999\n",
      "training: 63 batch 295 batch_loss: 0.21064463257789612\n",
      "training: 63 batch 296 batch_loss: 0.20795023441314697\n",
      "training: 63 batch 297 batch_loss: 0.20094141364097595\n",
      "training: 63 batch 298 batch_loss: 0.20647215843200684\n",
      "training: 63 batch 299 batch_loss: 0.20820355415344238\n",
      "training: 63 batch 300 batch_loss: 0.20676741003990173\n",
      "training: 63 batch 301 batch_loss: 0.2024494707584381\n",
      "training: 63 batch 302 batch_loss: 0.2054341733455658\n",
      "training: 63 batch 303 batch_loss: 0.2051936686038971\n",
      "training: 63 batch 304 batch_loss: 0.20599982142448425\n",
      "training: 63 batch 305 batch_loss: 0.20698711276054382\n",
      "training: 63 batch 306 batch_loss: 0.20543748140335083\n",
      "training: 63 batch 307 batch_loss: 0.20696622133255005\n",
      "training: 63 batch 308 batch_loss: 0.20918837189674377\n",
      "training: 63 batch 309 batch_loss: 0.2071654200553894\n",
      "training: 63 batch 310 batch_loss: 0.20819038152694702\n",
      "training: 63 batch 311 batch_loss: 0.20226559042930603\n",
      "training: 63 batch 312 batch_loss: 0.20526400208473206\n",
      "training: 63 batch 313 batch_loss: 0.20409831404685974\n",
      "training: 63 batch 314 batch_loss: 0.20453345775604248\n",
      "training: 63 batch 315 batch_loss: 0.20383000373840332\n",
      "training: 63 batch 316 batch_loss: 0.2096378207206726\n",
      "training: 63 batch 317 batch_loss: 0.20750290155410767\n",
      "training: 63 batch 318 batch_loss: 0.20771104097366333\n",
      "training: 63 batch 319 batch_loss: 0.20788058638572693\n",
      "training: 63 batch 320 batch_loss: 0.20936855673789978\n",
      "training: 63 batch 321 batch_loss: 0.20759695768356323\n",
      "training: 63 batch 322 batch_loss: 0.20578870177268982\n",
      "training: 63 batch 323 batch_loss: 0.20941579341888428\n",
      "training: 63 batch 324 batch_loss: 0.20838525891304016\n",
      "training: 63 batch 325 batch_loss: 0.20565056800842285\n",
      "training: 63 batch 326 batch_loss: 0.20648157596588135\n",
      "training: 63 batch 327 batch_loss: 0.20915931463241577\n",
      "training: 63 batch 328 batch_loss: 0.21082830429077148\n",
      "training: 63 batch 329 batch_loss: 0.2087438404560089\n",
      "training: 63 batch 330 batch_loss: 0.20678314566612244\n",
      "training: 63 batch 331 batch_loss: 0.20668768882751465\n",
      "training: 63 batch 332 batch_loss: 0.20714843273162842\n",
      "training: 63 batch 333 batch_loss: 0.20325025916099548\n",
      "training: 63 batch 334 batch_loss: 0.20416826009750366\n",
      "training: 63 batch 335 batch_loss: 0.2138240933418274\n",
      "training: 63 batch 336 batch_loss: 0.20386797189712524\n",
      "training: 63 batch 337 batch_loss: 0.209855318069458\n",
      "training: 63 batch 338 batch_loss: 0.2117289900779724\n",
      "training: 63 batch 339 batch_loss: 0.20695972442626953\n",
      "training: 63 batch 340 batch_loss: 0.20768830180168152\n",
      "training: 63 batch 341 batch_loss: 0.2056993544101715\n",
      "training: 63 batch 342 batch_loss: 0.20633110404014587\n",
      "training: 63 batch 343 batch_loss: 0.21053248643875122\n",
      "training: 63 batch 344 batch_loss: 0.2061888873577118\n",
      "training: 63 batch 345 batch_loss: 0.20775607228279114\n",
      "training: 63 batch 346 batch_loss: 0.20571336150169373\n",
      "training: 63 batch 347 batch_loss: 0.2077205777168274\n",
      "training: 63 batch 348 batch_loss: 0.20604172348976135\n",
      "training: 63 batch 349 batch_loss: 0.20466399192810059\n",
      "training: 63 batch 350 batch_loss: 0.212051123380661\n",
      "training: 63 batch 351 batch_loss: 0.20610767602920532\n",
      "training: 63 batch 352 batch_loss: 0.20666471123695374\n",
      "training: 63 batch 353 batch_loss: 0.20366743206977844\n",
      "training: 63 batch 354 batch_loss: 0.20652195811271667\n",
      "training: 63 batch 355 batch_loss: 0.20642155408859253\n",
      "training: 63 batch 356 batch_loss: 0.2082585096359253\n",
      "training: 63 batch 357 batch_loss: 0.21119722723960876\n",
      "training: 63 batch 358 batch_loss: 0.21077024936676025\n",
      "training: 63 batch 359 batch_loss: 0.20882853865623474\n",
      "training: 63 batch 360 batch_loss: 0.20596355199813843\n",
      "training: 63 batch 361 batch_loss: 0.20733243227005005\n",
      "training: 63 batch 362 batch_loss: 0.20920521020889282\n",
      "training: 63 batch 363 batch_loss: 0.20681557059288025\n",
      "training: 63 batch 364 batch_loss: 0.20694291591644287\n",
      "training: 63 batch 365 batch_loss: 0.2109488844871521\n",
      "training: 63 batch 366 batch_loss: 0.2039160132408142\n",
      "training: 63 batch 367 batch_loss: 0.20953842997550964\n",
      "training: 63 batch 368 batch_loss: 0.20880991220474243\n",
      "training: 63 batch 369 batch_loss: 0.20887604355812073\n",
      "training: 63 batch 370 batch_loss: 0.20718589425086975\n",
      "training: 63 batch 371 batch_loss: 0.21171310544013977\n",
      "training: 63 batch 372 batch_loss: 0.20625445246696472\n",
      "training: 63 batch 373 batch_loss: 0.20663511753082275\n",
      "training: 63 batch 374 batch_loss: 0.20694905519485474\n",
      "training: 63 batch 375 batch_loss: 0.20797684788703918\n",
      "training: 63 batch 376 batch_loss: 0.20578226447105408\n",
      "training: 63 batch 377 batch_loss: 0.206265389919281\n",
      "training: 63 batch 378 batch_loss: 0.21033260226249695\n",
      "training: 63 batch 379 batch_loss: 0.20812451839447021\n",
      "training: 63 batch 380 batch_loss: 0.20976540446281433\n",
      "training: 63 batch 381 batch_loss: 0.20438268780708313\n",
      "training: 63 batch 382 batch_loss: 0.20260488986968994\n",
      "training: 63 batch 383 batch_loss: 0.20592042803764343\n",
      "training: 63 batch 384 batch_loss: 0.20701396465301514\n",
      "training: 63 batch 385 batch_loss: 0.2073180079460144\n",
      "training: 63 batch 386 batch_loss: 0.20465612411499023\n",
      "training: 63 batch 387 batch_loss: 0.20808684825897217\n",
      "training: 63 batch 388 batch_loss: 0.20527592301368713\n",
      "training: 63 batch 389 batch_loss: 0.20646026730537415\n",
      "training: 63 batch 390 batch_loss: 0.20774561166763306\n",
      "training: 63 batch 391 batch_loss: 0.20711436867713928\n",
      "training: 63 batch 392 batch_loss: 0.20705056190490723\n",
      "training: 63 batch 393 batch_loss: 0.20252519845962524\n",
      "training: 63 batch 394 batch_loss: 0.20406392216682434\n",
      "training: 63 batch 395 batch_loss: 0.214695543050766\n",
      "training: 63 batch 396 batch_loss: 0.2071821689605713\n",
      "training: 63 batch 397 batch_loss: 0.20674875378608704\n",
      "training: 63 batch 398 batch_loss: 0.21088320016860962\n",
      "training: 63 batch 399 batch_loss: 0.20720207691192627\n",
      "training: 63 batch 400 batch_loss: 0.20731744170188904\n",
      "training: 63 batch 401 batch_loss: 0.2086799144744873\n",
      "training: 63 batch 402 batch_loss: 0.20455101132392883\n",
      "training: 63 batch 403 batch_loss: 0.20812880992889404\n",
      "training: 63 batch 404 batch_loss: 0.20862287282943726\n",
      "training: 63 batch 405 batch_loss: 0.20639467239379883\n",
      "training: 63 batch 406 batch_loss: 0.21062695980072021\n",
      "training: 63 batch 407 batch_loss: 0.2052278220653534\n",
      "training: 63 batch 408 batch_loss: 0.21006646752357483\n",
      "training: 63 batch 409 batch_loss: 0.20459246635437012\n",
      "training: 63 batch 410 batch_loss: 0.20707666873931885\n",
      "training: 63 batch 411 batch_loss: 0.20430254936218262\n",
      "training: 63 batch 412 batch_loss: 0.20610439777374268\n",
      "training: 63 batch 413 batch_loss: 0.20532870292663574\n",
      "training: 63 batch 414 batch_loss: 0.20778784155845642\n",
      "training: 63 batch 415 batch_loss: 0.20627814531326294\n",
      "training: 63 batch 416 batch_loss: 0.20904317498207092\n",
      "training: 63 batch 417 batch_loss: 0.20657169818878174\n",
      "training: 63 batch 418 batch_loss: 0.20624065399169922\n",
      "training: 63 batch 419 batch_loss: 0.21041584014892578\n",
      "training: 63 batch 420 batch_loss: 0.20676898956298828\n",
      "training: 63 batch 421 batch_loss: 0.21017232537269592\n",
      "training: 63 batch 422 batch_loss: 0.21005797386169434\n",
      "training: 63 batch 423 batch_loss: 0.20623427629470825\n",
      "training: 63 batch 424 batch_loss: 0.20930838584899902\n",
      "training: 63 batch 425 batch_loss: 0.20227202773094177\n",
      "training: 63 batch 426 batch_loss: 0.20581942796707153\n",
      "training: 63 batch 427 batch_loss: 0.2058122158050537\n",
      "training: 63 batch 428 batch_loss: 0.20599579811096191\n",
      "training: 63 batch 429 batch_loss: 0.20553243160247803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 63 batch 430 batch_loss: 0.20984897017478943\n",
      "training: 63 batch 431 batch_loss: 0.20747977495193481\n",
      "training: 63 batch 432 batch_loss: 0.20696216821670532\n",
      "training: 63 batch 433 batch_loss: 0.20563799142837524\n",
      "training: 63 batch 434 batch_loss: 0.20587095618247986\n",
      "training: 63 batch 435 batch_loss: 0.2047058343887329\n",
      "training: 63 batch 436 batch_loss: 0.20920288562774658\n",
      "training: 63 batch 437 batch_loss: 0.2100294828414917\n",
      "training: 63 batch 438 batch_loss: 0.2078922688961029\n",
      "training: 63 batch 439 batch_loss: 0.21160855889320374\n",
      "training: 63 batch 440 batch_loss: 0.2069002389907837\n",
      "training: 63 batch 441 batch_loss: 0.20871758460998535\n",
      "training: 63 batch 442 batch_loss: 0.20863911509513855\n",
      "training: 63 batch 443 batch_loss: 0.20831352472305298\n",
      "training: 63 batch 444 batch_loss: 0.2100476622581482\n",
      "training: 63 batch 445 batch_loss: 0.20539534091949463\n",
      "training: 63 batch 446 batch_loss: 0.2085171937942505\n",
      "training: 63 batch 447 batch_loss: 0.206266850233078\n",
      "training: 63 batch 448 batch_loss: 0.20495092868804932\n",
      "training: 63 batch 449 batch_loss: 0.20976099371910095\n",
      "training: 63 batch 450 batch_loss: 0.20509088039398193\n",
      "training: 63 batch 451 batch_loss: 0.20416715741157532\n",
      "training: 63 batch 452 batch_loss: 0.2075451910495758\n",
      "training: 63 batch 453 batch_loss: 0.21097159385681152\n",
      "training: 63 batch 454 batch_loss: 0.2056836187839508\n",
      "training: 63 batch 455 batch_loss: 0.20778509974479675\n",
      "training: 63 batch 456 batch_loss: 0.20747599005699158\n",
      "training: 63 batch 457 batch_loss: 0.2072548270225525\n",
      "training: 63 batch 458 batch_loss: 0.20940393209457397\n",
      "training: 63 batch 459 batch_loss: 0.2072734534740448\n",
      "training: 63 batch 460 batch_loss: 0.2121567726135254\n",
      "training: 63 batch 461 batch_loss: 0.2100948989391327\n",
      "training: 63 batch 462 batch_loss: 0.20647203922271729\n",
      "training: 63 batch 463 batch_loss: 0.20852994918823242\n",
      "training: 63 batch 464 batch_loss: 0.20821484923362732\n",
      "training: 63 batch 465 batch_loss: 0.20895537734031677\n",
      "training: 63 batch 466 batch_loss: 0.20767229795455933\n",
      "training: 63 batch 467 batch_loss: 0.21073278784751892\n",
      "training: 63 batch 468 batch_loss: 0.20350658893585205\n",
      "training: 63 batch 469 batch_loss: 0.20948725938796997\n",
      "training: 63 batch 470 batch_loss: 0.20785942673683167\n",
      "training: 63 batch 471 batch_loss: 0.20569413900375366\n",
      "training: 63 batch 472 batch_loss: 0.21040457487106323\n",
      "training: 63 batch 473 batch_loss: 0.2068769633769989\n",
      "training: 63 batch 474 batch_loss: 0.2058449387550354\n",
      "training: 63 batch 475 batch_loss: 0.2062268853187561\n",
      "training: 63 batch 476 batch_loss: 0.21189674735069275\n",
      "training: 63 batch 477 batch_loss: 0.20971229672431946\n",
      "training: 63 batch 478 batch_loss: 0.20884081721305847\n",
      "training: 63 batch 479 batch_loss: 0.20444464683532715\n",
      "training: 63 batch 480 batch_loss: 0.20748299360275269\n",
      "training: 63 batch 481 batch_loss: 0.21391263604164124\n",
      "training: 63 batch 482 batch_loss: 0.20584985613822937\n",
      "training: 63 batch 483 batch_loss: 0.20713922381401062\n",
      "training: 63 batch 484 batch_loss: 0.20648980140686035\n",
      "training: 63 batch 485 batch_loss: 0.20557308197021484\n",
      "training: 63 batch 486 batch_loss: 0.2129109799861908\n",
      "training: 63 batch 487 batch_loss: 0.20863056182861328\n",
      "training: 63 batch 488 batch_loss: 0.2068612277507782\n",
      "training: 63 batch 489 batch_loss: 0.20846861600875854\n",
      "training: 63 batch 490 batch_loss: 0.2079961597919464\n",
      "training: 63 batch 491 batch_loss: 0.20903077721595764\n",
      "training: 63 batch 492 batch_loss: 0.2098580300807953\n",
      "training: 63 batch 493 batch_loss: 0.20651507377624512\n",
      "training: 63 batch 494 batch_loss: 0.2081700563430786\n",
      "training: 63 batch 495 batch_loss: 0.21160602569580078\n",
      "training: 63 batch 496 batch_loss: 0.2101340889930725\n",
      "training: 63 batch 497 batch_loss: 0.20656493306159973\n",
      "training: 63 batch 498 batch_loss: 0.20493006706237793\n",
      "training: 63 batch 499 batch_loss: 0.20582342147827148\n",
      "training: 63 batch 500 batch_loss: 0.20461252331733704\n",
      "training: 63 batch 501 batch_loss: 0.20853227376937866\n",
      "training: 63 batch 502 batch_loss: 0.20758473873138428\n",
      "training: 63 batch 503 batch_loss: 0.2075318694114685\n",
      "training: 63 batch 504 batch_loss: 0.20981630682945251\n",
      "training: 63 batch 505 batch_loss: 0.20566219091415405\n",
      "training: 63 batch 506 batch_loss: 0.20388445258140564\n",
      "training: 63 batch 507 batch_loss: 0.20910561084747314\n",
      "training: 63 batch 508 batch_loss: 0.20320338010787964\n",
      "training: 63 batch 509 batch_loss: 0.20899325609207153\n",
      "training: 63 batch 510 batch_loss: 0.20560038089752197\n",
      "training: 63 batch 511 batch_loss: 0.20856842398643494\n",
      "training: 63 batch 512 batch_loss: 0.20879226922988892\n",
      "training: 63 batch 513 batch_loss: 0.20772692561149597\n",
      "training: 63 batch 514 batch_loss: 0.2088724672794342\n",
      "training: 63 batch 515 batch_loss: 0.20978954434394836\n",
      "training: 63 batch 516 batch_loss: 0.20772412419319153\n",
      "training: 63 batch 517 batch_loss: 0.20763173699378967\n",
      "training: 63 batch 518 batch_loss: 0.2117622196674347\n",
      "training: 63 batch 519 batch_loss: 0.20732110738754272\n",
      "training: 63 batch 520 batch_loss: 0.2049621045589447\n",
      "training: 63 batch 521 batch_loss: 0.20609638094902039\n",
      "training: 63 batch 522 batch_loss: 0.20540007948875427\n",
      "training: 63 batch 523 batch_loss: 0.2042279839515686\n",
      "training: 63 batch 524 batch_loss: 0.2079431414604187\n",
      "training: 63 batch 525 batch_loss: 0.20700111985206604\n",
      "training: 63 batch 526 batch_loss: 0.2061925232410431\n",
      "training: 63 batch 527 batch_loss: 0.20401260256767273\n",
      "training: 63 batch 528 batch_loss: 0.20669186115264893\n",
      "training: 63 batch 529 batch_loss: 0.2042117714881897\n",
      "training: 63 batch 530 batch_loss: 0.20874455571174622\n",
      "training: 63 batch 531 batch_loss: 0.20741039514541626\n",
      "training: 63 batch 532 batch_loss: 0.20713168382644653\n",
      "training: 63 batch 533 batch_loss: 0.20408645272254944\n",
      "training: 63 batch 534 batch_loss: 0.2063743770122528\n",
      "training: 63 batch 535 batch_loss: 0.20503857731819153\n",
      "training: 63 batch 536 batch_loss: 0.20858532190322876\n",
      "training: 63 batch 537 batch_loss: 0.2073834240436554\n",
      "training: 63 batch 538 batch_loss: 0.21054017543792725\n",
      "training: 63 batch 539 batch_loss: 0.20898392796516418\n",
      "training: 63 batch 540 batch_loss: 0.20591408014297485\n",
      "training: 63 batch 541 batch_loss: 0.20796123147010803\n",
      "training: 63 batch 542 batch_loss: 0.2076907455921173\n",
      "training: 63 batch 543 batch_loss: 0.2101912498474121\n",
      "training: 63 batch 544 batch_loss: 0.20444735884666443\n",
      "training: 63 batch 545 batch_loss: 0.20525524020195007\n",
      "training: 63 batch 546 batch_loss: 0.20524242520332336\n",
      "training: 63 batch 547 batch_loss: 0.2067386507987976\n",
      "training: 63 batch 548 batch_loss: 0.21020054817199707\n",
      "training: 63 batch 549 batch_loss: 0.2101549506187439\n",
      "training: 63 batch 550 batch_loss: 0.2105139195919037\n",
      "training: 63 batch 551 batch_loss: 0.20144903659820557\n",
      "training: 63 batch 552 batch_loss: 0.20674443244934082\n",
      "training: 63 batch 553 batch_loss: 0.2071627378463745\n",
      "training: 63 batch 554 batch_loss: 0.21095743775367737\n",
      "training: 63 batch 555 batch_loss: 0.21097153425216675\n",
      "training: 63 batch 556 batch_loss: 0.2092779278755188\n",
      "training: 63 batch 557 batch_loss: 0.20576006174087524\n",
      "training: 63 batch 558 batch_loss: 0.20952457189559937\n",
      "training: 63 batch 559 batch_loss: 0.20853161811828613\n",
      "training: 63 batch 560 batch_loss: 0.2110520601272583\n",
      "training: 63 batch 561 batch_loss: 0.2082580327987671\n",
      "training: 63 batch 562 batch_loss: 0.21158137917518616\n",
      "training: 63 batch 563 batch_loss: 0.20438426733016968\n",
      "training: 63 batch 564 batch_loss: 0.20982679724693298\n",
      "training: 63 batch 565 batch_loss: 0.20750319957733154\n",
      "training: 63 batch 566 batch_loss: 0.20788753032684326\n",
      "training: 63 batch 567 batch_loss: 0.20872417092323303\n",
      "training: 63 batch 568 batch_loss: 0.21063697338104248\n",
      "training: 63 batch 569 batch_loss: 0.2100764513015747\n",
      "training: 63 batch 570 batch_loss: 0.20990705490112305\n",
      "training: 63 batch 571 batch_loss: 0.2102334201335907\n",
      "training: 63 batch 572 batch_loss: 0.21264231204986572\n",
      "training: 63 batch 573 batch_loss: 0.2025146782398224\n",
      "training: 63 batch 574 batch_loss: 0.20763111114501953\n",
      "training: 63 batch 575 batch_loss: 0.20604759454727173\n",
      "training: 63 batch 576 batch_loss: 0.20666271448135376\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 63, Hit Ratio:0.03314867435283301 | Precision:0.04890887643762902 | Recall:0.06453052674329508 | NDCG:0.06334668157502814\n",
      "*Best Performance* \n",
      "Epoch: 61, Hit Ratio:0.033261934821931224 | Precision:0.04907598545168584 | Recall:0.06479544314235144 | MDCG:0.06348668156373174\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 64 batch 0 batch_loss: 0.2068149447441101\n",
      "training: 64 batch 1 batch_loss: 0.20340704917907715\n",
      "training: 64 batch 2 batch_loss: 0.2074483335018158\n",
      "training: 64 batch 3 batch_loss: 0.20611873269081116\n",
      "training: 64 batch 4 batch_loss: 0.20654550194740295\n",
      "training: 64 batch 5 batch_loss: 0.20523077249526978\n",
      "training: 64 batch 6 batch_loss: 0.20583677291870117\n",
      "training: 64 batch 7 batch_loss: 0.20953896641731262\n",
      "training: 64 batch 8 batch_loss: 0.20308586955070496\n",
      "training: 64 batch 9 batch_loss: 0.20653036236763\n",
      "training: 64 batch 10 batch_loss: 0.20655137300491333\n",
      "training: 64 batch 11 batch_loss: 0.20583242177963257\n",
      "training: 64 batch 12 batch_loss: 0.20136606693267822\n",
      "training: 64 batch 13 batch_loss: 0.20921126008033752\n",
      "training: 64 batch 14 batch_loss: 0.20321321487426758\n",
      "training: 64 batch 15 batch_loss: 0.2058839201927185\n",
      "training: 64 batch 16 batch_loss: 0.20344406366348267\n",
      "training: 64 batch 17 batch_loss: 0.20687055587768555\n",
      "training: 64 batch 18 batch_loss: 0.2087670862674713\n",
      "training: 64 batch 19 batch_loss: 0.20197078585624695\n",
      "training: 64 batch 20 batch_loss: 0.20259839296340942\n",
      "training: 64 batch 21 batch_loss: 0.20469599962234497\n",
      "training: 64 batch 22 batch_loss: 0.2047673761844635\n",
      "training: 64 batch 23 batch_loss: 0.21275293827056885\n",
      "training: 64 batch 24 batch_loss: 0.20475655794143677\n",
      "training: 64 batch 25 batch_loss: 0.20599082112312317\n",
      "training: 64 batch 26 batch_loss: 0.20760038495063782\n",
      "training: 64 batch 27 batch_loss: 0.20630377531051636\n",
      "training: 64 batch 28 batch_loss: 0.2025851607322693\n",
      "training: 64 batch 29 batch_loss: 0.20417815446853638\n",
      "training: 64 batch 30 batch_loss: 0.20720309019088745\n",
      "training: 64 batch 31 batch_loss: 0.20037609338760376\n",
      "training: 64 batch 32 batch_loss: 0.20862603187561035\n",
      "training: 64 batch 33 batch_loss: 0.20739522576332092\n",
      "training: 64 batch 34 batch_loss: 0.20345115661621094\n",
      "training: 64 batch 35 batch_loss: 0.2071693241596222\n",
      "training: 64 batch 36 batch_loss: 0.20473119616508484\n",
      "training: 64 batch 37 batch_loss: 0.2017529010772705\n",
      "training: 64 batch 38 batch_loss: 0.20891731977462769\n",
      "training: 64 batch 39 batch_loss: 0.20828181505203247\n",
      "training: 64 batch 40 batch_loss: 0.20503485202789307\n",
      "training: 64 batch 41 batch_loss: 0.2018072009086609\n",
      "training: 64 batch 42 batch_loss: 0.20892709493637085\n",
      "training: 64 batch 43 batch_loss: 0.21053868532180786\n",
      "training: 64 batch 44 batch_loss: 0.20549863576889038\n",
      "training: 64 batch 45 batch_loss: 0.2073574960231781\n",
      "training: 64 batch 46 batch_loss: 0.20146876573562622\n",
      "training: 64 batch 47 batch_loss: 0.20690664649009705\n",
      "training: 64 batch 48 batch_loss: 0.20847982168197632\n",
      "training: 64 batch 49 batch_loss: 0.20835602283477783\n",
      "training: 64 batch 50 batch_loss: 0.20506295561790466\n",
      "training: 64 batch 51 batch_loss: 0.2086317241191864\n",
      "training: 64 batch 52 batch_loss: 0.20705661177635193\n",
      "training: 64 batch 53 batch_loss: 0.20527392625808716\n",
      "training: 64 batch 54 batch_loss: 0.20595702528953552\n",
      "training: 64 batch 55 batch_loss: 0.20493420958518982\n",
      "training: 64 batch 56 batch_loss: 0.20789724588394165\n",
      "training: 64 batch 57 batch_loss: 0.20939481258392334\n",
      "training: 64 batch 58 batch_loss: 0.2035568356513977\n",
      "training: 64 batch 59 batch_loss: 0.20930397510528564\n",
      "training: 64 batch 60 batch_loss: 0.20880916714668274\n",
      "training: 64 batch 61 batch_loss: 0.20059707760810852\n",
      "training: 64 batch 62 batch_loss: 0.20836186408996582\n",
      "training: 64 batch 63 batch_loss: 0.20785671472549438\n",
      "training: 64 batch 64 batch_loss: 0.20668503642082214\n",
      "training: 64 batch 65 batch_loss: 0.2050400972366333\n",
      "training: 64 batch 66 batch_loss: 0.2044816017150879\n",
      "training: 64 batch 67 batch_loss: 0.2083444595336914\n",
      "training: 64 batch 68 batch_loss: 0.20684415102005005\n",
      "training: 64 batch 69 batch_loss: 0.20716619491577148\n",
      "training: 64 batch 70 batch_loss: 0.20602020621299744\n",
      "training: 64 batch 71 batch_loss: 0.20666521787643433\n",
      "training: 64 batch 72 batch_loss: 0.2046394646167755\n",
      "training: 64 batch 73 batch_loss: 0.20515570044517517\n",
      "training: 64 batch 74 batch_loss: 0.20261773467063904\n",
      "training: 64 batch 75 batch_loss: 0.20525825023651123\n",
      "training: 64 batch 76 batch_loss: 0.2045964002609253\n",
      "training: 64 batch 77 batch_loss: 0.2063998579978943\n",
      "training: 64 batch 78 batch_loss: 0.20677676796913147\n",
      "training: 64 batch 79 batch_loss: 0.20455798506736755\n",
      "training: 64 batch 80 batch_loss: 0.21148467063903809\n",
      "training: 64 batch 81 batch_loss: 0.2062329649925232\n",
      "training: 64 batch 82 batch_loss: 0.20700466632843018\n",
      "training: 64 batch 83 batch_loss: 0.20814895629882812\n",
      "training: 64 batch 84 batch_loss: 0.20728790760040283\n",
      "training: 64 batch 85 batch_loss: 0.20436686277389526\n",
      "training: 64 batch 86 batch_loss: 0.2091437578201294\n",
      "training: 64 batch 87 batch_loss: 0.20897477865219116\n",
      "training: 64 batch 88 batch_loss: 0.20416522026062012\n",
      "training: 64 batch 89 batch_loss: 0.2065853476524353\n",
      "training: 64 batch 90 batch_loss: 0.20288053154945374\n",
      "training: 64 batch 91 batch_loss: 0.2137068510055542\n",
      "training: 64 batch 92 batch_loss: 0.20736733078956604\n",
      "training: 64 batch 93 batch_loss: 0.20499232411384583\n",
      "training: 64 batch 94 batch_loss: 0.20789140462875366\n",
      "training: 64 batch 95 batch_loss: 0.2055327296257019\n",
      "training: 64 batch 96 batch_loss: 0.2062164843082428\n",
      "training: 64 batch 97 batch_loss: 0.20524263381958008\n",
      "training: 64 batch 98 batch_loss: 0.20779889822006226\n",
      "training: 64 batch 99 batch_loss: 0.20903271436691284\n",
      "training: 64 batch 100 batch_loss: 0.2082306444644928\n",
      "training: 64 batch 101 batch_loss: 0.20909550786018372\n",
      "training: 64 batch 102 batch_loss: 0.20509546995162964\n",
      "training: 64 batch 103 batch_loss: 0.20950454473495483\n",
      "training: 64 batch 104 batch_loss: 0.20857354998588562\n",
      "training: 64 batch 105 batch_loss: 0.20166367292404175\n",
      "training: 64 batch 106 batch_loss: 0.20211565494537354\n",
      "training: 64 batch 107 batch_loss: 0.20580846071243286\n",
      "training: 64 batch 108 batch_loss: 0.20622336864471436\n",
      "training: 64 batch 109 batch_loss: 0.2090945541858673\n",
      "training: 64 batch 110 batch_loss: 0.2098204791545868\n",
      "training: 64 batch 111 batch_loss: 0.20948311686515808\n",
      "training: 64 batch 112 batch_loss: 0.2072148323059082\n",
      "training: 64 batch 113 batch_loss: 0.20721891522407532\n",
      "training: 64 batch 114 batch_loss: 0.20313063263893127\n",
      "training: 64 batch 115 batch_loss: 0.2107734978199005\n",
      "training: 64 batch 116 batch_loss: 0.20739617943763733\n",
      "training: 64 batch 117 batch_loss: 0.20643582940101624\n",
      "training: 64 batch 118 batch_loss: 0.20952147245407104\n",
      "training: 64 batch 119 batch_loss: 0.20468935370445251\n",
      "training: 64 batch 120 batch_loss: 0.20565956830978394\n",
      "training: 64 batch 121 batch_loss: 0.2071918547153473\n",
      "training: 64 batch 122 batch_loss: 0.20757907629013062\n",
      "training: 64 batch 123 batch_loss: 0.2064349353313446\n",
      "training: 64 batch 124 batch_loss: 0.20679616928100586\n",
      "training: 64 batch 125 batch_loss: 0.2061311900615692\n",
      "training: 64 batch 126 batch_loss: 0.20341134071350098\n",
      "training: 64 batch 127 batch_loss: 0.20528897643089294\n",
      "training: 64 batch 128 batch_loss: 0.20906811952590942\n",
      "training: 64 batch 129 batch_loss: 0.2102409303188324\n",
      "training: 64 batch 130 batch_loss: 0.20800817012786865\n",
      "training: 64 batch 131 batch_loss: 0.20771697163581848\n",
      "training: 64 batch 132 batch_loss: 0.2058652937412262\n",
      "training: 64 batch 133 batch_loss: 0.20412832498550415\n",
      "training: 64 batch 134 batch_loss: 0.2109721302986145\n",
      "training: 64 batch 135 batch_loss: 0.20440995693206787\n",
      "training: 64 batch 136 batch_loss: 0.2064235806465149\n",
      "training: 64 batch 137 batch_loss: 0.20686855912208557\n",
      "training: 64 batch 138 batch_loss: 0.20696747303009033\n",
      "training: 64 batch 139 batch_loss: 0.2095029652118683\n",
      "training: 64 batch 140 batch_loss: 0.20428559184074402\n",
      "training: 64 batch 141 batch_loss: 0.20921185612678528\n",
      "training: 64 batch 142 batch_loss: 0.2067599892616272\n",
      "training: 64 batch 143 batch_loss: 0.20653235912322998\n",
      "training: 64 batch 144 batch_loss: 0.20411598682403564\n",
      "training: 64 batch 145 batch_loss: 0.20292353630065918\n",
      "training: 64 batch 146 batch_loss: 0.2031261920928955\n",
      "training: 64 batch 147 batch_loss: 0.2032080590724945\n",
      "training: 64 batch 148 batch_loss: 0.20757648348808289\n",
      "training: 64 batch 149 batch_loss: 0.20771777629852295\n",
      "training: 64 batch 150 batch_loss: 0.2138318121433258\n",
      "training: 64 batch 151 batch_loss: 0.20469629764556885\n",
      "training: 64 batch 152 batch_loss: 0.2051781415939331\n",
      "training: 64 batch 153 batch_loss: 0.20577096939086914\n",
      "training: 64 batch 154 batch_loss: 0.20204472541809082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 64 batch 155 batch_loss: 0.2030947208404541\n",
      "training: 64 batch 156 batch_loss: 0.21030372381210327\n",
      "training: 64 batch 157 batch_loss: 0.20516517758369446\n",
      "training: 64 batch 158 batch_loss: 0.2057822346687317\n",
      "training: 64 batch 159 batch_loss: 0.20569130778312683\n",
      "training: 64 batch 160 batch_loss: 0.21070727705955505\n",
      "training: 64 batch 161 batch_loss: 0.20696377754211426\n",
      "training: 64 batch 162 batch_loss: 0.2092297077178955\n",
      "training: 64 batch 163 batch_loss: 0.20405727624893188\n",
      "training: 64 batch 164 batch_loss: 0.20716750621795654\n",
      "training: 64 batch 165 batch_loss: 0.2060229480266571\n",
      "training: 64 batch 166 batch_loss: 0.20606577396392822\n",
      "training: 64 batch 167 batch_loss: 0.20811378955841064\n",
      "training: 64 batch 168 batch_loss: 0.20737400650978088\n",
      "training: 64 batch 169 batch_loss: 0.20946165919303894\n",
      "training: 64 batch 170 batch_loss: 0.2079588770866394\n",
      "training: 64 batch 171 batch_loss: 0.2063319981098175\n",
      "training: 64 batch 172 batch_loss: 0.20885580778121948\n",
      "training: 64 batch 173 batch_loss: 0.20962679386138916\n",
      "training: 64 batch 174 batch_loss: 0.20555338263511658\n",
      "training: 64 batch 175 batch_loss: 0.2022818922996521\n",
      "training: 64 batch 176 batch_loss: 0.20624780654907227\n",
      "training: 64 batch 177 batch_loss: 0.21063551306724548\n",
      "training: 64 batch 178 batch_loss: 0.21055489778518677\n",
      "training: 64 batch 179 batch_loss: 0.21201211214065552\n",
      "training: 64 batch 180 batch_loss: 0.20744037628173828\n",
      "training: 64 batch 181 batch_loss: 0.2099953293800354\n",
      "training: 64 batch 182 batch_loss: 0.2084311842918396\n",
      "training: 64 batch 183 batch_loss: 0.2123984694480896\n",
      "training: 64 batch 184 batch_loss: 0.2091776728630066\n",
      "training: 64 batch 185 batch_loss: 0.20437073707580566\n",
      "training: 64 batch 186 batch_loss: 0.20894205570220947\n",
      "training: 64 batch 187 batch_loss: 0.20705381035804749\n",
      "training: 64 batch 188 batch_loss: 0.20840871334075928\n",
      "training: 64 batch 189 batch_loss: 0.20522242784500122\n",
      "training: 64 batch 190 batch_loss: 0.2115451693534851\n",
      "training: 64 batch 191 batch_loss: 0.2051391899585724\n",
      "training: 64 batch 192 batch_loss: 0.20916232466697693\n",
      "training: 64 batch 193 batch_loss: 0.2063748836517334\n",
      "training: 64 batch 194 batch_loss: 0.20774656534194946\n",
      "training: 64 batch 195 batch_loss: 0.20831763744354248\n",
      "training: 64 batch 196 batch_loss: 0.20051956176757812\n",
      "training: 64 batch 197 batch_loss: 0.20410102605819702\n",
      "training: 64 batch 198 batch_loss: 0.2079157829284668\n",
      "training: 64 batch 199 batch_loss: 0.21037223935127258\n",
      "training: 64 batch 200 batch_loss: 0.2067125141620636\n",
      "training: 64 batch 201 batch_loss: 0.20527249574661255\n",
      "training: 64 batch 202 batch_loss: 0.20458295941352844\n",
      "training: 64 batch 203 batch_loss: 0.20946058630943298\n",
      "training: 64 batch 204 batch_loss: 0.20728737115859985\n",
      "training: 64 batch 205 batch_loss: 0.20564043521881104\n",
      "training: 64 batch 206 batch_loss: 0.2119753360748291\n",
      "training: 64 batch 207 batch_loss: 0.2042413353919983\n",
      "training: 64 batch 208 batch_loss: 0.2070440649986267\n",
      "training: 64 batch 209 batch_loss: 0.2072383165359497\n",
      "training: 64 batch 210 batch_loss: 0.2090775966644287\n",
      "training: 64 batch 211 batch_loss: 0.20602017641067505\n",
      "training: 64 batch 212 batch_loss: 0.20695191621780396\n",
      "training: 64 batch 213 batch_loss: 0.20832321047782898\n",
      "training: 64 batch 214 batch_loss: 0.20791810750961304\n",
      "training: 64 batch 215 batch_loss: 0.20849567651748657\n",
      "training: 64 batch 216 batch_loss: 0.20834732055664062\n",
      "training: 64 batch 217 batch_loss: 0.20666125416755676\n",
      "training: 64 batch 218 batch_loss: 0.2051302194595337\n",
      "training: 64 batch 219 batch_loss: 0.20317474007606506\n",
      "training: 64 batch 220 batch_loss: 0.2094649374485016\n",
      "training: 64 batch 221 batch_loss: 0.20817452669143677\n",
      "training: 64 batch 222 batch_loss: 0.2120060920715332\n",
      "training: 64 batch 223 batch_loss: 0.20871075987815857\n",
      "training: 64 batch 224 batch_loss: 0.2056310772895813\n",
      "training: 64 batch 225 batch_loss: 0.20779547095298767\n",
      "training: 64 batch 226 batch_loss: 0.20849007368087769\n",
      "training: 64 batch 227 batch_loss: 0.20551306009292603\n",
      "training: 64 batch 228 batch_loss: 0.20767593383789062\n",
      "training: 64 batch 229 batch_loss: 0.20932602882385254\n",
      "training: 64 batch 230 batch_loss: 0.20726829767227173\n",
      "training: 64 batch 231 batch_loss: 0.21002668142318726\n",
      "training: 64 batch 232 batch_loss: 0.20774531364440918\n",
      "training: 64 batch 233 batch_loss: 0.20499056577682495\n",
      "training: 64 batch 234 batch_loss: 0.205702543258667\n",
      "training: 64 batch 235 batch_loss: 0.2066451907157898\n",
      "training: 64 batch 236 batch_loss: 0.2087537944316864\n",
      "training: 64 batch 237 batch_loss: 0.20596981048583984\n",
      "training: 64 batch 238 batch_loss: 0.19987386465072632\n",
      "training: 64 batch 239 batch_loss: 0.20672541856765747\n",
      "training: 64 batch 240 batch_loss: 0.20492041110992432\n",
      "training: 64 batch 241 batch_loss: 0.20593222975730896\n",
      "training: 64 batch 242 batch_loss: 0.21146294474601746\n",
      "training: 64 batch 243 batch_loss: 0.20733419060707092\n",
      "training: 64 batch 244 batch_loss: 0.2021457552909851\n",
      "training: 64 batch 245 batch_loss: 0.2059953808784485\n",
      "training: 64 batch 246 batch_loss: 0.20758682489395142\n",
      "training: 64 batch 247 batch_loss: 0.20592010021209717\n",
      "training: 64 batch 248 batch_loss: 0.2062912881374359\n",
      "training: 64 batch 249 batch_loss: 0.2080467939376831\n",
      "training: 64 batch 250 batch_loss: 0.20778650045394897\n",
      "training: 64 batch 251 batch_loss: 0.20920675992965698\n",
      "training: 64 batch 252 batch_loss: 0.20691746473312378\n",
      "training: 64 batch 253 batch_loss: 0.2054060399532318\n",
      "training: 64 batch 254 batch_loss: 0.2060481607913971\n",
      "training: 64 batch 255 batch_loss: 0.20510917901992798\n",
      "training: 64 batch 256 batch_loss: 0.20610293745994568\n",
      "training: 64 batch 257 batch_loss: 0.2086920142173767\n",
      "training: 64 batch 258 batch_loss: 0.2098398208618164\n",
      "training: 64 batch 259 batch_loss: 0.206377774477005\n",
      "training: 64 batch 260 batch_loss: 0.21201512217521667\n",
      "training: 64 batch 261 batch_loss: 0.20714670419692993\n",
      "training: 64 batch 262 batch_loss: 0.20384559035301208\n",
      "training: 64 batch 263 batch_loss: 0.20568040013313293\n",
      "training: 64 batch 264 batch_loss: 0.20895260572433472\n",
      "training: 64 batch 265 batch_loss: 0.20852801203727722\n",
      "training: 64 batch 266 batch_loss: 0.2061934769153595\n",
      "training: 64 batch 267 batch_loss: 0.20672717690467834\n",
      "training: 64 batch 268 batch_loss: 0.20487695932388306\n",
      "training: 64 batch 269 batch_loss: 0.2074616551399231\n",
      "training: 64 batch 270 batch_loss: 0.20952540636062622\n",
      "training: 64 batch 271 batch_loss: 0.20436179637908936\n",
      "training: 64 batch 272 batch_loss: 0.20691132545471191\n",
      "training: 64 batch 273 batch_loss: 0.2094452977180481\n",
      "training: 64 batch 274 batch_loss: 0.20782563090324402\n",
      "training: 64 batch 275 batch_loss: 0.21124881505966187\n",
      "training: 64 batch 276 batch_loss: 0.20998862385749817\n",
      "training: 64 batch 277 batch_loss: 0.20416554808616638\n",
      "training: 64 batch 278 batch_loss: 0.20734775066375732\n",
      "training: 64 batch 279 batch_loss: 0.20616376399993896\n",
      "training: 64 batch 280 batch_loss: 0.21239370107650757\n",
      "training: 64 batch 281 batch_loss: 0.2079276740550995\n",
      "training: 64 batch 282 batch_loss: 0.20508241653442383\n",
      "training: 64 batch 283 batch_loss: 0.206549733877182\n",
      "training: 64 batch 284 batch_loss: 0.21245798468589783\n",
      "training: 64 batch 285 batch_loss: 0.20712491869926453\n",
      "training: 64 batch 286 batch_loss: 0.20557430386543274\n",
      "training: 64 batch 287 batch_loss: 0.2112576961517334\n",
      "training: 64 batch 288 batch_loss: 0.20360010862350464\n",
      "training: 64 batch 289 batch_loss: 0.20682787895202637\n",
      "training: 64 batch 290 batch_loss: 0.20862999558448792\n",
      "training: 64 batch 291 batch_loss: 0.2095302939414978\n",
      "training: 64 batch 292 batch_loss: 0.21131813526153564\n",
      "training: 64 batch 293 batch_loss: 0.20559939742088318\n",
      "training: 64 batch 294 batch_loss: 0.20408973097801208\n",
      "training: 64 batch 295 batch_loss: 0.20859137177467346\n",
      "training: 64 batch 296 batch_loss: 0.2097265124320984\n",
      "training: 64 batch 297 batch_loss: 0.21189510822296143\n",
      "training: 64 batch 298 batch_loss: 0.2094355821609497\n",
      "training: 64 batch 299 batch_loss: 0.20564556121826172\n",
      "training: 64 batch 300 batch_loss: 0.2079142928123474\n",
      "training: 64 batch 301 batch_loss: 0.20726579427719116\n",
      "training: 64 batch 302 batch_loss: 0.20708301663398743\n",
      "training: 64 batch 303 batch_loss: 0.2061881721019745\n",
      "training: 64 batch 304 batch_loss: 0.2058125138282776\n",
      "training: 64 batch 305 batch_loss: 0.21060538291931152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 64 batch 306 batch_loss: 0.2085750699043274\n",
      "training: 64 batch 307 batch_loss: 0.21184048056602478\n",
      "training: 64 batch 308 batch_loss: 0.2131386399269104\n",
      "training: 64 batch 309 batch_loss: 0.20289000868797302\n",
      "training: 64 batch 310 batch_loss: 0.20998522639274597\n",
      "training: 64 batch 311 batch_loss: 0.20805677771568298\n",
      "training: 64 batch 312 batch_loss: 0.21067696809768677\n",
      "training: 64 batch 313 batch_loss: 0.20921698212623596\n",
      "training: 64 batch 314 batch_loss: 0.20863652229309082\n",
      "training: 64 batch 315 batch_loss: 0.20566627383232117\n",
      "training: 64 batch 316 batch_loss: 0.20880812406539917\n",
      "training: 64 batch 317 batch_loss: 0.2099505364894867\n",
      "training: 64 batch 318 batch_loss: 0.20617681741714478\n",
      "training: 64 batch 319 batch_loss: 0.2032914161682129\n",
      "training: 64 batch 320 batch_loss: 0.20611581206321716\n",
      "training: 64 batch 321 batch_loss: 0.20536568760871887\n",
      "training: 64 batch 322 batch_loss: 0.20684263110160828\n",
      "training: 64 batch 323 batch_loss: 0.20438513159751892\n",
      "training: 64 batch 324 batch_loss: 0.20776835083961487\n",
      "training: 64 batch 325 batch_loss: 0.20304033160209656\n",
      "training: 64 batch 326 batch_loss: 0.2068939507007599\n",
      "training: 64 batch 327 batch_loss: 0.2043393850326538\n",
      "training: 64 batch 328 batch_loss: 0.20949295163154602\n",
      "training: 64 batch 329 batch_loss: 0.20493796467781067\n",
      "training: 64 batch 330 batch_loss: 0.20724254846572876\n",
      "training: 64 batch 331 batch_loss: 0.21000006794929504\n",
      "training: 64 batch 332 batch_loss: 0.20712485909461975\n",
      "training: 64 batch 333 batch_loss: 0.20783016085624695\n",
      "training: 64 batch 334 batch_loss: 0.21344131231307983\n",
      "training: 64 batch 335 batch_loss: 0.20559906959533691\n",
      "training: 64 batch 336 batch_loss: 0.20663952827453613\n",
      "training: 64 batch 337 batch_loss: 0.2062971591949463\n",
      "training: 64 batch 338 batch_loss: 0.20872437953948975\n",
      "training: 64 batch 339 batch_loss: 0.2043028473854065\n",
      "training: 64 batch 340 batch_loss: 0.20784562826156616\n",
      "training: 64 batch 341 batch_loss: 0.20333892107009888\n",
      "training: 64 batch 342 batch_loss: 0.20313239097595215\n",
      "training: 64 batch 343 batch_loss: 0.21248406171798706\n",
      "training: 64 batch 344 batch_loss: 0.20535627007484436\n",
      "training: 64 batch 345 batch_loss: 0.21300292015075684\n",
      "training: 64 batch 346 batch_loss: 0.21041211485862732\n",
      "training: 64 batch 347 batch_loss: 0.20745328068733215\n",
      "training: 64 batch 348 batch_loss: 0.20683619379997253\n",
      "training: 64 batch 349 batch_loss: 0.2067996859550476\n",
      "training: 64 batch 350 batch_loss: 0.20703566074371338\n",
      "training: 64 batch 351 batch_loss: 0.20358121395111084\n",
      "training: 64 batch 352 batch_loss: 0.20797276496887207\n",
      "training: 64 batch 353 batch_loss: 0.2079792618751526\n",
      "training: 64 batch 354 batch_loss: 0.21071094274520874\n",
      "training: 64 batch 355 batch_loss: 0.20900225639343262\n",
      "training: 64 batch 356 batch_loss: 0.20914989709854126\n",
      "training: 64 batch 357 batch_loss: 0.20673230290412903\n",
      "training: 64 batch 358 batch_loss: 0.20803076028823853\n",
      "training: 64 batch 359 batch_loss: 0.20754915475845337\n",
      "training: 64 batch 360 batch_loss: 0.20959609746932983\n",
      "training: 64 batch 361 batch_loss: 0.2116805613040924\n",
      "training: 64 batch 362 batch_loss: 0.21008697152137756\n",
      "training: 64 batch 363 batch_loss: 0.21000728011131287\n",
      "training: 64 batch 364 batch_loss: 0.204980731010437\n",
      "training: 64 batch 365 batch_loss: 0.20948678255081177\n",
      "training: 64 batch 366 batch_loss: 0.20709627866744995\n",
      "training: 64 batch 367 batch_loss: 0.20960670709609985\n",
      "training: 64 batch 368 batch_loss: 0.20965665578842163\n",
      "training: 64 batch 369 batch_loss: 0.20654207468032837\n",
      "training: 64 batch 370 batch_loss: 0.21362119913101196\n",
      "training: 64 batch 371 batch_loss: 0.2065250277519226\n",
      "training: 64 batch 372 batch_loss: 0.20327222347259521\n",
      "training: 64 batch 373 batch_loss: 0.20700427889823914\n",
      "training: 64 batch 374 batch_loss: 0.20841264724731445\n",
      "training: 64 batch 375 batch_loss: 0.20759254693984985\n",
      "training: 64 batch 376 batch_loss: 0.20563271641731262\n",
      "training: 64 batch 377 batch_loss: 0.20825263857841492\n",
      "training: 64 batch 378 batch_loss: 0.21114221215248108\n",
      "training: 64 batch 379 batch_loss: 0.20685559511184692\n",
      "training: 64 batch 380 batch_loss: 0.20903295278549194\n",
      "training: 64 batch 381 batch_loss: 0.20963260531425476\n",
      "training: 64 batch 382 batch_loss: 0.20934590697288513\n",
      "training: 64 batch 383 batch_loss: 0.20753827691078186\n",
      "training: 64 batch 384 batch_loss: 0.2064475119113922\n",
      "training: 64 batch 385 batch_loss: 0.21119564771652222\n",
      "training: 64 batch 386 batch_loss: 0.21121624112129211\n",
      "training: 64 batch 387 batch_loss: 0.20752903819084167\n",
      "training: 64 batch 388 batch_loss: 0.20800644159317017\n",
      "training: 64 batch 389 batch_loss: 0.20901384949684143\n",
      "training: 64 batch 390 batch_loss: 0.20753097534179688\n",
      "training: 64 batch 391 batch_loss: 0.2091626226902008\n",
      "training: 64 batch 392 batch_loss: 0.21070104837417603\n",
      "training: 64 batch 393 batch_loss: 0.2077467143535614\n",
      "training: 64 batch 394 batch_loss: 0.20302894711494446\n",
      "training: 64 batch 395 batch_loss: 0.2102804183959961\n",
      "training: 64 batch 396 batch_loss: 0.20783942937850952\n",
      "training: 64 batch 397 batch_loss: 0.20863991975784302\n",
      "training: 64 batch 398 batch_loss: 0.20457538962364197\n",
      "training: 64 batch 399 batch_loss: 0.20528912544250488\n",
      "training: 64 batch 400 batch_loss: 0.2065521478652954\n",
      "training: 64 batch 401 batch_loss: 0.20745620131492615\n",
      "training: 64 batch 402 batch_loss: 0.20827355980873108\n",
      "training: 64 batch 403 batch_loss: 0.20844343304634094\n",
      "training: 64 batch 404 batch_loss: 0.21312084794044495\n",
      "training: 64 batch 405 batch_loss: 0.2086172103881836\n",
      "training: 64 batch 406 batch_loss: 0.2101764678955078\n",
      "training: 64 batch 407 batch_loss: 0.21062761545181274\n",
      "training: 64 batch 408 batch_loss: 0.21070295572280884\n",
      "training: 64 batch 409 batch_loss: 0.20774462819099426\n",
      "training: 64 batch 410 batch_loss: 0.2071867287158966\n",
      "training: 64 batch 411 batch_loss: 0.20657077431678772\n",
      "training: 64 batch 412 batch_loss: 0.209417462348938\n",
      "training: 64 batch 413 batch_loss: 0.20466065406799316\n",
      "training: 64 batch 414 batch_loss: 0.20619672536849976\n",
      "training: 64 batch 415 batch_loss: 0.21031880378723145\n",
      "training: 64 batch 416 batch_loss: 0.2082202136516571\n",
      "training: 64 batch 417 batch_loss: 0.20342189073562622\n",
      "training: 64 batch 418 batch_loss: 0.20732635259628296\n",
      "training: 64 batch 419 batch_loss: 0.21191969513893127\n",
      "training: 64 batch 420 batch_loss: 0.20862653851509094\n",
      "training: 64 batch 421 batch_loss: 0.21068847179412842\n",
      "training: 64 batch 422 batch_loss: 0.21173369884490967\n",
      "training: 64 batch 423 batch_loss: 0.20904546976089478\n",
      "training: 64 batch 424 batch_loss: 0.20823171734809875\n",
      "training: 64 batch 425 batch_loss: 0.2087586224079132\n",
      "training: 64 batch 426 batch_loss: 0.20724698901176453\n",
      "training: 64 batch 427 batch_loss: 0.21182259917259216\n",
      "training: 64 batch 428 batch_loss: 0.2117549479007721\n",
      "training: 64 batch 429 batch_loss: 0.20892927050590515\n",
      "training: 64 batch 430 batch_loss: 0.21022319793701172\n",
      "training: 64 batch 431 batch_loss: 0.2076035737991333\n",
      "training: 64 batch 432 batch_loss: 0.20680126547813416\n",
      "training: 64 batch 433 batch_loss: 0.21498680114746094\n",
      "training: 64 batch 434 batch_loss: 0.20465099811553955\n",
      "training: 64 batch 435 batch_loss: 0.20508557558059692\n",
      "training: 64 batch 436 batch_loss: 0.20473811030387878\n",
      "training: 64 batch 437 batch_loss: 0.21299713850021362\n",
      "training: 64 batch 438 batch_loss: 0.20496666431427002\n",
      "training: 64 batch 439 batch_loss: 0.20890679955482483\n",
      "training: 64 batch 440 batch_loss: 0.20842894911766052\n",
      "training: 64 batch 441 batch_loss: 0.211664080619812\n",
      "training: 64 batch 442 batch_loss: 0.20432689785957336\n",
      "training: 64 batch 443 batch_loss: 0.20988386869430542\n",
      "training: 64 batch 444 batch_loss: 0.20714586973190308\n",
      "training: 64 batch 445 batch_loss: 0.20439964532852173\n",
      "training: 64 batch 446 batch_loss: 0.20932048559188843\n",
      "training: 64 batch 447 batch_loss: 0.2060883641242981\n",
      "training: 64 batch 448 batch_loss: 0.20952185988426208\n",
      "training: 64 batch 449 batch_loss: 0.20546257495880127\n",
      "training: 64 batch 450 batch_loss: 0.20831677317619324\n",
      "training: 64 batch 451 batch_loss: 0.2057240605354309\n",
      "training: 64 batch 452 batch_loss: 0.20415446162223816\n",
      "training: 64 batch 453 batch_loss: 0.2114637792110443\n",
      "training: 64 batch 454 batch_loss: 0.21334275603294373\n",
      "training: 64 batch 455 batch_loss: 0.2051999866962433\n",
      "training: 64 batch 456 batch_loss: 0.20816358923912048\n",
      "training: 64 batch 457 batch_loss: 0.20467272400856018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 64 batch 458 batch_loss: 0.20623046159744263\n",
      "training: 64 batch 459 batch_loss: 0.21187105774879456\n",
      "training: 64 batch 460 batch_loss: 0.2069254219532013\n",
      "training: 64 batch 461 batch_loss: 0.21350544691085815\n",
      "training: 64 batch 462 batch_loss: 0.20647689700126648\n",
      "training: 64 batch 463 batch_loss: 0.20797640085220337\n",
      "training: 64 batch 464 batch_loss: 0.20912405848503113\n",
      "training: 64 batch 465 batch_loss: 0.21026691794395447\n",
      "training: 64 batch 466 batch_loss: 0.20619267225265503\n",
      "training: 64 batch 467 batch_loss: 0.2050623893737793\n",
      "training: 64 batch 468 batch_loss: 0.2085176408290863\n",
      "training: 64 batch 469 batch_loss: 0.20911604166030884\n",
      "training: 64 batch 470 batch_loss: 0.2065536081790924\n",
      "training: 64 batch 471 batch_loss: 0.21322473883628845\n",
      "training: 64 batch 472 batch_loss: 0.2115345001220703\n",
      "training: 64 batch 473 batch_loss: 0.20762309432029724\n",
      "training: 64 batch 474 batch_loss: 0.20761942863464355\n",
      "training: 64 batch 475 batch_loss: 0.21177685260772705\n",
      "training: 64 batch 476 batch_loss: 0.21030354499816895\n",
      "training: 64 batch 477 batch_loss: 0.2081873118877411\n",
      "training: 64 batch 478 batch_loss: 0.21393132209777832\n",
      "training: 64 batch 479 batch_loss: 0.20733708143234253\n",
      "training: 64 batch 480 batch_loss: 0.20880967378616333\n",
      "training: 64 batch 481 batch_loss: 0.2053414285182953\n",
      "training: 64 batch 482 batch_loss: 0.20414000749588013\n",
      "training: 64 batch 483 batch_loss: 0.20762580633163452\n",
      "training: 64 batch 484 batch_loss: 0.21057140827178955\n",
      "training: 64 batch 485 batch_loss: 0.2039112150669098\n",
      "training: 64 batch 486 batch_loss: 0.20604169368743896\n",
      "training: 64 batch 487 batch_loss: 0.20714962482452393\n",
      "training: 64 batch 488 batch_loss: 0.20655465126037598\n",
      "training: 64 batch 489 batch_loss: 0.21387773752212524\n",
      "training: 64 batch 490 batch_loss: 0.20854270458221436\n",
      "training: 64 batch 491 batch_loss: 0.20755457878112793\n",
      "training: 64 batch 492 batch_loss: 0.20520725846290588\n",
      "training: 64 batch 493 batch_loss: 0.20892104506492615\n",
      "training: 64 batch 494 batch_loss: 0.2073976993560791\n",
      "training: 64 batch 495 batch_loss: 0.20849990844726562\n",
      "training: 64 batch 496 batch_loss: 0.20765668153762817\n",
      "training: 64 batch 497 batch_loss: 0.2095583975315094\n",
      "training: 64 batch 498 batch_loss: 0.21372035145759583\n",
      "training: 64 batch 499 batch_loss: 0.21169310808181763\n",
      "training: 64 batch 500 batch_loss: 0.20688936114311218\n",
      "training: 64 batch 501 batch_loss: 0.20968350768089294\n",
      "training: 64 batch 502 batch_loss: 0.20948418974876404\n",
      "training: 64 batch 503 batch_loss: 0.20701506733894348\n",
      "training: 64 batch 504 batch_loss: 0.20974063873291016\n",
      "training: 64 batch 505 batch_loss: 0.20848721265792847\n",
      "training: 64 batch 506 batch_loss: 0.20976334810256958\n",
      "training: 64 batch 507 batch_loss: 0.20931634306907654\n",
      "training: 64 batch 508 batch_loss: 0.20809093117713928\n",
      "training: 64 batch 509 batch_loss: 0.20762312412261963\n",
      "training: 64 batch 510 batch_loss: 0.21115869283676147\n",
      "training: 64 batch 511 batch_loss: 0.2045552134513855\n",
      "training: 64 batch 512 batch_loss: 0.2095896601676941\n",
      "training: 64 batch 513 batch_loss: 0.2107105851173401\n",
      "training: 64 batch 514 batch_loss: 0.2098749577999115\n",
      "training: 64 batch 515 batch_loss: 0.20708143711090088\n",
      "training: 64 batch 516 batch_loss: 0.21089637279510498\n",
      "training: 64 batch 517 batch_loss: 0.2039104700088501\n",
      "training: 64 batch 518 batch_loss: 0.20899012684822083\n",
      "training: 64 batch 519 batch_loss: 0.21108606457710266\n",
      "training: 64 batch 520 batch_loss: 0.21179381012916565\n",
      "training: 64 batch 521 batch_loss: 0.20814645290374756\n",
      "training: 64 batch 522 batch_loss: 0.20837774872779846\n",
      "training: 64 batch 523 batch_loss: 0.20632442831993103\n",
      "training: 64 batch 524 batch_loss: 0.2151452600955963\n",
      "training: 64 batch 525 batch_loss: 0.21054673194885254\n",
      "training: 64 batch 526 batch_loss: 0.20775550603866577\n",
      "training: 64 batch 527 batch_loss: 0.20838844776153564\n",
      "training: 64 batch 528 batch_loss: 0.2073689103126526\n",
      "training: 64 batch 529 batch_loss: 0.2069612741470337\n",
      "training: 64 batch 530 batch_loss: 0.20716053247451782\n",
      "training: 64 batch 531 batch_loss: 0.2071760892868042\n",
      "training: 64 batch 532 batch_loss: 0.21002352237701416\n",
      "training: 64 batch 533 batch_loss: 0.20784658193588257\n",
      "training: 64 batch 534 batch_loss: 0.20648744702339172\n",
      "training: 64 batch 535 batch_loss: 0.2138373851776123\n",
      "training: 64 batch 536 batch_loss: 0.20731663703918457\n",
      "training: 64 batch 537 batch_loss: 0.20873522758483887\n",
      "training: 64 batch 538 batch_loss: 0.21230155229568481\n",
      "training: 64 batch 539 batch_loss: 0.21054187417030334\n",
      "training: 64 batch 540 batch_loss: 0.20808041095733643\n",
      "training: 64 batch 541 batch_loss: 0.20891067385673523\n",
      "training: 64 batch 542 batch_loss: 0.2070700228214264\n",
      "training: 64 batch 543 batch_loss: 0.21071884036064148\n",
      "training: 64 batch 544 batch_loss: 0.20681369304656982\n",
      "training: 64 batch 545 batch_loss: 0.2070997655391693\n",
      "training: 64 batch 546 batch_loss: 0.2101682722568512\n",
      "training: 64 batch 547 batch_loss: 0.21039700508117676\n",
      "training: 64 batch 548 batch_loss: 0.2027648687362671\n",
      "training: 64 batch 549 batch_loss: 0.20844650268554688\n",
      "training: 64 batch 550 batch_loss: 0.20651355385780334\n",
      "training: 64 batch 551 batch_loss: 0.21007293462753296\n",
      "training: 64 batch 552 batch_loss: 0.20790988206863403\n",
      "training: 64 batch 553 batch_loss: 0.2128942310810089\n",
      "training: 64 batch 554 batch_loss: 0.20805931091308594\n",
      "training: 64 batch 555 batch_loss: 0.20904237031936646\n",
      "training: 64 batch 556 batch_loss: 0.2083931267261505\n",
      "training: 64 batch 557 batch_loss: 0.20444336533546448\n",
      "training: 64 batch 558 batch_loss: 0.20563814043998718\n",
      "training: 64 batch 559 batch_loss: 0.20795410871505737\n",
      "training: 64 batch 560 batch_loss: 0.21022909879684448\n",
      "training: 64 batch 561 batch_loss: 0.20581886172294617\n",
      "training: 64 batch 562 batch_loss: 0.2096710503101349\n",
      "training: 64 batch 563 batch_loss: 0.2093837857246399\n",
      "training: 64 batch 564 batch_loss: 0.2054990828037262\n",
      "training: 64 batch 565 batch_loss: 0.2089451253414154\n",
      "training: 64 batch 566 batch_loss: 0.21104300022125244\n",
      "training: 64 batch 567 batch_loss: 0.21339276432991028\n",
      "training: 64 batch 568 batch_loss: 0.21340727806091309\n",
      "training: 64 batch 569 batch_loss: 0.20436587929725647\n",
      "training: 64 batch 570 batch_loss: 0.20702078938484192\n",
      "training: 64 batch 571 batch_loss: 0.20837250351905823\n",
      "training: 64 batch 572 batch_loss: 0.2079530954360962\n",
      "training: 64 batch 573 batch_loss: 0.20939898490905762\n",
      "training: 64 batch 574 batch_loss: 0.20970839262008667\n",
      "training: 64 batch 575 batch_loss: 0.20940715074539185\n",
      "training: 64 batch 576 batch_loss: 0.20056864619255066\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 64, Hit Ratio:0.03318531744577655 | Precision:0.0489629411186474 | Recall:0.06456042490444434 | NDCG:0.06365016508037749\n",
      "*Best Performance* \n",
      "Epoch: 61, Hit Ratio:0.033261934821931224 | Precision:0.04907598545168584 | Recall:0.06479544314235144 | MDCG:0.06348668156373174\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 65 batch 0 batch_loss: 0.208624005317688\n",
      "training: 65 batch 1 batch_loss: 0.20360782742500305\n",
      "training: 65 batch 2 batch_loss: 0.20620587468147278\n",
      "training: 65 batch 3 batch_loss: 0.20716065168380737\n",
      "training: 65 batch 4 batch_loss: 0.20696333050727844\n",
      "training: 65 batch 5 batch_loss: 0.20761165022850037\n",
      "training: 65 batch 6 batch_loss: 0.205670565366745\n",
      "training: 65 batch 7 batch_loss: 0.2050066590309143\n",
      "training: 65 batch 8 batch_loss: 0.2062739133834839\n",
      "training: 65 batch 9 batch_loss: 0.20521417260169983\n",
      "training: 65 batch 10 batch_loss: 0.20483052730560303\n",
      "training: 65 batch 11 batch_loss: 0.2074025273323059\n",
      "training: 65 batch 12 batch_loss: 0.20668885111808777\n",
      "training: 65 batch 13 batch_loss: 0.20710492134094238\n",
      "training: 65 batch 14 batch_loss: 0.20547136664390564\n",
      "training: 65 batch 15 batch_loss: 0.20588070154190063\n",
      "training: 65 batch 16 batch_loss: 0.20287764072418213\n",
      "training: 65 batch 17 batch_loss: 0.20916742086410522\n",
      "training: 65 batch 18 batch_loss: 0.20454680919647217\n",
      "training: 65 batch 19 batch_loss: 0.20435810089111328\n",
      "training: 65 batch 20 batch_loss: 0.20835191011428833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 65 batch 21 batch_loss: 0.20934635400772095\n",
      "training: 65 batch 22 batch_loss: 0.21003979444503784\n",
      "training: 65 batch 23 batch_loss: 0.20719203352928162\n",
      "training: 65 batch 24 batch_loss: 0.21114006638526917\n",
      "training: 65 batch 25 batch_loss: 0.20708945393562317\n",
      "training: 65 batch 26 batch_loss: 0.20633089542388916\n",
      "training: 65 batch 27 batch_loss: 0.2082098126411438\n",
      "training: 65 batch 28 batch_loss: 0.20075130462646484\n",
      "training: 65 batch 29 batch_loss: 0.20769602060317993\n",
      "training: 65 batch 30 batch_loss: 0.20902222394943237\n",
      "training: 65 batch 31 batch_loss: 0.2076038122177124\n",
      "training: 65 batch 32 batch_loss: 0.2100948989391327\n",
      "training: 65 batch 33 batch_loss: 0.20926371216773987\n",
      "training: 65 batch 34 batch_loss: 0.20707637071609497\n",
      "training: 65 batch 35 batch_loss: 0.20400738716125488\n",
      "training: 65 batch 36 batch_loss: 0.20740559697151184\n",
      "training: 65 batch 37 batch_loss: 0.20960819721221924\n",
      "training: 65 batch 38 batch_loss: 0.21074649691581726\n",
      "training: 65 batch 39 batch_loss: 0.20528855919837952\n",
      "training: 65 batch 40 batch_loss: 0.20317411422729492\n",
      "training: 65 batch 41 batch_loss: 0.20891010761260986\n",
      "training: 65 batch 42 batch_loss: 0.21086111664772034\n",
      "training: 65 batch 43 batch_loss: 0.20472395420074463\n",
      "training: 65 batch 44 batch_loss: 0.21166566014289856\n",
      "training: 65 batch 45 batch_loss: 0.20729383826255798\n",
      "training: 65 batch 46 batch_loss: 0.20810779929161072\n",
      "training: 65 batch 47 batch_loss: 0.20880267024040222\n",
      "training: 65 batch 48 batch_loss: 0.21357125043869019\n",
      "training: 65 batch 49 batch_loss: 0.20575588941574097\n",
      "training: 65 batch 50 batch_loss: 0.20501911640167236\n",
      "training: 65 batch 51 batch_loss: 0.20686393976211548\n",
      "training: 65 batch 52 batch_loss: 0.21047917008399963\n",
      "training: 65 batch 53 batch_loss: 0.20829644799232483\n",
      "training: 65 batch 54 batch_loss: 0.21198239922523499\n",
      "training: 65 batch 55 batch_loss: 0.20470717549324036\n",
      "training: 65 batch 56 batch_loss: 0.21150729060173035\n",
      "training: 65 batch 57 batch_loss: 0.20567604899406433\n",
      "training: 65 batch 58 batch_loss: 0.20775502920150757\n",
      "training: 65 batch 59 batch_loss: 0.208963543176651\n",
      "training: 65 batch 60 batch_loss: 0.2070930004119873\n",
      "training: 65 batch 61 batch_loss: 0.20702478289604187\n",
      "training: 65 batch 62 batch_loss: 0.20667731761932373\n",
      "training: 65 batch 63 batch_loss: 0.2039700746536255\n",
      "training: 65 batch 64 batch_loss: 0.2102174162864685\n",
      "training: 65 batch 65 batch_loss: 0.20616909861564636\n",
      "training: 65 batch 66 batch_loss: 0.20630216598510742\n",
      "training: 65 batch 67 batch_loss: 0.21072426438331604\n",
      "training: 65 batch 68 batch_loss: 0.20647349953651428\n",
      "training: 65 batch 69 batch_loss: 0.20725440979003906\n",
      "training: 65 batch 70 batch_loss: 0.2087453007698059\n",
      "training: 65 batch 71 batch_loss: 0.20793071389198303\n",
      "training: 65 batch 72 batch_loss: 0.2085168957710266\n",
      "training: 65 batch 73 batch_loss: 0.20781707763671875\n",
      "training: 65 batch 74 batch_loss: 0.2054998278617859\n",
      "training: 65 batch 75 batch_loss: 0.2072918713092804\n",
      "training: 65 batch 76 batch_loss: 0.20670470595359802\n",
      "training: 65 batch 77 batch_loss: 0.20622766017913818\n",
      "training: 65 batch 78 batch_loss: 0.20265993475914001\n",
      "training: 65 batch 79 batch_loss: 0.20928415656089783\n",
      "training: 65 batch 80 batch_loss: 0.20560184121131897\n",
      "training: 65 batch 81 batch_loss: 0.21251028776168823\n",
      "training: 65 batch 82 batch_loss: 0.20878994464874268\n",
      "training: 65 batch 83 batch_loss: 0.2116413116455078\n",
      "training: 65 batch 84 batch_loss: 0.20604369044303894\n",
      "training: 65 batch 85 batch_loss: 0.20778587460517883\n",
      "training: 65 batch 86 batch_loss: 0.20552203059196472\n",
      "training: 65 batch 87 batch_loss: 0.2104056477546692\n",
      "training: 65 batch 88 batch_loss: 0.21294903755187988\n",
      "training: 65 batch 89 batch_loss: 0.2084898054599762\n",
      "training: 65 batch 90 batch_loss: 0.20891129970550537\n",
      "training: 65 batch 91 batch_loss: 0.20623445510864258\n",
      "training: 65 batch 92 batch_loss: 0.2087821364402771\n",
      "training: 65 batch 93 batch_loss: 0.20741799473762512\n",
      "training: 65 batch 94 batch_loss: 0.20257946848869324\n",
      "training: 65 batch 95 batch_loss: 0.20877620577812195\n",
      "training: 65 batch 96 batch_loss: 0.2060914933681488\n",
      "training: 65 batch 97 batch_loss: 0.20842254161834717\n",
      "training: 65 batch 98 batch_loss: 0.2083953320980072\n",
      "training: 65 batch 99 batch_loss: 0.20519647002220154\n",
      "training: 65 batch 100 batch_loss: 0.20540401339530945\n",
      "training: 65 batch 101 batch_loss: 0.2056327760219574\n",
      "training: 65 batch 102 batch_loss: 0.20906072854995728\n",
      "training: 65 batch 103 batch_loss: 0.20529556274414062\n",
      "training: 65 batch 104 batch_loss: 0.21299710869789124\n",
      "training: 65 batch 105 batch_loss: 0.20912784337997437\n",
      "training: 65 batch 106 batch_loss: 0.2083124816417694\n",
      "training: 65 batch 107 batch_loss: 0.20729506015777588\n",
      "training: 65 batch 108 batch_loss: 0.2078511118888855\n",
      "training: 65 batch 109 batch_loss: 0.2057986557483673\n",
      "training: 65 batch 110 batch_loss: 0.20857024192810059\n",
      "training: 65 batch 111 batch_loss: 0.20467472076416016\n",
      "training: 65 batch 112 batch_loss: 0.20738700032234192\n",
      "training: 65 batch 113 batch_loss: 0.20691394805908203\n",
      "training: 65 batch 114 batch_loss: 0.20798295736312866\n",
      "training: 65 batch 115 batch_loss: 0.20898103713989258\n",
      "training: 65 batch 116 batch_loss: 0.20421725511550903\n",
      "training: 65 batch 117 batch_loss: 0.20628094673156738\n",
      "training: 65 batch 118 batch_loss: 0.21002411842346191\n",
      "training: 65 batch 119 batch_loss: 0.21035835146903992\n",
      "training: 65 batch 120 batch_loss: 0.20961907505989075\n",
      "training: 65 batch 121 batch_loss: 0.20983344316482544\n",
      "training: 65 batch 122 batch_loss: 0.21289366483688354\n",
      "training: 65 batch 123 batch_loss: 0.21017098426818848\n",
      "training: 65 batch 124 batch_loss: 0.2040972113609314\n",
      "training: 65 batch 125 batch_loss: 0.21207711100578308\n",
      "training: 65 batch 126 batch_loss: 0.20514452457427979\n",
      "training: 65 batch 127 batch_loss: 0.20930087566375732\n",
      "training: 65 batch 128 batch_loss: 0.2088170349597931\n",
      "training: 65 batch 129 batch_loss: 0.20410999655723572\n",
      "training: 65 batch 130 batch_loss: 0.20749354362487793\n",
      "training: 65 batch 131 batch_loss: 0.21129870414733887\n",
      "training: 65 batch 132 batch_loss: 0.20685553550720215\n",
      "training: 65 batch 133 batch_loss: 0.20444485545158386\n",
      "training: 65 batch 134 batch_loss: 0.20162037014961243\n",
      "training: 65 batch 135 batch_loss: 0.20822596549987793\n",
      "training: 65 batch 136 batch_loss: 0.20705515146255493\n",
      "training: 65 batch 137 batch_loss: 0.20867964625358582\n",
      "training: 65 batch 138 batch_loss: 0.21093085408210754\n",
      "training: 65 batch 139 batch_loss: 0.20431506633758545\n",
      "training: 65 batch 140 batch_loss: 0.20832404494285583\n",
      "training: 65 batch 141 batch_loss: 0.2063414752483368\n",
      "training: 65 batch 142 batch_loss: 0.20399215817451477\n",
      "training: 65 batch 143 batch_loss: 0.20719870924949646\n",
      "training: 65 batch 144 batch_loss: 0.2132728397846222\n",
      "training: 65 batch 145 batch_loss: 0.2062019407749176\n",
      "training: 65 batch 146 batch_loss: 0.20802223682403564\n",
      "training: 65 batch 147 batch_loss: 0.20638173818588257\n",
      "training: 65 batch 148 batch_loss: 0.20729440450668335\n",
      "training: 65 batch 149 batch_loss: 0.2090277075767517\n",
      "training: 65 batch 150 batch_loss: 0.2045501470565796\n",
      "training: 65 batch 151 batch_loss: 0.20757144689559937\n",
      "training: 65 batch 152 batch_loss: 0.20844897627830505\n",
      "training: 65 batch 153 batch_loss: 0.20754081010818481\n",
      "training: 65 batch 154 batch_loss: 0.20560139417648315\n",
      "training: 65 batch 155 batch_loss: 0.20724943280220032\n",
      "training: 65 batch 156 batch_loss: 0.21028491854667664\n",
      "training: 65 batch 157 batch_loss: 0.20781239867210388\n",
      "training: 65 batch 158 batch_loss: 0.20918497443199158\n",
      "training: 65 batch 159 batch_loss: 0.21153584122657776\n",
      "training: 65 batch 160 batch_loss: 0.20841288566589355\n",
      "training: 65 batch 161 batch_loss: 0.2077968418598175\n",
      "training: 65 batch 162 batch_loss: 0.2089506983757019\n",
      "training: 65 batch 163 batch_loss: 0.20499646663665771\n",
      "training: 65 batch 164 batch_loss: 0.20866116881370544\n",
      "training: 65 batch 165 batch_loss: 0.20799043774604797\n",
      "training: 65 batch 166 batch_loss: 0.20682331919670105\n",
      "training: 65 batch 167 batch_loss: 0.20871740579605103\n",
      "training: 65 batch 168 batch_loss: 0.2103242576122284\n",
      "training: 65 batch 169 batch_loss: 0.20809844136238098\n",
      "training: 65 batch 170 batch_loss: 0.20696896314620972\n",
      "training: 65 batch 171 batch_loss: 0.20417839288711548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 65 batch 172 batch_loss: 0.20969080924987793\n",
      "training: 65 batch 173 batch_loss: 0.2108020782470703\n",
      "training: 65 batch 174 batch_loss: 0.20609772205352783\n",
      "training: 65 batch 175 batch_loss: 0.2090967297554016\n",
      "training: 65 batch 176 batch_loss: 0.20946812629699707\n",
      "training: 65 batch 177 batch_loss: 0.20778217911720276\n",
      "training: 65 batch 178 batch_loss: 0.20869603753089905\n",
      "training: 65 batch 179 batch_loss: 0.20787468552589417\n",
      "training: 65 batch 180 batch_loss: 0.20803114771842957\n",
      "training: 65 batch 181 batch_loss: 0.20849564671516418\n",
      "training: 65 batch 182 batch_loss: 0.20573875308036804\n",
      "training: 65 batch 183 batch_loss: 0.20525646209716797\n",
      "training: 65 batch 184 batch_loss: 0.20610737800598145\n",
      "training: 65 batch 185 batch_loss: 0.2085123360157013\n",
      "training: 65 batch 186 batch_loss: 0.20653104782104492\n",
      "training: 65 batch 187 batch_loss: 0.20789319276809692\n",
      "training: 65 batch 188 batch_loss: 0.2114487588405609\n",
      "training: 65 batch 189 batch_loss: 0.2003590166568756\n",
      "training: 65 batch 190 batch_loss: 0.20575129985809326\n",
      "training: 65 batch 191 batch_loss: 0.21561214327812195\n",
      "training: 65 batch 192 batch_loss: 0.2050306499004364\n",
      "training: 65 batch 193 batch_loss: 0.21012413501739502\n",
      "training: 65 batch 194 batch_loss: 0.20871499180793762\n",
      "training: 65 batch 195 batch_loss: 0.20337599515914917\n",
      "training: 65 batch 196 batch_loss: 0.20940551161766052\n",
      "training: 65 batch 197 batch_loss: 0.21372711658477783\n",
      "training: 65 batch 198 batch_loss: 0.20803746581077576\n",
      "training: 65 batch 199 batch_loss: 0.21004542708396912\n",
      "training: 65 batch 200 batch_loss: 0.20775026082992554\n",
      "training: 65 batch 201 batch_loss: 0.20875459909439087\n",
      "training: 65 batch 202 batch_loss: 0.2063148319721222\n",
      "training: 65 batch 203 batch_loss: 0.208329975605011\n",
      "training: 65 batch 204 batch_loss: 0.21123641729354858\n",
      "training: 65 batch 205 batch_loss: 0.20692181587219238\n",
      "training: 65 batch 206 batch_loss: 0.20821893215179443\n",
      "training: 65 batch 207 batch_loss: 0.21040821075439453\n",
      "training: 65 batch 208 batch_loss: 0.21335405111312866\n",
      "training: 65 batch 209 batch_loss: 0.20608949661254883\n",
      "training: 65 batch 210 batch_loss: 0.20603948831558228\n",
      "training: 65 batch 211 batch_loss: 0.21183639764785767\n",
      "training: 65 batch 212 batch_loss: 0.20269626379013062\n",
      "training: 65 batch 213 batch_loss: 0.20846247673034668\n",
      "training: 65 batch 214 batch_loss: 0.21097934246063232\n",
      "training: 65 batch 215 batch_loss: 0.2057236135005951\n",
      "training: 65 batch 216 batch_loss: 0.20748010277748108\n",
      "training: 65 batch 217 batch_loss: 0.2080133557319641\n",
      "training: 65 batch 218 batch_loss: 0.2082774043083191\n",
      "training: 65 batch 219 batch_loss: 0.21020841598510742\n",
      "training: 65 batch 220 batch_loss: 0.20909041166305542\n",
      "training: 65 batch 221 batch_loss: 0.207881897687912\n",
      "training: 65 batch 222 batch_loss: 0.20726454257965088\n",
      "training: 65 batch 223 batch_loss: 0.20465180277824402\n",
      "training: 65 batch 224 batch_loss: 0.2127523422241211\n",
      "training: 65 batch 225 batch_loss: 0.20930978655815125\n",
      "training: 65 batch 226 batch_loss: 0.2096712589263916\n",
      "training: 65 batch 227 batch_loss: 0.20872762799263\n",
      "training: 65 batch 228 batch_loss: 0.20701062679290771\n",
      "training: 65 batch 229 batch_loss: 0.2076425552368164\n",
      "training: 65 batch 230 batch_loss: 0.2082144320011139\n",
      "training: 65 batch 231 batch_loss: 0.2083573341369629\n",
      "training: 65 batch 232 batch_loss: 0.20771238207817078\n",
      "training: 65 batch 233 batch_loss: 0.20431113243103027\n",
      "training: 65 batch 234 batch_loss: 0.2056477665901184\n",
      "training: 65 batch 235 batch_loss: 0.21347260475158691\n",
      "training: 65 batch 236 batch_loss: 0.2065500020980835\n",
      "training: 65 batch 237 batch_loss: 0.2096163034439087\n",
      "training: 65 batch 238 batch_loss: 0.2072802186012268\n",
      "training: 65 batch 239 batch_loss: 0.20947831869125366\n",
      "training: 65 batch 240 batch_loss: 0.20612028241157532\n",
      "training: 65 batch 241 batch_loss: 0.2060805857181549\n",
      "training: 65 batch 242 batch_loss: 0.21158316731452942\n",
      "training: 65 batch 243 batch_loss: 0.2104020118713379\n",
      "training: 65 batch 244 batch_loss: 0.2085651159286499\n",
      "training: 65 batch 245 batch_loss: 0.20682629942893982\n",
      "training: 65 batch 246 batch_loss: 0.2074684202671051\n",
      "training: 65 batch 247 batch_loss: 0.21050918102264404\n",
      "training: 65 batch 248 batch_loss: 0.20725354552268982\n",
      "training: 65 batch 249 batch_loss: 0.2063789665699005\n",
      "training: 65 batch 250 batch_loss: 0.20991545915603638\n",
      "training: 65 batch 251 batch_loss: 0.2062988579273224\n",
      "training: 65 batch 252 batch_loss: 0.2094372808933258\n",
      "training: 65 batch 253 batch_loss: 0.21099954843521118\n",
      "training: 65 batch 254 batch_loss: 0.21193861961364746\n",
      "training: 65 batch 255 batch_loss: 0.20686769485473633\n",
      "training: 65 batch 256 batch_loss: 0.21225211024284363\n",
      "training: 65 batch 257 batch_loss: 0.2084348499774933\n",
      "training: 65 batch 258 batch_loss: 0.20844736695289612\n",
      "training: 65 batch 259 batch_loss: 0.20682761073112488\n",
      "training: 65 batch 260 batch_loss: 0.20702984929084778\n",
      "training: 65 batch 261 batch_loss: 0.20570224523544312\n",
      "training: 65 batch 262 batch_loss: 0.2108226716518402\n",
      "training: 65 batch 263 batch_loss: 0.20920804142951965\n",
      "training: 65 batch 264 batch_loss: 0.20710650086402893\n",
      "training: 65 batch 265 batch_loss: 0.20694345235824585\n",
      "training: 65 batch 266 batch_loss: 0.20607581734657288\n",
      "training: 65 batch 267 batch_loss: 0.2068241536617279\n",
      "training: 65 batch 268 batch_loss: 0.2065555453300476\n",
      "training: 65 batch 269 batch_loss: 0.2072308361530304\n",
      "training: 65 batch 270 batch_loss: 0.20808887481689453\n",
      "training: 65 batch 271 batch_loss: 0.205104798078537\n",
      "training: 65 batch 272 batch_loss: 0.20924469828605652\n",
      "training: 65 batch 273 batch_loss: 0.20563969016075134\n",
      "training: 65 batch 274 batch_loss: 0.2120627760887146\n",
      "training: 65 batch 275 batch_loss: 0.21215412020683289\n",
      "training: 65 batch 276 batch_loss: 0.20937150716781616\n",
      "training: 65 batch 277 batch_loss: 0.20588278770446777\n",
      "training: 65 batch 278 batch_loss: 0.20812124013900757\n",
      "training: 65 batch 279 batch_loss: 0.20720481872558594\n",
      "training: 65 batch 280 batch_loss: 0.2050008773803711\n",
      "training: 65 batch 281 batch_loss: 0.2104901671409607\n",
      "training: 65 batch 282 batch_loss: 0.21292057633399963\n",
      "training: 65 batch 283 batch_loss: 0.20925083756446838\n",
      "training: 65 batch 284 batch_loss: 0.20561033487319946\n",
      "training: 65 batch 285 batch_loss: 0.20974111557006836\n",
      "training: 65 batch 286 batch_loss: 0.20979174971580505\n",
      "training: 65 batch 287 batch_loss: 0.2093505561351776\n",
      "training: 65 batch 288 batch_loss: 0.2076869010925293\n",
      "training: 65 batch 289 batch_loss: 0.21212148666381836\n",
      "training: 65 batch 290 batch_loss: 0.2070501744747162\n",
      "training: 65 batch 291 batch_loss: 0.20617663860321045\n",
      "training: 65 batch 292 batch_loss: 0.20817318558692932\n",
      "training: 65 batch 293 batch_loss: 0.20716968178749084\n",
      "training: 65 batch 294 batch_loss: 0.20932236313819885\n",
      "training: 65 batch 295 batch_loss: 0.20793256163597107\n",
      "training: 65 batch 296 batch_loss: 0.21306103467941284\n",
      "training: 65 batch 297 batch_loss: 0.21057572960853577\n",
      "training: 65 batch 298 batch_loss: 0.2131749391555786\n",
      "training: 65 batch 299 batch_loss: 0.20277172327041626\n",
      "training: 65 batch 300 batch_loss: 0.20831793546676636\n",
      "training: 65 batch 301 batch_loss: 0.20508962869644165\n",
      "training: 65 batch 302 batch_loss: 0.20995214581489563\n",
      "training: 65 batch 303 batch_loss: 0.21162092685699463\n",
      "training: 65 batch 304 batch_loss: 0.20836278796195984\n",
      "training: 65 batch 305 batch_loss: 0.20946034789085388\n",
      "training: 65 batch 306 batch_loss: 0.2080232799053192\n",
      "training: 65 batch 307 batch_loss: 0.21401017904281616\n",
      "training: 65 batch 308 batch_loss: 0.2083837389945984\n",
      "training: 65 batch 309 batch_loss: 0.2090909481048584\n",
      "training: 65 batch 310 batch_loss: 0.20487448573112488\n",
      "training: 65 batch 311 batch_loss: 0.20881375670433044\n",
      "training: 65 batch 312 batch_loss: 0.20624849200248718\n",
      "training: 65 batch 313 batch_loss: 0.21222209930419922\n",
      "training: 65 batch 314 batch_loss: 0.21120241284370422\n",
      "training: 65 batch 315 batch_loss: 0.207229346036911\n",
      "training: 65 batch 316 batch_loss: 0.20329433679580688\n",
      "training: 65 batch 317 batch_loss: 0.20392286777496338\n",
      "training: 65 batch 318 batch_loss: 0.21413201093673706\n",
      "training: 65 batch 319 batch_loss: 0.20997923612594604\n",
      "training: 65 batch 320 batch_loss: 0.2101655900478363\n",
      "training: 65 batch 321 batch_loss: 0.212273508310318\n",
      "training: 65 batch 322 batch_loss: 0.2091602385044098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 65 batch 323 batch_loss: 0.20840871334075928\n",
      "training: 65 batch 324 batch_loss: 0.210353821516037\n",
      "training: 65 batch 325 batch_loss: 0.20911115407943726\n",
      "training: 65 batch 326 batch_loss: 0.20913910865783691\n",
      "training: 65 batch 327 batch_loss: 0.2117709517478943\n",
      "training: 65 batch 328 batch_loss: 0.2123720943927765\n",
      "training: 65 batch 329 batch_loss: 0.209011048078537\n",
      "training: 65 batch 330 batch_loss: 0.20788612961769104\n",
      "training: 65 batch 331 batch_loss: 0.20959323644638062\n",
      "training: 65 batch 332 batch_loss: 0.2099302113056183\n",
      "training: 65 batch 333 batch_loss: 0.2105502486228943\n",
      "training: 65 batch 334 batch_loss: 0.20918798446655273\n",
      "training: 65 batch 335 batch_loss: 0.21351587772369385\n",
      "training: 65 batch 336 batch_loss: 0.20641428232192993\n",
      "training: 65 batch 337 batch_loss: 0.21201211214065552\n",
      "training: 65 batch 338 batch_loss: 0.21170151233673096\n",
      "training: 65 batch 339 batch_loss: 0.2161724865436554\n",
      "training: 65 batch 340 batch_loss: 0.21043407917022705\n",
      "training: 65 batch 341 batch_loss: 0.20573508739471436\n",
      "training: 65 batch 342 batch_loss: 0.21057790517807007\n",
      "training: 65 batch 343 batch_loss: 0.20903456211090088\n",
      "training: 65 batch 344 batch_loss: 0.20154938101768494\n",
      "training: 65 batch 345 batch_loss: 0.20624437928199768\n",
      "training: 65 batch 346 batch_loss: 0.20819374918937683\n",
      "training: 65 batch 347 batch_loss: 0.20523184537887573\n",
      "training: 65 batch 348 batch_loss: 0.2121620774269104\n",
      "training: 65 batch 349 batch_loss: 0.20843863487243652\n",
      "training: 65 batch 350 batch_loss: 0.21402409672737122\n",
      "training: 65 batch 351 batch_loss: 0.20959919691085815\n",
      "training: 65 batch 352 batch_loss: 0.20524582266807556\n",
      "training: 65 batch 353 batch_loss: 0.20865008234977722\n",
      "training: 65 batch 354 batch_loss: 0.20485925674438477\n",
      "training: 65 batch 355 batch_loss: 0.20869892835617065\n",
      "training: 65 batch 356 batch_loss: 0.2089274525642395\n",
      "training: 65 batch 357 batch_loss: 0.2103794813156128\n",
      "training: 65 batch 358 batch_loss: 0.20670759677886963\n",
      "training: 65 batch 359 batch_loss: 0.20916596055030823\n",
      "training: 65 batch 360 batch_loss: 0.2091192901134491\n",
      "training: 65 batch 361 batch_loss: 0.2080804705619812\n",
      "training: 65 batch 362 batch_loss: 0.20812290906906128\n",
      "training: 65 batch 363 batch_loss: 0.2090165913105011\n",
      "training: 65 batch 364 batch_loss: 0.20892667770385742\n",
      "training: 65 batch 365 batch_loss: 0.20822376012802124\n",
      "training: 65 batch 366 batch_loss: 0.2082996964454651\n",
      "training: 65 batch 367 batch_loss: 0.20796126127243042\n",
      "training: 65 batch 368 batch_loss: 0.21056976914405823\n",
      "training: 65 batch 369 batch_loss: 0.20928829908370972\n",
      "training: 65 batch 370 batch_loss: 0.2101885974407196\n",
      "training: 65 batch 371 batch_loss: 0.208368718624115\n",
      "training: 65 batch 372 batch_loss: 0.20686748623847961\n",
      "training: 65 batch 373 batch_loss: 0.20769983530044556\n",
      "training: 65 batch 374 batch_loss: 0.2140432596206665\n",
      "training: 65 batch 375 batch_loss: 0.2060261368751526\n",
      "training: 65 batch 376 batch_loss: 0.20964094996452332\n",
      "training: 65 batch 377 batch_loss: 0.2070431411266327\n",
      "training: 65 batch 378 batch_loss: 0.211529940366745\n",
      "training: 65 batch 379 batch_loss: 0.20700329542160034\n",
      "training: 65 batch 380 batch_loss: 0.20747211575508118\n",
      "training: 65 batch 381 batch_loss: 0.2081318497657776\n",
      "training: 65 batch 382 batch_loss: 0.21424046158790588\n",
      "training: 65 batch 383 batch_loss: 0.2079939842224121\n",
      "training: 65 batch 384 batch_loss: 0.2076358199119568\n",
      "training: 65 batch 385 batch_loss: 0.21086132526397705\n",
      "training: 65 batch 386 batch_loss: 0.2037680745124817\n",
      "training: 65 batch 387 batch_loss: 0.20768633484840393\n",
      "training: 65 batch 388 batch_loss: 0.2081916332244873\n",
      "training: 65 batch 389 batch_loss: 0.20871689915657043\n",
      "training: 65 batch 390 batch_loss: 0.21194764971733093\n",
      "training: 65 batch 391 batch_loss: 0.2103799283504486\n",
      "training: 65 batch 392 batch_loss: 0.20934763550758362\n",
      "training: 65 batch 393 batch_loss: 0.20875582098960876\n",
      "training: 65 batch 394 batch_loss: 0.2097594141960144\n",
      "training: 65 batch 395 batch_loss: 0.20896312594413757\n",
      "training: 65 batch 396 batch_loss: 0.21148204803466797\n",
      "training: 65 batch 397 batch_loss: 0.20935407280921936\n",
      "training: 65 batch 398 batch_loss: 0.20807495713233948\n",
      "training: 65 batch 399 batch_loss: 0.20572936534881592\n",
      "training: 65 batch 400 batch_loss: 0.2103421688079834\n",
      "training: 65 batch 401 batch_loss: 0.2113586664199829\n",
      "training: 65 batch 402 batch_loss: 0.21132013201713562\n",
      "training: 65 batch 403 batch_loss: 0.21372568607330322\n",
      "training: 65 batch 404 batch_loss: 0.20756369829177856\n",
      "training: 65 batch 405 batch_loss: 0.21124786138534546\n",
      "training: 65 batch 406 batch_loss: 0.21126866340637207\n",
      "training: 65 batch 407 batch_loss: 0.20745137333869934\n",
      "training: 65 batch 408 batch_loss: 0.20673048496246338\n",
      "training: 65 batch 409 batch_loss: 0.20843178033828735\n",
      "training: 65 batch 410 batch_loss: 0.20840111374855042\n",
      "training: 65 batch 411 batch_loss: 0.20160844922065735\n",
      "training: 65 batch 412 batch_loss: 0.21058964729309082\n",
      "training: 65 batch 413 batch_loss: 0.20866543054580688\n",
      "training: 65 batch 414 batch_loss: 0.20679908990859985\n",
      "training: 65 batch 415 batch_loss: 0.2092854082584381\n",
      "training: 65 batch 416 batch_loss: 0.20733460783958435\n",
      "training: 65 batch 417 batch_loss: 0.21075016260147095\n",
      "training: 65 batch 418 batch_loss: 0.20831915736198425\n",
      "training: 65 batch 419 batch_loss: 0.20684978365898132\n",
      "training: 65 batch 420 batch_loss: 0.21108970046043396\n",
      "training: 65 batch 421 batch_loss: 0.20706582069396973\n",
      "training: 65 batch 422 batch_loss: 0.20971572399139404\n",
      "training: 65 batch 423 batch_loss: 0.20869305729866028\n",
      "training: 65 batch 424 batch_loss: 0.20932930707931519\n",
      "training: 65 batch 425 batch_loss: 0.20855355262756348\n",
      "training: 65 batch 426 batch_loss: 0.20883166790008545\n",
      "training: 65 batch 427 batch_loss: 0.20672649145126343\n",
      "training: 65 batch 428 batch_loss: 0.21292632818222046\n",
      "training: 65 batch 429 batch_loss: 0.2101292908191681\n",
      "training: 65 batch 430 batch_loss: 0.21334058046340942\n",
      "training: 65 batch 431 batch_loss: 0.21024024486541748\n",
      "training: 65 batch 432 batch_loss: 0.20947599411010742\n",
      "training: 65 batch 433 batch_loss: 0.21276864409446716\n",
      "training: 65 batch 434 batch_loss: 0.21117043495178223\n",
      "training: 65 batch 435 batch_loss: 0.21220099925994873\n",
      "training: 65 batch 436 batch_loss: 0.207831472158432\n",
      "training: 65 batch 437 batch_loss: 0.20916876196861267\n",
      "training: 65 batch 438 batch_loss: 0.21072280406951904\n",
      "training: 65 batch 439 batch_loss: 0.21032094955444336\n",
      "training: 65 batch 440 batch_loss: 0.2050989270210266\n",
      "training: 65 batch 441 batch_loss: 0.20998141169548035\n",
      "training: 65 batch 442 batch_loss: 0.20936179161071777\n",
      "training: 65 batch 443 batch_loss: 0.2064785659313202\n",
      "training: 65 batch 444 batch_loss: 0.20537936687469482\n",
      "training: 65 batch 445 batch_loss: 0.20741108059883118\n",
      "training: 65 batch 446 batch_loss: 0.20818516612052917\n",
      "training: 65 batch 447 batch_loss: 0.20713776350021362\n",
      "training: 65 batch 448 batch_loss: 0.20596736669540405\n",
      "training: 65 batch 449 batch_loss: 0.20686125755310059\n",
      "training: 65 batch 450 batch_loss: 0.21196773648262024\n",
      "training: 65 batch 451 batch_loss: 0.2076910138130188\n",
      "training: 65 batch 452 batch_loss: 0.20780032873153687\n",
      "training: 65 batch 453 batch_loss: 0.21326938271522522\n",
      "training: 65 batch 454 batch_loss: 0.2039501667022705\n",
      "training: 65 batch 455 batch_loss: 0.206231951713562\n",
      "training: 65 batch 456 batch_loss: 0.21077901124954224\n",
      "training: 65 batch 457 batch_loss: 0.20523446798324585\n",
      "training: 65 batch 458 batch_loss: 0.20710477232933044\n",
      "training: 65 batch 459 batch_loss: 0.21008947491645813\n",
      "training: 65 batch 460 batch_loss: 0.21063685417175293\n",
      "training: 65 batch 461 batch_loss: 0.2091355323791504\n",
      "training: 65 batch 462 batch_loss: 0.21070605516433716\n",
      "training: 65 batch 463 batch_loss: 0.20915475487709045\n",
      "training: 65 batch 464 batch_loss: 0.2102893888950348\n",
      "training: 65 batch 465 batch_loss: 0.2109578251838684\n",
      "training: 65 batch 466 batch_loss: 0.21010097861289978\n",
      "training: 65 batch 467 batch_loss: 0.21347257494926453\n",
      "training: 65 batch 468 batch_loss: 0.21170520782470703\n",
      "training: 65 batch 469 batch_loss: 0.20823559165000916\n",
      "training: 65 batch 470 batch_loss: 0.20670649409294128\n",
      "training: 65 batch 471 batch_loss: 0.20793485641479492\n",
      "training: 65 batch 472 batch_loss: 0.2087085247039795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 65 batch 473 batch_loss: 0.20969441533088684\n",
      "training: 65 batch 474 batch_loss: 0.21353894472122192\n",
      "training: 65 batch 475 batch_loss: 0.20999935269355774\n",
      "training: 65 batch 476 batch_loss: 0.21160674095153809\n",
      "training: 65 batch 477 batch_loss: 0.20742329955101013\n",
      "training: 65 batch 478 batch_loss: 0.20640814304351807\n",
      "training: 65 batch 479 batch_loss: 0.21113324165344238\n",
      "training: 65 batch 480 batch_loss: 0.20727667212486267\n",
      "training: 65 batch 481 batch_loss: 0.2116709053516388\n",
      "training: 65 batch 482 batch_loss: 0.20865494012832642\n",
      "training: 65 batch 483 batch_loss: 0.20865106582641602\n",
      "training: 65 batch 484 batch_loss: 0.21249616146087646\n",
      "training: 65 batch 485 batch_loss: 0.21009373664855957\n",
      "training: 65 batch 486 batch_loss: 0.21074900031089783\n",
      "training: 65 batch 487 batch_loss: 0.209642231464386\n",
      "training: 65 batch 488 batch_loss: 0.20911556482315063\n",
      "training: 65 batch 489 batch_loss: 0.2056163251399994\n",
      "training: 65 batch 490 batch_loss: 0.21149498224258423\n",
      "training: 65 batch 491 batch_loss: 0.20807814598083496\n",
      "training: 65 batch 492 batch_loss: 0.21091657876968384\n",
      "training: 65 batch 493 batch_loss: 0.2109423279762268\n",
      "training: 65 batch 494 batch_loss: 0.20957332849502563\n",
      "training: 65 batch 495 batch_loss: 0.2123343050479889\n",
      "training: 65 batch 496 batch_loss: 0.20846599340438843\n",
      "training: 65 batch 497 batch_loss: 0.21099647879600525\n",
      "training: 65 batch 498 batch_loss: 0.20638549327850342\n",
      "training: 65 batch 499 batch_loss: 0.20588278770446777\n",
      "training: 65 batch 500 batch_loss: 0.2109197974205017\n",
      "training: 65 batch 501 batch_loss: 0.21150356531143188\n",
      "training: 65 batch 502 batch_loss: 0.20939460396766663\n",
      "training: 65 batch 503 batch_loss: 0.21324431896209717\n",
      "training: 65 batch 504 batch_loss: 0.20712566375732422\n",
      "training: 65 batch 505 batch_loss: 0.210699200630188\n",
      "training: 65 batch 506 batch_loss: 0.21335682272911072\n",
      "training: 65 batch 507 batch_loss: 0.21245113015174866\n",
      "training: 65 batch 508 batch_loss: 0.21085503697395325\n",
      "training: 65 batch 509 batch_loss: 0.20912891626358032\n",
      "training: 65 batch 510 batch_loss: 0.21004989743232727\n",
      "training: 65 batch 511 batch_loss: 0.20903605222702026\n",
      "training: 65 batch 512 batch_loss: 0.20915237069129944\n",
      "training: 65 batch 513 batch_loss: 0.21058520674705505\n",
      "training: 65 batch 514 batch_loss: 0.2061019241809845\n",
      "training: 65 batch 515 batch_loss: 0.20725154876708984\n",
      "training: 65 batch 516 batch_loss: 0.2132003903388977\n",
      "training: 65 batch 517 batch_loss: 0.2081005871295929\n",
      "training: 65 batch 518 batch_loss: 0.20994329452514648\n",
      "training: 65 batch 519 batch_loss: 0.2116955816745758\n",
      "training: 65 batch 520 batch_loss: 0.2087119221687317\n",
      "training: 65 batch 521 batch_loss: 0.21073845028877258\n",
      "training: 65 batch 522 batch_loss: 0.21320033073425293\n",
      "training: 65 batch 523 batch_loss: 0.21233612298965454\n",
      "training: 65 batch 524 batch_loss: 0.20899179577827454\n",
      "training: 65 batch 525 batch_loss: 0.20927026867866516\n",
      "training: 65 batch 526 batch_loss: 0.20732566714286804\n",
      "training: 65 batch 527 batch_loss: 0.21230670809745789\n",
      "training: 65 batch 528 batch_loss: 0.205954909324646\n",
      "training: 65 batch 529 batch_loss: 0.2116139829158783\n",
      "training: 65 batch 530 batch_loss: 0.21002471446990967\n",
      "training: 65 batch 531 batch_loss: 0.21080884337425232\n",
      "training: 65 batch 532 batch_loss: 0.2078857123851776\n",
      "training: 65 batch 533 batch_loss: 0.20771825313568115\n",
      "training: 65 batch 534 batch_loss: 0.20905059576034546\n",
      "training: 65 batch 535 batch_loss: 0.20739641785621643\n",
      "training: 65 batch 536 batch_loss: 0.21186301112174988\n",
      "training: 65 batch 537 batch_loss: 0.20907017588615417\n",
      "training: 65 batch 538 batch_loss: 0.2105080485343933\n",
      "training: 65 batch 539 batch_loss: 0.20777800679206848\n",
      "training: 65 batch 540 batch_loss: 0.20808246731758118\n",
      "training: 65 batch 541 batch_loss: 0.20968788862228394\n",
      "training: 65 batch 542 batch_loss: 0.21203893423080444\n",
      "training: 65 batch 543 batch_loss: 0.20981177687644958\n",
      "training: 65 batch 544 batch_loss: 0.20612046122550964\n",
      "training: 65 batch 545 batch_loss: 0.21268650889396667\n",
      "training: 65 batch 546 batch_loss: 0.20961180329322815\n",
      "training: 65 batch 547 batch_loss: 0.21234282851219177\n",
      "training: 65 batch 548 batch_loss: 0.2070941925048828\n",
      "training: 65 batch 549 batch_loss: 0.20805925130844116\n",
      "training: 65 batch 550 batch_loss: 0.21075007319450378\n",
      "training: 65 batch 551 batch_loss: 0.20969468355178833\n",
      "training: 65 batch 552 batch_loss: 0.20965653657913208\n",
      "training: 65 batch 553 batch_loss: 0.20817464590072632\n",
      "training: 65 batch 554 batch_loss: 0.20843389630317688\n",
      "training: 65 batch 555 batch_loss: 0.2112157940864563\n",
      "training: 65 batch 556 batch_loss: 0.2076479196548462\n",
      "training: 65 batch 557 batch_loss: 0.20737558603286743\n",
      "training: 65 batch 558 batch_loss: 0.21096950769424438\n",
      "training: 65 batch 559 batch_loss: 0.206650048494339\n",
      "training: 65 batch 560 batch_loss: 0.20690369606018066\n",
      "training: 65 batch 561 batch_loss: 0.21136906743049622\n",
      "training: 65 batch 562 batch_loss: 0.210374116897583\n",
      "training: 65 batch 563 batch_loss: 0.20893153548240662\n",
      "training: 65 batch 564 batch_loss: 0.20781436562538147\n",
      "training: 65 batch 565 batch_loss: 0.2114447057247162\n",
      "training: 65 batch 566 batch_loss: 0.21076640486717224\n",
      "training: 65 batch 567 batch_loss: 0.20744526386260986\n",
      "training: 65 batch 568 batch_loss: 0.2082124650478363\n",
      "training: 65 batch 569 batch_loss: 0.20809030532836914\n",
      "training: 65 batch 570 batch_loss: 0.2088385820388794\n",
      "training: 65 batch 571 batch_loss: 0.20805230736732483\n",
      "training: 65 batch 572 batch_loss: 0.21015724539756775\n",
      "training: 65 batch 573 batch_loss: 0.20789718627929688\n",
      "training: 65 batch 574 batch_loss: 0.2125578224658966\n",
      "training: 65 batch 575 batch_loss: 0.21053478121757507\n",
      "training: 65 batch 576 batch_loss: 0.20946738123893738\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 65, Hit Ratio:0.03296212769784772 | Precision:0.048633638061535436 | Recall:0.06408306959447993 | NDCG:0.06317620767754861\n",
      "*Best Performance* \n",
      "Epoch: 61, Hit Ratio:0.033261934821931224 | Precision:0.04907598545168584 | Recall:0.06479544314235144 | MDCG:0.06348668156373174\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 66 batch 0 batch_loss: 0.20836669206619263\n",
      "training: 66 batch 1 batch_loss: 0.20943570137023926\n",
      "training: 66 batch 2 batch_loss: 0.20970827341079712\n",
      "training: 66 batch 3 batch_loss: 0.20655545592308044\n",
      "training: 66 batch 4 batch_loss: 0.2161087691783905\n",
      "training: 66 batch 5 batch_loss: 0.21248680353164673\n",
      "training: 66 batch 6 batch_loss: 0.20830559730529785\n",
      "training: 66 batch 7 batch_loss: 0.20980697870254517\n",
      "training: 66 batch 8 batch_loss: 0.20762336254119873\n",
      "training: 66 batch 9 batch_loss: 0.20467188954353333\n",
      "training: 66 batch 10 batch_loss: 0.20387372374534607\n",
      "training: 66 batch 11 batch_loss: 0.20830681920051575\n",
      "training: 66 batch 12 batch_loss: 0.20856112241744995\n",
      "training: 66 batch 13 batch_loss: 0.20621907711029053\n",
      "training: 66 batch 14 batch_loss: 0.2075653076171875\n",
      "training: 66 batch 15 batch_loss: 0.20941424369812012\n",
      "training: 66 batch 16 batch_loss: 0.20514929294586182\n",
      "training: 66 batch 17 batch_loss: 0.2103167176246643\n",
      "training: 66 batch 18 batch_loss: 0.21216243505477905\n",
      "training: 66 batch 19 batch_loss: 0.20602959394454956\n",
      "training: 66 batch 20 batch_loss: 0.20952409505844116\n",
      "training: 66 batch 21 batch_loss: 0.20633471012115479\n",
      "training: 66 batch 22 batch_loss: 0.20826295018196106\n",
      "training: 66 batch 23 batch_loss: 0.20787352323532104\n",
      "training: 66 batch 24 batch_loss: 0.20725059509277344\n",
      "training: 66 batch 25 batch_loss: 0.20869702100753784\n",
      "training: 66 batch 26 batch_loss: 0.20563626289367676\n",
      "training: 66 batch 27 batch_loss: 0.20459195971488953\n",
      "training: 66 batch 28 batch_loss: 0.20679527521133423\n",
      "training: 66 batch 29 batch_loss: 0.20973199605941772\n",
      "training: 66 batch 30 batch_loss: 0.20896852016448975\n",
      "training: 66 batch 31 batch_loss: 0.2077060341835022\n",
      "training: 66 batch 32 batch_loss: 0.2031802237033844\n",
      "training: 66 batch 33 batch_loss: 0.20661389827728271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 66 batch 34 batch_loss: 0.2081952691078186\n",
      "training: 66 batch 35 batch_loss: 0.20895087718963623\n",
      "training: 66 batch 36 batch_loss: 0.20944339036941528\n",
      "training: 66 batch 37 batch_loss: 0.21484017372131348\n",
      "training: 66 batch 38 batch_loss: 0.20531579852104187\n",
      "training: 66 batch 39 batch_loss: 0.20999294519424438\n",
      "training: 66 batch 40 batch_loss: 0.2048373520374298\n",
      "training: 66 batch 41 batch_loss: 0.20893999934196472\n",
      "training: 66 batch 42 batch_loss: 0.21029263734817505\n",
      "training: 66 batch 43 batch_loss: 0.20536941289901733\n",
      "training: 66 batch 44 batch_loss: 0.21048909425735474\n",
      "training: 66 batch 45 batch_loss: 0.20657360553741455\n",
      "training: 66 batch 46 batch_loss: 0.20716428756713867\n",
      "training: 66 batch 47 batch_loss: 0.20535412430763245\n",
      "training: 66 batch 48 batch_loss: 0.2046893835067749\n",
      "training: 66 batch 49 batch_loss: 0.20956584811210632\n",
      "training: 66 batch 50 batch_loss: 0.2113397717475891\n",
      "training: 66 batch 51 batch_loss: 0.20777058601379395\n",
      "training: 66 batch 52 batch_loss: 0.20988526940345764\n",
      "training: 66 batch 53 batch_loss: 0.20659810304641724\n",
      "training: 66 batch 54 batch_loss: 0.20628982782363892\n",
      "training: 66 batch 55 batch_loss: 0.20927894115447998\n",
      "training: 66 batch 56 batch_loss: 0.21247103810310364\n",
      "training: 66 batch 57 batch_loss: 0.20323911309242249\n",
      "training: 66 batch 58 batch_loss: 0.21094971895217896\n",
      "training: 66 batch 59 batch_loss: 0.20617267489433289\n",
      "training: 66 batch 60 batch_loss: 0.2052125334739685\n",
      "training: 66 batch 61 batch_loss: 0.2064749002456665\n",
      "training: 66 batch 62 batch_loss: 0.20862120389938354\n",
      "training: 66 batch 63 batch_loss: 0.20968669652938843\n",
      "training: 66 batch 64 batch_loss: 0.20864707231521606\n",
      "training: 66 batch 65 batch_loss: 0.2107301950454712\n",
      "training: 66 batch 66 batch_loss: 0.20838898420333862\n",
      "training: 66 batch 67 batch_loss: 0.2091962695121765\n",
      "training: 66 batch 68 batch_loss: 0.20764347910881042\n",
      "training: 66 batch 69 batch_loss: 0.2062613070011139\n",
      "training: 66 batch 70 batch_loss: 0.20564770698547363\n",
      "training: 66 batch 71 batch_loss: 0.21037060022354126\n",
      "training: 66 batch 72 batch_loss: 0.20945951342582703\n",
      "training: 66 batch 73 batch_loss: 0.20588257908821106\n",
      "training: 66 batch 74 batch_loss: 0.20382732152938843\n",
      "training: 66 batch 75 batch_loss: 0.20942002534866333\n",
      "training: 66 batch 76 batch_loss: 0.2083185315132141\n",
      "training: 66 batch 77 batch_loss: 0.21297186613082886\n",
      "training: 66 batch 78 batch_loss: 0.20561686158180237\n",
      "training: 66 batch 79 batch_loss: 0.21195954084396362\n",
      "training: 66 batch 80 batch_loss: 0.2069668173789978\n",
      "training: 66 batch 81 batch_loss: 0.20896390080451965\n",
      "training: 66 batch 82 batch_loss: 0.20479243993759155\n",
      "training: 66 batch 83 batch_loss: 0.20755934715270996\n",
      "training: 66 batch 84 batch_loss: 0.2045862078666687\n",
      "training: 66 batch 85 batch_loss: 0.2063552439212799\n",
      "training: 66 batch 86 batch_loss: 0.20405733585357666\n",
      "training: 66 batch 87 batch_loss: 0.20859262347221375\n",
      "training: 66 batch 88 batch_loss: 0.20999625325202942\n",
      "training: 66 batch 89 batch_loss: 0.21254268288612366\n",
      "training: 66 batch 90 batch_loss: 0.20686861872673035\n",
      "training: 66 batch 91 batch_loss: 0.2086871862411499\n",
      "training: 66 batch 92 batch_loss: 0.2076834738254547\n",
      "training: 66 batch 93 batch_loss: 0.21076583862304688\n",
      "training: 66 batch 94 batch_loss: 0.20918530225753784\n",
      "training: 66 batch 95 batch_loss: 0.21026191115379333\n",
      "training: 66 batch 96 batch_loss: 0.20641574263572693\n",
      "training: 66 batch 97 batch_loss: 0.20737096667289734\n",
      "training: 66 batch 98 batch_loss: 0.21240997314453125\n",
      "training: 66 batch 99 batch_loss: 0.2089017927646637\n",
      "training: 66 batch 100 batch_loss: 0.2086058259010315\n",
      "training: 66 batch 101 batch_loss: 0.21040502190589905\n",
      "training: 66 batch 102 batch_loss: 0.20816728472709656\n",
      "training: 66 batch 103 batch_loss: 0.20745277404785156\n",
      "training: 66 batch 104 batch_loss: 0.20724359154701233\n",
      "training: 66 batch 105 batch_loss: 0.2108859419822693\n",
      "training: 66 batch 106 batch_loss: 0.21083718538284302\n",
      "training: 66 batch 107 batch_loss: 0.2083398699760437\n",
      "training: 66 batch 108 batch_loss: 0.21217483282089233\n",
      "training: 66 batch 109 batch_loss: 0.20865613222122192\n",
      "training: 66 batch 110 batch_loss: 0.20850703120231628\n",
      "training: 66 batch 111 batch_loss: 0.20880040526390076\n",
      "training: 66 batch 112 batch_loss: 0.20856979489326477\n",
      "training: 66 batch 113 batch_loss: 0.20948079228401184\n",
      "training: 66 batch 114 batch_loss: 0.205458402633667\n",
      "training: 66 batch 115 batch_loss: 0.2056884467601776\n",
      "training: 66 batch 116 batch_loss: 0.2121647596359253\n",
      "training: 66 batch 117 batch_loss: 0.2085106372833252\n",
      "training: 66 batch 118 batch_loss: 0.20705458521842957\n",
      "training: 66 batch 119 batch_loss: 0.2065509855747223\n",
      "training: 66 batch 120 batch_loss: 0.21236437559127808\n",
      "training: 66 batch 121 batch_loss: 0.1986386477947235\n",
      "training: 66 batch 122 batch_loss: 0.21292990446090698\n",
      "training: 66 batch 123 batch_loss: 0.20795586705207825\n",
      "training: 66 batch 124 batch_loss: 0.21186867356300354\n",
      "training: 66 batch 125 batch_loss: 0.20897233486175537\n",
      "training: 66 batch 126 batch_loss: 0.20749002695083618\n",
      "training: 66 batch 127 batch_loss: 0.2078726887702942\n",
      "training: 66 batch 128 batch_loss: 0.2059074342250824\n",
      "training: 66 batch 129 batch_loss: 0.20895111560821533\n",
      "training: 66 batch 130 batch_loss: 0.20843550562858582\n",
      "training: 66 batch 131 batch_loss: 0.20934635400772095\n",
      "training: 66 batch 132 batch_loss: 0.21012413501739502\n",
      "training: 66 batch 133 batch_loss: 0.20991680026054382\n",
      "training: 66 batch 134 batch_loss: 0.20383727550506592\n",
      "training: 66 batch 135 batch_loss: 0.21076184511184692\n",
      "training: 66 batch 136 batch_loss: 0.20940536260604858\n",
      "training: 66 batch 137 batch_loss: 0.20743435621261597\n",
      "training: 66 batch 138 batch_loss: 0.21233254671096802\n",
      "training: 66 batch 139 batch_loss: 0.2103588581085205\n",
      "training: 66 batch 140 batch_loss: 0.20735880732536316\n",
      "training: 66 batch 141 batch_loss: 0.20890694856643677\n",
      "training: 66 batch 142 batch_loss: 0.20906344056129456\n",
      "training: 66 batch 143 batch_loss: 0.21053743362426758\n",
      "training: 66 batch 144 batch_loss: 0.20968329906463623\n",
      "training: 66 batch 145 batch_loss: 0.2035394310951233\n",
      "training: 66 batch 146 batch_loss: 0.20586338639259338\n",
      "training: 66 batch 147 batch_loss: 0.20614027976989746\n",
      "training: 66 batch 148 batch_loss: 0.20802107453346252\n",
      "training: 66 batch 149 batch_loss: 0.2096712589263916\n",
      "training: 66 batch 150 batch_loss: 0.20659369230270386\n",
      "training: 66 batch 151 batch_loss: 0.20373940467834473\n",
      "training: 66 batch 152 batch_loss: 0.21244755387306213\n",
      "training: 66 batch 153 batch_loss: 0.2096235156059265\n",
      "training: 66 batch 154 batch_loss: 0.21086058020591736\n",
      "training: 66 batch 155 batch_loss: 0.2040289044380188\n",
      "training: 66 batch 156 batch_loss: 0.212105393409729\n",
      "training: 66 batch 157 batch_loss: 0.20719316601753235\n",
      "training: 66 batch 158 batch_loss: 0.2109956443309784\n",
      "training: 66 batch 159 batch_loss: 0.2082918882369995\n",
      "training: 66 batch 160 batch_loss: 0.2092042863368988\n",
      "training: 66 batch 161 batch_loss: 0.2124119997024536\n",
      "training: 66 batch 162 batch_loss: 0.20657294988632202\n",
      "training: 66 batch 163 batch_loss: 0.21167397499084473\n",
      "training: 66 batch 164 batch_loss: 0.20739170908927917\n",
      "training: 66 batch 165 batch_loss: 0.21035239100456238\n",
      "training: 66 batch 166 batch_loss: 0.21245020627975464\n",
      "training: 66 batch 167 batch_loss: 0.20930218696594238\n",
      "training: 66 batch 168 batch_loss: 0.20855361223220825\n",
      "training: 66 batch 169 batch_loss: 0.20876675844192505\n",
      "training: 66 batch 170 batch_loss: 0.21080583333969116\n",
      "training: 66 batch 171 batch_loss: 0.20755818486213684\n",
      "training: 66 batch 172 batch_loss: 0.20993542671203613\n",
      "training: 66 batch 173 batch_loss: 0.20842072367668152\n",
      "training: 66 batch 174 batch_loss: 0.21143889427185059\n",
      "training: 66 batch 175 batch_loss: 0.21306848526000977\n",
      "training: 66 batch 176 batch_loss: 0.20188727974891663\n",
      "training: 66 batch 177 batch_loss: 0.2069661021232605\n",
      "training: 66 batch 178 batch_loss: 0.2143515646457672\n",
      "training: 66 batch 179 batch_loss: 0.2096380591392517\n",
      "training: 66 batch 180 batch_loss: 0.20733296871185303\n",
      "training: 66 batch 181 batch_loss: 0.20927569270133972\n",
      "training: 66 batch 182 batch_loss: 0.20559656620025635\n",
      "training: 66 batch 183 batch_loss: 0.21063080430030823\n",
      "training: 66 batch 184 batch_loss: 0.20834821462631226\n",
      "training: 66 batch 185 batch_loss: 0.2099606990814209\n",
      "training: 66 batch 186 batch_loss: 0.20773470401763916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 66 batch 187 batch_loss: 0.21044039726257324\n",
      "training: 66 batch 188 batch_loss: 0.20926839113235474\n",
      "training: 66 batch 189 batch_loss: 0.20864880084991455\n",
      "training: 66 batch 190 batch_loss: 0.20968934893608093\n",
      "training: 66 batch 191 batch_loss: 0.2087607979774475\n",
      "training: 66 batch 192 batch_loss: 0.2089153528213501\n",
      "training: 66 batch 193 batch_loss: 0.21070286631584167\n",
      "training: 66 batch 194 batch_loss: 0.21125423908233643\n",
      "training: 66 batch 195 batch_loss: 0.2073161005973816\n",
      "training: 66 batch 196 batch_loss: 0.20639705657958984\n",
      "training: 66 batch 197 batch_loss: 0.2067388892173767\n",
      "training: 66 batch 198 batch_loss: 0.20901426672935486\n",
      "training: 66 batch 199 batch_loss: 0.21221521496772766\n",
      "training: 66 batch 200 batch_loss: 0.20944008231163025\n",
      "training: 66 batch 201 batch_loss: 0.20694580674171448\n",
      "training: 66 batch 202 batch_loss: 0.21387243270874023\n",
      "training: 66 batch 203 batch_loss: 0.20575612783432007\n",
      "training: 66 batch 204 batch_loss: 0.20916658639907837\n",
      "training: 66 batch 205 batch_loss: 0.20708486437797546\n",
      "training: 66 batch 206 batch_loss: 0.2082093060016632\n",
      "training: 66 batch 207 batch_loss: 0.20635446906089783\n",
      "training: 66 batch 208 batch_loss: 0.20948749780654907\n",
      "training: 66 batch 209 batch_loss: 0.205390065908432\n",
      "training: 66 batch 210 batch_loss: 0.2097131311893463\n",
      "training: 66 batch 211 batch_loss: 0.2127668857574463\n",
      "training: 66 batch 212 batch_loss: 0.20781931281089783\n",
      "training: 66 batch 213 batch_loss: 0.2071385383605957\n",
      "training: 66 batch 214 batch_loss: 0.20922911167144775\n",
      "training: 66 batch 215 batch_loss: 0.21081194281578064\n",
      "training: 66 batch 216 batch_loss: 0.20944449305534363\n",
      "training: 66 batch 217 batch_loss: 0.20459091663360596\n",
      "training: 66 batch 218 batch_loss: 0.20670661330223083\n",
      "training: 66 batch 219 batch_loss: 0.20618173480033875\n",
      "training: 66 batch 220 batch_loss: 0.20622029900550842\n",
      "training: 66 batch 221 batch_loss: 0.21214625239372253\n",
      "training: 66 batch 222 batch_loss: 0.21224024891853333\n",
      "training: 66 batch 223 batch_loss: 0.21399635076522827\n",
      "training: 66 batch 224 batch_loss: 0.21052539348602295\n",
      "training: 66 batch 225 batch_loss: 0.2109857201576233\n",
      "training: 66 batch 226 batch_loss: 0.20672693848609924\n",
      "training: 66 batch 227 batch_loss: 0.20579713582992554\n",
      "training: 66 batch 228 batch_loss: 0.21033936738967896\n",
      "training: 66 batch 229 batch_loss: 0.20905441045761108\n",
      "training: 66 batch 230 batch_loss: 0.20753008127212524\n",
      "training: 66 batch 231 batch_loss: 0.21019545197486877\n",
      "training: 66 batch 232 batch_loss: 0.20882850885391235\n",
      "training: 66 batch 233 batch_loss: 0.2101295292377472\n",
      "training: 66 batch 234 batch_loss: 0.21113154292106628\n",
      "training: 66 batch 235 batch_loss: 0.21408069133758545\n",
      "training: 66 batch 236 batch_loss: 0.21057194471359253\n",
      "training: 66 batch 237 batch_loss: 0.20976665616035461\n",
      "training: 66 batch 238 batch_loss: 0.2080250382423401\n",
      "training: 66 batch 239 batch_loss: 0.20983096957206726\n",
      "training: 66 batch 240 batch_loss: 0.20825716853141785\n",
      "training: 66 batch 241 batch_loss: 0.21204647421836853\n",
      "training: 66 batch 242 batch_loss: 0.20862028002738953\n",
      "training: 66 batch 243 batch_loss: 0.20825070142745972\n",
      "training: 66 batch 244 batch_loss: 0.2087094783782959\n",
      "training: 66 batch 245 batch_loss: 0.20831987261772156\n",
      "training: 66 batch 246 batch_loss: 0.20795491337776184\n",
      "training: 66 batch 247 batch_loss: 0.21090167760849\n",
      "training: 66 batch 248 batch_loss: 0.20662200450897217\n",
      "training: 66 batch 249 batch_loss: 0.20704203844070435\n",
      "training: 66 batch 250 batch_loss: 0.209687739610672\n",
      "training: 66 batch 251 batch_loss: 0.20720109343528748\n",
      "training: 66 batch 252 batch_loss: 0.20902597904205322\n",
      "training: 66 batch 253 batch_loss: 0.20961284637451172\n",
      "training: 66 batch 254 batch_loss: 0.21124482154846191\n",
      "training: 66 batch 255 batch_loss: 0.2099824845790863\n",
      "training: 66 batch 256 batch_loss: 0.20885571837425232\n",
      "training: 66 batch 257 batch_loss: 0.20732071995735168\n",
      "training: 66 batch 258 batch_loss: 0.210008442401886\n",
      "training: 66 batch 259 batch_loss: 0.2094351053237915\n",
      "training: 66 batch 260 batch_loss: 0.20686012506484985\n",
      "training: 66 batch 261 batch_loss: 0.20889979600906372\n",
      "training: 66 batch 262 batch_loss: 0.20806285738945007\n",
      "training: 66 batch 263 batch_loss: 0.207955002784729\n",
      "training: 66 batch 264 batch_loss: 0.20646250247955322\n",
      "training: 66 batch 265 batch_loss: 0.20833754539489746\n",
      "training: 66 batch 266 batch_loss: 0.2128075659275055\n",
      "training: 66 batch 267 batch_loss: 0.21318280696868896\n",
      "training: 66 batch 268 batch_loss: 0.20727556943893433\n",
      "training: 66 batch 269 batch_loss: 0.21154826879501343\n",
      "training: 66 batch 270 batch_loss: 0.20937097072601318\n",
      "training: 66 batch 271 batch_loss: 0.20806759595870972\n",
      "training: 66 batch 272 batch_loss: 0.21101433038711548\n",
      "training: 66 batch 273 batch_loss: 0.21029135584831238\n",
      "training: 66 batch 274 batch_loss: 0.20995715260505676\n",
      "training: 66 batch 275 batch_loss: 0.21121978759765625\n",
      "training: 66 batch 276 batch_loss: 0.20948076248168945\n",
      "training: 66 batch 277 batch_loss: 0.21243619918823242\n",
      "training: 66 batch 278 batch_loss: 0.21046975255012512\n",
      "training: 66 batch 279 batch_loss: 0.2125139832496643\n",
      "training: 66 batch 280 batch_loss: 0.21394851803779602\n",
      "training: 66 batch 281 batch_loss: 0.20866242051124573\n",
      "training: 66 batch 282 batch_loss: 0.20728015899658203\n",
      "training: 66 batch 283 batch_loss: 0.21175172924995422\n",
      "training: 66 batch 284 batch_loss: 0.21005374193191528\n",
      "training: 66 batch 285 batch_loss: 0.21251484751701355\n",
      "training: 66 batch 286 batch_loss: 0.20918703079223633\n",
      "training: 66 batch 287 batch_loss: 0.21018865704536438\n",
      "training: 66 batch 288 batch_loss: 0.21148645877838135\n",
      "training: 66 batch 289 batch_loss: 0.20992934703826904\n",
      "training: 66 batch 290 batch_loss: 0.20628094673156738\n",
      "training: 66 batch 291 batch_loss: 0.20348381996154785\n",
      "training: 66 batch 292 batch_loss: 0.2066420316696167\n",
      "training: 66 batch 293 batch_loss: 0.2078622281551361\n",
      "training: 66 batch 294 batch_loss: 0.20956528186798096\n",
      "training: 66 batch 295 batch_loss: 0.20750993490219116\n",
      "training: 66 batch 296 batch_loss: 0.20563507080078125\n",
      "training: 66 batch 297 batch_loss: 0.21298718452453613\n",
      "training: 66 batch 298 batch_loss: 0.21180373430252075\n",
      "training: 66 batch 299 batch_loss: 0.20818522572517395\n",
      "training: 66 batch 300 batch_loss: 0.2101062536239624\n",
      "training: 66 batch 301 batch_loss: 0.21027737855911255\n",
      "training: 66 batch 302 batch_loss: 0.20486050844192505\n",
      "training: 66 batch 303 batch_loss: 0.20818805694580078\n",
      "training: 66 batch 304 batch_loss: 0.2041291892528534\n",
      "training: 66 batch 305 batch_loss: 0.2130100429058075\n",
      "training: 66 batch 306 batch_loss: 0.21014639735221863\n",
      "training: 66 batch 307 batch_loss: 0.20734712481498718\n",
      "training: 66 batch 308 batch_loss: 0.20984631776809692\n",
      "training: 66 batch 309 batch_loss: 0.2068479061126709\n",
      "training: 66 batch 310 batch_loss: 0.21294358372688293\n",
      "training: 66 batch 311 batch_loss: 0.20541933178901672\n",
      "training: 66 batch 312 batch_loss: 0.21144616603851318\n",
      "training: 66 batch 313 batch_loss: 0.21127918362617493\n",
      "training: 66 batch 314 batch_loss: 0.20690160989761353\n",
      "training: 66 batch 315 batch_loss: 0.21397998929023743\n",
      "training: 66 batch 316 batch_loss: 0.20952919125556946\n",
      "training: 66 batch 317 batch_loss: 0.20969390869140625\n",
      "training: 66 batch 318 batch_loss: 0.20636212825775146\n",
      "training: 66 batch 319 batch_loss: 0.21232128143310547\n",
      "training: 66 batch 320 batch_loss: 0.206601083278656\n",
      "training: 66 batch 321 batch_loss: 0.21098816394805908\n",
      "training: 66 batch 322 batch_loss: 0.21158212423324585\n",
      "training: 66 batch 323 batch_loss: 0.21083253622055054\n",
      "training: 66 batch 324 batch_loss: 0.2158776819705963\n",
      "training: 66 batch 325 batch_loss: 0.21066465973854065\n",
      "training: 66 batch 326 batch_loss: 0.21134313941001892\n",
      "training: 66 batch 327 batch_loss: 0.21204549074172974\n",
      "training: 66 batch 328 batch_loss: 0.2079106867313385\n",
      "training: 66 batch 329 batch_loss: 0.20588207244873047\n",
      "training: 66 batch 330 batch_loss: 0.20423662662506104\n",
      "training: 66 batch 331 batch_loss: 0.20737653970718384\n",
      "training: 66 batch 332 batch_loss: 0.2079528272151947\n",
      "training: 66 batch 333 batch_loss: 0.210893452167511\n",
      "training: 66 batch 334 batch_loss: 0.2088797688484192\n",
      "training: 66 batch 335 batch_loss: 0.20758667588233948\n",
      "training: 66 batch 336 batch_loss: 0.20693320035934448\n",
      "training: 66 batch 337 batch_loss: 0.21065860986709595\n",
      "training: 66 batch 338 batch_loss: 0.20614978671073914\n",
      "training: 66 batch 339 batch_loss: 0.20588716864585876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 66 batch 340 batch_loss: 0.21146631240844727\n",
      "training: 66 batch 341 batch_loss: 0.21466240286827087\n",
      "training: 66 batch 342 batch_loss: 0.2100931704044342\n",
      "training: 66 batch 343 batch_loss: 0.21147215366363525\n",
      "training: 66 batch 344 batch_loss: 0.21396315097808838\n",
      "training: 66 batch 345 batch_loss: 0.2072232961654663\n",
      "training: 66 batch 346 batch_loss: 0.21207362413406372\n",
      "training: 66 batch 347 batch_loss: 0.21463102102279663\n",
      "training: 66 batch 348 batch_loss: 0.204012930393219\n",
      "training: 66 batch 349 batch_loss: 0.21031495928764343\n",
      "training: 66 batch 350 batch_loss: 0.20958787202835083\n",
      "training: 66 batch 351 batch_loss: 0.210458904504776\n",
      "training: 66 batch 352 batch_loss: 0.21125894784927368\n",
      "training: 66 batch 353 batch_loss: 0.20882850885391235\n",
      "training: 66 batch 354 batch_loss: 0.20888125896453857\n",
      "training: 66 batch 355 batch_loss: 0.21041110157966614\n",
      "training: 66 batch 356 batch_loss: 0.20900610089302063\n",
      "training: 66 batch 357 batch_loss: 0.21087509393692017\n",
      "training: 66 batch 358 batch_loss: 0.21164298057556152\n",
      "training: 66 batch 359 batch_loss: 0.2087213397026062\n",
      "training: 66 batch 360 batch_loss: 0.20967811346054077\n",
      "training: 66 batch 361 batch_loss: 0.2077377736568451\n",
      "training: 66 batch 362 batch_loss: 0.2118721604347229\n",
      "training: 66 batch 363 batch_loss: 0.20605453848838806\n",
      "training: 66 batch 364 batch_loss: 0.2107972800731659\n",
      "training: 66 batch 365 batch_loss: 0.210822194814682\n",
      "training: 66 batch 366 batch_loss: 0.20923486351966858\n",
      "training: 66 batch 367 batch_loss: 0.2071605622768402\n",
      "training: 66 batch 368 batch_loss: 0.20919835567474365\n",
      "training: 66 batch 369 batch_loss: 0.20944127440452576\n",
      "training: 66 batch 370 batch_loss: 0.20533627271652222\n",
      "training: 66 batch 371 batch_loss: 0.21471863985061646\n",
      "training: 66 batch 372 batch_loss: 0.20968756079673767\n",
      "training: 66 batch 373 batch_loss: 0.21355029940605164\n",
      "training: 66 batch 374 batch_loss: 0.2069772183895111\n",
      "training: 66 batch 375 batch_loss: 0.2091561257839203\n",
      "training: 66 batch 376 batch_loss: 0.2070510983467102\n",
      "training: 66 batch 377 batch_loss: 0.2150384783744812\n",
      "training: 66 batch 378 batch_loss: 0.2107783854007721\n",
      "training: 66 batch 379 batch_loss: 0.21367236971855164\n",
      "training: 66 batch 380 batch_loss: 0.21155965328216553\n",
      "training: 66 batch 381 batch_loss: 0.20491409301757812\n",
      "training: 66 batch 382 batch_loss: 0.2098643183708191\n",
      "training: 66 batch 383 batch_loss: 0.21025249361991882\n",
      "training: 66 batch 384 batch_loss: 0.21189448237419128\n",
      "training: 66 batch 385 batch_loss: 0.2117774486541748\n",
      "training: 66 batch 386 batch_loss: 0.21279215812683105\n",
      "training: 66 batch 387 batch_loss: 0.20449429750442505\n",
      "training: 66 batch 388 batch_loss: 0.2111113965511322\n",
      "training: 66 batch 389 batch_loss: 0.2134190797805786\n",
      "training: 66 batch 390 batch_loss: 0.21251899003982544\n",
      "training: 66 batch 391 batch_loss: 0.20599564909934998\n",
      "training: 66 batch 392 batch_loss: 0.2080288827419281\n",
      "training: 66 batch 393 batch_loss: 0.2099786102771759\n",
      "training: 66 batch 394 batch_loss: 0.2112247347831726\n",
      "training: 66 batch 395 batch_loss: 0.21112793684005737\n",
      "training: 66 batch 396 batch_loss: 0.21250838041305542\n",
      "training: 66 batch 397 batch_loss: 0.20952194929122925\n",
      "training: 66 batch 398 batch_loss: 0.21203535795211792\n",
      "training: 66 batch 399 batch_loss: 0.2129381000995636\n",
      "training: 66 batch 400 batch_loss: 0.20804890990257263\n",
      "training: 66 batch 401 batch_loss: 0.21209561824798584\n",
      "training: 66 batch 402 batch_loss: 0.20959249138832092\n",
      "training: 66 batch 403 batch_loss: 0.2090984284877777\n",
      "training: 66 batch 404 batch_loss: 0.21318158507347107\n",
      "training: 66 batch 405 batch_loss: 0.21105048060417175\n",
      "training: 66 batch 406 batch_loss: 0.20984646677970886\n",
      "training: 66 batch 407 batch_loss: 0.20901507139205933\n",
      "training: 66 batch 408 batch_loss: 0.20536267757415771\n",
      "training: 66 batch 409 batch_loss: 0.21230581402778625\n",
      "training: 66 batch 410 batch_loss: 0.21303418278694153\n",
      "training: 66 batch 411 batch_loss: 0.20983773469924927\n",
      "training: 66 batch 412 batch_loss: 0.2093028426170349\n",
      "training: 66 batch 413 batch_loss: 0.2083887755870819\n",
      "training: 66 batch 414 batch_loss: 0.21047282218933105\n",
      "training: 66 batch 415 batch_loss: 0.213998943567276\n",
      "training: 66 batch 416 batch_loss: 0.20913994312286377\n",
      "training: 66 batch 417 batch_loss: 0.21163126826286316\n",
      "training: 66 batch 418 batch_loss: 0.2109341025352478\n",
      "training: 66 batch 419 batch_loss: 0.21195554733276367\n",
      "training: 66 batch 420 batch_loss: 0.21590691804885864\n",
      "training: 66 batch 421 batch_loss: 0.20862028002738953\n",
      "training: 66 batch 422 batch_loss: 0.20755267143249512\n",
      "training: 66 batch 423 batch_loss: 0.2073332667350769\n",
      "training: 66 batch 424 batch_loss: 0.21115097403526306\n",
      "training: 66 batch 425 batch_loss: 0.21083614230155945\n",
      "training: 66 batch 426 batch_loss: 0.20784145593643188\n",
      "training: 66 batch 427 batch_loss: 0.20370078086853027\n",
      "training: 66 batch 428 batch_loss: 0.21454206109046936\n",
      "training: 66 batch 429 batch_loss: 0.20520490407943726\n",
      "training: 66 batch 430 batch_loss: 0.2117437720298767\n",
      "training: 66 batch 431 batch_loss: 0.21016636490821838\n",
      "training: 66 batch 432 batch_loss: 0.20711278915405273\n",
      "training: 66 batch 433 batch_loss: 0.20812493562698364\n",
      "training: 66 batch 434 batch_loss: 0.2110576629638672\n",
      "training: 66 batch 435 batch_loss: 0.210525780916214\n",
      "training: 66 batch 436 batch_loss: 0.20554709434509277\n",
      "training: 66 batch 437 batch_loss: 0.20878008008003235\n",
      "training: 66 batch 438 batch_loss: 0.2129528522491455\n",
      "training: 66 batch 439 batch_loss: 0.20820742845535278\n",
      "training: 66 batch 440 batch_loss: 0.21340718865394592\n",
      "training: 66 batch 441 batch_loss: 0.20924454927444458\n",
      "training: 66 batch 442 batch_loss: 0.2111189365386963\n",
      "training: 66 batch 443 batch_loss: 0.2092721164226532\n",
      "training: 66 batch 444 batch_loss: 0.21479853987693787\n",
      "training: 66 batch 445 batch_loss: 0.2103455662727356\n",
      "training: 66 batch 446 batch_loss: 0.21066883206367493\n",
      "training: 66 batch 447 batch_loss: 0.21266621351242065\n",
      "training: 66 batch 448 batch_loss: 0.21133625507354736\n",
      "training: 66 batch 449 batch_loss: 0.20936796069145203\n",
      "training: 66 batch 450 batch_loss: 0.2133275866508484\n",
      "training: 66 batch 451 batch_loss: 0.20895355939865112\n",
      "training: 66 batch 452 batch_loss: 0.20785796642303467\n",
      "training: 66 batch 453 batch_loss: 0.20773860812187195\n",
      "training: 66 batch 454 batch_loss: 0.20764976739883423\n",
      "training: 66 batch 455 batch_loss: 0.2107379138469696\n",
      "training: 66 batch 456 batch_loss: 0.20962107181549072\n",
      "training: 66 batch 457 batch_loss: 0.2137090563774109\n",
      "training: 66 batch 458 batch_loss: 0.21134382486343384\n",
      "training: 66 batch 459 batch_loss: 0.20860755443572998\n",
      "training: 66 batch 460 batch_loss: 0.21067124605178833\n",
      "training: 66 batch 461 batch_loss: 0.21103382110595703\n",
      "training: 66 batch 462 batch_loss: 0.21458929777145386\n",
      "training: 66 batch 463 batch_loss: 0.21644282341003418\n",
      "training: 66 batch 464 batch_loss: 0.21214324235916138\n",
      "training: 66 batch 465 batch_loss: 0.21099460124969482\n",
      "training: 66 batch 466 batch_loss: 0.21478521823883057\n",
      "training: 66 batch 467 batch_loss: 0.21252045035362244\n",
      "training: 66 batch 468 batch_loss: 0.20908230543136597\n",
      "training: 66 batch 469 batch_loss: 0.20939624309539795\n",
      "training: 66 batch 470 batch_loss: 0.20767360925674438\n",
      "training: 66 batch 471 batch_loss: 0.20864295959472656\n",
      "training: 66 batch 472 batch_loss: 0.2116401493549347\n",
      "training: 66 batch 473 batch_loss: 0.21351858973503113\n",
      "training: 66 batch 474 batch_loss: 0.2006397843360901\n",
      "training: 66 batch 475 batch_loss: 0.21215057373046875\n",
      "training: 66 batch 476 batch_loss: 0.21054089069366455\n",
      "training: 66 batch 477 batch_loss: 0.21243345737457275\n",
      "training: 66 batch 478 batch_loss: 0.21004748344421387\n",
      "training: 66 batch 479 batch_loss: 0.20523518323898315\n",
      "training: 66 batch 480 batch_loss: 0.2111305594444275\n",
      "training: 66 batch 481 batch_loss: 0.21107524633407593\n",
      "training: 66 batch 482 batch_loss: 0.20804786682128906\n",
      "training: 66 batch 483 batch_loss: 0.21024489402770996\n",
      "training: 66 batch 484 batch_loss: 0.21186894178390503\n",
      "training: 66 batch 485 batch_loss: 0.2128872275352478\n",
      "training: 66 batch 486 batch_loss: 0.21297329664230347\n",
      "training: 66 batch 487 batch_loss: 0.21304240822792053\n",
      "training: 66 batch 488 batch_loss: 0.20952075719833374\n",
      "training: 66 batch 489 batch_loss: 0.21794670820236206\n",
      "training: 66 batch 490 batch_loss: 0.20719927549362183\n",
      "training: 66 batch 491 batch_loss: 0.20930922031402588\n",
      "training: 66 batch 492 batch_loss: 0.2122313678264618\n",
      "training: 66 batch 493 batch_loss: 0.20769423246383667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 66 batch 494 batch_loss: 0.21201381087303162\n",
      "training: 66 batch 495 batch_loss: 0.21157610416412354\n",
      "training: 66 batch 496 batch_loss: 0.20992523431777954\n",
      "training: 66 batch 497 batch_loss: 0.21023845672607422\n",
      "training: 66 batch 498 batch_loss: 0.210222065448761\n",
      "training: 66 batch 499 batch_loss: 0.21086880564689636\n",
      "training: 66 batch 500 batch_loss: 0.21027159690856934\n",
      "training: 66 batch 501 batch_loss: 0.20834830403327942\n",
      "training: 66 batch 502 batch_loss: 0.21031251549720764\n",
      "training: 66 batch 503 batch_loss: 0.2106805145740509\n",
      "training: 66 batch 504 batch_loss: 0.21314853429794312\n",
      "training: 66 batch 505 batch_loss: 0.21084249019622803\n",
      "training: 66 batch 506 batch_loss: 0.21098625659942627\n",
      "training: 66 batch 507 batch_loss: 0.2057015597820282\n",
      "training: 66 batch 508 batch_loss: 0.20299243927001953\n",
      "training: 66 batch 509 batch_loss: 0.21418440341949463\n",
      "training: 66 batch 510 batch_loss: 0.21223971247673035\n",
      "training: 66 batch 511 batch_loss: 0.2116273045539856\n",
      "training: 66 batch 512 batch_loss: 0.21090945601463318\n",
      "training: 66 batch 513 batch_loss: 0.21073922514915466\n",
      "training: 66 batch 514 batch_loss: 0.20984160900115967\n",
      "training: 66 batch 515 batch_loss: 0.2108561396598816\n",
      "training: 66 batch 516 batch_loss: 0.21158620715141296\n",
      "training: 66 batch 517 batch_loss: 0.20861339569091797\n",
      "training: 66 batch 518 batch_loss: 0.21119970083236694\n",
      "training: 66 batch 519 batch_loss: 0.20995408296585083\n",
      "training: 66 batch 520 batch_loss: 0.20975461602210999\n",
      "training: 66 batch 521 batch_loss: 0.21339434385299683\n",
      "training: 66 batch 522 batch_loss: 0.20951688289642334\n",
      "training: 66 batch 523 batch_loss: 0.21253424882888794\n",
      "training: 66 batch 524 batch_loss: 0.2143983542919159\n",
      "training: 66 batch 525 batch_loss: 0.2086849808692932\n",
      "training: 66 batch 526 batch_loss: 0.21162092685699463\n",
      "training: 66 batch 527 batch_loss: 0.20806336402893066\n",
      "training: 66 batch 528 batch_loss: 0.2078443467617035\n",
      "training: 66 batch 529 batch_loss: 0.20930182933807373\n",
      "training: 66 batch 530 batch_loss: 0.21302491426467896\n",
      "training: 66 batch 531 batch_loss: 0.21158477663993835\n",
      "training: 66 batch 532 batch_loss: 0.20755523443222046\n",
      "training: 66 batch 533 batch_loss: 0.2085200548171997\n",
      "training: 66 batch 534 batch_loss: 0.2078193724155426\n",
      "training: 66 batch 535 batch_loss: 0.2130727469921112\n",
      "training: 66 batch 536 batch_loss: 0.20655682682991028\n",
      "training: 66 batch 537 batch_loss: 0.2072572112083435\n",
      "training: 66 batch 538 batch_loss: 0.2107168436050415\n",
      "training: 66 batch 539 batch_loss: 0.2109943926334381\n",
      "training: 66 batch 540 batch_loss: 0.20838481187820435\n",
      "training: 66 batch 541 batch_loss: 0.2114351987838745\n",
      "training: 66 batch 542 batch_loss: 0.21068480610847473\n",
      "training: 66 batch 543 batch_loss: 0.21283096075057983\n",
      "training: 66 batch 544 batch_loss: 0.2103179693222046\n",
      "training: 66 batch 545 batch_loss: 0.20712712407112122\n",
      "training: 66 batch 546 batch_loss: 0.20819205045700073\n",
      "training: 66 batch 547 batch_loss: 0.21182316541671753\n",
      "training: 66 batch 548 batch_loss: 0.20359587669372559\n",
      "training: 66 batch 549 batch_loss: 0.21115779876708984\n",
      "training: 66 batch 550 batch_loss: 0.2111089825630188\n",
      "training: 66 batch 551 batch_loss: 0.21028268337249756\n",
      "training: 66 batch 552 batch_loss: 0.21028411388397217\n",
      "training: 66 batch 553 batch_loss: 0.210274338722229\n",
      "training: 66 batch 554 batch_loss: 0.2122558057308197\n",
      "training: 66 batch 555 batch_loss: 0.21491685509681702\n",
      "training: 66 batch 556 batch_loss: 0.20869672298431396\n",
      "training: 66 batch 557 batch_loss: 0.2124396562576294\n",
      "training: 66 batch 558 batch_loss: 0.21648156642913818\n",
      "training: 66 batch 559 batch_loss: 0.2110322117805481\n",
      "training: 66 batch 560 batch_loss: 0.21152612566947937\n",
      "training: 66 batch 561 batch_loss: 0.2139090895652771\n",
      "training: 66 batch 562 batch_loss: 0.2082885205745697\n",
      "training: 66 batch 563 batch_loss: 0.208651602268219\n",
      "training: 66 batch 564 batch_loss: 0.21135136485099792\n",
      "training: 66 batch 565 batch_loss: 0.21400609612464905\n",
      "training: 66 batch 566 batch_loss: 0.2113950550556183\n",
      "training: 66 batch 567 batch_loss: 0.21030715107917786\n",
      "training: 66 batch 568 batch_loss: 0.2048693299293518\n",
      "training: 66 batch 569 batch_loss: 0.20745044946670532\n",
      "training: 66 batch 570 batch_loss: 0.20747560262680054\n",
      "training: 66 batch 571 batch_loss: 0.20998525619506836\n",
      "training: 66 batch 572 batch_loss: 0.20869293808937073\n",
      "training: 66 batch 573 batch_loss: 0.21034744381904602\n",
      "training: 66 batch 574 batch_loss: 0.2134113609790802\n",
      "training: 66 batch 575 batch_loss: 0.21108239889144897\n",
      "training: 66 batch 576 batch_loss: 0.2122008502483368\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 66, Hit Ratio:0.03323195410952287 | Precision:0.049031750712670796 | Recall:0.06516047003545757 | NDCG:0.063864949692942\n",
      "*Best Performance* \n",
      "Epoch: 61, Hit Ratio:0.033261934821931224 | Precision:0.04907598545168584 | Recall:0.06479544314235144 | MDCG:0.06348668156373174\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 67 batch 0 batch_loss: 0.2093941569328308\n",
      "training: 67 batch 1 batch_loss: 0.21064510941505432\n",
      "training: 67 batch 2 batch_loss: 0.2097359299659729\n",
      "training: 67 batch 3 batch_loss: 0.20414653420448303\n",
      "training: 67 batch 4 batch_loss: 0.20759165287017822\n",
      "training: 67 batch 5 batch_loss: 0.20380479097366333\n",
      "training: 67 batch 6 batch_loss: 0.2057112455368042\n",
      "training: 67 batch 7 batch_loss: 0.2120058536529541\n",
      "training: 67 batch 8 batch_loss: 0.20543181896209717\n",
      "training: 67 batch 9 batch_loss: 0.21094435453414917\n",
      "training: 67 batch 10 batch_loss: 0.20649677515029907\n",
      "training: 67 batch 11 batch_loss: 0.20831483602523804\n",
      "training: 67 batch 12 batch_loss: 0.20849978923797607\n",
      "training: 67 batch 13 batch_loss: 0.20989227294921875\n",
      "training: 67 batch 14 batch_loss: 0.2094159722328186\n",
      "training: 67 batch 15 batch_loss: 0.2085198163986206\n",
      "training: 67 batch 16 batch_loss: 0.20716363191604614\n",
      "training: 67 batch 17 batch_loss: 0.21131697297096252\n",
      "training: 67 batch 18 batch_loss: 0.20927953720092773\n",
      "training: 67 batch 19 batch_loss: 0.21003103256225586\n",
      "training: 67 batch 20 batch_loss: 0.2091549038887024\n",
      "training: 67 batch 21 batch_loss: 0.21216243505477905\n",
      "training: 67 batch 22 batch_loss: 0.20883339643478394\n",
      "training: 67 batch 23 batch_loss: 0.2038615643978119\n",
      "training: 67 batch 24 batch_loss: 0.20752227306365967\n",
      "training: 67 batch 25 batch_loss: 0.2087106704711914\n",
      "training: 67 batch 26 batch_loss: 0.21125522255897522\n",
      "training: 67 batch 27 batch_loss: 0.20602980256080627\n",
      "training: 67 batch 28 batch_loss: 0.20478644967079163\n",
      "training: 67 batch 29 batch_loss: 0.2075306475162506\n",
      "training: 67 batch 30 batch_loss: 0.21523034572601318\n",
      "training: 67 batch 31 batch_loss: 0.20785197615623474\n",
      "training: 67 batch 32 batch_loss: 0.20753297209739685\n",
      "training: 67 batch 33 batch_loss: 0.20798039436340332\n",
      "training: 67 batch 34 batch_loss: 0.21144500374794006\n",
      "training: 67 batch 35 batch_loss: 0.2083427608013153\n",
      "training: 67 batch 36 batch_loss: 0.21151235699653625\n",
      "training: 67 batch 37 batch_loss: 0.20960652828216553\n",
      "training: 67 batch 38 batch_loss: 0.21064496040344238\n",
      "training: 67 batch 39 batch_loss: 0.20643386244773865\n",
      "training: 67 batch 40 batch_loss: 0.2060704529285431\n",
      "training: 67 batch 41 batch_loss: 0.21233251690864563\n",
      "training: 67 batch 42 batch_loss: 0.2106073498725891\n",
      "training: 67 batch 43 batch_loss: 0.2082727551460266\n",
      "training: 67 batch 44 batch_loss: 0.20767605304718018\n",
      "training: 67 batch 45 batch_loss: 0.2121366262435913\n",
      "training: 67 batch 46 batch_loss: 0.20719465613365173\n",
      "training: 67 batch 47 batch_loss: 0.2109350562095642\n",
      "training: 67 batch 48 batch_loss: 0.20745792984962463\n",
      "training: 67 batch 49 batch_loss: 0.2099229395389557\n",
      "training: 67 batch 50 batch_loss: 0.20947155356407166\n",
      "training: 67 batch 51 batch_loss: 0.20897245407104492\n",
      "training: 67 batch 52 batch_loss: 0.2096826136112213\n",
      "training: 67 batch 53 batch_loss: 0.21239840984344482\n",
      "training: 67 batch 54 batch_loss: 0.20799922943115234\n",
      "training: 67 batch 55 batch_loss: 0.21523350477218628\n",
      "training: 67 batch 56 batch_loss: 0.20825794339179993\n",
      "training: 67 batch 57 batch_loss: 0.20622646808624268\n",
      "training: 67 batch 58 batch_loss: 0.21179047226905823\n",
      "training: 67 batch 59 batch_loss: 0.21168074011802673\n",
      "training: 67 batch 60 batch_loss: 0.2063569724559784\n",
      "training: 67 batch 61 batch_loss: 0.21145355701446533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 67 batch 62 batch_loss: 0.2090989053249359\n",
      "training: 67 batch 63 batch_loss: 0.2087312638759613\n",
      "training: 67 batch 64 batch_loss: 0.2109421193599701\n",
      "training: 67 batch 65 batch_loss: 0.21148210763931274\n",
      "training: 67 batch 66 batch_loss: 0.21148979663848877\n",
      "training: 67 batch 67 batch_loss: 0.20861166715621948\n",
      "training: 67 batch 68 batch_loss: 0.21054640412330627\n",
      "training: 67 batch 69 batch_loss: 0.20527306199073792\n",
      "training: 67 batch 70 batch_loss: 0.20833104848861694\n",
      "training: 67 batch 71 batch_loss: 0.20971879363059998\n",
      "training: 67 batch 72 batch_loss: 0.20944267511367798\n",
      "training: 67 batch 73 batch_loss: 0.20521587133407593\n",
      "training: 67 batch 74 batch_loss: 0.2124798595905304\n",
      "training: 67 batch 75 batch_loss: 0.2083074450492859\n",
      "training: 67 batch 76 batch_loss: 0.20731839537620544\n",
      "training: 67 batch 77 batch_loss: 0.20853614807128906\n",
      "training: 67 batch 78 batch_loss: 0.20833221077919006\n",
      "training: 67 batch 79 batch_loss: 0.2059536576271057\n",
      "training: 67 batch 80 batch_loss: 0.21006140112876892\n",
      "training: 67 batch 81 batch_loss: 0.20627069473266602\n",
      "training: 67 batch 82 batch_loss: 0.20536288619041443\n",
      "training: 67 batch 83 batch_loss: 0.20364883542060852\n",
      "training: 67 batch 84 batch_loss: 0.20483633875846863\n",
      "training: 67 batch 85 batch_loss: 0.20911896228790283\n",
      "training: 67 batch 86 batch_loss: 0.2077760100364685\n",
      "training: 67 batch 87 batch_loss: 0.20895633101463318\n",
      "training: 67 batch 88 batch_loss: 0.2074868083000183\n",
      "training: 67 batch 89 batch_loss: 0.20975849032402039\n",
      "training: 67 batch 90 batch_loss: 0.21100634336471558\n",
      "training: 67 batch 91 batch_loss: 0.21287518739700317\n",
      "training: 67 batch 92 batch_loss: 0.21408039331436157\n",
      "training: 67 batch 93 batch_loss: 0.204263836145401\n",
      "training: 67 batch 94 batch_loss: 0.212301105260849\n",
      "training: 67 batch 95 batch_loss: 0.20885759592056274\n",
      "training: 67 batch 96 batch_loss: 0.21049174666404724\n",
      "training: 67 batch 97 batch_loss: 0.20981431007385254\n",
      "training: 67 batch 98 batch_loss: 0.21388667821884155\n",
      "training: 67 batch 99 batch_loss: 0.20680752396583557\n",
      "training: 67 batch 100 batch_loss: 0.20935848355293274\n",
      "training: 67 batch 101 batch_loss: 0.21046388149261475\n",
      "training: 67 batch 102 batch_loss: 0.211189866065979\n",
      "training: 67 batch 103 batch_loss: 0.2097017467021942\n",
      "training: 67 batch 104 batch_loss: 0.2082653045654297\n",
      "training: 67 batch 105 batch_loss: 0.21435365080833435\n",
      "training: 67 batch 106 batch_loss: 0.21166211366653442\n",
      "training: 67 batch 107 batch_loss: 0.20937037467956543\n",
      "training: 67 batch 108 batch_loss: 0.20987474918365479\n",
      "training: 67 batch 109 batch_loss: 0.20957162976264954\n",
      "training: 67 batch 110 batch_loss: 0.21236175298690796\n",
      "training: 67 batch 111 batch_loss: 0.20850923657417297\n",
      "training: 67 batch 112 batch_loss: 0.2067740559577942\n",
      "training: 67 batch 113 batch_loss: 0.21426081657409668\n",
      "training: 67 batch 114 batch_loss: 0.20282721519470215\n",
      "training: 67 batch 115 batch_loss: 0.20996826887130737\n",
      "training: 67 batch 116 batch_loss: 0.20756733417510986\n",
      "training: 67 batch 117 batch_loss: 0.2103087306022644\n",
      "training: 67 batch 118 batch_loss: 0.20762020349502563\n",
      "training: 67 batch 119 batch_loss: 0.2102210521697998\n",
      "training: 67 batch 120 batch_loss: 0.20977196097373962\n",
      "training: 67 batch 121 batch_loss: 0.20932745933532715\n",
      "training: 67 batch 122 batch_loss: 0.20963338017463684\n",
      "training: 67 batch 123 batch_loss: 0.20735210180282593\n",
      "training: 67 batch 124 batch_loss: 0.20744842290878296\n",
      "training: 67 batch 125 batch_loss: 0.21166670322418213\n",
      "training: 67 batch 126 batch_loss: 0.21425318717956543\n",
      "training: 67 batch 127 batch_loss: 0.20717310905456543\n",
      "training: 67 batch 128 batch_loss: 0.21085524559020996\n",
      "training: 67 batch 129 batch_loss: 0.21177861094474792\n",
      "training: 67 batch 130 batch_loss: 0.20833048224449158\n",
      "training: 67 batch 131 batch_loss: 0.20897606015205383\n",
      "training: 67 batch 132 batch_loss: 0.21150749921798706\n",
      "training: 67 batch 133 batch_loss: 0.20741510391235352\n",
      "training: 67 batch 134 batch_loss: 0.21124190092086792\n",
      "training: 67 batch 135 batch_loss: 0.20925265550613403\n",
      "training: 67 batch 136 batch_loss: 0.2110198736190796\n",
      "training: 67 batch 137 batch_loss: 0.20753967761993408\n",
      "training: 67 batch 138 batch_loss: 0.2116979956626892\n",
      "training: 67 batch 139 batch_loss: 0.21126186847686768\n",
      "training: 67 batch 140 batch_loss: 0.20999813079833984\n",
      "training: 67 batch 141 batch_loss: 0.20953232049942017\n",
      "training: 67 batch 142 batch_loss: 0.20991671085357666\n",
      "training: 67 batch 143 batch_loss: 0.2067107856273651\n",
      "training: 67 batch 144 batch_loss: 0.21072545647621155\n",
      "training: 67 batch 145 batch_loss: 0.20790034532546997\n",
      "training: 67 batch 146 batch_loss: 0.21093735098838806\n",
      "training: 67 batch 147 batch_loss: 0.20718646049499512\n",
      "training: 67 batch 148 batch_loss: 0.21231317520141602\n",
      "training: 67 batch 149 batch_loss: 0.20910918712615967\n",
      "training: 67 batch 150 batch_loss: 0.20974844694137573\n",
      "training: 67 batch 151 batch_loss: 0.21342021226882935\n",
      "training: 67 batch 152 batch_loss: 0.2063939869403839\n",
      "training: 67 batch 153 batch_loss: 0.20787203311920166\n",
      "training: 67 batch 154 batch_loss: 0.20649516582489014\n",
      "training: 67 batch 155 batch_loss: 0.20728960633277893\n",
      "training: 67 batch 156 batch_loss: 0.2120867669582367\n",
      "training: 67 batch 157 batch_loss: 0.21457329392433167\n",
      "training: 67 batch 158 batch_loss: 0.21378585696220398\n",
      "training: 67 batch 159 batch_loss: 0.20724743604660034\n",
      "training: 67 batch 160 batch_loss: 0.21175193786621094\n",
      "training: 67 batch 161 batch_loss: 0.2067214548587799\n",
      "training: 67 batch 162 batch_loss: 0.21113020181655884\n",
      "training: 67 batch 163 batch_loss: 0.20961129665374756\n",
      "training: 67 batch 164 batch_loss: 0.20874574780464172\n",
      "training: 67 batch 165 batch_loss: 0.20658168196678162\n",
      "training: 67 batch 166 batch_loss: 0.2060019075870514\n",
      "training: 67 batch 167 batch_loss: 0.20697945356369019\n",
      "training: 67 batch 168 batch_loss: 0.2104109823703766\n",
      "training: 67 batch 169 batch_loss: 0.2054644227027893\n",
      "training: 67 batch 170 batch_loss: 0.21309775114059448\n",
      "training: 67 batch 171 batch_loss: 0.21079415082931519\n",
      "training: 67 batch 172 batch_loss: 0.2083982527256012\n",
      "training: 67 batch 173 batch_loss: 0.20684698224067688\n",
      "training: 67 batch 174 batch_loss: 0.20880302786827087\n",
      "training: 67 batch 175 batch_loss: 0.2120865285396576\n",
      "training: 67 batch 176 batch_loss: 0.20905569195747375\n",
      "training: 67 batch 177 batch_loss: 0.2061867117881775\n",
      "training: 67 batch 178 batch_loss: 0.2091141641139984\n",
      "training: 67 batch 179 batch_loss: 0.2059268355369568\n",
      "training: 67 batch 180 batch_loss: 0.2084742784500122\n",
      "training: 67 batch 181 batch_loss: 0.21492144465446472\n",
      "training: 67 batch 182 batch_loss: 0.20840716361999512\n",
      "training: 67 batch 183 batch_loss: 0.2125747799873352\n",
      "training: 67 batch 184 batch_loss: 0.2107946276664734\n",
      "training: 67 batch 185 batch_loss: 0.2096373736858368\n",
      "training: 67 batch 186 batch_loss: 0.20988169312477112\n",
      "training: 67 batch 187 batch_loss: 0.21274521946907043\n",
      "training: 67 batch 188 batch_loss: 0.20795989036560059\n",
      "training: 67 batch 189 batch_loss: 0.2111804485321045\n",
      "training: 67 batch 190 batch_loss: 0.21388211846351624\n",
      "training: 67 batch 191 batch_loss: 0.208901047706604\n",
      "training: 67 batch 192 batch_loss: 0.20764005184173584\n",
      "training: 67 batch 193 batch_loss: 0.209833562374115\n",
      "training: 67 batch 194 batch_loss: 0.2099725902080536\n",
      "training: 67 batch 195 batch_loss: 0.2134048044681549\n",
      "training: 67 batch 196 batch_loss: 0.20890194177627563\n",
      "training: 67 batch 197 batch_loss: 0.20924517512321472\n",
      "training: 67 batch 198 batch_loss: 0.21065756678581238\n",
      "training: 67 batch 199 batch_loss: 0.2147250473499298\n",
      "training: 67 batch 200 batch_loss: 0.20896807312965393\n",
      "training: 67 batch 201 batch_loss: 0.20490390062332153\n",
      "training: 67 batch 202 batch_loss: 0.20636528730392456\n",
      "training: 67 batch 203 batch_loss: 0.21151989698410034\n",
      "training: 67 batch 204 batch_loss: 0.2078961730003357\n",
      "training: 67 batch 205 batch_loss: 0.21076899766921997\n",
      "training: 67 batch 206 batch_loss: 0.2101587951183319\n",
      "training: 67 batch 207 batch_loss: 0.2112511396408081\n",
      "training: 67 batch 208 batch_loss: 0.21361690759658813\n",
      "training: 67 batch 209 batch_loss: 0.21107792854309082\n",
      "training: 67 batch 210 batch_loss: 0.210860937833786\n",
      "training: 67 batch 211 batch_loss: 0.20981788635253906\n",
      "training: 67 batch 212 batch_loss: 0.20818370580673218\n",
      "training: 67 batch 213 batch_loss: 0.2140764594078064\n",
      "training: 67 batch 214 batch_loss: 0.20860600471496582\n",
      "training: 67 batch 215 batch_loss: 0.21038347482681274\n",
      "training: 67 batch 216 batch_loss: 0.20688381791114807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 67 batch 217 batch_loss: 0.20957505702972412\n",
      "training: 67 batch 218 batch_loss: 0.20728656649589539\n",
      "training: 67 batch 219 batch_loss: 0.210575670003891\n",
      "training: 67 batch 220 batch_loss: 0.2103227972984314\n",
      "training: 67 batch 221 batch_loss: 0.21240001916885376\n",
      "training: 67 batch 222 batch_loss: 0.211156964302063\n",
      "training: 67 batch 223 batch_loss: 0.2080320119857788\n",
      "training: 67 batch 224 batch_loss: 0.21064740419387817\n",
      "training: 67 batch 225 batch_loss: 0.21052351593971252\n",
      "training: 67 batch 226 batch_loss: 0.21282890439033508\n",
      "training: 67 batch 227 batch_loss: 0.208856463432312\n",
      "training: 67 batch 228 batch_loss: 0.21042287349700928\n",
      "training: 67 batch 229 batch_loss: 0.2077871561050415\n",
      "training: 67 batch 230 batch_loss: 0.21186649799346924\n",
      "training: 67 batch 231 batch_loss: 0.20851227641105652\n",
      "training: 67 batch 232 batch_loss: 0.20660516619682312\n",
      "training: 67 batch 233 batch_loss: 0.21201318502426147\n",
      "training: 67 batch 234 batch_loss: 0.21112483739852905\n",
      "training: 67 batch 235 batch_loss: 0.21083861589431763\n",
      "training: 67 batch 236 batch_loss: 0.21351873874664307\n",
      "training: 67 batch 237 batch_loss: 0.2113495171070099\n",
      "training: 67 batch 238 batch_loss: 0.20798102021217346\n",
      "training: 67 batch 239 batch_loss: 0.21197015047073364\n",
      "training: 67 batch 240 batch_loss: 0.20853835344314575\n",
      "training: 67 batch 241 batch_loss: 0.211137056350708\n",
      "training: 67 batch 242 batch_loss: 0.2073971927165985\n",
      "training: 67 batch 243 batch_loss: 0.20905113220214844\n",
      "training: 67 batch 244 batch_loss: 0.21493655443191528\n",
      "training: 67 batch 245 batch_loss: 0.20975083112716675\n",
      "training: 67 batch 246 batch_loss: 0.20526421070098877\n",
      "training: 67 batch 247 batch_loss: 0.21544578671455383\n",
      "training: 67 batch 248 batch_loss: 0.20872384309768677\n",
      "training: 67 batch 249 batch_loss: 0.20901045203208923\n",
      "training: 67 batch 250 batch_loss: 0.2138219177722931\n",
      "training: 67 batch 251 batch_loss: 0.21024483442306519\n",
      "training: 67 batch 252 batch_loss: 0.21110963821411133\n",
      "training: 67 batch 253 batch_loss: 0.2083163857460022\n",
      "training: 67 batch 254 batch_loss: 0.20765194296836853\n",
      "training: 67 batch 255 batch_loss: 0.20311817526817322\n",
      "training: 67 batch 256 batch_loss: 0.20682454109191895\n",
      "training: 67 batch 257 batch_loss: 0.21341624855995178\n",
      "training: 67 batch 258 batch_loss: 0.2127026617527008\n",
      "training: 67 batch 259 batch_loss: 0.21077582240104675\n",
      "training: 67 batch 260 batch_loss: 0.21294498443603516\n",
      "training: 67 batch 261 batch_loss: 0.21014726161956787\n",
      "training: 67 batch 262 batch_loss: 0.20778343081474304\n",
      "training: 67 batch 263 batch_loss: 0.21087482571601868\n",
      "training: 67 batch 264 batch_loss: 0.20494931936264038\n",
      "training: 67 batch 265 batch_loss: 0.20925980806350708\n",
      "training: 67 batch 266 batch_loss: 0.21084824204444885\n",
      "training: 67 batch 267 batch_loss: 0.21045085787773132\n",
      "training: 67 batch 268 batch_loss: 0.20669704675674438\n",
      "training: 67 batch 269 batch_loss: 0.20661014318466187\n",
      "training: 67 batch 270 batch_loss: 0.2094390094280243\n",
      "training: 67 batch 271 batch_loss: 0.21062666177749634\n",
      "training: 67 batch 272 batch_loss: 0.21047499775886536\n",
      "training: 67 batch 273 batch_loss: 0.20955201983451843\n",
      "training: 67 batch 274 batch_loss: 0.20889359712600708\n",
      "training: 67 batch 275 batch_loss: 0.2084147334098816\n",
      "training: 67 batch 276 batch_loss: 0.211003839969635\n",
      "training: 67 batch 277 batch_loss: 0.21153292059898376\n",
      "training: 67 batch 278 batch_loss: 0.20949220657348633\n",
      "training: 67 batch 279 batch_loss: 0.20892027020454407\n",
      "training: 67 batch 280 batch_loss: 0.21288087964057922\n",
      "training: 67 batch 281 batch_loss: 0.21295014023780823\n",
      "training: 67 batch 282 batch_loss: 0.210911363363266\n",
      "training: 67 batch 283 batch_loss: 0.2082417905330658\n",
      "training: 67 batch 284 batch_loss: 0.21216684579849243\n",
      "training: 67 batch 285 batch_loss: 0.2089056372642517\n",
      "training: 67 batch 286 batch_loss: 0.2079504430294037\n",
      "training: 67 batch 287 batch_loss: 0.2096535563468933\n",
      "training: 67 batch 288 batch_loss: 0.21231567859649658\n",
      "training: 67 batch 289 batch_loss: 0.20899486541748047\n",
      "training: 67 batch 290 batch_loss: 0.2054702639579773\n",
      "training: 67 batch 291 batch_loss: 0.2061263918876648\n",
      "training: 67 batch 292 batch_loss: 0.20764371752738953\n",
      "training: 67 batch 293 batch_loss: 0.2154834270477295\n",
      "training: 67 batch 294 batch_loss: 0.20813924074172974\n",
      "training: 67 batch 295 batch_loss: 0.21035867929458618\n",
      "training: 67 batch 296 batch_loss: 0.21041420102119446\n",
      "training: 67 batch 297 batch_loss: 0.21297550201416016\n",
      "training: 67 batch 298 batch_loss: 0.2086414098739624\n",
      "training: 67 batch 299 batch_loss: 0.2107030153274536\n",
      "training: 67 batch 300 batch_loss: 0.21346276998519897\n",
      "training: 67 batch 301 batch_loss: 0.21340340375900269\n",
      "training: 67 batch 302 batch_loss: 0.21052128076553345\n",
      "training: 67 batch 303 batch_loss: 0.21322345733642578\n",
      "training: 67 batch 304 batch_loss: 0.21571922302246094\n",
      "training: 67 batch 305 batch_loss: 0.2093074917793274\n",
      "training: 67 batch 306 batch_loss: 0.2116696834564209\n",
      "training: 67 batch 307 batch_loss: 0.21068811416625977\n",
      "training: 67 batch 308 batch_loss: 0.2116713523864746\n",
      "training: 67 batch 309 batch_loss: 0.2130441665649414\n",
      "training: 67 batch 310 batch_loss: 0.20654615759849548\n",
      "training: 67 batch 311 batch_loss: 0.21145737171173096\n",
      "training: 67 batch 312 batch_loss: 0.21027129888534546\n",
      "training: 67 batch 313 batch_loss: 0.2097645401954651\n",
      "training: 67 batch 314 batch_loss: 0.21253830194473267\n",
      "training: 67 batch 315 batch_loss: 0.21155276894569397\n",
      "training: 67 batch 316 batch_loss: 0.21297568082809448\n",
      "training: 67 batch 317 batch_loss: 0.20812973380088806\n",
      "training: 67 batch 318 batch_loss: 0.20954987406730652\n",
      "training: 67 batch 319 batch_loss: 0.21093636751174927\n",
      "training: 67 batch 320 batch_loss: 0.2105714976787567\n",
      "training: 67 batch 321 batch_loss: 0.2100440263748169\n",
      "training: 67 batch 322 batch_loss: 0.20917052030563354\n",
      "training: 67 batch 323 batch_loss: 0.21314886212348938\n",
      "training: 67 batch 324 batch_loss: 0.2138940393924713\n",
      "training: 67 batch 325 batch_loss: 0.21258598566055298\n",
      "training: 67 batch 326 batch_loss: 0.2144964635372162\n",
      "training: 67 batch 327 batch_loss: 0.2141750156879425\n",
      "training: 67 batch 328 batch_loss: 0.20751255750656128\n",
      "training: 67 batch 329 batch_loss: 0.21029961109161377\n",
      "training: 67 batch 330 batch_loss: 0.21081331372261047\n",
      "training: 67 batch 331 batch_loss: 0.20744580030441284\n",
      "training: 67 batch 332 batch_loss: 0.20971804857254028\n",
      "training: 67 batch 333 batch_loss: 0.20676392316818237\n",
      "training: 67 batch 334 batch_loss: 0.20766806602478027\n",
      "training: 67 batch 335 batch_loss: 0.21180015802383423\n",
      "training: 67 batch 336 batch_loss: 0.20978069305419922\n",
      "training: 67 batch 337 batch_loss: 0.2093660831451416\n",
      "training: 67 batch 338 batch_loss: 0.21039515733718872\n",
      "training: 67 batch 339 batch_loss: 0.21330219507217407\n",
      "training: 67 batch 340 batch_loss: 0.21170955896377563\n",
      "training: 67 batch 341 batch_loss: 0.2140577733516693\n",
      "training: 67 batch 342 batch_loss: 0.20864349603652954\n",
      "training: 67 batch 343 batch_loss: 0.21252340078353882\n",
      "training: 67 batch 344 batch_loss: 0.21333876252174377\n",
      "training: 67 batch 345 batch_loss: 0.20787549018859863\n",
      "training: 67 batch 346 batch_loss: 0.21108120679855347\n",
      "training: 67 batch 347 batch_loss: 0.20816272497177124\n",
      "training: 67 batch 348 batch_loss: 0.20807421207427979\n",
      "training: 67 batch 349 batch_loss: 0.20898112654685974\n",
      "training: 67 batch 350 batch_loss: 0.21208810806274414\n",
      "training: 67 batch 351 batch_loss: 0.20951995253562927\n",
      "training: 67 batch 352 batch_loss: 0.2054547667503357\n",
      "training: 67 batch 353 batch_loss: 0.21113380789756775\n",
      "training: 67 batch 354 batch_loss: 0.20667016506195068\n",
      "training: 67 batch 355 batch_loss: 0.20936015248298645\n",
      "training: 67 batch 356 batch_loss: 0.20742356777191162\n",
      "training: 67 batch 357 batch_loss: 0.2108357846736908\n",
      "training: 67 batch 358 batch_loss: 0.20841050148010254\n",
      "training: 67 batch 359 batch_loss: 0.2127600610256195\n",
      "training: 67 batch 360 batch_loss: 0.21159282326698303\n",
      "training: 67 batch 361 batch_loss: 0.21102848649024963\n",
      "training: 67 batch 362 batch_loss: 0.21544119715690613\n",
      "training: 67 batch 363 batch_loss: 0.21098309755325317\n",
      "training: 67 batch 364 batch_loss: 0.21450167894363403\n",
      "training: 67 batch 365 batch_loss: 0.205814391374588\n",
      "training: 67 batch 366 batch_loss: 0.20977291464805603\n",
      "training: 67 batch 367 batch_loss: 0.21062535047531128\n",
      "training: 67 batch 368 batch_loss: 0.21146363019943237\n",
      "training: 67 batch 369 batch_loss: 0.21250736713409424\n",
      "training: 67 batch 370 batch_loss: 0.20786148309707642\n",
      "training: 67 batch 371 batch_loss: 0.21334880590438843\n",
      "training: 67 batch 372 batch_loss: 0.2131553590297699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 67 batch 373 batch_loss: 0.21003544330596924\n",
      "training: 67 batch 374 batch_loss: 0.20958948135375977\n",
      "training: 67 batch 375 batch_loss: 0.21166470646858215\n",
      "training: 67 batch 376 batch_loss: 0.2135390043258667\n",
      "training: 67 batch 377 batch_loss: 0.21071729063987732\n",
      "training: 67 batch 378 batch_loss: 0.2084820568561554\n",
      "training: 67 batch 379 batch_loss: 0.21359512209892273\n",
      "training: 67 batch 380 batch_loss: 0.20993104577064514\n",
      "training: 67 batch 381 batch_loss: 0.20883482694625854\n",
      "training: 67 batch 382 batch_loss: 0.20597326755523682\n",
      "training: 67 batch 383 batch_loss: 0.2146962583065033\n",
      "training: 67 batch 384 batch_loss: 0.21238771080970764\n",
      "training: 67 batch 385 batch_loss: 0.21282804012298584\n",
      "training: 67 batch 386 batch_loss: 0.21250653266906738\n",
      "training: 67 batch 387 batch_loss: 0.209967702627182\n",
      "training: 67 batch 388 batch_loss: 0.2103779911994934\n",
      "training: 67 batch 389 batch_loss: 0.2065359354019165\n",
      "training: 67 batch 390 batch_loss: 0.21036413311958313\n",
      "training: 67 batch 391 batch_loss: 0.21155041456222534\n",
      "training: 67 batch 392 batch_loss: 0.21393081545829773\n",
      "training: 67 batch 393 batch_loss: 0.21137654781341553\n",
      "training: 67 batch 394 batch_loss: 0.2096118927001953\n",
      "training: 67 batch 395 batch_loss: 0.21304824948310852\n",
      "training: 67 batch 396 batch_loss: 0.2129397988319397\n",
      "training: 67 batch 397 batch_loss: 0.21067321300506592\n",
      "training: 67 batch 398 batch_loss: 0.21013084053993225\n",
      "training: 67 batch 399 batch_loss: 0.21180656552314758\n",
      "training: 67 batch 400 batch_loss: 0.21271589398384094\n",
      "training: 67 batch 401 batch_loss: 0.2063092589378357\n",
      "training: 67 batch 402 batch_loss: 0.21269962191581726\n",
      "training: 67 batch 403 batch_loss: 0.20823270082473755\n",
      "training: 67 batch 404 batch_loss: 0.2092302441596985\n",
      "training: 67 batch 405 batch_loss: 0.2114267349243164\n",
      "training: 67 batch 406 batch_loss: 0.20525947213172913\n",
      "training: 67 batch 407 batch_loss: 0.20842352509498596\n",
      "training: 67 batch 408 batch_loss: 0.20992770791053772\n",
      "training: 67 batch 409 batch_loss: 0.20873332023620605\n",
      "training: 67 batch 410 batch_loss: 0.20874983072280884\n",
      "training: 67 batch 411 batch_loss: 0.2109048068523407\n",
      "training: 67 batch 412 batch_loss: 0.21010509133338928\n",
      "training: 67 batch 413 batch_loss: 0.21133098006248474\n",
      "training: 67 batch 414 batch_loss: 0.21155297756195068\n",
      "training: 67 batch 415 batch_loss: 0.2141546607017517\n",
      "training: 67 batch 416 batch_loss: 0.21500623226165771\n",
      "training: 67 batch 417 batch_loss: 0.20849817991256714\n",
      "training: 67 batch 418 batch_loss: 0.21123230457305908\n",
      "training: 67 batch 419 batch_loss: 0.20717689394950867\n",
      "training: 67 batch 420 batch_loss: 0.20966839790344238\n",
      "training: 67 batch 421 batch_loss: 0.2070334553718567\n",
      "training: 67 batch 422 batch_loss: 0.21287408471107483\n",
      "training: 67 batch 423 batch_loss: 0.2100803554058075\n",
      "training: 67 batch 424 batch_loss: 0.21318107843399048\n",
      "training: 67 batch 425 batch_loss: 0.2094811201095581\n",
      "training: 67 batch 426 batch_loss: 0.20678851008415222\n",
      "training: 67 batch 427 batch_loss: 0.21626225113868713\n",
      "training: 67 batch 428 batch_loss: 0.20975008606910706\n",
      "training: 67 batch 429 batch_loss: 0.21203947067260742\n",
      "training: 67 batch 430 batch_loss: 0.21140772104263306\n",
      "training: 67 batch 431 batch_loss: 0.21253210306167603\n",
      "training: 67 batch 432 batch_loss: 0.20979350805282593\n",
      "training: 67 batch 433 batch_loss: 0.21624791622161865\n",
      "training: 67 batch 434 batch_loss: 0.20838195085525513\n",
      "training: 67 batch 435 batch_loss: 0.21194082498550415\n",
      "training: 67 batch 436 batch_loss: 0.2111557424068451\n",
      "training: 67 batch 437 batch_loss: 0.21021050214767456\n",
      "training: 67 batch 438 batch_loss: 0.21038120985031128\n",
      "training: 67 batch 439 batch_loss: 0.21267876029014587\n",
      "training: 67 batch 440 batch_loss: 0.20945993065834045\n",
      "training: 67 batch 441 batch_loss: 0.20929557085037231\n",
      "training: 67 batch 442 batch_loss: 0.21028074622154236\n",
      "training: 67 batch 443 batch_loss: 0.21019813418388367\n",
      "training: 67 batch 444 batch_loss: 0.20988479256629944\n",
      "training: 67 batch 445 batch_loss: 0.21130022406578064\n",
      "training: 67 batch 446 batch_loss: 0.21467867493629456\n",
      "training: 67 batch 447 batch_loss: 0.21260839700698853\n",
      "training: 67 batch 448 batch_loss: 0.20795190334320068\n",
      "training: 67 batch 449 batch_loss: 0.20747637748718262\n",
      "training: 67 batch 450 batch_loss: 0.21154838800430298\n",
      "training: 67 batch 451 batch_loss: 0.20641440153121948\n",
      "training: 67 batch 452 batch_loss: 0.2156619131565094\n",
      "training: 67 batch 453 batch_loss: 0.2048586905002594\n",
      "training: 67 batch 454 batch_loss: 0.21255889534950256\n",
      "training: 67 batch 455 batch_loss: 0.21000605821609497\n",
      "training: 67 batch 456 batch_loss: 0.21352621912956238\n",
      "training: 67 batch 457 batch_loss: 0.21107739210128784\n",
      "training: 67 batch 458 batch_loss: 0.2079843282699585\n",
      "training: 67 batch 459 batch_loss: 0.20860567688941956\n",
      "training: 67 batch 460 batch_loss: 0.21233227849006653\n",
      "training: 67 batch 461 batch_loss: 0.21211844682693481\n",
      "training: 67 batch 462 batch_loss: 0.2114275097846985\n",
      "training: 67 batch 463 batch_loss: 0.20951497554779053\n",
      "training: 67 batch 464 batch_loss: 0.21667256951332092\n",
      "training: 67 batch 465 batch_loss: 0.21436792612075806\n",
      "training: 67 batch 466 batch_loss: 0.21645009517669678\n",
      "training: 67 batch 467 batch_loss: 0.21008199453353882\n",
      "training: 67 batch 468 batch_loss: 0.21594339609146118\n",
      "training: 67 batch 469 batch_loss: 0.2119205892086029\n",
      "training: 67 batch 470 batch_loss: 0.21443483233451843\n",
      "training: 67 batch 471 batch_loss: 0.2132015824317932\n",
      "training: 67 batch 472 batch_loss: 0.2132069170475006\n",
      "training: 67 batch 473 batch_loss: 0.2109760046005249\n",
      "training: 67 batch 474 batch_loss: 0.2086176872253418\n",
      "training: 67 batch 475 batch_loss: 0.21804159879684448\n",
      "training: 67 batch 476 batch_loss: 0.21119120717048645\n",
      "training: 67 batch 477 batch_loss: 0.20688214898109436\n",
      "training: 67 batch 478 batch_loss: 0.21004587411880493\n",
      "training: 67 batch 479 batch_loss: 0.2133806347846985\n",
      "training: 67 batch 480 batch_loss: 0.20748582482337952\n",
      "training: 67 batch 481 batch_loss: 0.20897921919822693\n",
      "training: 67 batch 482 batch_loss: 0.21232950687408447\n",
      "training: 67 batch 483 batch_loss: 0.21129730343818665\n",
      "training: 67 batch 484 batch_loss: 0.20937451720237732\n",
      "training: 67 batch 485 batch_loss: 0.20783600211143494\n",
      "training: 67 batch 486 batch_loss: 0.20952442288398743\n",
      "training: 67 batch 487 batch_loss: 0.21360576152801514\n",
      "training: 67 batch 488 batch_loss: 0.20966511964797974\n",
      "training: 67 batch 489 batch_loss: 0.21631798148155212\n",
      "training: 67 batch 490 batch_loss: 0.2109890878200531\n",
      "training: 67 batch 491 batch_loss: 0.21179375052452087\n",
      "training: 67 batch 492 batch_loss: 0.20892280340194702\n",
      "training: 67 batch 493 batch_loss: 0.2133774757385254\n",
      "training: 67 batch 494 batch_loss: 0.2125261127948761\n",
      "training: 67 batch 495 batch_loss: 0.20789098739624023\n",
      "training: 67 batch 496 batch_loss: 0.2095116674900055\n",
      "training: 67 batch 497 batch_loss: 0.20877757668495178\n",
      "training: 67 batch 498 batch_loss: 0.20978310704231262\n",
      "training: 67 batch 499 batch_loss: 0.21066242456436157\n",
      "training: 67 batch 500 batch_loss: 0.21004420518875122\n",
      "training: 67 batch 501 batch_loss: 0.21024814248085022\n",
      "training: 67 batch 502 batch_loss: 0.21483466029167175\n",
      "training: 67 batch 503 batch_loss: 0.21132314205169678\n",
      "training: 67 batch 504 batch_loss: 0.21147215366363525\n",
      "training: 67 batch 505 batch_loss: 0.21117770671844482\n",
      "training: 67 batch 506 batch_loss: 0.21349447965621948\n",
      "training: 67 batch 507 batch_loss: 0.20872247219085693\n",
      "training: 67 batch 508 batch_loss: 0.21282953023910522\n",
      "training: 67 batch 509 batch_loss: 0.2070123553276062\n",
      "training: 67 batch 510 batch_loss: 0.20975086092948914\n",
      "training: 67 batch 511 batch_loss: 0.21055495738983154\n",
      "training: 67 batch 512 batch_loss: 0.2113337218761444\n",
      "training: 67 batch 513 batch_loss: 0.20747140049934387\n",
      "training: 67 batch 514 batch_loss: 0.21480706334114075\n",
      "training: 67 batch 515 batch_loss: 0.21116477251052856\n",
      "training: 67 batch 516 batch_loss: 0.21393507719039917\n",
      "training: 67 batch 517 batch_loss: 0.21131309866905212\n",
      "training: 67 batch 518 batch_loss: 0.2144208550453186\n",
      "training: 67 batch 519 batch_loss: 0.2110024392604828\n",
      "training: 67 batch 520 batch_loss: 0.21086597442626953\n",
      "training: 67 batch 521 batch_loss: 0.2135642170906067\n",
      "training: 67 batch 522 batch_loss: 0.21050119400024414\n",
      "training: 67 batch 523 batch_loss: 0.21613705158233643\n",
      "training: 67 batch 524 batch_loss: 0.21259823441505432\n",
      "training: 67 batch 525 batch_loss: 0.21349316835403442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 67 batch 526 batch_loss: 0.21291294693946838\n",
      "training: 67 batch 527 batch_loss: 0.21339169144630432\n",
      "training: 67 batch 528 batch_loss: 0.21255972981452942\n",
      "training: 67 batch 529 batch_loss: 0.20864930748939514\n",
      "training: 67 batch 530 batch_loss: 0.2130371630191803\n",
      "training: 67 batch 531 batch_loss: 0.20982831716537476\n",
      "training: 67 batch 532 batch_loss: 0.21167060732841492\n",
      "training: 67 batch 533 batch_loss: 0.2084837257862091\n",
      "training: 67 batch 534 batch_loss: 0.20620188117027283\n",
      "training: 67 batch 535 batch_loss: 0.21202409267425537\n",
      "training: 67 batch 536 batch_loss: 0.20775079727172852\n",
      "training: 67 batch 537 batch_loss: 0.20901989936828613\n",
      "training: 67 batch 538 batch_loss: 0.2116852104663849\n",
      "training: 67 batch 539 batch_loss: 0.21186938881874084\n",
      "training: 67 batch 540 batch_loss: 0.21511512994766235\n",
      "training: 67 batch 541 batch_loss: 0.2131676971912384\n",
      "training: 67 batch 542 batch_loss: 0.21523401141166687\n",
      "training: 67 batch 543 batch_loss: 0.21112099289894104\n",
      "training: 67 batch 544 batch_loss: 0.21051150560379028\n",
      "training: 67 batch 545 batch_loss: 0.21314632892608643\n",
      "training: 67 batch 546 batch_loss: 0.2091529369354248\n",
      "training: 67 batch 547 batch_loss: 0.21221423149108887\n",
      "training: 67 batch 548 batch_loss: 0.2072715163230896\n",
      "training: 67 batch 549 batch_loss: 0.21074536442756653\n",
      "training: 67 batch 550 batch_loss: 0.21048802137374878\n",
      "training: 67 batch 551 batch_loss: 0.21068871021270752\n",
      "training: 67 batch 552 batch_loss: 0.21561306715011597\n",
      "training: 67 batch 553 batch_loss: 0.20860958099365234\n",
      "training: 67 batch 554 batch_loss: 0.2164853811264038\n",
      "training: 67 batch 555 batch_loss: 0.20972940325737\n",
      "training: 67 batch 556 batch_loss: 0.21164321899414062\n",
      "training: 67 batch 557 batch_loss: 0.20772790908813477\n",
      "training: 67 batch 558 batch_loss: 0.2090752124786377\n",
      "training: 67 batch 559 batch_loss: 0.21136122941970825\n",
      "training: 67 batch 560 batch_loss: 0.21298927068710327\n",
      "training: 67 batch 561 batch_loss: 0.2112121880054474\n",
      "training: 67 batch 562 batch_loss: 0.2100914716720581\n",
      "training: 67 batch 563 batch_loss: 0.21257522702217102\n",
      "training: 67 batch 564 batch_loss: 0.212741881608963\n",
      "training: 67 batch 565 batch_loss: 0.2096412181854248\n",
      "training: 67 batch 566 batch_loss: 0.21050366759300232\n",
      "training: 67 batch 567 batch_loss: 0.21334004402160645\n",
      "training: 67 batch 568 batch_loss: 0.20793774724006653\n",
      "training: 67 batch 569 batch_loss: 0.21380311250686646\n",
      "training: 67 batch 570 batch_loss: 0.20989775657653809\n",
      "training: 67 batch 571 batch_loss: 0.21090009808540344\n",
      "training: 67 batch 572 batch_loss: 0.2139354944229126\n",
      "training: 67 batch 573 batch_loss: 0.21225512027740479\n",
      "training: 67 batch 574 batch_loss: 0.2152470350265503\n",
      "training: 67 batch 575 batch_loss: 0.2104966640472412\n",
      "training: 67 batch 576 batch_loss: 0.20724022388458252\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 67, Hit Ratio:0.033201973397114524 | Precision:0.048987515973655756 | Recall:0.0646964146106986 | NDCG:0.063680426410055\n",
      "*Best Performance* \n",
      "Epoch: 61, Hit Ratio:0.033261934821931224 | Precision:0.04907598545168584 | Recall:0.06479544314235144 | MDCG:0.06348668156373174\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 68 batch 0 batch_loss: 0.2081862986087799\n",
      "training: 68 batch 1 batch_loss: 0.21074450016021729\n",
      "training: 68 batch 2 batch_loss: 0.21121779084205627\n",
      "training: 68 batch 3 batch_loss: 0.20894908905029297\n",
      "training: 68 batch 4 batch_loss: 0.20583605766296387\n",
      "training: 68 batch 5 batch_loss: 0.20795181393623352\n",
      "training: 68 batch 6 batch_loss: 0.20922264456748962\n",
      "training: 68 batch 7 batch_loss: 0.21008947491645813\n",
      "training: 68 batch 8 batch_loss: 0.20138296484947205\n",
      "training: 68 batch 9 batch_loss: 0.21138590574264526\n",
      "training: 68 batch 10 batch_loss: 0.21343034505844116\n",
      "training: 68 batch 11 batch_loss: 0.20996889472007751\n",
      "training: 68 batch 12 batch_loss: 0.20438092947006226\n",
      "training: 68 batch 13 batch_loss: 0.20744764804840088\n",
      "training: 68 batch 14 batch_loss: 0.20741784572601318\n",
      "training: 68 batch 15 batch_loss: 0.2087932527065277\n",
      "training: 68 batch 16 batch_loss: 0.21072980761528015\n",
      "training: 68 batch 17 batch_loss: 0.21328210830688477\n",
      "training: 68 batch 18 batch_loss: 0.20984038710594177\n",
      "training: 68 batch 19 batch_loss: 0.21153271198272705\n",
      "training: 68 batch 20 batch_loss: 0.21177014708518982\n",
      "training: 68 batch 21 batch_loss: 0.20716595649719238\n",
      "training: 68 batch 22 batch_loss: 0.21230489015579224\n",
      "training: 68 batch 23 batch_loss: 0.2099919617176056\n",
      "training: 68 batch 24 batch_loss: 0.20892947912216187\n",
      "training: 68 batch 25 batch_loss: 0.2066655158996582\n",
      "training: 68 batch 26 batch_loss: 0.21105265617370605\n",
      "training: 68 batch 27 batch_loss: 0.21122696995735168\n",
      "training: 68 batch 28 batch_loss: 0.20689734816551208\n",
      "training: 68 batch 29 batch_loss: 0.21088463068008423\n",
      "training: 68 batch 30 batch_loss: 0.20785951614379883\n",
      "training: 68 batch 31 batch_loss: 0.2125878632068634\n",
      "training: 68 batch 32 batch_loss: 0.20797735452651978\n",
      "training: 68 batch 33 batch_loss: 0.20835721492767334\n",
      "training: 68 batch 34 batch_loss: 0.20300832390785217\n",
      "training: 68 batch 35 batch_loss: 0.20912504196166992\n",
      "training: 68 batch 36 batch_loss: 0.20385122299194336\n",
      "training: 68 batch 37 batch_loss: 0.20845270156860352\n",
      "training: 68 batch 38 batch_loss: 0.20934244990348816\n",
      "training: 68 batch 39 batch_loss: 0.21071308851242065\n",
      "training: 68 batch 40 batch_loss: 0.21210616827011108\n",
      "training: 68 batch 41 batch_loss: 0.2089264988899231\n",
      "training: 68 batch 42 batch_loss: 0.20965030789375305\n",
      "training: 68 batch 43 batch_loss: 0.20952782034873962\n",
      "training: 68 batch 44 batch_loss: 0.20545554161071777\n",
      "training: 68 batch 45 batch_loss: 0.20882770419120789\n",
      "training: 68 batch 46 batch_loss: 0.20879411697387695\n",
      "training: 68 batch 47 batch_loss: 0.21002641320228577\n",
      "training: 68 batch 48 batch_loss: 0.20864394307136536\n",
      "training: 68 batch 49 batch_loss: 0.20863407850265503\n",
      "training: 68 batch 50 batch_loss: 0.21164214611053467\n",
      "training: 68 batch 51 batch_loss: 0.20827215909957886\n",
      "training: 68 batch 52 batch_loss: 0.2078404426574707\n",
      "training: 68 batch 53 batch_loss: 0.2118571698665619\n",
      "training: 68 batch 54 batch_loss: 0.2067725658416748\n",
      "training: 68 batch 55 batch_loss: 0.21214061975479126\n",
      "training: 68 batch 56 batch_loss: 0.21075409650802612\n",
      "training: 68 batch 57 batch_loss: 0.20840269327163696\n",
      "training: 68 batch 58 batch_loss: 0.21182239055633545\n",
      "training: 68 batch 59 batch_loss: 0.20782315731048584\n",
      "training: 68 batch 60 batch_loss: 0.21115612983703613\n",
      "training: 68 batch 61 batch_loss: 0.21088376641273499\n",
      "training: 68 batch 62 batch_loss: 0.21141594648361206\n",
      "training: 68 batch 63 batch_loss: 0.215961754322052\n",
      "training: 68 batch 64 batch_loss: 0.21017149090766907\n",
      "training: 68 batch 65 batch_loss: 0.21053367853164673\n",
      "training: 68 batch 66 batch_loss: 0.21301117539405823\n",
      "training: 68 batch 67 batch_loss: 0.21431958675384521\n",
      "training: 68 batch 68 batch_loss: 0.20889869332313538\n",
      "training: 68 batch 69 batch_loss: 0.2081265151500702\n",
      "training: 68 batch 70 batch_loss: 0.2091720998287201\n",
      "training: 68 batch 71 batch_loss: 0.20887163281440735\n",
      "training: 68 batch 72 batch_loss: 0.2153511941432953\n",
      "training: 68 batch 73 batch_loss: 0.2108302116394043\n",
      "training: 68 batch 74 batch_loss: 0.21344643831253052\n",
      "training: 68 batch 75 batch_loss: 0.21011024713516235\n",
      "training: 68 batch 76 batch_loss: 0.2122516632080078\n",
      "training: 68 batch 77 batch_loss: 0.20461854338645935\n",
      "training: 68 batch 78 batch_loss: 0.20903879404067993\n",
      "training: 68 batch 79 batch_loss: 0.2121526598930359\n",
      "training: 68 batch 80 batch_loss: 0.2185612916946411\n",
      "training: 68 batch 81 batch_loss: 0.21056362986564636\n",
      "training: 68 batch 82 batch_loss: 0.2122977077960968\n",
      "training: 68 batch 83 batch_loss: 0.21177002787590027\n",
      "training: 68 batch 84 batch_loss: 0.20921379327774048\n",
      "training: 68 batch 85 batch_loss: 0.2137497365474701\n",
      "training: 68 batch 86 batch_loss: 0.2116582691669464\n",
      "training: 68 batch 87 batch_loss: 0.21130871772766113\n",
      "training: 68 batch 88 batch_loss: 0.2083302140235901\n",
      "training: 68 batch 89 batch_loss: 0.2110116183757782\n",
      "training: 68 batch 90 batch_loss: 0.21328049898147583\n",
      "training: 68 batch 91 batch_loss: 0.21043848991394043\n",
      "training: 68 batch 92 batch_loss: 0.2119758129119873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 68 batch 93 batch_loss: 0.2117018699645996\n",
      "training: 68 batch 94 batch_loss: 0.20447787642478943\n",
      "training: 68 batch 95 batch_loss: 0.2054716944694519\n",
      "training: 68 batch 96 batch_loss: 0.20844760537147522\n",
      "training: 68 batch 97 batch_loss: 0.20994237065315247\n",
      "training: 68 batch 98 batch_loss: 0.20923221111297607\n",
      "training: 68 batch 99 batch_loss: 0.21054542064666748\n",
      "training: 68 batch 100 batch_loss: 0.20899692177772522\n",
      "training: 68 batch 101 batch_loss: 0.21561062335968018\n",
      "training: 68 batch 102 batch_loss: 0.21002870798110962\n",
      "training: 68 batch 103 batch_loss: 0.2087996006011963\n",
      "training: 68 batch 104 batch_loss: 0.21261626482009888\n",
      "training: 68 batch 105 batch_loss: 0.21017104387283325\n",
      "training: 68 batch 106 batch_loss: 0.20787155628204346\n",
      "training: 68 batch 107 batch_loss: 0.21231094002723694\n",
      "training: 68 batch 108 batch_loss: 0.21295064687728882\n",
      "training: 68 batch 109 batch_loss: 0.2101544439792633\n",
      "training: 68 batch 110 batch_loss: 0.21427685022354126\n",
      "training: 68 batch 111 batch_loss: 0.20648539066314697\n",
      "training: 68 batch 112 batch_loss: 0.21069538593292236\n",
      "training: 68 batch 113 batch_loss: 0.2120133638381958\n",
      "training: 68 batch 114 batch_loss: 0.21473294496536255\n",
      "training: 68 batch 115 batch_loss: 0.21051818132400513\n",
      "training: 68 batch 116 batch_loss: 0.21144691109657288\n",
      "training: 68 batch 117 batch_loss: 0.20895841717720032\n",
      "training: 68 batch 118 batch_loss: 0.20994096994400024\n",
      "training: 68 batch 119 batch_loss: 0.21255365014076233\n",
      "training: 68 batch 120 batch_loss: 0.21443596482276917\n",
      "training: 68 batch 121 batch_loss: 0.21018853783607483\n",
      "training: 68 batch 122 batch_loss: 0.21129056811332703\n",
      "training: 68 batch 123 batch_loss: 0.21030807495117188\n",
      "training: 68 batch 124 batch_loss: 0.2139076292514801\n",
      "training: 68 batch 125 batch_loss: 0.20989447832107544\n",
      "training: 68 batch 126 batch_loss: 0.2086799144744873\n",
      "training: 68 batch 127 batch_loss: 0.2102532982826233\n",
      "training: 68 batch 128 batch_loss: 0.21058064699172974\n",
      "training: 68 batch 129 batch_loss: 0.20978838205337524\n",
      "training: 68 batch 130 batch_loss: 0.20779529213905334\n",
      "training: 68 batch 131 batch_loss: 0.2084566354751587\n",
      "training: 68 batch 132 batch_loss: 0.21090567111968994\n",
      "training: 68 batch 133 batch_loss: 0.21050435304641724\n",
      "training: 68 batch 134 batch_loss: 0.20428863167762756\n",
      "training: 68 batch 135 batch_loss: 0.21262794733047485\n",
      "training: 68 batch 136 batch_loss: 0.21205425262451172\n",
      "training: 68 batch 137 batch_loss: 0.21336743235588074\n",
      "training: 68 batch 138 batch_loss: 0.21267297863960266\n",
      "training: 68 batch 139 batch_loss: 0.21281284093856812\n",
      "training: 68 batch 140 batch_loss: 0.2106732726097107\n",
      "training: 68 batch 141 batch_loss: 0.21037229895591736\n",
      "training: 68 batch 142 batch_loss: 0.2051236927509308\n",
      "training: 68 batch 143 batch_loss: 0.20993691682815552\n",
      "training: 68 batch 144 batch_loss: 0.20829373598098755\n",
      "training: 68 batch 145 batch_loss: 0.21061360836029053\n",
      "training: 68 batch 146 batch_loss: 0.21101388335227966\n",
      "training: 68 batch 147 batch_loss: 0.20939180254936218\n",
      "training: 68 batch 148 batch_loss: 0.2108825445175171\n",
      "training: 68 batch 149 batch_loss: 0.21160098910331726\n",
      "training: 68 batch 150 batch_loss: 0.2118673324584961\n",
      "training: 68 batch 151 batch_loss: 0.20735499262809753\n",
      "training: 68 batch 152 batch_loss: 0.20826727151870728\n",
      "training: 68 batch 153 batch_loss: 0.2115989625453949\n",
      "training: 68 batch 154 batch_loss: 0.20792770385742188\n",
      "training: 68 batch 155 batch_loss: 0.20530802011489868\n",
      "training: 68 batch 156 batch_loss: 0.2134251594543457\n",
      "training: 68 batch 157 batch_loss: 0.21179115772247314\n",
      "training: 68 batch 158 batch_loss: 0.21518582105636597\n",
      "training: 68 batch 159 batch_loss: 0.21190562844276428\n",
      "training: 68 batch 160 batch_loss: 0.21182706952095032\n",
      "training: 68 batch 161 batch_loss: 0.20994555950164795\n",
      "training: 68 batch 162 batch_loss: 0.21301084756851196\n",
      "training: 68 batch 163 batch_loss: 0.2146570086479187\n",
      "training: 68 batch 164 batch_loss: 0.21451687812805176\n",
      "training: 68 batch 165 batch_loss: 0.21158498525619507\n",
      "training: 68 batch 166 batch_loss: 0.21101418137550354\n",
      "training: 68 batch 167 batch_loss: 0.2116120159626007\n",
      "training: 68 batch 168 batch_loss: 0.21222591400146484\n",
      "training: 68 batch 169 batch_loss: 0.2100234031677246\n",
      "training: 68 batch 170 batch_loss: 0.20796388387680054\n",
      "training: 68 batch 171 batch_loss: 0.21444007754325867\n",
      "training: 68 batch 172 batch_loss: 0.2065075933933258\n",
      "training: 68 batch 173 batch_loss: 0.21073293685913086\n",
      "training: 68 batch 174 batch_loss: 0.21421727538108826\n",
      "training: 68 batch 175 batch_loss: 0.21185174584388733\n",
      "training: 68 batch 176 batch_loss: 0.2121526598930359\n",
      "training: 68 batch 177 batch_loss: 0.21122592687606812\n",
      "training: 68 batch 178 batch_loss: 0.20881229639053345\n",
      "training: 68 batch 179 batch_loss: 0.20992803573608398\n",
      "training: 68 batch 180 batch_loss: 0.2112940549850464\n",
      "training: 68 batch 181 batch_loss: 0.20776236057281494\n",
      "training: 68 batch 182 batch_loss: 0.2136395275592804\n",
      "training: 68 batch 183 batch_loss: 0.21020621061325073\n",
      "training: 68 batch 184 batch_loss: 0.20978277921676636\n",
      "training: 68 batch 185 batch_loss: 0.21108552813529968\n",
      "training: 68 batch 186 batch_loss: 0.21350786089897156\n",
      "training: 68 batch 187 batch_loss: 0.21046477556228638\n",
      "training: 68 batch 188 batch_loss: 0.2100009024143219\n",
      "training: 68 batch 189 batch_loss: 0.2113877534866333\n",
      "training: 68 batch 190 batch_loss: 0.20832785964012146\n",
      "training: 68 batch 191 batch_loss: 0.20943289995193481\n",
      "training: 68 batch 192 batch_loss: 0.21380287408828735\n",
      "training: 68 batch 193 batch_loss: 0.21068105101585388\n",
      "training: 68 batch 194 batch_loss: 0.20478922128677368\n",
      "training: 68 batch 195 batch_loss: 0.21471548080444336\n",
      "training: 68 batch 196 batch_loss: 0.2116667628288269\n",
      "training: 68 batch 197 batch_loss: 0.21107059717178345\n",
      "training: 68 batch 198 batch_loss: 0.20768946409225464\n",
      "training: 68 batch 199 batch_loss: 0.21030476689338684\n",
      "training: 68 batch 200 batch_loss: 0.2141665816307068\n",
      "training: 68 batch 201 batch_loss: 0.21639922261238098\n",
      "training: 68 batch 202 batch_loss: 0.21161401271820068\n",
      "training: 68 batch 203 batch_loss: 0.2080792784690857\n",
      "training: 68 batch 204 batch_loss: 0.21079784631729126\n",
      "training: 68 batch 205 batch_loss: 0.21312206983566284\n",
      "training: 68 batch 206 batch_loss: 0.2096322774887085\n",
      "training: 68 batch 207 batch_loss: 0.2121758759021759\n",
      "training: 68 batch 208 batch_loss: 0.21084177494049072\n",
      "training: 68 batch 209 batch_loss: 0.21022343635559082\n",
      "training: 68 batch 210 batch_loss: 0.21447864174842834\n",
      "training: 68 batch 211 batch_loss: 0.20944342017173767\n",
      "training: 68 batch 212 batch_loss: 0.21383607387542725\n",
      "training: 68 batch 213 batch_loss: 0.21105870604515076\n",
      "training: 68 batch 214 batch_loss: 0.21283438801765442\n",
      "training: 68 batch 215 batch_loss: 0.20779910683631897\n",
      "training: 68 batch 216 batch_loss: 0.2077072560787201\n",
      "training: 68 batch 217 batch_loss: 0.2099679410457611\n",
      "training: 68 batch 218 batch_loss: 0.20937049388885498\n",
      "training: 68 batch 219 batch_loss: 0.21020928025245667\n",
      "training: 68 batch 220 batch_loss: 0.21029916405677795\n",
      "training: 68 batch 221 batch_loss: 0.20878592133522034\n",
      "training: 68 batch 222 batch_loss: 0.20876383781433105\n",
      "training: 68 batch 223 batch_loss: 0.214161217212677\n",
      "training: 68 batch 224 batch_loss: 0.21323883533477783\n",
      "training: 68 batch 225 batch_loss: 0.2102125883102417\n",
      "training: 68 batch 226 batch_loss: 0.21361315250396729\n",
      "training: 68 batch 227 batch_loss: 0.21040239930152893\n",
      "training: 68 batch 228 batch_loss: 0.20595324039459229\n",
      "training: 68 batch 229 batch_loss: 0.21100765466690063\n",
      "training: 68 batch 230 batch_loss: 0.20865848660469055\n",
      "training: 68 batch 231 batch_loss: 0.213781476020813\n",
      "training: 68 batch 232 batch_loss: 0.20979785919189453\n",
      "training: 68 batch 233 batch_loss: 0.21476000547409058\n",
      "training: 68 batch 234 batch_loss: 0.20494186878204346\n",
      "training: 68 batch 235 batch_loss: 0.21310192346572876\n",
      "training: 68 batch 236 batch_loss: 0.21098917722702026\n",
      "training: 68 batch 237 batch_loss: 0.21471214294433594\n",
      "training: 68 batch 238 batch_loss: 0.20475813746452332\n",
      "training: 68 batch 239 batch_loss: 0.21042141318321228\n",
      "training: 68 batch 240 batch_loss: 0.21373063325881958\n",
      "training: 68 batch 241 batch_loss: 0.2105068564414978\n",
      "training: 68 batch 242 batch_loss: 0.20583641529083252\n",
      "training: 68 batch 243 batch_loss: 0.21181675791740417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 68 batch 244 batch_loss: 0.21359452605247498\n",
      "training: 68 batch 245 batch_loss: 0.21082818508148193\n",
      "training: 68 batch 246 batch_loss: 0.20997577905654907\n",
      "training: 68 batch 247 batch_loss: 0.21691778302192688\n",
      "training: 68 batch 248 batch_loss: 0.2122095823287964\n",
      "training: 68 batch 249 batch_loss: 0.2161865234375\n",
      "training: 68 batch 250 batch_loss: 0.20939937233924866\n",
      "training: 68 batch 251 batch_loss: 0.2112884819507599\n",
      "training: 68 batch 252 batch_loss: 0.2144356071949005\n",
      "training: 68 batch 253 batch_loss: 0.21066224575042725\n",
      "training: 68 batch 254 batch_loss: 0.2154795527458191\n",
      "training: 68 batch 255 batch_loss: 0.21169406175613403\n",
      "training: 68 batch 256 batch_loss: 0.2101011574268341\n",
      "training: 68 batch 257 batch_loss: 0.2096710503101349\n",
      "training: 68 batch 258 batch_loss: 0.21029770374298096\n",
      "training: 68 batch 259 batch_loss: 0.21201997995376587\n",
      "training: 68 batch 260 batch_loss: 0.20768222212791443\n",
      "training: 68 batch 261 batch_loss: 0.21024960279464722\n",
      "training: 68 batch 262 batch_loss: 0.20586097240447998\n",
      "training: 68 batch 263 batch_loss: 0.21239548921585083\n",
      "training: 68 batch 264 batch_loss: 0.21240156888961792\n",
      "training: 68 batch 265 batch_loss: 0.212930828332901\n",
      "training: 68 batch 266 batch_loss: 0.21018481254577637\n",
      "training: 68 batch 267 batch_loss: 0.20809149742126465\n",
      "training: 68 batch 268 batch_loss: 0.21289530396461487\n",
      "training: 68 batch 269 batch_loss: 0.206547349691391\n",
      "training: 68 batch 270 batch_loss: 0.21072900295257568\n",
      "training: 68 batch 271 batch_loss: 0.21257394552230835\n",
      "training: 68 batch 272 batch_loss: 0.20876997709274292\n",
      "training: 68 batch 273 batch_loss: 0.20833832025527954\n",
      "training: 68 batch 274 batch_loss: 0.21322762966156006\n",
      "training: 68 batch 275 batch_loss: 0.20946037769317627\n",
      "training: 68 batch 276 batch_loss: 0.20887959003448486\n",
      "training: 68 batch 277 batch_loss: 0.20958542823791504\n",
      "training: 68 batch 278 batch_loss: 0.21320343017578125\n",
      "training: 68 batch 279 batch_loss: 0.21313226222991943\n",
      "training: 68 batch 280 batch_loss: 0.21194982528686523\n",
      "training: 68 batch 281 batch_loss: 0.2094576358795166\n",
      "training: 68 batch 282 batch_loss: 0.2046036720275879\n",
      "training: 68 batch 283 batch_loss: 0.21292254328727722\n",
      "training: 68 batch 284 batch_loss: 0.21364834904670715\n",
      "training: 68 batch 285 batch_loss: 0.20982173085212708\n",
      "training: 68 batch 286 batch_loss: 0.2097635567188263\n",
      "training: 68 batch 287 batch_loss: 0.21200281381607056\n",
      "training: 68 batch 288 batch_loss: 0.21453005075454712\n",
      "training: 68 batch 289 batch_loss: 0.2101273238658905\n",
      "training: 68 batch 290 batch_loss: 0.21067988872528076\n",
      "training: 68 batch 291 batch_loss: 0.21070608496665955\n",
      "training: 68 batch 292 batch_loss: 0.20637542009353638\n",
      "training: 68 batch 293 batch_loss: 0.20931744575500488\n",
      "training: 68 batch 294 batch_loss: 0.2088947296142578\n",
      "training: 68 batch 295 batch_loss: 0.2111726701259613\n",
      "training: 68 batch 296 batch_loss: 0.21077683568000793\n",
      "training: 68 batch 297 batch_loss: 0.2102820873260498\n",
      "training: 68 batch 298 batch_loss: 0.21163517236709595\n",
      "training: 68 batch 299 batch_loss: 0.210860937833786\n",
      "training: 68 batch 300 batch_loss: 0.21041607856750488\n",
      "training: 68 batch 301 batch_loss: 0.2106306254863739\n",
      "training: 68 batch 302 batch_loss: 0.20949837565422058\n",
      "training: 68 batch 303 batch_loss: 0.20769309997558594\n",
      "training: 68 batch 304 batch_loss: 0.20878934860229492\n",
      "training: 68 batch 305 batch_loss: 0.20615336298942566\n",
      "training: 68 batch 306 batch_loss: 0.21141451597213745\n",
      "training: 68 batch 307 batch_loss: 0.21854856610298157\n",
      "training: 68 batch 308 batch_loss: 0.20829331874847412\n",
      "training: 68 batch 309 batch_loss: 0.21121135354042053\n",
      "training: 68 batch 310 batch_loss: 0.21063172817230225\n",
      "training: 68 batch 311 batch_loss: 0.2107069492340088\n",
      "training: 68 batch 312 batch_loss: 0.20926323533058167\n",
      "training: 68 batch 313 batch_loss: 0.21055257320404053\n",
      "training: 68 batch 314 batch_loss: 0.2090798020362854\n",
      "training: 68 batch 315 batch_loss: 0.21327245235443115\n",
      "training: 68 batch 316 batch_loss: 0.2071053385734558\n",
      "training: 68 batch 317 batch_loss: 0.21289318799972534\n",
      "training: 68 batch 318 batch_loss: 0.21070930361747742\n",
      "training: 68 batch 319 batch_loss: 0.2125493288040161\n",
      "training: 68 batch 320 batch_loss: 0.21424245834350586\n",
      "training: 68 batch 321 batch_loss: 0.21328598260879517\n",
      "training: 68 batch 322 batch_loss: 0.21017974615097046\n",
      "training: 68 batch 323 batch_loss: 0.21131747961044312\n",
      "training: 68 batch 324 batch_loss: 0.21364900469779968\n",
      "training: 68 batch 325 batch_loss: 0.20851412415504456\n",
      "training: 68 batch 326 batch_loss: 0.21485310792922974\n",
      "training: 68 batch 327 batch_loss: 0.2049243450164795\n",
      "training: 68 batch 328 batch_loss: 0.21138715744018555\n",
      "training: 68 batch 329 batch_loss: 0.21151167154312134\n",
      "training: 68 batch 330 batch_loss: 0.20819538831710815\n",
      "training: 68 batch 331 batch_loss: 0.21233677864074707\n",
      "training: 68 batch 332 batch_loss: 0.21249186992645264\n",
      "training: 68 batch 333 batch_loss: 0.21073848009109497\n",
      "training: 68 batch 334 batch_loss: 0.21281909942626953\n",
      "training: 68 batch 335 batch_loss: 0.2106027603149414\n",
      "training: 68 batch 336 batch_loss: 0.2101459503173828\n",
      "training: 68 batch 337 batch_loss: 0.21289801597595215\n",
      "training: 68 batch 338 batch_loss: 0.21475782990455627\n",
      "training: 68 batch 339 batch_loss: 0.21263962984085083\n",
      "training: 68 batch 340 batch_loss: 0.21168529987335205\n",
      "training: 68 batch 341 batch_loss: 0.21062767505645752\n",
      "training: 68 batch 342 batch_loss: 0.20457804203033447\n",
      "training: 68 batch 343 batch_loss: 0.21640005707740784\n",
      "training: 68 batch 344 batch_loss: 0.21542775630950928\n",
      "training: 68 batch 345 batch_loss: 0.21075439453125\n",
      "training: 68 batch 346 batch_loss: 0.21786090731620789\n",
      "training: 68 batch 347 batch_loss: 0.21311205625534058\n",
      "training: 68 batch 348 batch_loss: 0.2091374397277832\n",
      "training: 68 batch 349 batch_loss: 0.21377232670783997\n",
      "training: 68 batch 350 batch_loss: 0.20564499497413635\n",
      "training: 68 batch 351 batch_loss: 0.21059024333953857\n",
      "training: 68 batch 352 batch_loss: 0.2081659734249115\n",
      "training: 68 batch 353 batch_loss: 0.21334055066108704\n",
      "training: 68 batch 354 batch_loss: 0.20999470353126526\n",
      "training: 68 batch 355 batch_loss: 0.21073055267333984\n",
      "training: 68 batch 356 batch_loss: 0.21155068278312683\n",
      "training: 68 batch 357 batch_loss: 0.20862314105033875\n",
      "training: 68 batch 358 batch_loss: 0.21269789338111877\n",
      "training: 68 batch 359 batch_loss: 0.21408039331436157\n",
      "training: 68 batch 360 batch_loss: 0.21192756295204163\n",
      "training: 68 batch 361 batch_loss: 0.21200528740882874\n",
      "training: 68 batch 362 batch_loss: 0.20907574892044067\n",
      "training: 68 batch 363 batch_loss: 0.21702146530151367\n",
      "training: 68 batch 364 batch_loss: 0.2098381221294403\n",
      "training: 68 batch 365 batch_loss: 0.21456673741340637\n",
      "training: 68 batch 366 batch_loss: 0.207998126745224\n",
      "training: 68 batch 367 batch_loss: 0.21368089318275452\n",
      "training: 68 batch 368 batch_loss: 0.21290671825408936\n",
      "training: 68 batch 369 batch_loss: 0.2059919238090515\n",
      "training: 68 batch 370 batch_loss: 0.21515411138534546\n",
      "training: 68 batch 371 batch_loss: 0.20916759967803955\n",
      "training: 68 batch 372 batch_loss: 0.21332097053527832\n",
      "training: 68 batch 373 batch_loss: 0.21262449026107788\n",
      "training: 68 batch 374 batch_loss: 0.21131324768066406\n",
      "training: 68 batch 375 batch_loss: 0.2133103311061859\n",
      "training: 68 batch 376 batch_loss: 0.21627292037010193\n",
      "training: 68 batch 377 batch_loss: 0.20905667543411255\n",
      "training: 68 batch 378 batch_loss: 0.21216589212417603\n",
      "training: 68 batch 379 batch_loss: 0.2120753526687622\n",
      "training: 68 batch 380 batch_loss: 0.2156582474708557\n",
      "training: 68 batch 381 batch_loss: 0.21083879470825195\n",
      "training: 68 batch 382 batch_loss: 0.21500101685523987\n",
      "training: 68 batch 383 batch_loss: 0.21235999464988708\n",
      "training: 68 batch 384 batch_loss: 0.2113496959209442\n",
      "training: 68 batch 385 batch_loss: 0.20918655395507812\n",
      "training: 68 batch 386 batch_loss: 0.2151271104812622\n",
      "training: 68 batch 387 batch_loss: 0.20826444029808044\n",
      "training: 68 batch 388 batch_loss: 0.20860543847084045\n",
      "training: 68 batch 389 batch_loss: 0.21362006664276123\n",
      "training: 68 batch 390 batch_loss: 0.210851788520813\n",
      "training: 68 batch 391 batch_loss: 0.20957890152931213\n",
      "training: 68 batch 392 batch_loss: 0.21039295196533203\n",
      "training: 68 batch 393 batch_loss: 0.21227788925170898\n",
      "training: 68 batch 394 batch_loss: 0.21105128526687622\n",
      "training: 68 batch 395 batch_loss: 0.21180051565170288\n",
      "training: 68 batch 396 batch_loss: 0.20926499366760254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 68 batch 397 batch_loss: 0.2105296552181244\n",
      "training: 68 batch 398 batch_loss: 0.2119944989681244\n",
      "training: 68 batch 399 batch_loss: 0.21458220481872559\n",
      "training: 68 batch 400 batch_loss: 0.2140691876411438\n",
      "training: 68 batch 401 batch_loss: 0.2095145583152771\n",
      "training: 68 batch 402 batch_loss: 0.21222060918807983\n",
      "training: 68 batch 403 batch_loss: 0.21404951810836792\n",
      "training: 68 batch 404 batch_loss: 0.21365615725517273\n",
      "training: 68 batch 405 batch_loss: 0.21289384365081787\n",
      "training: 68 batch 406 batch_loss: 0.2101556360721588\n",
      "training: 68 batch 407 batch_loss: 0.20751264691352844\n",
      "training: 68 batch 408 batch_loss: 0.210049569606781\n",
      "training: 68 batch 409 batch_loss: 0.21163833141326904\n",
      "training: 68 batch 410 batch_loss: 0.21420133113861084\n",
      "training: 68 batch 411 batch_loss: 0.2156224250793457\n",
      "training: 68 batch 412 batch_loss: 0.2107018232345581\n",
      "training: 68 batch 413 batch_loss: 0.21162936091423035\n",
      "training: 68 batch 414 batch_loss: 0.20711085200309753\n",
      "training: 68 batch 415 batch_loss: 0.20818334817886353\n",
      "training: 68 batch 416 batch_loss: 0.20828846096992493\n",
      "training: 68 batch 417 batch_loss: 0.21416383981704712\n",
      "training: 68 batch 418 batch_loss: 0.21272245049476624\n",
      "training: 68 batch 419 batch_loss: 0.21320068836212158\n",
      "training: 68 batch 420 batch_loss: 0.21028730273246765\n",
      "training: 68 batch 421 batch_loss: 0.21172070503234863\n",
      "training: 68 batch 422 batch_loss: 0.214591383934021\n",
      "training: 68 batch 423 batch_loss: 0.2164229452610016\n",
      "training: 68 batch 424 batch_loss: 0.21338015794754028\n",
      "training: 68 batch 425 batch_loss: 0.21066045761108398\n",
      "training: 68 batch 426 batch_loss: 0.2164396345615387\n",
      "training: 68 batch 427 batch_loss: 0.21006757020950317\n",
      "training: 68 batch 428 batch_loss: 0.21354418992996216\n",
      "training: 68 batch 429 batch_loss: 0.21072250604629517\n",
      "training: 68 batch 430 batch_loss: 0.2087351679801941\n",
      "training: 68 batch 431 batch_loss: 0.21078255772590637\n",
      "training: 68 batch 432 batch_loss: 0.21312355995178223\n",
      "training: 68 batch 433 batch_loss: 0.21626654267311096\n",
      "training: 68 batch 434 batch_loss: 0.21199846267700195\n",
      "training: 68 batch 435 batch_loss: 0.21373504400253296\n",
      "training: 68 batch 436 batch_loss: 0.21250522136688232\n",
      "training: 68 batch 437 batch_loss: 0.21078938245773315\n",
      "training: 68 batch 438 batch_loss: 0.2140943706035614\n",
      "training: 68 batch 439 batch_loss: 0.21006780862808228\n",
      "training: 68 batch 440 batch_loss: 0.21120679378509521\n",
      "training: 68 batch 441 batch_loss: 0.21227511763572693\n",
      "training: 68 batch 442 batch_loss: 0.2131037712097168\n",
      "training: 68 batch 443 batch_loss: 0.21365413069725037\n",
      "training: 68 batch 444 batch_loss: 0.21758127212524414\n",
      "training: 68 batch 445 batch_loss: 0.21716254949569702\n",
      "training: 68 batch 446 batch_loss: 0.21135783195495605\n",
      "training: 68 batch 447 batch_loss: 0.2167688012123108\n",
      "training: 68 batch 448 batch_loss: 0.21145474910736084\n",
      "training: 68 batch 449 batch_loss: 0.21205872297286987\n",
      "training: 68 batch 450 batch_loss: 0.20444133877754211\n",
      "training: 68 batch 451 batch_loss: 0.20906788110733032\n",
      "training: 68 batch 452 batch_loss: 0.2075798213481903\n",
      "training: 68 batch 453 batch_loss: 0.20715349912643433\n",
      "training: 68 batch 454 batch_loss: 0.2087644338607788\n",
      "training: 68 batch 455 batch_loss: 0.20875480771064758\n",
      "training: 68 batch 456 batch_loss: 0.21341902017593384\n",
      "training: 68 batch 457 batch_loss: 0.21439534425735474\n",
      "training: 68 batch 458 batch_loss: 0.2149023711681366\n",
      "training: 68 batch 459 batch_loss: 0.21029430627822876\n",
      "training: 68 batch 460 batch_loss: 0.21253404021263123\n",
      "training: 68 batch 461 batch_loss: 0.2163403034210205\n",
      "training: 68 batch 462 batch_loss: 0.20913439989089966\n",
      "training: 68 batch 463 batch_loss: 0.20917856693267822\n",
      "training: 68 batch 464 batch_loss: 0.21528735756874084\n",
      "training: 68 batch 465 batch_loss: 0.21247366070747375\n",
      "training: 68 batch 466 batch_loss: 0.20711442828178406\n",
      "training: 68 batch 467 batch_loss: 0.20780599117279053\n",
      "training: 68 batch 468 batch_loss: 0.21096310019493103\n",
      "training: 68 batch 469 batch_loss: 0.21611332893371582\n",
      "training: 68 batch 470 batch_loss: 0.2113552689552307\n",
      "training: 68 batch 471 batch_loss: 0.21089434623718262\n",
      "training: 68 batch 472 batch_loss: 0.20849475264549255\n",
      "training: 68 batch 473 batch_loss: 0.21101701259613037\n",
      "training: 68 batch 474 batch_loss: 0.21229925751686096\n",
      "training: 68 batch 475 batch_loss: 0.213130384683609\n",
      "training: 68 batch 476 batch_loss: 0.21152415871620178\n",
      "training: 68 batch 477 batch_loss: 0.213420569896698\n",
      "training: 68 batch 478 batch_loss: 0.21398743987083435\n",
      "training: 68 batch 479 batch_loss: 0.21281608939170837\n",
      "training: 68 batch 480 batch_loss: 0.20837441086769104\n",
      "training: 68 batch 481 batch_loss: 0.21492862701416016\n",
      "training: 68 batch 482 batch_loss: 0.21003222465515137\n",
      "training: 68 batch 483 batch_loss: 0.21286672353744507\n",
      "training: 68 batch 484 batch_loss: 0.2114276885986328\n",
      "training: 68 batch 485 batch_loss: 0.2080712914466858\n",
      "training: 68 batch 486 batch_loss: 0.21137860417366028\n",
      "training: 68 batch 487 batch_loss: 0.20972636342048645\n",
      "training: 68 batch 488 batch_loss: 0.21496447920799255\n",
      "training: 68 batch 489 batch_loss: 0.21006396412849426\n",
      "training: 68 batch 490 batch_loss: 0.2115510106086731\n",
      "training: 68 batch 491 batch_loss: 0.2040347456932068\n",
      "training: 68 batch 492 batch_loss: 0.20667147636413574\n",
      "training: 68 batch 493 batch_loss: 0.210135817527771\n",
      "training: 68 batch 494 batch_loss: 0.21485882997512817\n",
      "training: 68 batch 495 batch_loss: 0.21209830045700073\n",
      "training: 68 batch 496 batch_loss: 0.20627859234809875\n",
      "training: 68 batch 497 batch_loss: 0.22267377376556396\n",
      "training: 68 batch 498 batch_loss: 0.2143217921257019\n",
      "training: 68 batch 499 batch_loss: 0.2196616530418396\n",
      "training: 68 batch 500 batch_loss: 0.2095632255077362\n",
      "training: 68 batch 501 batch_loss: 0.21171975135803223\n",
      "training: 68 batch 502 batch_loss: 0.21946850419044495\n",
      "training: 68 batch 503 batch_loss: 0.21294435858726501\n",
      "training: 68 batch 504 batch_loss: 0.21226835250854492\n",
      "training: 68 batch 505 batch_loss: 0.21425634622573853\n",
      "training: 68 batch 506 batch_loss: 0.2093483805656433\n",
      "training: 68 batch 507 batch_loss: 0.21162551641464233\n",
      "training: 68 batch 508 batch_loss: 0.20913437008857727\n",
      "training: 68 batch 509 batch_loss: 0.2119770050048828\n",
      "training: 68 batch 510 batch_loss: 0.21181505918502808\n",
      "training: 68 batch 511 batch_loss: 0.21117109060287476\n",
      "training: 68 batch 512 batch_loss: 0.210758775472641\n",
      "training: 68 batch 513 batch_loss: 0.21033546328544617\n",
      "training: 68 batch 514 batch_loss: 0.20618537068367004\n",
      "training: 68 batch 515 batch_loss: 0.20978900790214539\n",
      "training: 68 batch 516 batch_loss: 0.20994681119918823\n",
      "training: 68 batch 517 batch_loss: 0.2114773392677307\n",
      "training: 68 batch 518 batch_loss: 0.21568182110786438\n",
      "training: 68 batch 519 batch_loss: 0.21120548248291016\n",
      "training: 68 batch 520 batch_loss: 0.21292635798454285\n",
      "training: 68 batch 521 batch_loss: 0.21043157577514648\n",
      "training: 68 batch 522 batch_loss: 0.21329182386398315\n",
      "training: 68 batch 523 batch_loss: 0.21247246861457825\n",
      "training: 68 batch 524 batch_loss: 0.2153312861919403\n",
      "training: 68 batch 525 batch_loss: 0.21360665559768677\n",
      "training: 68 batch 526 batch_loss: 0.21359211206436157\n",
      "training: 68 batch 527 batch_loss: 0.21148741245269775\n",
      "training: 68 batch 528 batch_loss: 0.21162331104278564\n",
      "training: 68 batch 529 batch_loss: 0.20998764038085938\n",
      "training: 68 batch 530 batch_loss: 0.21157589554786682\n",
      "training: 68 batch 531 batch_loss: 0.21173980832099915\n",
      "training: 68 batch 532 batch_loss: 0.2123589813709259\n",
      "training: 68 batch 533 batch_loss: 0.21495148539543152\n",
      "training: 68 batch 534 batch_loss: 0.2111414670944214\n",
      "training: 68 batch 535 batch_loss: 0.21527504920959473\n",
      "training: 68 batch 536 batch_loss: 0.21137836575508118\n",
      "training: 68 batch 537 batch_loss: 0.21024459600448608\n",
      "training: 68 batch 538 batch_loss: 0.21361589431762695\n",
      "training: 68 batch 539 batch_loss: 0.2144521176815033\n",
      "training: 68 batch 540 batch_loss: 0.21863022446632385\n",
      "training: 68 batch 541 batch_loss: 0.21173223853111267\n",
      "training: 68 batch 542 batch_loss: 0.20784690976142883\n",
      "training: 68 batch 543 batch_loss: 0.2119036316871643\n",
      "training: 68 batch 544 batch_loss: 0.21098193526268005\n",
      "training: 68 batch 545 batch_loss: 0.2174118161201477\n",
      "training: 68 batch 546 batch_loss: 0.2097589373588562\n",
      "training: 68 batch 547 batch_loss: 0.210906982421875\n",
      "training: 68 batch 548 batch_loss: 0.21259379386901855\n",
      "training: 68 batch 549 batch_loss: 0.21054816246032715\n",
      "training: 68 batch 550 batch_loss: 0.2097066044807434\n",
      "training: 68 batch 551 batch_loss: 0.21072638034820557\n",
      "training: 68 batch 552 batch_loss: 0.21154454350471497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 68 batch 553 batch_loss: 0.21626052260398865\n",
      "training: 68 batch 554 batch_loss: 0.21299797296524048\n",
      "training: 68 batch 555 batch_loss: 0.21530351042747498\n",
      "training: 68 batch 556 batch_loss: 0.21412602066993713\n",
      "training: 68 batch 557 batch_loss: 0.21689963340759277\n",
      "training: 68 batch 558 batch_loss: 0.21557599306106567\n",
      "training: 68 batch 559 batch_loss: 0.21652284264564514\n",
      "training: 68 batch 560 batch_loss: 0.21001297235488892\n",
      "training: 68 batch 561 batch_loss: 0.21290400624275208\n",
      "training: 68 batch 562 batch_loss: 0.21358811855316162\n",
      "training: 68 batch 563 batch_loss: 0.2105216085910797\n",
      "training: 68 batch 564 batch_loss: 0.21351420879364014\n",
      "training: 68 batch 565 batch_loss: 0.2139989137649536\n",
      "training: 68 batch 566 batch_loss: 0.21045103669166565\n",
      "training: 68 batch 567 batch_loss: 0.2151855230331421\n",
      "training: 68 batch 568 batch_loss: 0.20802271366119385\n",
      "training: 68 batch 569 batch_loss: 0.2107064127922058\n",
      "training: 68 batch 570 batch_loss: 0.2058444619178772\n",
      "training: 68 batch 571 batch_loss: 0.21519461274147034\n",
      "training: 68 batch 572 batch_loss: 0.21311748027801514\n",
      "training: 68 batch 573 batch_loss: 0.21498480439186096\n",
      "training: 68 batch 574 batch_loss: 0.21343189477920532\n",
      "training: 68 batch 575 batch_loss: 0.21154850721359253\n",
      "training: 68 batch 576 batch_loss: 0.22231921553611755\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 68, Hit Ratio:0.03353176123360638 | Precision:0.04947409810282119 | Recall:0.06511509304686922 | NDCG:0.0641621394246606\n",
      "*Best Performance* \n",
      "Epoch: 68, Hit Ratio:0.03353176123360638 | Precision:0.04947409810282119 | Recall:0.06511509304686922 | MDCG:0.0641621394246606\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 69 batch 0 batch_loss: 0.21467500925064087\n",
      "training: 69 batch 1 batch_loss: 0.20753717422485352\n",
      "training: 69 batch 2 batch_loss: 0.20761248469352722\n",
      "training: 69 batch 3 batch_loss: 0.2115313708782196\n",
      "training: 69 batch 4 batch_loss: 0.2098177671432495\n",
      "training: 69 batch 5 batch_loss: 0.20911061763763428\n",
      "training: 69 batch 6 batch_loss: 0.21441861987113953\n",
      "training: 69 batch 7 batch_loss: 0.21330910921096802\n",
      "training: 69 batch 8 batch_loss: 0.2108374834060669\n",
      "training: 69 batch 9 batch_loss: 0.20890873670578003\n",
      "training: 69 batch 10 batch_loss: 0.2096894085407257\n",
      "training: 69 batch 11 batch_loss: 0.21372109651565552\n",
      "training: 69 batch 12 batch_loss: 0.21086519956588745\n",
      "training: 69 batch 13 batch_loss: 0.21113204956054688\n",
      "training: 69 batch 14 batch_loss: 0.20631006360054016\n",
      "training: 69 batch 15 batch_loss: 0.21443045139312744\n",
      "training: 69 batch 16 batch_loss: 0.20322197675704956\n",
      "training: 69 batch 17 batch_loss: 0.20774495601654053\n",
      "training: 69 batch 18 batch_loss: 0.21009090542793274\n",
      "training: 69 batch 19 batch_loss: 0.21421310305595398\n",
      "training: 69 batch 20 batch_loss: 0.21171221137046814\n",
      "training: 69 batch 21 batch_loss: 0.21401196718215942\n",
      "training: 69 batch 22 batch_loss: 0.21020102500915527\n",
      "training: 69 batch 23 batch_loss: 0.2123049795627594\n",
      "training: 69 batch 24 batch_loss: 0.2122071385383606\n",
      "training: 69 batch 25 batch_loss: 0.21115463972091675\n",
      "training: 69 batch 26 batch_loss: 0.20967823266983032\n",
      "training: 69 batch 27 batch_loss: 0.20982131361961365\n",
      "training: 69 batch 28 batch_loss: 0.21386480331420898\n",
      "training: 69 batch 29 batch_loss: 0.21594589948654175\n",
      "training: 69 batch 30 batch_loss: 0.20963144302368164\n",
      "training: 69 batch 31 batch_loss: 0.21663525700569153\n",
      "training: 69 batch 32 batch_loss: 0.21085026860237122\n",
      "training: 69 batch 33 batch_loss: 0.21057787537574768\n",
      "training: 69 batch 34 batch_loss: 0.2121538519859314\n",
      "training: 69 batch 35 batch_loss: 0.20917746424674988\n",
      "training: 69 batch 36 batch_loss: 0.21582871675491333\n",
      "training: 69 batch 37 batch_loss: 0.21323108673095703\n",
      "training: 69 batch 38 batch_loss: 0.20222991704940796\n",
      "training: 69 batch 39 batch_loss: 0.21465858817100525\n",
      "training: 69 batch 40 batch_loss: 0.21251237392425537\n",
      "training: 69 batch 41 batch_loss: 0.20829322934150696\n",
      "training: 69 batch 42 batch_loss: 0.2094060182571411\n",
      "training: 69 batch 43 batch_loss: 0.20861202478408813\n",
      "training: 69 batch 44 batch_loss: 0.21305280923843384\n",
      "training: 69 batch 45 batch_loss: 0.21180856227874756\n",
      "training: 69 batch 46 batch_loss: 0.21228694915771484\n",
      "training: 69 batch 47 batch_loss: 0.20514962077140808\n",
      "training: 69 batch 48 batch_loss: 0.21055227518081665\n",
      "training: 69 batch 49 batch_loss: 0.2137402892112732\n",
      "training: 69 batch 50 batch_loss: 0.21260395646095276\n",
      "training: 69 batch 51 batch_loss: 0.2113601565361023\n",
      "training: 69 batch 52 batch_loss: 0.20969650149345398\n",
      "training: 69 batch 53 batch_loss: 0.2130351960659027\n",
      "training: 69 batch 54 batch_loss: 0.21520915627479553\n",
      "training: 69 batch 55 batch_loss: 0.2136663794517517\n",
      "training: 69 batch 56 batch_loss: 0.21145009994506836\n",
      "training: 69 batch 57 batch_loss: 0.20843765139579773\n",
      "training: 69 batch 58 batch_loss: 0.21202018857002258\n",
      "training: 69 batch 59 batch_loss: 0.21189647912979126\n",
      "training: 69 batch 60 batch_loss: 0.21102845668792725\n",
      "training: 69 batch 61 batch_loss: 0.2122347354888916\n",
      "training: 69 batch 62 batch_loss: 0.2097567915916443\n",
      "training: 69 batch 63 batch_loss: 0.21128231287002563\n",
      "training: 69 batch 64 batch_loss: 0.21084627509117126\n",
      "training: 69 batch 65 batch_loss: 0.20921942591667175\n",
      "training: 69 batch 66 batch_loss: 0.20937779545783997\n",
      "training: 69 batch 67 batch_loss: 0.20796281099319458\n",
      "training: 69 batch 68 batch_loss: 0.21178752183914185\n",
      "training: 69 batch 69 batch_loss: 0.20752573013305664\n",
      "training: 69 batch 70 batch_loss: 0.21246477961540222\n",
      "training: 69 batch 71 batch_loss: 0.20986086130142212\n",
      "training: 69 batch 72 batch_loss: 0.21657472848892212\n",
      "training: 69 batch 73 batch_loss: 0.21367624402046204\n",
      "training: 69 batch 74 batch_loss: 0.21154063940048218\n",
      "training: 69 batch 75 batch_loss: 0.21559149026870728\n",
      "training: 69 batch 76 batch_loss: 0.2121930718421936\n",
      "training: 69 batch 77 batch_loss: 0.21263229846954346\n",
      "training: 69 batch 78 batch_loss: 0.20826011896133423\n",
      "training: 69 batch 79 batch_loss: 0.21223962306976318\n",
      "training: 69 batch 80 batch_loss: 0.21230652928352356\n",
      "training: 69 batch 81 batch_loss: 0.20673441886901855\n",
      "training: 69 batch 82 batch_loss: 0.20920300483703613\n",
      "training: 69 batch 83 batch_loss: 0.21213805675506592\n",
      "training: 69 batch 84 batch_loss: 0.2108985185623169\n",
      "training: 69 batch 85 batch_loss: 0.2076331079006195\n",
      "training: 69 batch 86 batch_loss: 0.20911380648612976\n",
      "training: 69 batch 87 batch_loss: 0.2139154076576233\n",
      "training: 69 batch 88 batch_loss: 0.21260038018226624\n",
      "training: 69 batch 89 batch_loss: 0.21054145693778992\n",
      "training: 69 batch 90 batch_loss: 0.21294748783111572\n",
      "training: 69 batch 91 batch_loss: 0.2112760841846466\n",
      "training: 69 batch 92 batch_loss: 0.20998573303222656\n",
      "training: 69 batch 93 batch_loss: 0.2092806100845337\n",
      "training: 69 batch 94 batch_loss: 0.21228131651878357\n",
      "training: 69 batch 95 batch_loss: 0.20946848392486572\n",
      "training: 69 batch 96 batch_loss: 0.208663672208786\n",
      "training: 69 batch 97 batch_loss: 0.21175968647003174\n",
      "training: 69 batch 98 batch_loss: 0.21450850367546082\n",
      "training: 69 batch 99 batch_loss: 0.21280935406684875\n",
      "training: 69 batch 100 batch_loss: 0.21211311221122742\n",
      "training: 69 batch 101 batch_loss: 0.21373045444488525\n",
      "training: 69 batch 102 batch_loss: 0.21425855159759521\n",
      "training: 69 batch 103 batch_loss: 0.20883339643478394\n",
      "training: 69 batch 104 batch_loss: 0.20909631252288818\n",
      "training: 69 batch 105 batch_loss: 0.2115376591682434\n",
      "training: 69 batch 106 batch_loss: 0.21744033694267273\n",
      "training: 69 batch 107 batch_loss: 0.2119484543800354\n",
      "training: 69 batch 108 batch_loss: 0.21196046471595764\n",
      "training: 69 batch 109 batch_loss: 0.210682213306427\n",
      "training: 69 batch 110 batch_loss: 0.21034321188926697\n",
      "training: 69 batch 111 batch_loss: 0.21371817588806152\n",
      "training: 69 batch 112 batch_loss: 0.21500509977340698\n",
      "training: 69 batch 113 batch_loss: 0.21352902054786682\n",
      "training: 69 batch 114 batch_loss: 0.21146300435066223\n",
      "training: 69 batch 115 batch_loss: 0.2115580439567566\n",
      "training: 69 batch 116 batch_loss: 0.2101116180419922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 69 batch 117 batch_loss: 0.20626404881477356\n",
      "training: 69 batch 118 batch_loss: 0.2102794647216797\n",
      "training: 69 batch 119 batch_loss: 0.21157506108283997\n",
      "training: 69 batch 120 batch_loss: 0.20917052030563354\n",
      "training: 69 batch 121 batch_loss: 0.2107016146183014\n",
      "training: 69 batch 122 batch_loss: 0.20844042301177979\n",
      "training: 69 batch 123 batch_loss: 0.21159455180168152\n",
      "training: 69 batch 124 batch_loss: 0.20600131154060364\n",
      "training: 69 batch 125 batch_loss: 0.21012040972709656\n",
      "training: 69 batch 126 batch_loss: 0.2076472043991089\n",
      "training: 69 batch 127 batch_loss: 0.21046605706214905\n",
      "training: 69 batch 128 batch_loss: 0.21235951781272888\n",
      "training: 69 batch 129 batch_loss: 0.21810847520828247\n",
      "training: 69 batch 130 batch_loss: 0.2156570851802826\n",
      "training: 69 batch 131 batch_loss: 0.21019220352172852\n",
      "training: 69 batch 132 batch_loss: 0.2119365632534027\n",
      "training: 69 batch 133 batch_loss: 0.20820075273513794\n",
      "training: 69 batch 134 batch_loss: 0.20631590485572815\n",
      "training: 69 batch 135 batch_loss: 0.21061930060386658\n",
      "training: 69 batch 136 batch_loss: 0.20792457461357117\n",
      "training: 69 batch 137 batch_loss: 0.2123241126537323\n",
      "training: 69 batch 138 batch_loss: 0.21017977595329285\n",
      "training: 69 batch 139 batch_loss: 0.2107880711555481\n",
      "training: 69 batch 140 batch_loss: 0.21024668216705322\n",
      "training: 69 batch 141 batch_loss: 0.21362841129302979\n",
      "training: 69 batch 142 batch_loss: 0.2128482162952423\n",
      "training: 69 batch 143 batch_loss: 0.21256116032600403\n",
      "training: 69 batch 144 batch_loss: 0.21435728669166565\n",
      "training: 69 batch 145 batch_loss: 0.2138558030128479\n",
      "training: 69 batch 146 batch_loss: 0.2144125998020172\n",
      "training: 69 batch 147 batch_loss: 0.21577033400535583\n",
      "training: 69 batch 148 batch_loss: 0.2072272002696991\n",
      "training: 69 batch 149 batch_loss: 0.20805802941322327\n",
      "training: 69 batch 150 batch_loss: 0.21333247423171997\n",
      "training: 69 batch 151 batch_loss: 0.2101021409034729\n",
      "training: 69 batch 152 batch_loss: 0.21080628037452698\n",
      "training: 69 batch 153 batch_loss: 0.21345895528793335\n",
      "training: 69 batch 154 batch_loss: 0.21245378255844116\n",
      "training: 69 batch 155 batch_loss: 0.2148175835609436\n",
      "training: 69 batch 156 batch_loss: 0.2110573947429657\n",
      "training: 69 batch 157 batch_loss: 0.21179020404815674\n",
      "training: 69 batch 158 batch_loss: 0.20626947283744812\n",
      "training: 69 batch 159 batch_loss: 0.21382084488868713\n",
      "training: 69 batch 160 batch_loss: 0.21049129962921143\n",
      "training: 69 batch 161 batch_loss: 0.2121792435646057\n",
      "training: 69 batch 162 batch_loss: 0.21423351764678955\n",
      "training: 69 batch 163 batch_loss: 0.21176865696907043\n",
      "training: 69 batch 164 batch_loss: 0.212843656539917\n",
      "training: 69 batch 165 batch_loss: 0.207444965839386\n",
      "training: 69 batch 166 batch_loss: 0.2123371660709381\n",
      "training: 69 batch 167 batch_loss: 0.212474524974823\n",
      "training: 69 batch 168 batch_loss: 0.21018871665000916\n",
      "training: 69 batch 169 batch_loss: 0.2154679000377655\n",
      "training: 69 batch 170 batch_loss: 0.21307116746902466\n",
      "training: 69 batch 171 batch_loss: 0.21547091007232666\n",
      "training: 69 batch 172 batch_loss: 0.21169915795326233\n",
      "training: 69 batch 173 batch_loss: 0.21429723501205444\n",
      "training: 69 batch 174 batch_loss: 0.21308287978172302\n",
      "training: 69 batch 175 batch_loss: 0.2125524878501892\n",
      "training: 69 batch 176 batch_loss: 0.20868438482284546\n",
      "training: 69 batch 177 batch_loss: 0.21452748775482178\n",
      "training: 69 batch 178 batch_loss: 0.21083593368530273\n",
      "training: 69 batch 179 batch_loss: 0.20783552527427673\n",
      "training: 69 batch 180 batch_loss: 0.2117762565612793\n",
      "training: 69 batch 181 batch_loss: 0.21192067861557007\n",
      "training: 69 batch 182 batch_loss: 0.20718589425086975\n",
      "training: 69 batch 183 batch_loss: 0.2143782377243042\n",
      "training: 69 batch 184 batch_loss: 0.20997026562690735\n",
      "training: 69 batch 185 batch_loss: 0.2124885618686676\n",
      "training: 69 batch 186 batch_loss: 0.21208417415618896\n",
      "training: 69 batch 187 batch_loss: 0.21361875534057617\n",
      "training: 69 batch 188 batch_loss: 0.20985770225524902\n",
      "training: 69 batch 189 batch_loss: 0.21406683325767517\n",
      "training: 69 batch 190 batch_loss: 0.21199831366539001\n",
      "training: 69 batch 191 batch_loss: 0.2125994861125946\n",
      "training: 69 batch 192 batch_loss: 0.21398448944091797\n",
      "training: 69 batch 193 batch_loss: 0.2138262689113617\n",
      "training: 69 batch 194 batch_loss: 0.21189305186271667\n",
      "training: 69 batch 195 batch_loss: 0.2153124213218689\n",
      "training: 69 batch 196 batch_loss: 0.21363359689712524\n",
      "training: 69 batch 197 batch_loss: 0.20957213640213013\n",
      "training: 69 batch 198 batch_loss: 0.21073991060256958\n",
      "training: 69 batch 199 batch_loss: 0.21229544281959534\n",
      "training: 69 batch 200 batch_loss: 0.2079838216304779\n",
      "training: 69 batch 201 batch_loss: 0.20976683497428894\n",
      "training: 69 batch 202 batch_loss: 0.20715850591659546\n",
      "training: 69 batch 203 batch_loss: 0.2094116508960724\n",
      "training: 69 batch 204 batch_loss: 0.21245303750038147\n",
      "training: 69 batch 205 batch_loss: 0.21208220720291138\n",
      "training: 69 batch 206 batch_loss: 0.21227732300758362\n",
      "training: 69 batch 207 batch_loss: 0.21407437324523926\n",
      "training: 69 batch 208 batch_loss: 0.2120082676410675\n",
      "training: 69 batch 209 batch_loss: 0.21307894587516785\n",
      "training: 69 batch 210 batch_loss: 0.21566113829612732\n",
      "training: 69 batch 211 batch_loss: 0.2096388041973114\n",
      "training: 69 batch 212 batch_loss: 0.21124181151390076\n",
      "training: 69 batch 213 batch_loss: 0.21494656801223755\n",
      "training: 69 batch 214 batch_loss: 0.21429121494293213\n",
      "training: 69 batch 215 batch_loss: 0.207570880651474\n",
      "training: 69 batch 216 batch_loss: 0.21313917636871338\n",
      "training: 69 batch 217 batch_loss: 0.21269690990447998\n",
      "training: 69 batch 218 batch_loss: 0.21060603857040405\n",
      "training: 69 batch 219 batch_loss: 0.21671777963638306\n",
      "training: 69 batch 220 batch_loss: 0.21282052993774414\n",
      "training: 69 batch 221 batch_loss: 0.2118152678012848\n",
      "training: 69 batch 222 batch_loss: 0.21130138635635376\n",
      "training: 69 batch 223 batch_loss: 0.20634308457374573\n",
      "training: 69 batch 224 batch_loss: 0.21096497774124146\n",
      "training: 69 batch 225 batch_loss: 0.21095556020736694\n",
      "training: 69 batch 226 batch_loss: 0.21392196416854858\n",
      "training: 69 batch 227 batch_loss: 0.21492308378219604\n",
      "training: 69 batch 228 batch_loss: 0.21677923202514648\n",
      "training: 69 batch 229 batch_loss: 0.21231693029403687\n",
      "training: 69 batch 230 batch_loss: 0.2138116955757141\n",
      "training: 69 batch 231 batch_loss: 0.21373194456100464\n",
      "training: 69 batch 232 batch_loss: 0.216050386428833\n",
      "training: 69 batch 233 batch_loss: 0.21073299646377563\n",
      "training: 69 batch 234 batch_loss: 0.2125059962272644\n",
      "training: 69 batch 235 batch_loss: 0.21457067131996155\n",
      "training: 69 batch 236 batch_loss: 0.21587377786636353\n",
      "training: 69 batch 237 batch_loss: 0.21462109684944153\n",
      "training: 69 batch 238 batch_loss: 0.21720421314239502\n",
      "training: 69 batch 239 batch_loss: 0.21347948908805847\n",
      "training: 69 batch 240 batch_loss: 0.2098020315170288\n",
      "training: 69 batch 241 batch_loss: 0.2123696506023407\n",
      "training: 69 batch 242 batch_loss: 0.21368038654327393\n",
      "training: 69 batch 243 batch_loss: 0.21400785446166992\n",
      "training: 69 batch 244 batch_loss: 0.2093248963356018\n",
      "training: 69 batch 245 batch_loss: 0.2109258770942688\n",
      "training: 69 batch 246 batch_loss: 0.21161898970603943\n",
      "training: 69 batch 247 batch_loss: 0.20751136541366577\n",
      "training: 69 batch 248 batch_loss: 0.21413657069206238\n",
      "training: 69 batch 249 batch_loss: 0.21616238355636597\n",
      "training: 69 batch 250 batch_loss: 0.20929813385009766\n",
      "training: 69 batch 251 batch_loss: 0.21206390857696533\n",
      "training: 69 batch 252 batch_loss: 0.21171095967292786\n",
      "training: 69 batch 253 batch_loss: 0.20940721035003662\n",
      "training: 69 batch 254 batch_loss: 0.2114889919757843\n",
      "training: 69 batch 255 batch_loss: 0.2097862958908081\n",
      "training: 69 batch 256 batch_loss: 0.2147185206413269\n",
      "training: 69 batch 257 batch_loss: 0.21051564812660217\n",
      "training: 69 batch 258 batch_loss: 0.21392548084259033\n",
      "training: 69 batch 259 batch_loss: 0.21402615308761597\n",
      "training: 69 batch 260 batch_loss: 0.2100481390953064\n",
      "training: 69 batch 261 batch_loss: 0.20709118247032166\n",
      "training: 69 batch 262 batch_loss: 0.21748048067092896\n",
      "training: 69 batch 263 batch_loss: 0.21297204494476318\n",
      "training: 69 batch 264 batch_loss: 0.21362000703811646\n",
      "training: 69 batch 265 batch_loss: 0.2088041603565216\n",
      "training: 69 batch 266 batch_loss: 0.21247947216033936\n",
      "training: 69 batch 267 batch_loss: 0.21099603176116943\n",
      "training: 69 batch 268 batch_loss: 0.21372628211975098\n",
      "training: 69 batch 269 batch_loss: 0.21201461553573608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 69 batch 270 batch_loss: 0.2109237015247345\n",
      "training: 69 batch 271 batch_loss: 0.2135014533996582\n",
      "training: 69 batch 272 batch_loss: 0.21552467346191406\n",
      "training: 69 batch 273 batch_loss: 0.2126096785068512\n",
      "training: 69 batch 274 batch_loss: 0.2069375216960907\n",
      "training: 69 batch 275 batch_loss: 0.20960581302642822\n",
      "training: 69 batch 276 batch_loss: 0.20899268984794617\n",
      "training: 69 batch 277 batch_loss: 0.21523165702819824\n",
      "training: 69 batch 278 batch_loss: 0.21229130029678345\n",
      "training: 69 batch 279 batch_loss: 0.2136366367340088\n",
      "training: 69 batch 280 batch_loss: 0.21236303448677063\n",
      "training: 69 batch 281 batch_loss: 0.2113865613937378\n",
      "training: 69 batch 282 batch_loss: 0.2116781771183014\n",
      "training: 69 batch 283 batch_loss: 0.21008026599884033\n",
      "training: 69 batch 284 batch_loss: 0.21531549096107483\n",
      "training: 69 batch 285 batch_loss: 0.21238645911216736\n",
      "training: 69 batch 286 batch_loss: 0.21166148781776428\n",
      "training: 69 batch 287 batch_loss: 0.21218129992485046\n",
      "training: 69 batch 288 batch_loss: 0.21414047479629517\n",
      "training: 69 batch 289 batch_loss: 0.2131366729736328\n",
      "training: 69 batch 290 batch_loss: 0.21173161268234253\n",
      "training: 69 batch 291 batch_loss: 0.21396926045417786\n",
      "training: 69 batch 292 batch_loss: 0.21082767844200134\n",
      "training: 69 batch 293 batch_loss: 0.216962069272995\n",
      "training: 69 batch 294 batch_loss: 0.21339058876037598\n",
      "training: 69 batch 295 batch_loss: 0.21066030859947205\n",
      "training: 69 batch 296 batch_loss: 0.20858541131019592\n",
      "training: 69 batch 297 batch_loss: 0.21554100513458252\n",
      "training: 69 batch 298 batch_loss: 0.21115127205848694\n",
      "training: 69 batch 299 batch_loss: 0.2168521285057068\n",
      "training: 69 batch 300 batch_loss: 0.21939346194267273\n",
      "training: 69 batch 301 batch_loss: 0.20932185649871826\n",
      "training: 69 batch 302 batch_loss: 0.2088121771812439\n",
      "training: 69 batch 303 batch_loss: 0.2125125527381897\n",
      "training: 69 batch 304 batch_loss: 0.21102622151374817\n",
      "training: 69 batch 305 batch_loss: 0.21550357341766357\n",
      "training: 69 batch 306 batch_loss: 0.21725884079933167\n",
      "training: 69 batch 307 batch_loss: 0.20883816480636597\n",
      "training: 69 batch 308 batch_loss: 0.20805373787879944\n",
      "training: 69 batch 309 batch_loss: 0.2124033272266388\n",
      "training: 69 batch 310 batch_loss: 0.2114940881729126\n",
      "training: 69 batch 311 batch_loss: 0.2130759060382843\n",
      "training: 69 batch 312 batch_loss: 0.2102106213569641\n",
      "training: 69 batch 313 batch_loss: 0.2100962996482849\n",
      "training: 69 batch 314 batch_loss: 0.20999038219451904\n",
      "training: 69 batch 315 batch_loss: 0.21196770668029785\n",
      "training: 69 batch 316 batch_loss: 0.21243593096733093\n",
      "training: 69 batch 317 batch_loss: 0.20817148685455322\n",
      "training: 69 batch 318 batch_loss: 0.2132876217365265\n",
      "training: 69 batch 319 batch_loss: 0.21519634127616882\n",
      "training: 69 batch 320 batch_loss: 0.21068108081817627\n",
      "training: 69 batch 321 batch_loss: 0.21217110753059387\n",
      "training: 69 batch 322 batch_loss: 0.21118766069412231\n",
      "training: 69 batch 323 batch_loss: 0.21172937750816345\n",
      "training: 69 batch 324 batch_loss: 0.21375462412834167\n",
      "training: 69 batch 325 batch_loss: 0.2110637128353119\n",
      "training: 69 batch 326 batch_loss: 0.20921170711517334\n",
      "training: 69 batch 327 batch_loss: 0.21228107810020447\n",
      "training: 69 batch 328 batch_loss: 0.2119131088256836\n",
      "training: 69 batch 329 batch_loss: 0.21128645539283752\n",
      "training: 69 batch 330 batch_loss: 0.21129247546195984\n",
      "training: 69 batch 331 batch_loss: 0.210954487323761\n",
      "training: 69 batch 332 batch_loss: 0.2109774351119995\n",
      "training: 69 batch 333 batch_loss: 0.21117541193962097\n",
      "training: 69 batch 334 batch_loss: 0.22053661942481995\n",
      "training: 69 batch 335 batch_loss: 0.21327203512191772\n",
      "training: 69 batch 336 batch_loss: 0.21082046627998352\n",
      "training: 69 batch 337 batch_loss: 0.2178189754486084\n",
      "training: 69 batch 338 batch_loss: 0.21238094568252563\n",
      "training: 69 batch 339 batch_loss: 0.21323752403259277\n",
      "training: 69 batch 340 batch_loss: 0.21414169669151306\n",
      "training: 69 batch 341 batch_loss: 0.2104443907737732\n",
      "training: 69 batch 342 batch_loss: 0.2095610797405243\n",
      "training: 69 batch 343 batch_loss: 0.2090790867805481\n",
      "training: 69 batch 344 batch_loss: 0.21179920434951782\n",
      "training: 69 batch 345 batch_loss: 0.210858553647995\n",
      "training: 69 batch 346 batch_loss: 0.2118983268737793\n",
      "training: 69 batch 347 batch_loss: 0.20998632907867432\n",
      "training: 69 batch 348 batch_loss: 0.21270477771759033\n",
      "training: 69 batch 349 batch_loss: 0.21157273650169373\n",
      "training: 69 batch 350 batch_loss: 0.2154707908630371\n",
      "training: 69 batch 351 batch_loss: 0.2116365134716034\n",
      "training: 69 batch 352 batch_loss: 0.2110016942024231\n",
      "training: 69 batch 353 batch_loss: 0.21107441186904907\n",
      "training: 69 batch 354 batch_loss: 0.2179018259048462\n",
      "training: 69 batch 355 batch_loss: 0.20827257633209229\n",
      "training: 69 batch 356 batch_loss: 0.20806416869163513\n",
      "training: 69 batch 357 batch_loss: 0.21176990866661072\n",
      "training: 69 batch 358 batch_loss: 0.21332699060440063\n",
      "training: 69 batch 359 batch_loss: 0.20993474125862122\n",
      "training: 69 batch 360 batch_loss: 0.2142854928970337\n",
      "training: 69 batch 361 batch_loss: 0.21484848856925964\n",
      "training: 69 batch 362 batch_loss: 0.21022695302963257\n",
      "training: 69 batch 363 batch_loss: 0.21003171801567078\n",
      "training: 69 batch 364 batch_loss: 0.20873814821243286\n",
      "training: 69 batch 365 batch_loss: 0.21280008554458618\n",
      "training: 69 batch 366 batch_loss: 0.2089146375656128\n",
      "training: 69 batch 367 batch_loss: 0.21356278657913208\n",
      "training: 69 batch 368 batch_loss: 0.21444988250732422\n",
      "training: 69 batch 369 batch_loss: 0.21173924207687378\n",
      "training: 69 batch 370 batch_loss: 0.21235966682434082\n",
      "training: 69 batch 371 batch_loss: 0.20852157473564148\n",
      "training: 69 batch 372 batch_loss: 0.2153516411781311\n",
      "training: 69 batch 373 batch_loss: 0.21162566542625427\n",
      "training: 69 batch 374 batch_loss: 0.20720016956329346\n",
      "training: 69 batch 375 batch_loss: 0.21650049090385437\n",
      "training: 69 batch 376 batch_loss: 0.21827420592308044\n",
      "training: 69 batch 377 batch_loss: 0.212701678276062\n",
      "training: 69 batch 378 batch_loss: 0.2093365490436554\n",
      "training: 69 batch 379 batch_loss: 0.2139664888381958\n",
      "training: 69 batch 380 batch_loss: 0.20995554327964783\n",
      "training: 69 batch 381 batch_loss: 0.21901270747184753\n",
      "training: 69 batch 382 batch_loss: 0.214401513338089\n",
      "training: 69 batch 383 batch_loss: 0.21123993396759033\n",
      "training: 69 batch 384 batch_loss: 0.21581828594207764\n",
      "training: 69 batch 385 batch_loss: 0.21714353561401367\n",
      "training: 69 batch 386 batch_loss: 0.2112557590007782\n",
      "training: 69 batch 387 batch_loss: 0.21709933876991272\n",
      "training: 69 batch 388 batch_loss: 0.21298444271087646\n",
      "training: 69 batch 389 batch_loss: 0.2100696861743927\n",
      "training: 69 batch 390 batch_loss: 0.20948666334152222\n",
      "training: 69 batch 391 batch_loss: 0.21083685755729675\n",
      "training: 69 batch 392 batch_loss: 0.20724499225616455\n",
      "training: 69 batch 393 batch_loss: 0.2096131443977356\n",
      "training: 69 batch 394 batch_loss: 0.20851528644561768\n",
      "training: 69 batch 395 batch_loss: 0.21234899759292603\n",
      "training: 69 batch 396 batch_loss: 0.20966318249702454\n",
      "training: 69 batch 397 batch_loss: 0.2150496244430542\n",
      "training: 69 batch 398 batch_loss: 0.21155071258544922\n",
      "training: 69 batch 399 batch_loss: 0.21254080533981323\n",
      "training: 69 batch 400 batch_loss: 0.21368664503097534\n",
      "training: 69 batch 401 batch_loss: 0.21323862671852112\n",
      "training: 69 batch 402 batch_loss: 0.21078768372535706\n",
      "training: 69 batch 403 batch_loss: 0.21292316913604736\n",
      "training: 69 batch 404 batch_loss: 0.2127334475517273\n",
      "training: 69 batch 405 batch_loss: 0.21094372868537903\n",
      "training: 69 batch 406 batch_loss: 0.2088528871536255\n",
      "training: 69 batch 407 batch_loss: 0.21489092707633972\n",
      "training: 69 batch 408 batch_loss: 0.21125653386116028\n",
      "training: 69 batch 409 batch_loss: 0.213926762342453\n",
      "training: 69 batch 410 batch_loss: 0.21412771940231323\n",
      "training: 69 batch 411 batch_loss: 0.21379202604293823\n",
      "training: 69 batch 412 batch_loss: 0.21626129746437073\n",
      "training: 69 batch 413 batch_loss: 0.21299761533737183\n",
      "training: 69 batch 414 batch_loss: 0.2122739851474762\n",
      "training: 69 batch 415 batch_loss: 0.2154597043991089\n",
      "training: 69 batch 416 batch_loss: 0.21262040734291077\n",
      "training: 69 batch 417 batch_loss: 0.20765787363052368\n",
      "training: 69 batch 418 batch_loss: 0.20943784713745117\n",
      "training: 69 batch 419 batch_loss: 0.21845421195030212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 69 batch 420 batch_loss: 0.20988523960113525\n",
      "training: 69 batch 421 batch_loss: 0.2155528962612152\n",
      "training: 69 batch 422 batch_loss: 0.21447408199310303\n",
      "training: 69 batch 423 batch_loss: 0.2124449610710144\n",
      "training: 69 batch 424 batch_loss: 0.21339315176010132\n",
      "training: 69 batch 425 batch_loss: 0.21077701449394226\n",
      "training: 69 batch 426 batch_loss: 0.20865222811698914\n",
      "training: 69 batch 427 batch_loss: 0.21143490076065063\n",
      "training: 69 batch 428 batch_loss: 0.2175547480583191\n",
      "training: 69 batch 429 batch_loss: 0.21555855870246887\n",
      "training: 69 batch 430 batch_loss: 0.21141773462295532\n",
      "training: 69 batch 431 batch_loss: 0.20953688025474548\n",
      "training: 69 batch 432 batch_loss: 0.2153528928756714\n",
      "training: 69 batch 433 batch_loss: 0.21021202206611633\n",
      "training: 69 batch 434 batch_loss: 0.2151048481464386\n",
      "training: 69 batch 435 batch_loss: 0.20844992995262146\n",
      "training: 69 batch 436 batch_loss: 0.20808666944503784\n",
      "training: 69 batch 437 batch_loss: 0.21206319332122803\n",
      "training: 69 batch 438 batch_loss: 0.21429282426834106\n",
      "training: 69 batch 439 batch_loss: 0.21257662773132324\n",
      "training: 69 batch 440 batch_loss: 0.20798352360725403\n",
      "training: 69 batch 441 batch_loss: 0.21116265654563904\n",
      "training: 69 batch 442 batch_loss: 0.2081882655620575\n",
      "training: 69 batch 443 batch_loss: 0.21246284246444702\n",
      "training: 69 batch 444 batch_loss: 0.2129499316215515\n",
      "training: 69 batch 445 batch_loss: 0.20950108766555786\n",
      "training: 69 batch 446 batch_loss: 0.2166319489479065\n",
      "training: 69 batch 447 batch_loss: 0.21461379528045654\n",
      "training: 69 batch 448 batch_loss: 0.21225589513778687\n",
      "training: 69 batch 449 batch_loss: 0.211518794298172\n",
      "training: 69 batch 450 batch_loss: 0.20944154262542725\n",
      "training: 69 batch 451 batch_loss: 0.2127077877521515\n",
      "training: 69 batch 452 batch_loss: 0.21624085307121277\n",
      "training: 69 batch 453 batch_loss: 0.21108010411262512\n",
      "training: 69 batch 454 batch_loss: 0.21668803691864014\n",
      "training: 69 batch 455 batch_loss: 0.21242469549179077\n",
      "training: 69 batch 456 batch_loss: 0.21027833223342896\n",
      "training: 69 batch 457 batch_loss: 0.2075665295124054\n",
      "training: 69 batch 458 batch_loss: 0.21086814999580383\n",
      "training: 69 batch 459 batch_loss: 0.21202877163887024\n",
      "training: 69 batch 460 batch_loss: 0.21161013841629028\n",
      "training: 69 batch 461 batch_loss: 0.21050673723220825\n",
      "training: 69 batch 462 batch_loss: 0.2134149670600891\n",
      "training: 69 batch 463 batch_loss: 0.21405214071273804\n",
      "training: 69 batch 464 batch_loss: 0.2133144736289978\n",
      "training: 69 batch 465 batch_loss: 0.2153862714767456\n",
      "training: 69 batch 466 batch_loss: 0.2108372449874878\n",
      "training: 69 batch 467 batch_loss: 0.21472063660621643\n",
      "training: 69 batch 468 batch_loss: 0.20851367712020874\n",
      "training: 69 batch 469 batch_loss: 0.21134358644485474\n",
      "training: 69 batch 470 batch_loss: 0.208357036113739\n",
      "training: 69 batch 471 batch_loss: 0.2136378288269043\n",
      "training: 69 batch 472 batch_loss: 0.21585801243782043\n",
      "training: 69 batch 473 batch_loss: 0.21192878484725952\n",
      "training: 69 batch 474 batch_loss: 0.213335782289505\n",
      "training: 69 batch 475 batch_loss: 0.21030616760253906\n",
      "training: 69 batch 476 batch_loss: 0.20733770728111267\n",
      "training: 69 batch 477 batch_loss: 0.21376407146453857\n",
      "training: 69 batch 478 batch_loss: 0.21360021829605103\n",
      "training: 69 batch 479 batch_loss: 0.21418344974517822\n",
      "training: 69 batch 480 batch_loss: 0.21556323766708374\n",
      "training: 69 batch 481 batch_loss: 0.20958822965621948\n",
      "training: 69 batch 482 batch_loss: 0.2112148106098175\n",
      "training: 69 batch 483 batch_loss: 0.2098407745361328\n",
      "training: 69 batch 484 batch_loss: 0.21282222867012024\n",
      "training: 69 batch 485 batch_loss: 0.21073684096336365\n",
      "training: 69 batch 486 batch_loss: 0.20773500204086304\n",
      "training: 69 batch 487 batch_loss: 0.21441566944122314\n",
      "training: 69 batch 488 batch_loss: 0.22067350149154663\n",
      "training: 69 batch 489 batch_loss: 0.21421295404434204\n",
      "training: 69 batch 490 batch_loss: 0.20937344431877136\n",
      "training: 69 batch 491 batch_loss: 0.20910876989364624\n",
      "training: 69 batch 492 batch_loss: 0.2151663899421692\n",
      "training: 69 batch 493 batch_loss: 0.21295970678329468\n",
      "training: 69 batch 494 batch_loss: 0.21278226375579834\n",
      "training: 69 batch 495 batch_loss: 0.2140180766582489\n",
      "training: 69 batch 496 batch_loss: 0.21338194608688354\n",
      "training: 69 batch 497 batch_loss: 0.21243545413017273\n",
      "training: 69 batch 498 batch_loss: 0.20917347073554993\n",
      "training: 69 batch 499 batch_loss: 0.2147589921951294\n",
      "training: 69 batch 500 batch_loss: 0.21324464678764343\n",
      "training: 69 batch 501 batch_loss: 0.2144019901752472\n",
      "training: 69 batch 502 batch_loss: 0.21377873420715332\n",
      "training: 69 batch 503 batch_loss: 0.21252021193504333\n",
      "training: 69 batch 504 batch_loss: 0.21257227659225464\n",
      "training: 69 batch 505 batch_loss: 0.21351200342178345\n",
      "training: 69 batch 506 batch_loss: 0.2170279622077942\n",
      "training: 69 batch 507 batch_loss: 0.21039503812789917\n",
      "training: 69 batch 508 batch_loss: 0.215110182762146\n",
      "training: 69 batch 509 batch_loss: 0.21092459559440613\n",
      "training: 69 batch 510 batch_loss: 0.21340593695640564\n",
      "training: 69 batch 511 batch_loss: 0.21422076225280762\n",
      "training: 69 batch 512 batch_loss: 0.21642053127288818\n",
      "training: 69 batch 513 batch_loss: 0.2120717167854309\n",
      "training: 69 batch 514 batch_loss: 0.21367570757865906\n",
      "training: 69 batch 515 batch_loss: 0.21327054500579834\n",
      "training: 69 batch 516 batch_loss: 0.21247398853302002\n",
      "training: 69 batch 517 batch_loss: 0.21250924468040466\n",
      "training: 69 batch 518 batch_loss: 0.2112025022506714\n",
      "training: 69 batch 519 batch_loss: 0.2081393599510193\n",
      "training: 69 batch 520 batch_loss: 0.21107959747314453\n",
      "training: 69 batch 521 batch_loss: 0.20784103870391846\n",
      "training: 69 batch 522 batch_loss: 0.21475136280059814\n",
      "training: 69 batch 523 batch_loss: 0.212468683719635\n",
      "training: 69 batch 524 batch_loss: 0.21219965815544128\n",
      "training: 69 batch 525 batch_loss: 0.2104569971561432\n",
      "training: 69 batch 526 batch_loss: 0.20683085918426514\n",
      "training: 69 batch 527 batch_loss: 0.2138093113899231\n",
      "training: 69 batch 528 batch_loss: 0.21814972162246704\n",
      "training: 69 batch 529 batch_loss: 0.21429520845413208\n",
      "training: 69 batch 530 batch_loss: 0.2151610553264618\n",
      "training: 69 batch 531 batch_loss: 0.2105109989643097\n",
      "training: 69 batch 532 batch_loss: 0.20872318744659424\n",
      "training: 69 batch 533 batch_loss: 0.2150692343711853\n",
      "training: 69 batch 534 batch_loss: 0.21299540996551514\n",
      "training: 69 batch 535 batch_loss: 0.21429434418678284\n",
      "training: 69 batch 536 batch_loss: 0.21434268355369568\n",
      "training: 69 batch 537 batch_loss: 0.21404218673706055\n",
      "training: 69 batch 538 batch_loss: 0.21299776434898376\n",
      "training: 69 batch 539 batch_loss: 0.21346992254257202\n",
      "training: 69 batch 540 batch_loss: 0.21115821599960327\n",
      "training: 69 batch 541 batch_loss: 0.21122169494628906\n",
      "training: 69 batch 542 batch_loss: 0.21223986148834229\n",
      "training: 69 batch 543 batch_loss: 0.21506637334823608\n",
      "training: 69 batch 544 batch_loss: 0.21418088674545288\n",
      "training: 69 batch 545 batch_loss: 0.2154606282711029\n",
      "training: 69 batch 546 batch_loss: 0.21579188108444214\n",
      "training: 69 batch 547 batch_loss: 0.2133350372314453\n",
      "training: 69 batch 548 batch_loss: 0.2120189666748047\n",
      "training: 69 batch 549 batch_loss: 0.20917975902557373\n",
      "training: 69 batch 550 batch_loss: 0.20840442180633545\n",
      "training: 69 batch 551 batch_loss: 0.21331754326820374\n",
      "training: 69 batch 552 batch_loss: 0.20975756645202637\n",
      "training: 69 batch 553 batch_loss: 0.21276968717575073\n",
      "training: 69 batch 554 batch_loss: 0.21401739120483398\n",
      "training: 69 batch 555 batch_loss: 0.21013155579566956\n",
      "training: 69 batch 556 batch_loss: 0.21172469854354858\n",
      "training: 69 batch 557 batch_loss: 0.2154572606086731\n",
      "training: 69 batch 558 batch_loss: 0.21054798364639282\n",
      "training: 69 batch 559 batch_loss: 0.21565228700637817\n",
      "training: 69 batch 560 batch_loss: 0.21105006337165833\n",
      "training: 69 batch 561 batch_loss: 0.21493080258369446\n",
      "training: 69 batch 562 batch_loss: 0.21344774961471558\n",
      "training: 69 batch 563 batch_loss: 0.21465429663658142\n",
      "training: 69 batch 564 batch_loss: 0.21239471435546875\n",
      "training: 69 batch 565 batch_loss: 0.2138383686542511\n",
      "training: 69 batch 566 batch_loss: 0.21810263395309448\n",
      "training: 69 batch 567 batch_loss: 0.21889427304267883\n",
      "training: 69 batch 568 batch_loss: 0.20977798104286194\n",
      "training: 69 batch 569 batch_loss: 0.2132594883441925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 69 batch 570 batch_loss: 0.2137758731842041\n",
      "training: 69 batch 571 batch_loss: 0.2145899534225464\n",
      "training: 69 batch 572 batch_loss: 0.2140965461730957\n",
      "training: 69 batch 573 batch_loss: 0.2141287624835968\n",
      "training: 69 batch 574 batch_loss: 0.21060726046562195\n",
      "training: 69 batch 575 batch_loss: 0.21358197927474976\n",
      "training: 69 batch 576 batch_loss: 0.22037506103515625\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 69, Hit Ratio:0.03334854576888868 | Precision:0.04920377469772928 | Recall:0.06499431647070661 | NDCG:0.06396839320991751\n",
      "*Best Performance* \n",
      "Epoch: 68, Hit Ratio:0.03353176123360638 | Precision:0.04947409810282119 | Recall:0.06511509304686922 | MDCG:0.0641621394246606\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 70 batch 0 batch_loss: 0.2084560990333557\n",
      "training: 70 batch 1 batch_loss: 0.20949915051460266\n",
      "training: 70 batch 2 batch_loss: 0.2117844521999359\n",
      "training: 70 batch 3 batch_loss: 0.21324613690376282\n",
      "training: 70 batch 4 batch_loss: 0.21634343266487122\n",
      "training: 70 batch 5 batch_loss: 0.21142441034317017\n",
      "training: 70 batch 6 batch_loss: 0.21410101652145386\n",
      "training: 70 batch 7 batch_loss: 0.21231794357299805\n",
      "training: 70 batch 8 batch_loss: 0.21072891354560852\n",
      "training: 70 batch 9 batch_loss: 0.20983561873435974\n",
      "training: 70 batch 10 batch_loss: 0.21562963724136353\n",
      "training: 70 batch 11 batch_loss: 0.21281522512435913\n",
      "training: 70 batch 12 batch_loss: 0.21195098757743835\n",
      "training: 70 batch 13 batch_loss: 0.20749202370643616\n",
      "training: 70 batch 14 batch_loss: 0.2075696885585785\n",
      "training: 70 batch 15 batch_loss: 0.20748978853225708\n",
      "training: 70 batch 16 batch_loss: 0.20773515105247498\n",
      "training: 70 batch 17 batch_loss: 0.2130412757396698\n",
      "training: 70 batch 18 batch_loss: 0.20839419960975647\n",
      "training: 70 batch 19 batch_loss: 0.2117461860179901\n",
      "training: 70 batch 20 batch_loss: 0.20616930723190308\n",
      "training: 70 batch 21 batch_loss: 0.20808184146881104\n",
      "training: 70 batch 22 batch_loss: 0.21118727326393127\n",
      "training: 70 batch 23 batch_loss: 0.20572656393051147\n",
      "training: 70 batch 24 batch_loss: 0.21282139420509338\n",
      "training: 70 batch 25 batch_loss: 0.21308159828186035\n",
      "training: 70 batch 26 batch_loss: 0.2134629487991333\n",
      "training: 70 batch 27 batch_loss: 0.21271628141403198\n",
      "training: 70 batch 28 batch_loss: 0.2171505093574524\n",
      "training: 70 batch 29 batch_loss: 0.20997384190559387\n",
      "training: 70 batch 30 batch_loss: 0.20942652225494385\n",
      "training: 70 batch 31 batch_loss: 0.20964348316192627\n",
      "training: 70 batch 32 batch_loss: 0.21002987027168274\n",
      "training: 70 batch 33 batch_loss: 0.20948243141174316\n",
      "training: 70 batch 34 batch_loss: 0.2071881890296936\n",
      "training: 70 batch 35 batch_loss: 0.2105158269405365\n",
      "training: 70 batch 36 batch_loss: 0.21358561515808105\n",
      "training: 70 batch 37 batch_loss: 0.2117873728275299\n",
      "training: 70 batch 38 batch_loss: 0.20701712369918823\n",
      "training: 70 batch 39 batch_loss: 0.21503278613090515\n",
      "training: 70 batch 40 batch_loss: 0.21351706981658936\n",
      "training: 70 batch 41 batch_loss: 0.21197542548179626\n",
      "training: 70 batch 42 batch_loss: 0.21384698152542114\n",
      "training: 70 batch 43 batch_loss: 0.21073251962661743\n",
      "training: 70 batch 44 batch_loss: 0.2132701277732849\n",
      "training: 70 batch 45 batch_loss: 0.214799702167511\n",
      "training: 70 batch 46 batch_loss: 0.20885878801345825\n",
      "training: 70 batch 47 batch_loss: 0.21361932158470154\n",
      "training: 70 batch 48 batch_loss: 0.21294981241226196\n",
      "training: 70 batch 49 batch_loss: 0.21069157123565674\n",
      "training: 70 batch 50 batch_loss: 0.20894107222557068\n",
      "training: 70 batch 51 batch_loss: 0.21420353651046753\n",
      "training: 70 batch 52 batch_loss: 0.21196386218070984\n",
      "training: 70 batch 53 batch_loss: 0.21514245867729187\n",
      "training: 70 batch 54 batch_loss: 0.21044570207595825\n",
      "training: 70 batch 55 batch_loss: 0.21049252152442932\n",
      "training: 70 batch 56 batch_loss: 0.2157352864742279\n",
      "training: 70 batch 57 batch_loss: 0.21469542384147644\n",
      "training: 70 batch 58 batch_loss: 0.21398240327835083\n",
      "training: 70 batch 59 batch_loss: 0.21282178163528442\n",
      "training: 70 batch 60 batch_loss: 0.21465522050857544\n",
      "training: 70 batch 61 batch_loss: 0.21156975626945496\n",
      "training: 70 batch 62 batch_loss: 0.2087695598602295\n",
      "training: 70 batch 63 batch_loss: 0.21159440279006958\n",
      "training: 70 batch 64 batch_loss: 0.20874452590942383\n",
      "training: 70 batch 65 batch_loss: 0.2084294557571411\n",
      "training: 70 batch 66 batch_loss: 0.21576201915740967\n",
      "training: 70 batch 67 batch_loss: 0.20967215299606323\n",
      "training: 70 batch 68 batch_loss: 0.21003872156143188\n",
      "training: 70 batch 69 batch_loss: 0.2095327377319336\n",
      "training: 70 batch 70 batch_loss: 0.21265271306037903\n",
      "training: 70 batch 71 batch_loss: 0.20838654041290283\n",
      "training: 70 batch 72 batch_loss: 0.20950037240982056\n",
      "training: 70 batch 73 batch_loss: 0.21348881721496582\n",
      "training: 70 batch 74 batch_loss: 0.21336930990219116\n",
      "training: 70 batch 75 batch_loss: 0.21227136254310608\n",
      "training: 70 batch 76 batch_loss: 0.20907694101333618\n",
      "training: 70 batch 77 batch_loss: 0.21332812309265137\n",
      "training: 70 batch 78 batch_loss: 0.2098950743675232\n",
      "training: 70 batch 79 batch_loss: 0.21058881282806396\n",
      "training: 70 batch 80 batch_loss: 0.21635085344314575\n",
      "training: 70 batch 81 batch_loss: 0.20898222923278809\n",
      "training: 70 batch 82 batch_loss: 0.20702970027923584\n",
      "training: 70 batch 83 batch_loss: 0.21062466502189636\n",
      "training: 70 batch 84 batch_loss: 0.21134567260742188\n",
      "training: 70 batch 85 batch_loss: 0.21139025688171387\n",
      "training: 70 batch 86 batch_loss: 0.21241596341133118\n",
      "training: 70 batch 87 batch_loss: 0.21315506100654602\n",
      "training: 70 batch 88 batch_loss: 0.20825040340423584\n",
      "training: 70 batch 89 batch_loss: 0.21651464700698853\n",
      "training: 70 batch 90 batch_loss: 0.20475569367408752\n",
      "training: 70 batch 91 batch_loss: 0.21084988117218018\n",
      "training: 70 batch 92 batch_loss: 0.2124636173248291\n",
      "training: 70 batch 93 batch_loss: 0.21563643217086792\n",
      "training: 70 batch 94 batch_loss: 0.21051472425460815\n",
      "training: 70 batch 95 batch_loss: 0.21316850185394287\n",
      "training: 70 batch 96 batch_loss: 0.21327674388885498\n",
      "training: 70 batch 97 batch_loss: 0.21276143193244934\n",
      "training: 70 batch 98 batch_loss: 0.2119724154472351\n",
      "training: 70 batch 99 batch_loss: 0.21472561359405518\n",
      "training: 70 batch 100 batch_loss: 0.21450015902519226\n",
      "training: 70 batch 101 batch_loss: 0.20809721946716309\n",
      "training: 70 batch 102 batch_loss: 0.21412578225135803\n",
      "training: 70 batch 103 batch_loss: 0.21080231666564941\n",
      "training: 70 batch 104 batch_loss: 0.2118529975414276\n",
      "training: 70 batch 105 batch_loss: 0.20949020981788635\n",
      "training: 70 batch 106 batch_loss: 0.21312260627746582\n",
      "training: 70 batch 107 batch_loss: 0.21281170845031738\n",
      "training: 70 batch 108 batch_loss: 0.2096986174583435\n",
      "training: 70 batch 109 batch_loss: 0.21321380138397217\n",
      "training: 70 batch 110 batch_loss: 0.21201974153518677\n",
      "training: 70 batch 111 batch_loss: 0.20928895473480225\n",
      "training: 70 batch 112 batch_loss: 0.21505168080329895\n",
      "training: 70 batch 113 batch_loss: 0.2117132544517517\n",
      "training: 70 batch 114 batch_loss: 0.21272248029708862\n",
      "training: 70 batch 115 batch_loss: 0.20957857370376587\n",
      "training: 70 batch 116 batch_loss: 0.20773613452911377\n",
      "training: 70 batch 117 batch_loss: 0.21431666612625122\n",
      "training: 70 batch 118 batch_loss: 0.2116931676864624\n",
      "training: 70 batch 119 batch_loss: 0.2094498872756958\n",
      "training: 70 batch 120 batch_loss: 0.20962491631507874\n",
      "training: 70 batch 121 batch_loss: 0.21015295386314392\n",
      "training: 70 batch 122 batch_loss: 0.20952248573303223\n",
      "training: 70 batch 123 batch_loss: 0.21124151349067688\n",
      "training: 70 batch 124 batch_loss: 0.21175715327262878\n",
      "training: 70 batch 125 batch_loss: 0.2137880027294159\n",
      "training: 70 batch 126 batch_loss: 0.20993459224700928\n",
      "training: 70 batch 127 batch_loss: 0.20644918084144592\n",
      "training: 70 batch 128 batch_loss: 0.20998415350914001\n",
      "training: 70 batch 129 batch_loss: 0.2094307243824005\n",
      "training: 70 batch 130 batch_loss: 0.21725493669509888\n",
      "training: 70 batch 131 batch_loss: 0.2106042504310608\n",
      "training: 70 batch 132 batch_loss: 0.21445247530937195\n",
      "training: 70 batch 133 batch_loss: 0.21466359496116638\n",
      "training: 70 batch 134 batch_loss: 0.21806401014328003\n",
      "training: 70 batch 135 batch_loss: 0.21343514323234558\n",
      "training: 70 batch 136 batch_loss: 0.21002179384231567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 batch 137 batch_loss: 0.21117204427719116\n",
      "training: 70 batch 138 batch_loss: 0.20736467838287354\n",
      "training: 70 batch 139 batch_loss: 0.2100943624973297\n",
      "training: 70 batch 140 batch_loss: 0.21003222465515137\n",
      "training: 70 batch 141 batch_loss: 0.21004068851470947\n",
      "training: 70 batch 142 batch_loss: 0.21390509605407715\n",
      "training: 70 batch 143 batch_loss: 0.21186327934265137\n",
      "training: 70 batch 144 batch_loss: 0.21027106046676636\n",
      "training: 70 batch 145 batch_loss: 0.21403715014457703\n",
      "training: 70 batch 146 batch_loss: 0.21426033973693848\n",
      "training: 70 batch 147 batch_loss: 0.2135714590549469\n",
      "training: 70 batch 148 batch_loss: 0.21184277534484863\n",
      "training: 70 batch 149 batch_loss: 0.21392172574996948\n",
      "training: 70 batch 150 batch_loss: 0.21074187755584717\n",
      "training: 70 batch 151 batch_loss: 0.21356192231178284\n",
      "training: 70 batch 152 batch_loss: 0.21203306317329407\n",
      "training: 70 batch 153 batch_loss: 0.2149985432624817\n",
      "training: 70 batch 154 batch_loss: 0.21326008439064026\n",
      "training: 70 batch 155 batch_loss: 0.2119382619857788\n",
      "training: 70 batch 156 batch_loss: 0.2134419083595276\n",
      "training: 70 batch 157 batch_loss: 0.21184667944908142\n",
      "training: 70 batch 158 batch_loss: 0.21085447072982788\n",
      "training: 70 batch 159 batch_loss: 0.21086609363555908\n",
      "training: 70 batch 160 batch_loss: 0.20955926179885864\n",
      "training: 70 batch 161 batch_loss: 0.21263861656188965\n",
      "training: 70 batch 162 batch_loss: 0.21269115805625916\n",
      "training: 70 batch 163 batch_loss: 0.2094472348690033\n",
      "training: 70 batch 164 batch_loss: 0.21368253231048584\n",
      "training: 70 batch 165 batch_loss: 0.21257328987121582\n",
      "training: 70 batch 166 batch_loss: 0.21142420172691345\n",
      "training: 70 batch 167 batch_loss: 0.22054922580718994\n",
      "training: 70 batch 168 batch_loss: 0.21112996339797974\n",
      "training: 70 batch 169 batch_loss: 0.2132345736026764\n",
      "training: 70 batch 170 batch_loss: 0.21263733506202698\n",
      "training: 70 batch 171 batch_loss: 0.21392184495925903\n",
      "training: 70 batch 172 batch_loss: 0.2163524627685547\n",
      "training: 70 batch 173 batch_loss: 0.2118656039237976\n",
      "training: 70 batch 174 batch_loss: 0.21430858969688416\n",
      "training: 70 batch 175 batch_loss: 0.21028736233711243\n",
      "training: 70 batch 176 batch_loss: 0.21693125367164612\n",
      "training: 70 batch 177 batch_loss: 0.21359437704086304\n",
      "training: 70 batch 178 batch_loss: 0.21405017375946045\n",
      "training: 70 batch 179 batch_loss: 0.20942610502243042\n",
      "training: 70 batch 180 batch_loss: 0.21504050493240356\n",
      "training: 70 batch 181 batch_loss: 0.21360993385314941\n",
      "training: 70 batch 182 batch_loss: 0.21200141310691833\n",
      "training: 70 batch 183 batch_loss: 0.21329089999198914\n",
      "training: 70 batch 184 batch_loss: 0.21753105521202087\n",
      "training: 70 batch 185 batch_loss: 0.20998287200927734\n",
      "training: 70 batch 186 batch_loss: 0.20907577872276306\n",
      "training: 70 batch 187 batch_loss: 0.2088167667388916\n",
      "training: 70 batch 188 batch_loss: 0.21483242511749268\n",
      "training: 70 batch 189 batch_loss: 0.2094530165195465\n",
      "training: 70 batch 190 batch_loss: 0.21641182899475098\n",
      "training: 70 batch 191 batch_loss: 0.21330475807189941\n",
      "training: 70 batch 192 batch_loss: 0.21560508012771606\n",
      "training: 70 batch 193 batch_loss: 0.20969638228416443\n",
      "training: 70 batch 194 batch_loss: 0.2125730812549591\n",
      "training: 70 batch 195 batch_loss: 0.21390098333358765\n",
      "training: 70 batch 196 batch_loss: 0.21020904183387756\n",
      "training: 70 batch 197 batch_loss: 0.212958425283432\n",
      "training: 70 batch 198 batch_loss: 0.21549248695373535\n",
      "training: 70 batch 199 batch_loss: 0.2113932967185974\n",
      "training: 70 batch 200 batch_loss: 0.21381241083145142\n",
      "training: 70 batch 201 batch_loss: 0.21597161889076233\n",
      "training: 70 batch 202 batch_loss: 0.21576905250549316\n",
      "training: 70 batch 203 batch_loss: 0.21426606178283691\n",
      "training: 70 batch 204 batch_loss: 0.21093156933784485\n",
      "training: 70 batch 205 batch_loss: 0.2109028697013855\n",
      "training: 70 batch 206 batch_loss: 0.21173173189163208\n",
      "training: 70 batch 207 batch_loss: 0.21428006887435913\n",
      "training: 70 batch 208 batch_loss: 0.21002376079559326\n",
      "training: 70 batch 209 batch_loss: 0.21388429403305054\n",
      "training: 70 batch 210 batch_loss: 0.21435391902923584\n",
      "training: 70 batch 211 batch_loss: 0.21086597442626953\n",
      "training: 70 batch 212 batch_loss: 0.20900240540504456\n",
      "training: 70 batch 213 batch_loss: 0.2094373106956482\n",
      "training: 70 batch 214 batch_loss: 0.213643878698349\n",
      "training: 70 batch 215 batch_loss: 0.21210601925849915\n",
      "training: 70 batch 216 batch_loss: 0.21696969866752625\n",
      "training: 70 batch 217 batch_loss: 0.2114173173904419\n",
      "training: 70 batch 218 batch_loss: 0.2089729905128479\n",
      "training: 70 batch 219 batch_loss: 0.21495753526687622\n",
      "training: 70 batch 220 batch_loss: 0.21367278695106506\n",
      "training: 70 batch 221 batch_loss: 0.2116101086139679\n",
      "training: 70 batch 222 batch_loss: 0.21123820543289185\n",
      "training: 70 batch 223 batch_loss: 0.21618840098381042\n",
      "training: 70 batch 224 batch_loss: 0.21510177850723267\n",
      "training: 70 batch 225 batch_loss: 0.21384412050247192\n",
      "training: 70 batch 226 batch_loss: 0.21399328112602234\n",
      "training: 70 batch 227 batch_loss: 0.2084348499774933\n",
      "training: 70 batch 228 batch_loss: 0.2136792540550232\n",
      "training: 70 batch 229 batch_loss: 0.21226927638053894\n",
      "training: 70 batch 230 batch_loss: 0.2109222412109375\n",
      "training: 70 batch 231 batch_loss: 0.21175497770309448\n",
      "training: 70 batch 232 batch_loss: 0.21199393272399902\n",
      "training: 70 batch 233 batch_loss: 0.20853155851364136\n",
      "training: 70 batch 234 batch_loss: 0.21730488538742065\n",
      "training: 70 batch 235 batch_loss: 0.2136879563331604\n",
      "training: 70 batch 236 batch_loss: 0.21294870972633362\n",
      "training: 70 batch 237 batch_loss: 0.21389412879943848\n",
      "training: 70 batch 238 batch_loss: 0.21440285444259644\n",
      "training: 70 batch 239 batch_loss: 0.21021437644958496\n",
      "training: 70 batch 240 batch_loss: 0.21321675181388855\n",
      "training: 70 batch 241 batch_loss: 0.21510383486747742\n",
      "training: 70 batch 242 batch_loss: 0.21092060208320618\n",
      "training: 70 batch 243 batch_loss: 0.20989230275154114\n",
      "training: 70 batch 244 batch_loss: 0.2098718285560608\n",
      "training: 70 batch 245 batch_loss: 0.2105419933795929\n",
      "training: 70 batch 246 batch_loss: 0.20957231521606445\n",
      "training: 70 batch 247 batch_loss: 0.21544432640075684\n",
      "training: 70 batch 248 batch_loss: 0.21518820524215698\n",
      "training: 70 batch 249 batch_loss: 0.21354940533638\n",
      "training: 70 batch 250 batch_loss: 0.21198466420173645\n",
      "training: 70 batch 251 batch_loss: 0.2054884433746338\n",
      "training: 70 batch 252 batch_loss: 0.21785151958465576\n",
      "training: 70 batch 253 batch_loss: 0.21160081028938293\n",
      "training: 70 batch 254 batch_loss: 0.21370577812194824\n",
      "training: 70 batch 255 batch_loss: 0.21080949902534485\n",
      "training: 70 batch 256 batch_loss: 0.21824216842651367\n",
      "training: 70 batch 257 batch_loss: 0.21358197927474976\n",
      "training: 70 batch 258 batch_loss: 0.2111700177192688\n",
      "training: 70 batch 259 batch_loss: 0.20962172746658325\n",
      "training: 70 batch 260 batch_loss: 0.2151569128036499\n",
      "training: 70 batch 261 batch_loss: 0.21058309078216553\n",
      "training: 70 batch 262 batch_loss: 0.21612519025802612\n",
      "training: 70 batch 263 batch_loss: 0.21387025713920593\n",
      "training: 70 batch 264 batch_loss: 0.20902925729751587\n",
      "training: 70 batch 265 batch_loss: 0.21845605969429016\n",
      "training: 70 batch 266 batch_loss: 0.2168813943862915\n",
      "training: 70 batch 267 batch_loss: 0.2124347686767578\n",
      "training: 70 batch 268 batch_loss: 0.21124878525733948\n",
      "training: 70 batch 269 batch_loss: 0.2140350341796875\n",
      "training: 70 batch 270 batch_loss: 0.21445953845977783\n",
      "training: 70 batch 271 batch_loss: 0.21131950616836548\n",
      "training: 70 batch 272 batch_loss: 0.2140686810016632\n",
      "training: 70 batch 273 batch_loss: 0.21460285782814026\n",
      "training: 70 batch 274 batch_loss: 0.21131980419158936\n",
      "training: 70 batch 275 batch_loss: 0.21577119827270508\n",
      "training: 70 batch 276 batch_loss: 0.21231380105018616\n",
      "training: 70 batch 277 batch_loss: 0.20972225069999695\n",
      "training: 70 batch 278 batch_loss: 0.2151288092136383\n",
      "training: 70 batch 279 batch_loss: 0.2126012146472931\n",
      "training: 70 batch 280 batch_loss: 0.21461358666419983\n",
      "training: 70 batch 281 batch_loss: 0.21266070008277893\n",
      "training: 70 batch 282 batch_loss: 0.21338042616844177\n",
      "training: 70 batch 283 batch_loss: 0.2149912416934967\n",
      "training: 70 batch 284 batch_loss: 0.21434739232063293\n",
      "training: 70 batch 285 batch_loss: 0.2137853503227234\n",
      "training: 70 batch 286 batch_loss: 0.2145029902458191\n",
      "training: 70 batch 287 batch_loss: 0.21389004588127136\n",
      "training: 70 batch 288 batch_loss: 0.21277275681495667\n",
      "training: 70 batch 289 batch_loss: 0.22052159905433655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 batch 290 batch_loss: 0.21513059735298157\n",
      "training: 70 batch 291 batch_loss: 0.2160356640815735\n",
      "training: 70 batch 292 batch_loss: 0.212155282497406\n",
      "training: 70 batch 293 batch_loss: 0.21155807375907898\n",
      "training: 70 batch 294 batch_loss: 0.21944698691368103\n",
      "training: 70 batch 295 batch_loss: 0.21441984176635742\n",
      "training: 70 batch 296 batch_loss: 0.21448418498039246\n",
      "training: 70 batch 297 batch_loss: 0.20808398723602295\n",
      "training: 70 batch 298 batch_loss: 0.21130895614624023\n",
      "training: 70 batch 299 batch_loss: 0.21258828043937683\n",
      "training: 70 batch 300 batch_loss: 0.21199524402618408\n",
      "training: 70 batch 301 batch_loss: 0.21530258655548096\n",
      "training: 70 batch 302 batch_loss: 0.21452757716178894\n",
      "training: 70 batch 303 batch_loss: 0.21152055263519287\n",
      "training: 70 batch 304 batch_loss: 0.21767529845237732\n",
      "training: 70 batch 305 batch_loss: 0.21178078651428223\n",
      "training: 70 batch 306 batch_loss: 0.21035462617874146\n",
      "training: 70 batch 307 batch_loss: 0.21895796060562134\n",
      "training: 70 batch 308 batch_loss: 0.21789228916168213\n",
      "training: 70 batch 309 batch_loss: 0.21172410249710083\n",
      "training: 70 batch 310 batch_loss: 0.20946121215820312\n",
      "training: 70 batch 311 batch_loss: 0.21433374285697937\n",
      "training: 70 batch 312 batch_loss: 0.20969054102897644\n",
      "training: 70 batch 313 batch_loss: 0.21294862031936646\n",
      "training: 70 batch 314 batch_loss: 0.21221619844436646\n",
      "training: 70 batch 315 batch_loss: 0.2110714316368103\n",
      "training: 70 batch 316 batch_loss: 0.21513023972511292\n",
      "training: 70 batch 317 batch_loss: 0.21130219101905823\n",
      "training: 70 batch 318 batch_loss: 0.21331822872161865\n",
      "training: 70 batch 319 batch_loss: 0.21363812685012817\n",
      "training: 70 batch 320 batch_loss: 0.21339917182922363\n",
      "training: 70 batch 321 batch_loss: 0.2111952006816864\n",
      "training: 70 batch 322 batch_loss: 0.21383368968963623\n",
      "training: 70 batch 323 batch_loss: 0.2133190631866455\n",
      "training: 70 batch 324 batch_loss: 0.20913735032081604\n",
      "training: 70 batch 325 batch_loss: 0.21283617615699768\n",
      "training: 70 batch 326 batch_loss: 0.20926272869110107\n",
      "training: 70 batch 327 batch_loss: 0.21587702631950378\n",
      "training: 70 batch 328 batch_loss: 0.2113301157951355\n",
      "training: 70 batch 329 batch_loss: 0.2128533124923706\n",
      "training: 70 batch 330 batch_loss: 0.21142813563346863\n",
      "training: 70 batch 331 batch_loss: 0.2113889455795288\n",
      "training: 70 batch 332 batch_loss: 0.2103882133960724\n",
      "training: 70 batch 333 batch_loss: 0.2102045714855194\n",
      "training: 70 batch 334 batch_loss: 0.21381646394729614\n",
      "training: 70 batch 335 batch_loss: 0.2139238715171814\n",
      "training: 70 batch 336 batch_loss: 0.21556782722473145\n",
      "training: 70 batch 337 batch_loss: 0.21060439944267273\n",
      "training: 70 batch 338 batch_loss: 0.21725937724113464\n",
      "training: 70 batch 339 batch_loss: 0.21448257565498352\n",
      "training: 70 batch 340 batch_loss: 0.21267223358154297\n",
      "training: 70 batch 341 batch_loss: 0.2104371190071106\n",
      "training: 70 batch 342 batch_loss: 0.21836930513381958\n",
      "training: 70 batch 343 batch_loss: 0.21817058324813843\n",
      "training: 70 batch 344 batch_loss: 0.21696439385414124\n",
      "training: 70 batch 345 batch_loss: 0.21282580494880676\n",
      "training: 70 batch 346 batch_loss: 0.2125803530216217\n",
      "training: 70 batch 347 batch_loss: 0.21280953288078308\n",
      "training: 70 batch 348 batch_loss: 0.2097075879573822\n",
      "training: 70 batch 349 batch_loss: 0.21455368399620056\n",
      "training: 70 batch 350 batch_loss: 0.2126486599445343\n",
      "training: 70 batch 351 batch_loss: 0.20799043774604797\n",
      "training: 70 batch 352 batch_loss: 0.21388256549835205\n",
      "training: 70 batch 353 batch_loss: 0.2107374370098114\n",
      "training: 70 batch 354 batch_loss: 0.21466392278671265\n",
      "training: 70 batch 355 batch_loss: 0.21438419818878174\n",
      "training: 70 batch 356 batch_loss: 0.21477577090263367\n",
      "training: 70 batch 357 batch_loss: 0.21102705597877502\n",
      "training: 70 batch 358 batch_loss: 0.21349066495895386\n",
      "training: 70 batch 359 batch_loss: 0.2165709137916565\n",
      "training: 70 batch 360 batch_loss: 0.21741312742233276\n",
      "training: 70 batch 361 batch_loss: 0.2135549783706665\n",
      "training: 70 batch 362 batch_loss: 0.21162626147270203\n",
      "training: 70 batch 363 batch_loss: 0.21169382333755493\n",
      "training: 70 batch 364 batch_loss: 0.21036216616630554\n",
      "training: 70 batch 365 batch_loss: 0.21120348572731018\n",
      "training: 70 batch 366 batch_loss: 0.21885430812835693\n",
      "training: 70 batch 367 batch_loss: 0.21599015593528748\n",
      "training: 70 batch 368 batch_loss: 0.21287322044372559\n",
      "training: 70 batch 369 batch_loss: 0.2172490358352661\n",
      "training: 70 batch 370 batch_loss: 0.211667001247406\n",
      "training: 70 batch 371 batch_loss: 0.21515235304832458\n",
      "training: 70 batch 372 batch_loss: 0.2161567211151123\n",
      "training: 70 batch 373 batch_loss: 0.21697920560836792\n",
      "training: 70 batch 374 batch_loss: 0.21090710163116455\n",
      "training: 70 batch 375 batch_loss: 0.21341657638549805\n",
      "training: 70 batch 376 batch_loss: 0.21456506848335266\n",
      "training: 70 batch 377 batch_loss: 0.21005547046661377\n",
      "training: 70 batch 378 batch_loss: 0.21381273865699768\n",
      "training: 70 batch 379 batch_loss: 0.2140086591243744\n",
      "training: 70 batch 380 batch_loss: 0.20966175198554993\n",
      "training: 70 batch 381 batch_loss: 0.2120736837387085\n",
      "training: 70 batch 382 batch_loss: 0.21363264322280884\n",
      "training: 70 batch 383 batch_loss: 0.2114727795124054\n",
      "training: 70 batch 384 batch_loss: 0.2158779799938202\n",
      "training: 70 batch 385 batch_loss: 0.21275758743286133\n",
      "training: 70 batch 386 batch_loss: 0.2170090675354004\n",
      "training: 70 batch 387 batch_loss: 0.21757975220680237\n",
      "training: 70 batch 388 batch_loss: 0.2100515365600586\n",
      "training: 70 batch 389 batch_loss: 0.21282044053077698\n",
      "training: 70 batch 390 batch_loss: 0.21058520674705505\n",
      "training: 70 batch 391 batch_loss: 0.21286797523498535\n",
      "training: 70 batch 392 batch_loss: 0.21085822582244873\n",
      "training: 70 batch 393 batch_loss: 0.21138879656791687\n",
      "training: 70 batch 394 batch_loss: 0.21645328402519226\n",
      "training: 70 batch 395 batch_loss: 0.21271947026252747\n",
      "training: 70 batch 396 batch_loss: 0.21500137448310852\n",
      "training: 70 batch 397 batch_loss: 0.21158945560455322\n",
      "training: 70 batch 398 batch_loss: 0.21499788761138916\n",
      "training: 70 batch 399 batch_loss: 0.21258121728897095\n",
      "training: 70 batch 400 batch_loss: 0.21495670080184937\n",
      "training: 70 batch 401 batch_loss: 0.21244603395462036\n",
      "training: 70 batch 402 batch_loss: 0.21624347567558289\n",
      "training: 70 batch 403 batch_loss: 0.21174165606498718\n",
      "training: 70 batch 404 batch_loss: 0.21353313326835632\n",
      "training: 70 batch 405 batch_loss: 0.21180549263954163\n",
      "training: 70 batch 406 batch_loss: 0.21516239643096924\n",
      "training: 70 batch 407 batch_loss: 0.2124251425266266\n",
      "training: 70 batch 408 batch_loss: 0.21259433031082153\n",
      "training: 70 batch 409 batch_loss: 0.2177354097366333\n",
      "training: 70 batch 410 batch_loss: 0.20894888043403625\n",
      "training: 70 batch 411 batch_loss: 0.21042782068252563\n",
      "training: 70 batch 412 batch_loss: 0.21685093641281128\n",
      "training: 70 batch 413 batch_loss: 0.2102225124835968\n",
      "training: 70 batch 414 batch_loss: 0.21378293633460999\n",
      "training: 70 batch 415 batch_loss: 0.2071194350719452\n",
      "training: 70 batch 416 batch_loss: 0.2167414426803589\n",
      "training: 70 batch 417 batch_loss: 0.21583083271980286\n",
      "training: 70 batch 418 batch_loss: 0.21547013521194458\n",
      "training: 70 batch 419 batch_loss: 0.21452942490577698\n",
      "training: 70 batch 420 batch_loss: 0.21357974410057068\n",
      "training: 70 batch 421 batch_loss: 0.2128395140171051\n",
      "training: 70 batch 422 batch_loss: 0.21707037091255188\n",
      "training: 70 batch 423 batch_loss: 0.2146790623664856\n",
      "training: 70 batch 424 batch_loss: 0.21305733919143677\n",
      "training: 70 batch 425 batch_loss: 0.21225818991661072\n",
      "training: 70 batch 426 batch_loss: 0.2114788293838501\n",
      "training: 70 batch 427 batch_loss: 0.21816322207450867\n",
      "training: 70 batch 428 batch_loss: 0.21450713276863098\n",
      "training: 70 batch 429 batch_loss: 0.21243232488632202\n",
      "training: 70 batch 430 batch_loss: 0.21559280157089233\n",
      "training: 70 batch 431 batch_loss: 0.21106669306755066\n",
      "training: 70 batch 432 batch_loss: 0.21373096108436584\n",
      "training: 70 batch 433 batch_loss: 0.21011608839035034\n",
      "training: 70 batch 434 batch_loss: 0.21594446897506714\n",
      "training: 70 batch 435 batch_loss: 0.20870983600616455\n",
      "training: 70 batch 436 batch_loss: 0.2144356369972229\n",
      "training: 70 batch 437 batch_loss: 0.21233582496643066\n",
      "training: 70 batch 438 batch_loss: 0.21129050850868225\n",
      "training: 70 batch 439 batch_loss: 0.21609002351760864\n",
      "training: 70 batch 440 batch_loss: 0.21469080448150635\n",
      "training: 70 batch 441 batch_loss: 0.2115073800086975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 70 batch 442 batch_loss: 0.2110196352005005\n",
      "training: 70 batch 443 batch_loss: 0.2099800705909729\n",
      "training: 70 batch 444 batch_loss: 0.21202632784843445\n",
      "training: 70 batch 445 batch_loss: 0.21424663066864014\n",
      "training: 70 batch 446 batch_loss: 0.21141493320465088\n",
      "training: 70 batch 447 batch_loss: 0.21228927373886108\n",
      "training: 70 batch 448 batch_loss: 0.21144181489944458\n",
      "training: 70 batch 449 batch_loss: 0.21437570452690125\n",
      "training: 70 batch 450 batch_loss: 0.2169634997844696\n",
      "training: 70 batch 451 batch_loss: 0.21314778923988342\n",
      "training: 70 batch 452 batch_loss: 0.21348372101783752\n",
      "training: 70 batch 453 batch_loss: 0.2129269242286682\n",
      "training: 70 batch 454 batch_loss: 0.2133857011795044\n",
      "training: 70 batch 455 batch_loss: 0.21407857537269592\n",
      "training: 70 batch 456 batch_loss: 0.21377810835838318\n",
      "training: 70 batch 457 batch_loss: 0.2183099389076233\n",
      "training: 70 batch 458 batch_loss: 0.21422842144966125\n",
      "training: 70 batch 459 batch_loss: 0.21310529112815857\n",
      "training: 70 batch 460 batch_loss: 0.2128908634185791\n",
      "training: 70 batch 461 batch_loss: 0.21362322568893433\n",
      "training: 70 batch 462 batch_loss: 0.21660077571868896\n",
      "training: 70 batch 463 batch_loss: 0.21400117874145508\n",
      "training: 70 batch 464 batch_loss: 0.212395578622818\n",
      "training: 70 batch 465 batch_loss: 0.2104489803314209\n",
      "training: 70 batch 466 batch_loss: 0.21260985732078552\n",
      "training: 70 batch 467 batch_loss: 0.21717658638954163\n",
      "training: 70 batch 468 batch_loss: 0.21339061856269836\n",
      "training: 70 batch 469 batch_loss: 0.21031957864761353\n",
      "training: 70 batch 470 batch_loss: 0.2146487832069397\n",
      "training: 70 batch 471 batch_loss: 0.21370863914489746\n",
      "training: 70 batch 472 batch_loss: 0.21773982048034668\n",
      "training: 70 batch 473 batch_loss: 0.21549981832504272\n",
      "training: 70 batch 474 batch_loss: 0.21175611019134521\n",
      "training: 70 batch 475 batch_loss: 0.21273279190063477\n",
      "training: 70 batch 476 batch_loss: 0.21783071756362915\n",
      "training: 70 batch 477 batch_loss: 0.21564549207687378\n",
      "training: 70 batch 478 batch_loss: 0.21228808164596558\n",
      "training: 70 batch 479 batch_loss: 0.2127014398574829\n",
      "training: 70 batch 480 batch_loss: 0.21392515301704407\n",
      "training: 70 batch 481 batch_loss: 0.20978134870529175\n",
      "training: 70 batch 482 batch_loss: 0.21693915128707886\n",
      "training: 70 batch 483 batch_loss: 0.21307316422462463\n",
      "training: 70 batch 484 batch_loss: 0.2132112681865692\n",
      "training: 70 batch 485 batch_loss: 0.21550071239471436\n",
      "training: 70 batch 486 batch_loss: 0.2105962038040161\n",
      "training: 70 batch 487 batch_loss: 0.21191370487213135\n",
      "training: 70 batch 488 batch_loss: 0.21847587823867798\n",
      "training: 70 batch 489 batch_loss: 0.21275225281715393\n",
      "training: 70 batch 490 batch_loss: 0.22032895684242249\n",
      "training: 70 batch 491 batch_loss: 0.20694339275360107\n",
      "training: 70 batch 492 batch_loss: 0.2115323543548584\n",
      "training: 70 batch 493 batch_loss: 0.21110191941261292\n",
      "training: 70 batch 494 batch_loss: 0.21231788396835327\n",
      "training: 70 batch 495 batch_loss: 0.21366167068481445\n",
      "training: 70 batch 496 batch_loss: 0.2168736457824707\n",
      "training: 70 batch 497 batch_loss: 0.21343070268630981\n",
      "training: 70 batch 498 batch_loss: 0.21218040585517883\n",
      "training: 70 batch 499 batch_loss: 0.2128496766090393\n",
      "training: 70 batch 500 batch_loss: 0.2138938009738922\n",
      "training: 70 batch 501 batch_loss: 0.21409833431243896\n",
      "training: 70 batch 502 batch_loss: 0.21267631649971008\n",
      "training: 70 batch 503 batch_loss: 0.2113601565361023\n",
      "training: 70 batch 504 batch_loss: 0.21151554584503174\n",
      "training: 70 batch 505 batch_loss: 0.21166688203811646\n",
      "training: 70 batch 506 batch_loss: 0.21515315771102905\n",
      "training: 70 batch 507 batch_loss: 0.21450433135032654\n",
      "training: 70 batch 508 batch_loss: 0.21342507004737854\n",
      "training: 70 batch 509 batch_loss: 0.20988565683364868\n",
      "training: 70 batch 510 batch_loss: 0.21204820275306702\n",
      "training: 70 batch 511 batch_loss: 0.21521592140197754\n",
      "training: 70 batch 512 batch_loss: 0.21438854932785034\n",
      "training: 70 batch 513 batch_loss: 0.21532702445983887\n",
      "training: 70 batch 514 batch_loss: 0.21219950914382935\n",
      "training: 70 batch 515 batch_loss: 0.2142840325832367\n",
      "training: 70 batch 516 batch_loss: 0.21452105045318604\n",
      "training: 70 batch 517 batch_loss: 0.21382880210876465\n",
      "training: 70 batch 518 batch_loss: 0.2161189317703247\n",
      "training: 70 batch 519 batch_loss: 0.21402543783187866\n",
      "training: 70 batch 520 batch_loss: 0.21848520636558533\n",
      "training: 70 batch 521 batch_loss: 0.21704241633415222\n",
      "training: 70 batch 522 batch_loss: 0.2130010426044464\n",
      "training: 70 batch 523 batch_loss: 0.2132965326309204\n",
      "training: 70 batch 524 batch_loss: 0.2136206030845642\n",
      "training: 70 batch 525 batch_loss: 0.21784162521362305\n",
      "training: 70 batch 526 batch_loss: 0.21288472414016724\n",
      "training: 70 batch 527 batch_loss: 0.2182624340057373\n",
      "training: 70 batch 528 batch_loss: 0.21565887331962585\n",
      "training: 70 batch 529 batch_loss: 0.21523988246917725\n",
      "training: 70 batch 530 batch_loss: 0.21588879823684692\n",
      "training: 70 batch 531 batch_loss: 0.21447518467903137\n",
      "training: 70 batch 532 batch_loss: 0.2188979685306549\n",
      "training: 70 batch 533 batch_loss: 0.2152027189731598\n",
      "training: 70 batch 534 batch_loss: 0.22068676352500916\n",
      "training: 70 batch 535 batch_loss: 0.21579596400260925\n",
      "training: 70 batch 536 batch_loss: 0.21226000785827637\n",
      "training: 70 batch 537 batch_loss: 0.21027448773384094\n",
      "training: 70 batch 538 batch_loss: 0.21683889627456665\n",
      "training: 70 batch 539 batch_loss: 0.20837128162384033\n",
      "training: 70 batch 540 batch_loss: 0.20763316750526428\n",
      "training: 70 batch 541 batch_loss: 0.20669180154800415\n",
      "training: 70 batch 542 batch_loss: 0.2140679657459259\n",
      "training: 70 batch 543 batch_loss: 0.2119002640247345\n",
      "training: 70 batch 544 batch_loss: 0.2153344452381134\n",
      "training: 70 batch 545 batch_loss: 0.21038013696670532\n",
      "training: 70 batch 546 batch_loss: 0.2165202796459198\n",
      "training: 70 batch 547 batch_loss: 0.2145964801311493\n",
      "training: 70 batch 548 batch_loss: 0.20846033096313477\n",
      "training: 70 batch 549 batch_loss: 0.21552368998527527\n",
      "training: 70 batch 550 batch_loss: 0.2124917209148407\n",
      "training: 70 batch 551 batch_loss: 0.21616071462631226\n",
      "training: 70 batch 552 batch_loss: 0.2147039771080017\n",
      "training: 70 batch 553 batch_loss: 0.21642374992370605\n",
      "training: 70 batch 554 batch_loss: 0.21109801530838013\n",
      "training: 70 batch 555 batch_loss: 0.21864265203475952\n",
      "training: 70 batch 556 batch_loss: 0.21608343720436096\n",
      "training: 70 batch 557 batch_loss: 0.2116740643978119\n",
      "training: 70 batch 558 batch_loss: 0.21561047434806824\n",
      "training: 70 batch 559 batch_loss: 0.2130906581878662\n",
      "training: 70 batch 560 batch_loss: 0.213914155960083\n",
      "training: 70 batch 561 batch_loss: 0.21496760845184326\n",
      "training: 70 batch 562 batch_loss: 0.2167869210243225\n",
      "training: 70 batch 563 batch_loss: 0.21217480301856995\n",
      "training: 70 batch 564 batch_loss: 0.210424542427063\n",
      "training: 70 batch 565 batch_loss: 0.2108498215675354\n",
      "training: 70 batch 566 batch_loss: 0.2149285078048706\n",
      "training: 70 batch 567 batch_loss: 0.21623364090919495\n",
      "training: 70 batch 568 batch_loss: 0.21139386296272278\n",
      "training: 70 batch 569 batch_loss: 0.21465149521827698\n",
      "training: 70 batch 570 batch_loss: 0.21401607990264893\n",
      "training: 70 batch 571 batch_loss: 0.21247389912605286\n",
      "training: 70 batch 572 batch_loss: 0.21211034059524536\n",
      "training: 70 batch 573 batch_loss: 0.21244728565216064\n",
      "training: 70 batch 574 batch_loss: 0.21743810176849365\n",
      "training: 70 batch 575 batch_loss: 0.21621090173721313\n",
      "training: 70 batch 576 batch_loss: 0.2113218903541565\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 70, Hit Ratio:0.0332152981581849 | Precision:0.04900717585766244 | Recall:0.06502445941751903 | NDCG:0.06380245270615073\n",
      "*Best Performance* \n",
      "Epoch: 68, Hit Ratio:0.03353176123360638 | Precision:0.04947409810282119 | Recall:0.06511509304686922 | MDCG:0.0641621394246606\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 71 batch 0 batch_loss: 0.20889422297477722\n",
      "training: 71 batch 1 batch_loss: 0.21120452880859375\n",
      "training: 71 batch 2 batch_loss: 0.2102872133255005\n",
      "training: 71 batch 3 batch_loss: 0.21258845925331116\n",
      "training: 71 batch 4 batch_loss: 0.2131110429763794\n",
      "training: 71 batch 5 batch_loss: 0.20939981937408447\n",
      "training: 71 batch 6 batch_loss: 0.210307776927948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 71 batch 7 batch_loss: 0.21353688836097717\n",
      "training: 71 batch 8 batch_loss: 0.21171069145202637\n",
      "training: 71 batch 9 batch_loss: 0.21216025948524475\n",
      "training: 71 batch 10 batch_loss: 0.21138820052146912\n",
      "training: 71 batch 11 batch_loss: 0.21164730191230774\n",
      "training: 71 batch 12 batch_loss: 0.21600809693336487\n",
      "training: 71 batch 13 batch_loss: 0.21778911352157593\n",
      "training: 71 batch 14 batch_loss: 0.21187034249305725\n",
      "training: 71 batch 15 batch_loss: 0.21492812037467957\n",
      "training: 71 batch 16 batch_loss: 0.21064108610153198\n",
      "training: 71 batch 17 batch_loss: 0.2054520547389984\n",
      "training: 71 batch 18 batch_loss: 0.21219250559806824\n",
      "training: 71 batch 19 batch_loss: 0.21234452724456787\n",
      "training: 71 batch 20 batch_loss: 0.21517908573150635\n",
      "training: 71 batch 21 batch_loss: 0.21377772092819214\n",
      "training: 71 batch 22 batch_loss: 0.2133426070213318\n",
      "training: 71 batch 23 batch_loss: 0.2106388509273529\n",
      "training: 71 batch 24 batch_loss: 0.21122297644615173\n",
      "training: 71 batch 25 batch_loss: 0.21340179443359375\n",
      "training: 71 batch 26 batch_loss: 0.2110394835472107\n",
      "training: 71 batch 27 batch_loss: 0.20671969652175903\n",
      "training: 71 batch 28 batch_loss: 0.21453899145126343\n",
      "training: 71 batch 29 batch_loss: 0.21442902088165283\n",
      "training: 71 batch 30 batch_loss: 0.21449339389801025\n",
      "training: 71 batch 31 batch_loss: 0.21495270729064941\n",
      "training: 71 batch 32 batch_loss: 0.21498680114746094\n",
      "training: 71 batch 33 batch_loss: 0.20824280381202698\n",
      "training: 71 batch 34 batch_loss: 0.21530985832214355\n",
      "training: 71 batch 35 batch_loss: 0.2131577730178833\n",
      "training: 71 batch 36 batch_loss: 0.21757221221923828\n",
      "training: 71 batch 37 batch_loss: 0.2160072922706604\n",
      "training: 71 batch 38 batch_loss: 0.2144109010696411\n",
      "training: 71 batch 39 batch_loss: 0.2118358016014099\n",
      "training: 71 batch 40 batch_loss: 0.21460068225860596\n",
      "training: 71 batch 41 batch_loss: 0.21032604575157166\n",
      "training: 71 batch 42 batch_loss: 0.21289300918579102\n",
      "training: 71 batch 43 batch_loss: 0.2137543261051178\n",
      "training: 71 batch 44 batch_loss: 0.21268266439437866\n",
      "training: 71 batch 45 batch_loss: 0.21052345633506775\n",
      "training: 71 batch 46 batch_loss: 0.21407368779182434\n",
      "training: 71 batch 47 batch_loss: 0.21318739652633667\n",
      "training: 71 batch 48 batch_loss: 0.21382328867912292\n",
      "training: 71 batch 49 batch_loss: 0.21424958109855652\n",
      "training: 71 batch 50 batch_loss: 0.2135116159915924\n",
      "training: 71 batch 51 batch_loss: 0.21510246396064758\n",
      "training: 71 batch 52 batch_loss: 0.21320879459381104\n",
      "training: 71 batch 53 batch_loss: 0.21733462810516357\n",
      "training: 71 batch 54 batch_loss: 0.2141057848930359\n",
      "training: 71 batch 55 batch_loss: 0.2143208384513855\n",
      "training: 71 batch 56 batch_loss: 0.21681714057922363\n",
      "training: 71 batch 57 batch_loss: 0.21460404992103577\n",
      "training: 71 batch 58 batch_loss: 0.21371573209762573\n",
      "training: 71 batch 59 batch_loss: 0.21039795875549316\n",
      "training: 71 batch 60 batch_loss: 0.21148830652236938\n",
      "training: 71 batch 61 batch_loss: 0.2112433910369873\n",
      "training: 71 batch 62 batch_loss: 0.20767340064048767\n",
      "training: 71 batch 63 batch_loss: 0.21058589220046997\n",
      "training: 71 batch 64 batch_loss: 0.21102935075759888\n",
      "training: 71 batch 65 batch_loss: 0.21300366520881653\n",
      "training: 71 batch 66 batch_loss: 0.21489235758781433\n",
      "training: 71 batch 67 batch_loss: 0.2169029712677002\n",
      "training: 71 batch 68 batch_loss: 0.21095716953277588\n",
      "training: 71 batch 69 batch_loss: 0.21086889505386353\n",
      "training: 71 batch 70 batch_loss: 0.213259756565094\n",
      "training: 71 batch 71 batch_loss: 0.2102545201778412\n",
      "training: 71 batch 72 batch_loss: 0.21341073513031006\n",
      "training: 71 batch 73 batch_loss: 0.21439218521118164\n",
      "training: 71 batch 74 batch_loss: 0.21364542841911316\n",
      "training: 71 batch 75 batch_loss: 0.21247565746307373\n",
      "training: 71 batch 76 batch_loss: 0.21367263793945312\n",
      "training: 71 batch 77 batch_loss: 0.21120288968086243\n",
      "training: 71 batch 78 batch_loss: 0.2135411500930786\n",
      "training: 71 batch 79 batch_loss: 0.21630924940109253\n",
      "training: 71 batch 80 batch_loss: 0.2146632969379425\n",
      "training: 71 batch 81 batch_loss: 0.20946237444877625\n",
      "training: 71 batch 82 batch_loss: 0.21273797750473022\n",
      "training: 71 batch 83 batch_loss: 0.20648372173309326\n",
      "training: 71 batch 84 batch_loss: 0.2143864631652832\n",
      "training: 71 batch 85 batch_loss: 0.21065399050712585\n",
      "training: 71 batch 86 batch_loss: 0.2114182412624359\n",
      "training: 71 batch 87 batch_loss: 0.21519848704338074\n",
      "training: 71 batch 88 batch_loss: 0.21133118867874146\n",
      "training: 71 batch 89 batch_loss: 0.20996776223182678\n",
      "training: 71 batch 90 batch_loss: 0.20897263288497925\n",
      "training: 71 batch 91 batch_loss: 0.21340322494506836\n",
      "training: 71 batch 92 batch_loss: 0.216781884431839\n",
      "training: 71 batch 93 batch_loss: 0.21544042229652405\n",
      "training: 71 batch 94 batch_loss: 0.21258065104484558\n",
      "training: 71 batch 95 batch_loss: 0.2124204933643341\n",
      "training: 71 batch 96 batch_loss: 0.21394792199134827\n",
      "training: 71 batch 97 batch_loss: 0.20978179574012756\n",
      "training: 71 batch 98 batch_loss: 0.21653425693511963\n",
      "training: 71 batch 99 batch_loss: 0.2115306854248047\n",
      "training: 71 batch 100 batch_loss: 0.21252131462097168\n",
      "training: 71 batch 101 batch_loss: 0.20972135663032532\n",
      "training: 71 batch 102 batch_loss: 0.2165622115135193\n",
      "training: 71 batch 103 batch_loss: 0.21220433712005615\n",
      "training: 71 batch 104 batch_loss: 0.21485543251037598\n",
      "training: 71 batch 105 batch_loss: 0.2084815502166748\n",
      "training: 71 batch 106 batch_loss: 0.2140301764011383\n",
      "training: 71 batch 107 batch_loss: 0.21333318948745728\n",
      "training: 71 batch 108 batch_loss: 0.21172675490379333\n",
      "training: 71 batch 109 batch_loss: 0.2125663161277771\n",
      "training: 71 batch 110 batch_loss: 0.21113154292106628\n",
      "training: 71 batch 111 batch_loss: 0.2175123691558838\n",
      "training: 71 batch 112 batch_loss: 0.2138778567314148\n",
      "training: 71 batch 113 batch_loss: 0.21234560012817383\n",
      "training: 71 batch 114 batch_loss: 0.21402508020401\n",
      "training: 71 batch 115 batch_loss: 0.21379902958869934\n",
      "training: 71 batch 116 batch_loss: 0.213454931974411\n",
      "training: 71 batch 117 batch_loss: 0.2094193398952484\n",
      "training: 71 batch 118 batch_loss: 0.2154255211353302\n",
      "training: 71 batch 119 batch_loss: 0.2118489146232605\n",
      "training: 71 batch 120 batch_loss: 0.21206694841384888\n",
      "training: 71 batch 121 batch_loss: 0.2169809639453888\n",
      "training: 71 batch 122 batch_loss: 0.21327093243598938\n",
      "training: 71 batch 123 batch_loss: 0.21128401160240173\n",
      "training: 71 batch 124 batch_loss: 0.21107035875320435\n",
      "training: 71 batch 125 batch_loss: 0.21681874990463257\n",
      "training: 71 batch 126 batch_loss: 0.21245595812797546\n",
      "training: 71 batch 127 batch_loss: 0.2113248109817505\n",
      "training: 71 batch 128 batch_loss: 0.21376162767410278\n",
      "training: 71 batch 129 batch_loss: 0.20727571845054626\n",
      "training: 71 batch 130 batch_loss: 0.2129218578338623\n",
      "training: 71 batch 131 batch_loss: 0.2174481749534607\n",
      "training: 71 batch 132 batch_loss: 0.21427187323570251\n",
      "training: 71 batch 133 batch_loss: 0.21102014183998108\n",
      "training: 71 batch 134 batch_loss: 0.21514731645584106\n",
      "training: 71 batch 135 batch_loss: 0.21320748329162598\n",
      "training: 71 batch 136 batch_loss: 0.21298518776893616\n",
      "training: 71 batch 137 batch_loss: 0.21208232641220093\n",
      "training: 71 batch 138 batch_loss: 0.21338558197021484\n",
      "training: 71 batch 139 batch_loss: 0.2112005650997162\n",
      "training: 71 batch 140 batch_loss: 0.2146768867969513\n",
      "training: 71 batch 141 batch_loss: 0.21531903743743896\n",
      "training: 71 batch 142 batch_loss: 0.21114686131477356\n",
      "training: 71 batch 143 batch_loss: 0.21089652180671692\n",
      "training: 71 batch 144 batch_loss: 0.21401622891426086\n",
      "training: 71 batch 145 batch_loss: 0.2115831971168518\n",
      "training: 71 batch 146 batch_loss: 0.2157098352909088\n",
      "training: 71 batch 147 batch_loss: 0.21113264560699463\n",
      "training: 71 batch 148 batch_loss: 0.21170338988304138\n",
      "training: 71 batch 149 batch_loss: 0.21364638209342957\n",
      "training: 71 batch 150 batch_loss: 0.21144333481788635\n",
      "training: 71 batch 151 batch_loss: 0.21261188387870789\n",
      "training: 71 batch 152 batch_loss: 0.20877444744110107\n",
      "training: 71 batch 153 batch_loss: 0.21655717492103577\n",
      "training: 71 batch 154 batch_loss: 0.2128150463104248\n",
      "training: 71 batch 155 batch_loss: 0.21302539110183716\n",
      "training: 71 batch 156 batch_loss: 0.21757125854492188\n",
      "training: 71 batch 157 batch_loss: 0.2147032618522644\n",
      "training: 71 batch 158 batch_loss: 0.21261823177337646\n",
      "training: 71 batch 159 batch_loss: 0.21182632446289062\n",
      "training: 71 batch 160 batch_loss: 0.2116701900959015\n",
      "training: 71 batch 161 batch_loss: 0.21141117811203003\n",
      "training: 71 batch 162 batch_loss: 0.20985960960388184\n",
      "training: 71 batch 163 batch_loss: 0.21615585684776306\n",
      "training: 71 batch 164 batch_loss: 0.2134837508201599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 71 batch 165 batch_loss: 0.214334636926651\n",
      "training: 71 batch 166 batch_loss: 0.2117730677127838\n",
      "training: 71 batch 167 batch_loss: 0.21180576086044312\n",
      "training: 71 batch 168 batch_loss: 0.20914191007614136\n",
      "training: 71 batch 169 batch_loss: 0.21038120985031128\n",
      "training: 71 batch 170 batch_loss: 0.21069198846817017\n",
      "training: 71 batch 171 batch_loss: 0.21255341172218323\n",
      "training: 71 batch 172 batch_loss: 0.21206191182136536\n",
      "training: 71 batch 173 batch_loss: 0.21598559617996216\n",
      "training: 71 batch 174 batch_loss: 0.21177595853805542\n",
      "training: 71 batch 175 batch_loss: 0.21521523594856262\n",
      "training: 71 batch 176 batch_loss: 0.214146226644516\n",
      "training: 71 batch 177 batch_loss: 0.21425703167915344\n",
      "training: 71 batch 178 batch_loss: 0.20976018905639648\n",
      "training: 71 batch 179 batch_loss: 0.20985504984855652\n",
      "training: 71 batch 180 batch_loss: 0.21149921417236328\n",
      "training: 71 batch 181 batch_loss: 0.21440917253494263\n",
      "training: 71 batch 182 batch_loss: 0.20979952812194824\n",
      "training: 71 batch 183 batch_loss: 0.20949161052703857\n",
      "training: 71 batch 184 batch_loss: 0.21459880471229553\n",
      "training: 71 batch 185 batch_loss: 0.21336165070533752\n",
      "training: 71 batch 186 batch_loss: 0.21169927716255188\n",
      "training: 71 batch 187 batch_loss: 0.2152375876903534\n",
      "training: 71 batch 188 batch_loss: 0.21334436535835266\n",
      "training: 71 batch 189 batch_loss: 0.2107892632484436\n",
      "training: 71 batch 190 batch_loss: 0.21475672721862793\n",
      "training: 71 batch 191 batch_loss: 0.21313601732254028\n",
      "training: 71 batch 192 batch_loss: 0.2114579677581787\n",
      "training: 71 batch 193 batch_loss: 0.21434670686721802\n",
      "training: 71 batch 194 batch_loss: 0.21492859721183777\n",
      "training: 71 batch 195 batch_loss: 0.21439915895462036\n",
      "training: 71 batch 196 batch_loss: 0.2110770344734192\n",
      "training: 71 batch 197 batch_loss: 0.2150004506111145\n",
      "training: 71 batch 198 batch_loss: 0.2161022126674652\n",
      "training: 71 batch 199 batch_loss: 0.21419036388397217\n",
      "training: 71 batch 200 batch_loss: 0.21452820301055908\n",
      "training: 71 batch 201 batch_loss: 0.2157903015613556\n",
      "training: 71 batch 202 batch_loss: 0.21593621373176575\n",
      "training: 71 batch 203 batch_loss: 0.2117728888988495\n",
      "training: 71 batch 204 batch_loss: 0.2104935646057129\n",
      "training: 71 batch 205 batch_loss: 0.21688076853752136\n",
      "training: 71 batch 206 batch_loss: 0.21874916553497314\n",
      "training: 71 batch 207 batch_loss: 0.216820627450943\n",
      "training: 71 batch 208 batch_loss: 0.21182194352149963\n",
      "training: 71 batch 209 batch_loss: 0.20927771925926208\n",
      "training: 71 batch 210 batch_loss: 0.21384719014167786\n",
      "training: 71 batch 211 batch_loss: 0.21276089549064636\n",
      "training: 71 batch 212 batch_loss: 0.21207007765769958\n",
      "training: 71 batch 213 batch_loss: 0.21453312039375305\n",
      "training: 71 batch 214 batch_loss: 0.2131780982017517\n",
      "training: 71 batch 215 batch_loss: 0.21379229426383972\n",
      "training: 71 batch 216 batch_loss: 0.2139582335948944\n",
      "training: 71 batch 217 batch_loss: 0.21447241306304932\n",
      "training: 71 batch 218 batch_loss: 0.2120131254196167\n",
      "training: 71 batch 219 batch_loss: 0.21196138858795166\n",
      "training: 71 batch 220 batch_loss: 0.21055170893669128\n",
      "training: 71 batch 221 batch_loss: 0.21426743268966675\n",
      "training: 71 batch 222 batch_loss: 0.21596136689186096\n",
      "training: 71 batch 223 batch_loss: 0.21663591265678406\n",
      "training: 71 batch 224 batch_loss: 0.2113141417503357\n",
      "training: 71 batch 225 batch_loss: 0.2109115719795227\n",
      "training: 71 batch 226 batch_loss: 0.21288150548934937\n",
      "training: 71 batch 227 batch_loss: 0.21379467844963074\n",
      "training: 71 batch 228 batch_loss: 0.21168464422225952\n",
      "training: 71 batch 229 batch_loss: 0.21500667929649353\n",
      "training: 71 batch 230 batch_loss: 0.21308133006095886\n",
      "training: 71 batch 231 batch_loss: 0.21484556794166565\n",
      "training: 71 batch 232 batch_loss: 0.21351251006126404\n",
      "training: 71 batch 233 batch_loss: 0.21430855989456177\n",
      "training: 71 batch 234 batch_loss: 0.20976218581199646\n",
      "training: 71 batch 235 batch_loss: 0.2079126238822937\n",
      "training: 71 batch 236 batch_loss: 0.2144336998462677\n",
      "training: 71 batch 237 batch_loss: 0.21018144488334656\n",
      "training: 71 batch 238 batch_loss: 0.20958879590034485\n",
      "training: 71 batch 239 batch_loss: 0.21331310272216797\n",
      "training: 71 batch 240 batch_loss: 0.2111201286315918\n",
      "training: 71 batch 241 batch_loss: 0.2117379903793335\n",
      "training: 71 batch 242 batch_loss: 0.21132555603981018\n",
      "training: 71 batch 243 batch_loss: 0.21180003881454468\n",
      "training: 71 batch 244 batch_loss: 0.21696066856384277\n",
      "training: 71 batch 245 batch_loss: 0.2134057879447937\n",
      "training: 71 batch 246 batch_loss: 0.21265605092048645\n",
      "training: 71 batch 247 batch_loss: 0.21180689334869385\n",
      "training: 71 batch 248 batch_loss: 0.21651199460029602\n",
      "training: 71 batch 249 batch_loss: 0.21900805830955505\n",
      "training: 71 batch 250 batch_loss: 0.21555495262145996\n",
      "training: 71 batch 251 batch_loss: 0.21550637483596802\n",
      "training: 71 batch 252 batch_loss: 0.21361824870109558\n",
      "training: 71 batch 253 batch_loss: 0.21491241455078125\n",
      "training: 71 batch 254 batch_loss: 0.2131006121635437\n",
      "training: 71 batch 255 batch_loss: 0.2151358723640442\n",
      "training: 71 batch 256 batch_loss: 0.2169555127620697\n",
      "training: 71 batch 257 batch_loss: 0.21430614590644836\n",
      "training: 71 batch 258 batch_loss: 0.21417009830474854\n",
      "training: 71 batch 259 batch_loss: 0.2128904163837433\n",
      "training: 71 batch 260 batch_loss: 0.21746623516082764\n",
      "training: 71 batch 261 batch_loss: 0.21218430995941162\n",
      "training: 71 batch 262 batch_loss: 0.21210607886314392\n",
      "training: 71 batch 263 batch_loss: 0.21587154269218445\n",
      "training: 71 batch 264 batch_loss: 0.21699321269989014\n",
      "training: 71 batch 265 batch_loss: 0.20787611603736877\n",
      "training: 71 batch 266 batch_loss: 0.21231716871261597\n",
      "training: 71 batch 267 batch_loss: 0.2148503065109253\n",
      "training: 71 batch 268 batch_loss: 0.21744012832641602\n",
      "training: 71 batch 269 batch_loss: 0.21273350715637207\n",
      "training: 71 batch 270 batch_loss: 0.21374112367630005\n",
      "training: 71 batch 271 batch_loss: 0.21306687593460083\n",
      "training: 71 batch 272 batch_loss: 0.21294420957565308\n",
      "training: 71 batch 273 batch_loss: 0.21002107858657837\n",
      "training: 71 batch 274 batch_loss: 0.21290281414985657\n",
      "training: 71 batch 275 batch_loss: 0.21772387623786926\n",
      "training: 71 batch 276 batch_loss: 0.21126022934913635\n",
      "training: 71 batch 277 batch_loss: 0.20969629287719727\n",
      "training: 71 batch 278 batch_loss: 0.21206459403038025\n",
      "training: 71 batch 279 batch_loss: 0.21032384037971497\n",
      "training: 71 batch 280 batch_loss: 0.21571564674377441\n",
      "training: 71 batch 281 batch_loss: 0.21405357122421265\n",
      "training: 71 batch 282 batch_loss: 0.21247491240501404\n",
      "training: 71 batch 283 batch_loss: 0.21336090564727783\n",
      "training: 71 batch 284 batch_loss: 0.2146194577217102\n",
      "training: 71 batch 285 batch_loss: 0.21663743257522583\n",
      "training: 71 batch 286 batch_loss: 0.21309533715248108\n",
      "training: 71 batch 287 batch_loss: 0.21200335025787354\n",
      "training: 71 batch 288 batch_loss: 0.21729248762130737\n",
      "training: 71 batch 289 batch_loss: 0.21504539251327515\n",
      "training: 71 batch 290 batch_loss: 0.21271520853042603\n",
      "training: 71 batch 291 batch_loss: 0.20910421013832092\n",
      "training: 71 batch 292 batch_loss: 0.20944806933403015\n",
      "training: 71 batch 293 batch_loss: 0.21390652656555176\n",
      "training: 71 batch 294 batch_loss: 0.21354055404663086\n",
      "training: 71 batch 295 batch_loss: 0.21341437101364136\n",
      "training: 71 batch 296 batch_loss: 0.21688011288642883\n",
      "training: 71 batch 297 batch_loss: 0.21387356519699097\n",
      "training: 71 batch 298 batch_loss: 0.21191424131393433\n",
      "training: 71 batch 299 batch_loss: 0.21270349621772766\n",
      "training: 71 batch 300 batch_loss: 0.21189165115356445\n",
      "training: 71 batch 301 batch_loss: 0.21492299437522888\n",
      "training: 71 batch 302 batch_loss: 0.21512725949287415\n",
      "training: 71 batch 303 batch_loss: 0.21315425634384155\n",
      "training: 71 batch 304 batch_loss: 0.2145788073539734\n",
      "training: 71 batch 305 batch_loss: 0.2143782377243042\n",
      "training: 71 batch 306 batch_loss: 0.21261048316955566\n",
      "training: 71 batch 307 batch_loss: 0.2114955186843872\n",
      "training: 71 batch 308 batch_loss: 0.21069687604904175\n",
      "training: 71 batch 309 batch_loss: 0.21377164125442505\n",
      "training: 71 batch 310 batch_loss: 0.21611613035202026\n",
      "training: 71 batch 311 batch_loss: 0.21141862869262695\n",
      "training: 71 batch 312 batch_loss: 0.21122336387634277\n",
      "training: 71 batch 313 batch_loss: 0.2096855342388153\n",
      "training: 71 batch 314 batch_loss: 0.21337547898292542\n",
      "training: 71 batch 315 batch_loss: 0.208778977394104\n",
      "training: 71 batch 316 batch_loss: 0.21312165260314941\n",
      "training: 71 batch 317 batch_loss: 0.2139800786972046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 71 batch 318 batch_loss: 0.21245506405830383\n",
      "training: 71 batch 319 batch_loss: 0.21184492111206055\n",
      "training: 71 batch 320 batch_loss: 0.21084129810333252\n",
      "training: 71 batch 321 batch_loss: 0.21816420555114746\n",
      "training: 71 batch 322 batch_loss: 0.21489065885543823\n",
      "training: 71 batch 323 batch_loss: 0.21195021271705627\n",
      "training: 71 batch 324 batch_loss: 0.21430134773254395\n",
      "training: 71 batch 325 batch_loss: 0.21326711773872375\n",
      "training: 71 batch 326 batch_loss: 0.21067440509796143\n",
      "training: 71 batch 327 batch_loss: 0.21061396598815918\n",
      "training: 71 batch 328 batch_loss: 0.2094259262084961\n",
      "training: 71 batch 329 batch_loss: 0.2134472131729126\n",
      "training: 71 batch 330 batch_loss: 0.2120618224143982\n",
      "training: 71 batch 331 batch_loss: 0.21569982171058655\n",
      "training: 71 batch 332 batch_loss: 0.2152719795703888\n",
      "training: 71 batch 333 batch_loss: 0.21755549311637878\n",
      "training: 71 batch 334 batch_loss: 0.21821945905685425\n",
      "training: 71 batch 335 batch_loss: 0.21388337016105652\n",
      "training: 71 batch 336 batch_loss: 0.2179943323135376\n",
      "training: 71 batch 337 batch_loss: 0.21177327632904053\n",
      "training: 71 batch 338 batch_loss: 0.21418029069900513\n",
      "training: 71 batch 339 batch_loss: 0.21590232849121094\n",
      "training: 71 batch 340 batch_loss: 0.21492639183998108\n",
      "training: 71 batch 341 batch_loss: 0.21534201502799988\n",
      "training: 71 batch 342 batch_loss: 0.2169722020626068\n",
      "training: 71 batch 343 batch_loss: 0.21065294742584229\n",
      "training: 71 batch 344 batch_loss: 0.21335816383361816\n",
      "training: 71 batch 345 batch_loss: 0.21451938152313232\n",
      "training: 71 batch 346 batch_loss: 0.21450001001358032\n",
      "training: 71 batch 347 batch_loss: 0.210849791765213\n",
      "training: 71 batch 348 batch_loss: 0.21663549542427063\n",
      "training: 71 batch 349 batch_loss: 0.21343180537223816\n",
      "training: 71 batch 350 batch_loss: 0.21397417783737183\n",
      "training: 71 batch 351 batch_loss: 0.21173512935638428\n",
      "training: 71 batch 352 batch_loss: 0.2167486548423767\n",
      "training: 71 batch 353 batch_loss: 0.21720275282859802\n",
      "training: 71 batch 354 batch_loss: 0.2136293351650238\n",
      "training: 71 batch 355 batch_loss: 0.2152704894542694\n",
      "training: 71 batch 356 batch_loss: 0.21460357308387756\n",
      "training: 71 batch 357 batch_loss: 0.21433714032173157\n",
      "training: 71 batch 358 batch_loss: 0.21091777086257935\n",
      "training: 71 batch 359 batch_loss: 0.21138674020767212\n",
      "training: 71 batch 360 batch_loss: 0.21382004022598267\n",
      "training: 71 batch 361 batch_loss: 0.2126709222793579\n",
      "training: 71 batch 362 batch_loss: 0.21750593185424805\n",
      "training: 71 batch 363 batch_loss: 0.2143409252166748\n",
      "training: 71 batch 364 batch_loss: 0.2130085527896881\n",
      "training: 71 batch 365 batch_loss: 0.2117052674293518\n",
      "training: 71 batch 366 batch_loss: 0.21053197979927063\n",
      "training: 71 batch 367 batch_loss: 0.21028724312782288\n",
      "training: 71 batch 368 batch_loss: 0.211707204580307\n",
      "training: 71 batch 369 batch_loss: 0.21427983045578003\n",
      "training: 71 batch 370 batch_loss: 0.20985901355743408\n",
      "training: 71 batch 371 batch_loss: 0.21802562475204468\n",
      "training: 71 batch 372 batch_loss: 0.21481114625930786\n",
      "training: 71 batch 373 batch_loss: 0.21376997232437134\n",
      "training: 71 batch 374 batch_loss: 0.21220338344573975\n",
      "training: 71 batch 375 batch_loss: 0.2139306664466858\n",
      "training: 71 batch 376 batch_loss: 0.2162778675556183\n",
      "training: 71 batch 377 batch_loss: 0.21189340949058533\n",
      "training: 71 batch 378 batch_loss: 0.2087443470954895\n",
      "training: 71 batch 379 batch_loss: 0.21279501914978027\n",
      "training: 71 batch 380 batch_loss: 0.21221774816513062\n",
      "training: 71 batch 381 batch_loss: 0.21432477235794067\n",
      "training: 71 batch 382 batch_loss: 0.2174665927886963\n",
      "training: 71 batch 383 batch_loss: 0.2150578796863556\n",
      "training: 71 batch 384 batch_loss: 0.2153310775756836\n",
      "training: 71 batch 385 batch_loss: 0.21229612827301025\n",
      "training: 71 batch 386 batch_loss: 0.2161698043346405\n",
      "training: 71 batch 387 batch_loss: 0.2136337161064148\n",
      "training: 71 batch 388 batch_loss: 0.21896076202392578\n",
      "training: 71 batch 389 batch_loss: 0.2110854983329773\n",
      "training: 71 batch 390 batch_loss: 0.21739450097084045\n",
      "training: 71 batch 391 batch_loss: 0.21339142322540283\n",
      "training: 71 batch 392 batch_loss: 0.21196666359901428\n",
      "training: 71 batch 393 batch_loss: 0.21443942189216614\n",
      "training: 71 batch 394 batch_loss: 0.21250563859939575\n",
      "training: 71 batch 395 batch_loss: 0.21433347463607788\n",
      "training: 71 batch 396 batch_loss: 0.21222516894340515\n",
      "training: 71 batch 397 batch_loss: 0.21498411893844604\n",
      "training: 71 batch 398 batch_loss: 0.21440571546554565\n",
      "training: 71 batch 399 batch_loss: 0.21335914731025696\n",
      "training: 71 batch 400 batch_loss: 0.2207958698272705\n",
      "training: 71 batch 401 batch_loss: 0.21687543392181396\n",
      "training: 71 batch 402 batch_loss: 0.21752500534057617\n",
      "training: 71 batch 403 batch_loss: 0.21564731001853943\n",
      "training: 71 batch 404 batch_loss: 0.21569809317588806\n",
      "training: 71 batch 405 batch_loss: 0.20994776487350464\n",
      "training: 71 batch 406 batch_loss: 0.21672981977462769\n",
      "training: 71 batch 407 batch_loss: 0.2145257592201233\n",
      "training: 71 batch 408 batch_loss: 0.20779719948768616\n",
      "training: 71 batch 409 batch_loss: 0.2130737006664276\n",
      "training: 71 batch 410 batch_loss: 0.2130270004272461\n",
      "training: 71 batch 411 batch_loss: 0.21939155459403992\n",
      "training: 71 batch 412 batch_loss: 0.21344676613807678\n",
      "training: 71 batch 413 batch_loss: 0.2186998426914215\n",
      "training: 71 batch 414 batch_loss: 0.21140548586845398\n",
      "training: 71 batch 415 batch_loss: 0.21405208110809326\n",
      "training: 71 batch 416 batch_loss: 0.21032994985580444\n",
      "training: 71 batch 417 batch_loss: 0.21315708756446838\n",
      "training: 71 batch 418 batch_loss: 0.21269363164901733\n",
      "training: 71 batch 419 batch_loss: 0.21466407179832458\n",
      "training: 71 batch 420 batch_loss: 0.21380868554115295\n",
      "training: 71 batch 421 batch_loss: 0.2159368097782135\n",
      "training: 71 batch 422 batch_loss: 0.21683865785598755\n",
      "training: 71 batch 423 batch_loss: 0.2147963047027588\n",
      "training: 71 batch 424 batch_loss: 0.21411409974098206\n",
      "training: 71 batch 425 batch_loss: 0.2124832272529602\n",
      "training: 71 batch 426 batch_loss: 0.2173651158809662\n",
      "training: 71 batch 427 batch_loss: 0.21759086847305298\n",
      "training: 71 batch 428 batch_loss: 0.21301472187042236\n",
      "training: 71 batch 429 batch_loss: 0.2149290144443512\n",
      "training: 71 batch 430 batch_loss: 0.21424072980880737\n",
      "training: 71 batch 431 batch_loss: 0.21364137530326843\n",
      "training: 71 batch 432 batch_loss: 0.21419188380241394\n",
      "training: 71 batch 433 batch_loss: 0.2209097146987915\n",
      "training: 71 batch 434 batch_loss: 0.2155047357082367\n",
      "training: 71 batch 435 batch_loss: 0.21467921137809753\n",
      "training: 71 batch 436 batch_loss: 0.2162860929965973\n",
      "training: 71 batch 437 batch_loss: 0.21411371231079102\n",
      "training: 71 batch 438 batch_loss: 0.21901148557662964\n",
      "training: 71 batch 439 batch_loss: 0.21416938304901123\n",
      "training: 71 batch 440 batch_loss: 0.21630048751831055\n",
      "training: 71 batch 441 batch_loss: 0.21308469772338867\n",
      "training: 71 batch 442 batch_loss: 0.21376433968544006\n",
      "training: 71 batch 443 batch_loss: 0.21445059776306152\n",
      "training: 71 batch 444 batch_loss: 0.21466830372810364\n",
      "training: 71 batch 445 batch_loss: 0.2123091220855713\n",
      "training: 71 batch 446 batch_loss: 0.212262362241745\n",
      "training: 71 batch 447 batch_loss: 0.21972247958183289\n",
      "training: 71 batch 448 batch_loss: 0.2157200276851654\n",
      "training: 71 batch 449 batch_loss: 0.21529239416122437\n",
      "training: 71 batch 450 batch_loss: 0.2154410183429718\n",
      "training: 71 batch 451 batch_loss: 0.21779599785804749\n",
      "training: 71 batch 452 batch_loss: 0.2148285210132599\n",
      "training: 71 batch 453 batch_loss: 0.2118847370147705\n",
      "training: 71 batch 454 batch_loss: 0.21282881498336792\n",
      "training: 71 batch 455 batch_loss: 0.21362906694412231\n",
      "training: 71 batch 456 batch_loss: 0.21404793858528137\n",
      "training: 71 batch 457 batch_loss: 0.21273210644721985\n",
      "training: 71 batch 458 batch_loss: 0.2223902940750122\n",
      "training: 71 batch 459 batch_loss: 0.21575427055358887\n",
      "training: 71 batch 460 batch_loss: 0.21403011679649353\n",
      "training: 71 batch 461 batch_loss: 0.21891939640045166\n",
      "training: 71 batch 462 batch_loss: 0.21088296175003052\n",
      "training: 71 batch 463 batch_loss: 0.21494317054748535\n",
      "training: 71 batch 464 batch_loss: 0.2177124321460724\n",
      "training: 71 batch 465 batch_loss: 0.21250292658805847\n",
      "training: 71 batch 466 batch_loss: 0.21482115983963013\n",
      "training: 71 batch 467 batch_loss: 0.21511510014533997\n",
      "training: 71 batch 468 batch_loss: 0.21305561065673828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 71 batch 469 batch_loss: 0.21535193920135498\n",
      "training: 71 batch 470 batch_loss: 0.21216559410095215\n",
      "training: 71 batch 471 batch_loss: 0.21612706780433655\n",
      "training: 71 batch 472 batch_loss: 0.2132282257080078\n",
      "training: 71 batch 473 batch_loss: 0.21501773595809937\n",
      "training: 71 batch 474 batch_loss: 0.21234941482543945\n",
      "training: 71 batch 475 batch_loss: 0.21551376581192017\n",
      "training: 71 batch 476 batch_loss: 0.21651586890220642\n",
      "training: 71 batch 477 batch_loss: 0.21637922525405884\n",
      "training: 71 batch 478 batch_loss: 0.21473604440689087\n",
      "training: 71 batch 479 batch_loss: 0.21341222524642944\n",
      "training: 71 batch 480 batch_loss: 0.21480396389961243\n",
      "training: 71 batch 481 batch_loss: 0.21229875087738037\n",
      "training: 71 batch 482 batch_loss: 0.2143203616142273\n",
      "training: 71 batch 483 batch_loss: 0.21303397417068481\n",
      "training: 71 batch 484 batch_loss: 0.2197566032409668\n",
      "training: 71 batch 485 batch_loss: 0.21670383214950562\n",
      "training: 71 batch 486 batch_loss: 0.21377944946289062\n",
      "training: 71 batch 487 batch_loss: 0.21189343929290771\n",
      "training: 71 batch 488 batch_loss: 0.21476173400878906\n",
      "training: 71 batch 489 batch_loss: 0.21786710619926453\n",
      "training: 71 batch 490 batch_loss: 0.21647968888282776\n",
      "training: 71 batch 491 batch_loss: 0.21315234899520874\n",
      "training: 71 batch 492 batch_loss: 0.212656170129776\n",
      "training: 71 batch 493 batch_loss: 0.20721504092216492\n",
      "training: 71 batch 494 batch_loss: 0.2187567949295044\n",
      "training: 71 batch 495 batch_loss: 0.21696355938911438\n",
      "training: 71 batch 496 batch_loss: 0.21339023113250732\n",
      "training: 71 batch 497 batch_loss: 0.21489283442497253\n",
      "training: 71 batch 498 batch_loss: 0.21333038806915283\n",
      "training: 71 batch 499 batch_loss: 0.2112615704536438\n",
      "training: 71 batch 500 batch_loss: 0.2186979055404663\n",
      "training: 71 batch 501 batch_loss: 0.21660977602005005\n",
      "training: 71 batch 502 batch_loss: 0.21656769514083862\n",
      "training: 71 batch 503 batch_loss: 0.211515873670578\n",
      "training: 71 batch 504 batch_loss: 0.2144262194633484\n",
      "training: 71 batch 505 batch_loss: 0.21590900421142578\n",
      "training: 71 batch 506 batch_loss: 0.21650654077529907\n",
      "training: 71 batch 507 batch_loss: 0.21141254901885986\n",
      "training: 71 batch 508 batch_loss: 0.21758300065994263\n",
      "training: 71 batch 509 batch_loss: 0.2126728594303131\n",
      "training: 71 batch 510 batch_loss: 0.21022063493728638\n",
      "training: 71 batch 511 batch_loss: 0.2139817476272583\n",
      "training: 71 batch 512 batch_loss: 0.21343308687210083\n",
      "training: 71 batch 513 batch_loss: 0.2187369465827942\n",
      "training: 71 batch 514 batch_loss: 0.21539399027824402\n",
      "training: 71 batch 515 batch_loss: 0.21715709567070007\n",
      "training: 71 batch 516 batch_loss: 0.21243718266487122\n",
      "training: 71 batch 517 batch_loss: 0.21679529547691345\n",
      "training: 71 batch 518 batch_loss: 0.21952733397483826\n",
      "training: 71 batch 519 batch_loss: 0.21334302425384521\n",
      "training: 71 batch 520 batch_loss: 0.20535492897033691\n",
      "training: 71 batch 521 batch_loss: 0.21138256788253784\n",
      "training: 71 batch 522 batch_loss: 0.2158944010734558\n",
      "training: 71 batch 523 batch_loss: 0.21723243594169617\n",
      "training: 71 batch 524 batch_loss: 0.21536865830421448\n",
      "training: 71 batch 525 batch_loss: 0.213826984167099\n",
      "training: 71 batch 526 batch_loss: 0.21434369683265686\n",
      "training: 71 batch 527 batch_loss: 0.21383735537528992\n",
      "training: 71 batch 528 batch_loss: 0.21672388911247253\n",
      "training: 71 batch 529 batch_loss: 0.21979469060897827\n",
      "training: 71 batch 530 batch_loss: 0.21690094470977783\n",
      "training: 71 batch 531 batch_loss: 0.21433430910110474\n",
      "training: 71 batch 532 batch_loss: 0.21386349201202393\n",
      "training: 71 batch 533 batch_loss: 0.21428701281547546\n",
      "training: 71 batch 534 batch_loss: 0.2150692343711853\n",
      "training: 71 batch 535 batch_loss: 0.21629592776298523\n",
      "training: 71 batch 536 batch_loss: 0.215856671333313\n",
      "training: 71 batch 537 batch_loss: 0.21606823801994324\n",
      "training: 71 batch 538 batch_loss: 0.2140951156616211\n",
      "training: 71 batch 539 batch_loss: 0.21521109342575073\n",
      "training: 71 batch 540 batch_loss: 0.21328073740005493\n",
      "training: 71 batch 541 batch_loss: 0.214494526386261\n",
      "training: 71 batch 542 batch_loss: 0.21078923344612122\n",
      "training: 71 batch 543 batch_loss: 0.21180397272109985\n",
      "training: 71 batch 544 batch_loss: 0.2149023413658142\n",
      "training: 71 batch 545 batch_loss: 0.21216288208961487\n",
      "training: 71 batch 546 batch_loss: 0.21515297889709473\n",
      "training: 71 batch 547 batch_loss: 0.21016117930412292\n",
      "training: 71 batch 548 batch_loss: 0.21667900681495667\n",
      "training: 71 batch 549 batch_loss: 0.21525681018829346\n",
      "training: 71 batch 550 batch_loss: 0.21625712513923645\n",
      "training: 71 batch 551 batch_loss: 0.21811926364898682\n",
      "training: 71 batch 552 batch_loss: 0.21518105268478394\n",
      "training: 71 batch 553 batch_loss: 0.21303409337997437\n",
      "training: 71 batch 554 batch_loss: 0.21329784393310547\n",
      "training: 71 batch 555 batch_loss: 0.2200983464717865\n",
      "training: 71 batch 556 batch_loss: 0.2175527811050415\n",
      "training: 71 batch 557 batch_loss: 0.21324902772903442\n",
      "training: 71 batch 558 batch_loss: 0.2148938775062561\n",
      "training: 71 batch 559 batch_loss: 0.21306413412094116\n",
      "training: 71 batch 560 batch_loss: 0.2130405604839325\n",
      "training: 71 batch 561 batch_loss: 0.21230003237724304\n",
      "training: 71 batch 562 batch_loss: 0.21234914660453796\n",
      "training: 71 batch 563 batch_loss: 0.21621695160865784\n",
      "training: 71 batch 564 batch_loss: 0.21386584639549255\n",
      "training: 71 batch 565 batch_loss: 0.21631160378456116\n",
      "training: 71 batch 566 batch_loss: 0.21122252941131592\n",
      "training: 71 batch 567 batch_loss: 0.21645048260688782\n",
      "training: 71 batch 568 batch_loss: 0.21075302362442017\n",
      "training: 71 batch 569 batch_loss: 0.21718859672546387\n",
      "training: 71 batch 570 batch_loss: 0.21626919507980347\n",
      "training: 71 batch 571 batch_loss: 0.21647411584854126\n",
      "training: 71 batch 572 batch_loss: 0.21669745445251465\n",
      "training: 71 batch 573 batch_loss: 0.2213810682296753\n",
      "training: 71 batch 574 batch_loss: 0.21476519107818604\n",
      "training: 71 batch 575 batch_loss: 0.21540457010269165\n",
      "training: 71 batch 576 batch_loss: 0.21785461902618408\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 71, Hit Ratio:0.033338552198085895 | Precision:0.04918902978472427 | Recall:0.06474887292028493 | NDCG:0.06384942799215455\n",
      "*Best Performance* \n",
      "Epoch: 68, Hit Ratio:0.03353176123360638 | Precision:0.04947409810282119 | Recall:0.06511509304686922 | MDCG:0.0641621394246606\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 72 batch 0 batch_loss: 0.21068355441093445\n",
      "training: 72 batch 1 batch_loss: 0.21220138669013977\n",
      "training: 72 batch 2 batch_loss: 0.21290647983551025\n",
      "training: 72 batch 3 batch_loss: 0.21191781759262085\n",
      "training: 72 batch 4 batch_loss: 0.20909926295280457\n",
      "training: 72 batch 5 batch_loss: 0.21582534909248352\n",
      "training: 72 batch 6 batch_loss: 0.21564224362373352\n",
      "training: 72 batch 7 batch_loss: 0.2108919620513916\n",
      "training: 72 batch 8 batch_loss: 0.21271157264709473\n",
      "training: 72 batch 9 batch_loss: 0.21141603589057922\n",
      "training: 72 batch 10 batch_loss: 0.2143106758594513\n",
      "training: 72 batch 11 batch_loss: 0.21011784672737122\n",
      "training: 72 batch 12 batch_loss: 0.21370857954025269\n",
      "training: 72 batch 13 batch_loss: 0.2110603153705597\n",
      "training: 72 batch 14 batch_loss: 0.2084711790084839\n",
      "training: 72 batch 15 batch_loss: 0.2103714942932129\n",
      "training: 72 batch 16 batch_loss: 0.21167483925819397\n",
      "training: 72 batch 17 batch_loss: 0.2139858901500702\n",
      "training: 72 batch 18 batch_loss: 0.21149364113807678\n",
      "training: 72 batch 19 batch_loss: 0.21214795112609863\n",
      "training: 72 batch 20 batch_loss: 0.21163463592529297\n",
      "training: 72 batch 21 batch_loss: 0.2131710648536682\n",
      "training: 72 batch 22 batch_loss: 0.21313714981079102\n",
      "training: 72 batch 23 batch_loss: 0.21413755416870117\n",
      "training: 72 batch 24 batch_loss: 0.21169859170913696\n",
      "training: 72 batch 25 batch_loss: 0.21488690376281738\n",
      "training: 72 batch 26 batch_loss: 0.21833088994026184\n",
      "training: 72 batch 27 batch_loss: 0.21274712681770325\n",
      "training: 72 batch 28 batch_loss: 0.21524003148078918\n",
      "training: 72 batch 29 batch_loss: 0.21885943412780762\n",
      "training: 72 batch 30 batch_loss: 0.21401792764663696\n",
      "training: 72 batch 31 batch_loss: 0.21468043327331543\n",
      "training: 72 batch 32 batch_loss: 0.21553978323936462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 72 batch 33 batch_loss: 0.21489059925079346\n",
      "training: 72 batch 34 batch_loss: 0.21100693941116333\n",
      "training: 72 batch 35 batch_loss: 0.21193867921829224\n",
      "training: 72 batch 36 batch_loss: 0.21338337659835815\n",
      "training: 72 batch 37 batch_loss: 0.2129082977771759\n",
      "training: 72 batch 38 batch_loss: 0.21585258841514587\n",
      "training: 72 batch 39 batch_loss: 0.21103966236114502\n",
      "training: 72 batch 40 batch_loss: 0.2137514054775238\n",
      "training: 72 batch 41 batch_loss: 0.21341103315353394\n",
      "training: 72 batch 42 batch_loss: 0.2081719934940338\n",
      "training: 72 batch 43 batch_loss: 0.21711063385009766\n",
      "training: 72 batch 44 batch_loss: 0.21356672048568726\n",
      "training: 72 batch 45 batch_loss: 0.21682646870613098\n",
      "training: 72 batch 46 batch_loss: 0.21258723735809326\n",
      "training: 72 batch 47 batch_loss: 0.21344727277755737\n",
      "training: 72 batch 48 batch_loss: 0.21380174160003662\n",
      "training: 72 batch 49 batch_loss: 0.2115914225578308\n",
      "training: 72 batch 50 batch_loss: 0.21279257535934448\n",
      "training: 72 batch 51 batch_loss: 0.21379145979881287\n",
      "training: 72 batch 52 batch_loss: 0.21431946754455566\n",
      "training: 72 batch 53 batch_loss: 0.2125672698020935\n",
      "training: 72 batch 54 batch_loss: 0.21205884218215942\n",
      "training: 72 batch 55 batch_loss: 0.21399495005607605\n",
      "training: 72 batch 56 batch_loss: 0.21417230367660522\n",
      "training: 72 batch 57 batch_loss: 0.21364843845367432\n",
      "training: 72 batch 58 batch_loss: 0.21138563752174377\n",
      "training: 72 batch 59 batch_loss: 0.218014657497406\n",
      "training: 72 batch 60 batch_loss: 0.2124229073524475\n",
      "training: 72 batch 61 batch_loss: 0.21724802255630493\n",
      "training: 72 batch 62 batch_loss: 0.21553781628608704\n",
      "training: 72 batch 63 batch_loss: 0.21654212474822998\n",
      "training: 72 batch 64 batch_loss: 0.2127026617527008\n",
      "training: 72 batch 65 batch_loss: 0.21326524019241333\n",
      "training: 72 batch 66 batch_loss: 0.21548596024513245\n",
      "training: 72 batch 67 batch_loss: 0.2121548354625702\n",
      "training: 72 batch 68 batch_loss: 0.21261534094810486\n",
      "training: 72 batch 69 batch_loss: 0.21545016765594482\n",
      "training: 72 batch 70 batch_loss: 0.21217742562294006\n",
      "training: 72 batch 71 batch_loss: 0.21673369407653809\n",
      "training: 72 batch 72 batch_loss: 0.2166152000427246\n",
      "training: 72 batch 73 batch_loss: 0.21307742595672607\n",
      "training: 72 batch 74 batch_loss: 0.21702536940574646\n",
      "training: 72 batch 75 batch_loss: 0.2137061357498169\n",
      "training: 72 batch 76 batch_loss: 0.21335247159004211\n",
      "training: 72 batch 77 batch_loss: 0.21692204475402832\n",
      "training: 72 batch 78 batch_loss: 0.21267682313919067\n",
      "training: 72 batch 79 batch_loss: 0.2118256390094757\n",
      "training: 72 batch 80 batch_loss: 0.21436059474945068\n",
      "training: 72 batch 81 batch_loss: 0.21003136038780212\n",
      "training: 72 batch 82 batch_loss: 0.21524590253829956\n",
      "training: 72 batch 83 batch_loss: 0.2161598801612854\n",
      "training: 72 batch 84 batch_loss: 0.21536412835121155\n",
      "training: 72 batch 85 batch_loss: 0.21564990282058716\n",
      "training: 72 batch 86 batch_loss: 0.21061813831329346\n",
      "training: 72 batch 87 batch_loss: 0.21534746885299683\n",
      "training: 72 batch 88 batch_loss: 0.21249577403068542\n",
      "training: 72 batch 89 batch_loss: 0.21595022082328796\n",
      "training: 72 batch 90 batch_loss: 0.21087664365768433\n",
      "training: 72 batch 91 batch_loss: 0.21650275588035583\n",
      "training: 72 batch 92 batch_loss: 0.2152484953403473\n",
      "training: 72 batch 93 batch_loss: 0.21575304865837097\n",
      "training: 72 batch 94 batch_loss: 0.21351534128189087\n",
      "training: 72 batch 95 batch_loss: 0.21664562821388245\n",
      "training: 72 batch 96 batch_loss: 0.21541696786880493\n",
      "training: 72 batch 97 batch_loss: 0.21597182750701904\n",
      "training: 72 batch 98 batch_loss: 0.21775862574577332\n",
      "training: 72 batch 99 batch_loss: 0.2142789661884308\n",
      "training: 72 batch 100 batch_loss: 0.21190586686134338\n",
      "training: 72 batch 101 batch_loss: 0.21039026975631714\n",
      "training: 72 batch 102 batch_loss: 0.21465396881103516\n",
      "training: 72 batch 103 batch_loss: 0.21852025389671326\n",
      "training: 72 batch 104 batch_loss: 0.21081748604774475\n",
      "training: 72 batch 105 batch_loss: 0.208624005317688\n",
      "training: 72 batch 106 batch_loss: 0.21259737014770508\n",
      "training: 72 batch 107 batch_loss: 0.217442125082016\n",
      "training: 72 batch 108 batch_loss: 0.21213549375534058\n",
      "training: 72 batch 109 batch_loss: 0.21395111083984375\n",
      "training: 72 batch 110 batch_loss: 0.21384572982788086\n",
      "training: 72 batch 111 batch_loss: 0.2153244912624359\n",
      "training: 72 batch 112 batch_loss: 0.2177724540233612\n",
      "training: 72 batch 113 batch_loss: 0.21214449405670166\n",
      "training: 72 batch 114 batch_loss: 0.20968806743621826\n",
      "training: 72 batch 115 batch_loss: 0.21683931350708008\n",
      "training: 72 batch 116 batch_loss: 0.21695807576179504\n",
      "training: 72 batch 117 batch_loss: 0.21150463819503784\n",
      "training: 72 batch 118 batch_loss: 0.2147526741027832\n",
      "training: 72 batch 119 batch_loss: 0.2134101688861847\n",
      "training: 72 batch 120 batch_loss: 0.213356614112854\n",
      "training: 72 batch 121 batch_loss: 0.21319067478179932\n",
      "training: 72 batch 122 batch_loss: 0.21408289670944214\n",
      "training: 72 batch 123 batch_loss: 0.216572105884552\n",
      "training: 72 batch 124 batch_loss: 0.2101476788520813\n",
      "training: 72 batch 125 batch_loss: 0.2137162983417511\n",
      "training: 72 batch 126 batch_loss: 0.21314075589179993\n",
      "training: 72 batch 127 batch_loss: 0.21417629718780518\n",
      "training: 72 batch 128 batch_loss: 0.20904484391212463\n",
      "training: 72 batch 129 batch_loss: 0.2162449061870575\n",
      "training: 72 batch 130 batch_loss: 0.2128819227218628\n",
      "training: 72 batch 131 batch_loss: 0.2145833969116211\n",
      "training: 72 batch 132 batch_loss: 0.21511831879615784\n",
      "training: 72 batch 133 batch_loss: 0.21394598484039307\n",
      "training: 72 batch 134 batch_loss: 0.21494296193122864\n",
      "training: 72 batch 135 batch_loss: 0.2114875614643097\n",
      "training: 72 batch 136 batch_loss: 0.21137204766273499\n",
      "training: 72 batch 137 batch_loss: 0.2133764624595642\n",
      "training: 72 batch 138 batch_loss: 0.21123164892196655\n",
      "training: 72 batch 139 batch_loss: 0.21245306730270386\n",
      "training: 72 batch 140 batch_loss: 0.20897802710533142\n",
      "training: 72 batch 141 batch_loss: 0.21433961391448975\n",
      "training: 72 batch 142 batch_loss: 0.21591639518737793\n",
      "training: 72 batch 143 batch_loss: 0.2146121859550476\n",
      "training: 72 batch 144 batch_loss: 0.2139136791229248\n",
      "training: 72 batch 145 batch_loss: 0.21047821640968323\n",
      "training: 72 batch 146 batch_loss: 0.2130916714668274\n",
      "training: 72 batch 147 batch_loss: 0.2155490517616272\n",
      "training: 72 batch 148 batch_loss: 0.21707302331924438\n",
      "training: 72 batch 149 batch_loss: 0.213913232088089\n",
      "training: 72 batch 150 batch_loss: 0.21089312434196472\n",
      "training: 72 batch 151 batch_loss: 0.21398630738258362\n",
      "training: 72 batch 152 batch_loss: 0.21374520659446716\n",
      "training: 72 batch 153 batch_loss: 0.2135743796825409\n",
      "training: 72 batch 154 batch_loss: 0.21362173557281494\n",
      "training: 72 batch 155 batch_loss: 0.21170765161514282\n",
      "training: 72 batch 156 batch_loss: 0.21732956171035767\n",
      "training: 72 batch 157 batch_loss: 0.212660551071167\n",
      "training: 72 batch 158 batch_loss: 0.21412935853004456\n",
      "training: 72 batch 159 batch_loss: 0.21141937375068665\n",
      "training: 72 batch 160 batch_loss: 0.2158796787261963\n",
      "training: 72 batch 161 batch_loss: 0.21465647220611572\n",
      "training: 72 batch 162 batch_loss: 0.21301794052124023\n",
      "training: 72 batch 163 batch_loss: 0.20830321311950684\n",
      "training: 72 batch 164 batch_loss: 0.21346276998519897\n",
      "training: 72 batch 165 batch_loss: 0.2102736532688141\n",
      "training: 72 batch 166 batch_loss: 0.2136164903640747\n",
      "training: 72 batch 167 batch_loss: 0.21569207310676575\n",
      "training: 72 batch 168 batch_loss: 0.2139267921447754\n",
      "training: 72 batch 169 batch_loss: 0.216077983379364\n",
      "training: 72 batch 170 batch_loss: 0.21583449840545654\n",
      "training: 72 batch 171 batch_loss: 0.21310698986053467\n",
      "training: 72 batch 172 batch_loss: 0.21432459354400635\n",
      "training: 72 batch 173 batch_loss: 0.21343669295310974\n",
      "training: 72 batch 174 batch_loss: 0.21484670042991638\n",
      "training: 72 batch 175 batch_loss: 0.21313431859016418\n",
      "training: 72 batch 176 batch_loss: 0.21625885367393494\n",
      "training: 72 batch 177 batch_loss: 0.21685093641281128\n",
      "training: 72 batch 178 batch_loss: 0.20984986424446106\n",
      "training: 72 batch 179 batch_loss: 0.21307998895645142\n",
      "training: 72 batch 180 batch_loss: 0.2114591896533966\n",
      "training: 72 batch 181 batch_loss: 0.21160396933555603\n",
      "training: 72 batch 182 batch_loss: 0.2122248411178589\n",
      "training: 72 batch 183 batch_loss: 0.21494722366333008\n",
      "training: 72 batch 184 batch_loss: 0.2139902114868164\n",
      "training: 72 batch 185 batch_loss: 0.21251454949378967\n",
      "training: 72 batch 186 batch_loss: 0.21629977226257324\n",
      "training: 72 batch 187 batch_loss: 0.21246230602264404\n",
      "training: 72 batch 188 batch_loss: 0.21547842025756836\n",
      "training: 72 batch 189 batch_loss: 0.21362045407295227\n",
      "training: 72 batch 190 batch_loss: 0.2131018340587616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 72 batch 191 batch_loss: 0.214217871427536\n",
      "training: 72 batch 192 batch_loss: 0.20914649963378906\n",
      "training: 72 batch 193 batch_loss: 0.21266990900039673\n",
      "training: 72 batch 194 batch_loss: 0.21502330899238586\n",
      "training: 72 batch 195 batch_loss: 0.219842791557312\n",
      "training: 72 batch 196 batch_loss: 0.2115936279296875\n",
      "training: 72 batch 197 batch_loss: 0.21756401658058167\n",
      "training: 72 batch 198 batch_loss: 0.21367710828781128\n",
      "training: 72 batch 199 batch_loss: 0.21874433755874634\n",
      "training: 72 batch 200 batch_loss: 0.2156568169593811\n",
      "training: 72 batch 201 batch_loss: 0.2131962776184082\n",
      "training: 72 batch 202 batch_loss: 0.2163861095905304\n",
      "training: 72 batch 203 batch_loss: 0.21490556001663208\n",
      "training: 72 batch 204 batch_loss: 0.2179364264011383\n",
      "training: 72 batch 205 batch_loss: 0.21152961254119873\n",
      "training: 72 batch 206 batch_loss: 0.21366965770721436\n",
      "training: 72 batch 207 batch_loss: 0.21415016055107117\n",
      "training: 72 batch 208 batch_loss: 0.2188255786895752\n",
      "training: 72 batch 209 batch_loss: 0.20759353041648865\n",
      "training: 72 batch 210 batch_loss: 0.20891237258911133\n",
      "training: 72 batch 211 batch_loss: 0.2136242389678955\n",
      "training: 72 batch 212 batch_loss: 0.2123396396636963\n",
      "training: 72 batch 213 batch_loss: 0.2151707410812378\n",
      "training: 72 batch 214 batch_loss: 0.21223172545433044\n",
      "training: 72 batch 215 batch_loss: 0.2171591818332672\n",
      "training: 72 batch 216 batch_loss: 0.21231049299240112\n",
      "training: 72 batch 217 batch_loss: 0.2101290225982666\n",
      "training: 72 batch 218 batch_loss: 0.21262624859809875\n",
      "training: 72 batch 219 batch_loss: 0.21332252025604248\n",
      "training: 72 batch 220 batch_loss: 0.2211708426475525\n",
      "training: 72 batch 221 batch_loss: 0.21458113193511963\n",
      "training: 72 batch 222 batch_loss: 0.21243950724601746\n",
      "training: 72 batch 223 batch_loss: 0.21802246570587158\n",
      "training: 72 batch 224 batch_loss: 0.2163037657737732\n",
      "training: 72 batch 225 batch_loss: 0.21493425965309143\n",
      "training: 72 batch 226 batch_loss: 0.2158387005329132\n",
      "training: 72 batch 227 batch_loss: 0.21289697289466858\n",
      "training: 72 batch 228 batch_loss: 0.21398505568504333\n",
      "training: 72 batch 229 batch_loss: 0.21457251906394958\n",
      "training: 72 batch 230 batch_loss: 0.21715566515922546\n",
      "training: 72 batch 231 batch_loss: 0.21772661805152893\n",
      "training: 72 batch 232 batch_loss: 0.21105796098709106\n",
      "training: 72 batch 233 batch_loss: 0.20914024114608765\n",
      "training: 72 batch 234 batch_loss: 0.2173786461353302\n",
      "training: 72 batch 235 batch_loss: 0.20957210659980774\n",
      "training: 72 batch 236 batch_loss: 0.20958960056304932\n",
      "training: 72 batch 237 batch_loss: 0.21113452315330505\n",
      "training: 72 batch 238 batch_loss: 0.21902155876159668\n",
      "training: 72 batch 239 batch_loss: 0.21579477190971375\n",
      "training: 72 batch 240 batch_loss: 0.21296066045761108\n",
      "training: 72 batch 241 batch_loss: 0.2151937484741211\n",
      "training: 72 batch 242 batch_loss: 0.21199274063110352\n",
      "training: 72 batch 243 batch_loss: 0.21592676639556885\n",
      "training: 72 batch 244 batch_loss: 0.21109279990196228\n",
      "training: 72 batch 245 batch_loss: 0.2160203754901886\n",
      "training: 72 batch 246 batch_loss: 0.21568071842193604\n",
      "training: 72 batch 247 batch_loss: 0.21663996577262878\n",
      "training: 72 batch 248 batch_loss: 0.2108161449432373\n",
      "training: 72 batch 249 batch_loss: 0.21735936403274536\n",
      "training: 72 batch 250 batch_loss: 0.2131548523902893\n",
      "training: 72 batch 251 batch_loss: 0.21288686990737915\n",
      "training: 72 batch 252 batch_loss: 0.210852712392807\n",
      "training: 72 batch 253 batch_loss: 0.21421971917152405\n",
      "training: 72 batch 254 batch_loss: 0.21567437052726746\n",
      "training: 72 batch 255 batch_loss: 0.2093416452407837\n",
      "training: 72 batch 256 batch_loss: 0.2133815586566925\n",
      "training: 72 batch 257 batch_loss: 0.21460586786270142\n",
      "training: 72 batch 258 batch_loss: 0.21581536531448364\n",
      "training: 72 batch 259 batch_loss: 0.21725961565971375\n",
      "training: 72 batch 260 batch_loss: 0.21241456270217896\n",
      "training: 72 batch 261 batch_loss: 0.22008758783340454\n",
      "training: 72 batch 262 batch_loss: 0.21727421879768372\n",
      "training: 72 batch 263 batch_loss: 0.21660363674163818\n",
      "training: 72 batch 264 batch_loss: 0.21719413995742798\n",
      "training: 72 batch 265 batch_loss: 0.21398919820785522\n",
      "training: 72 batch 266 batch_loss: 0.21538597345352173\n",
      "training: 72 batch 267 batch_loss: 0.21595335006713867\n",
      "training: 72 batch 268 batch_loss: 0.21181386709213257\n",
      "training: 72 batch 269 batch_loss: 0.21127301454544067\n",
      "training: 72 batch 270 batch_loss: 0.21895575523376465\n",
      "training: 72 batch 271 batch_loss: 0.21766182780265808\n",
      "training: 72 batch 272 batch_loss: 0.20929843187332153\n",
      "training: 72 batch 273 batch_loss: 0.2173655927181244\n",
      "training: 72 batch 274 batch_loss: 0.2163703739643097\n",
      "training: 72 batch 275 batch_loss: 0.21600493788719177\n",
      "training: 72 batch 276 batch_loss: 0.21591496467590332\n",
      "training: 72 batch 277 batch_loss: 0.2185952067375183\n",
      "training: 72 batch 278 batch_loss: 0.21227943897247314\n",
      "training: 72 batch 279 batch_loss: 0.21638837456703186\n",
      "training: 72 batch 280 batch_loss: 0.2140519618988037\n",
      "training: 72 batch 281 batch_loss: 0.21082669496536255\n",
      "training: 72 batch 282 batch_loss: 0.21585485339164734\n",
      "training: 72 batch 283 batch_loss: 0.21145862340927124\n",
      "training: 72 batch 284 batch_loss: 0.21700739860534668\n",
      "training: 72 batch 285 batch_loss: 0.21434438228607178\n",
      "training: 72 batch 286 batch_loss: 0.2207779586315155\n",
      "training: 72 batch 287 batch_loss: 0.21517622470855713\n",
      "training: 72 batch 288 batch_loss: 0.21455228328704834\n",
      "training: 72 batch 289 batch_loss: 0.21430256962776184\n",
      "training: 72 batch 290 batch_loss: 0.21682098507881165\n",
      "training: 72 batch 291 batch_loss: 0.21405234932899475\n",
      "training: 72 batch 292 batch_loss: 0.21553096175193787\n",
      "training: 72 batch 293 batch_loss: 0.2184978723526001\n",
      "training: 72 batch 294 batch_loss: 0.21382376551628113\n",
      "training: 72 batch 295 batch_loss: 0.2144973874092102\n",
      "training: 72 batch 296 batch_loss: 0.21525034308433533\n",
      "training: 72 batch 297 batch_loss: 0.2099478840827942\n",
      "training: 72 batch 298 batch_loss: 0.22065776586532593\n",
      "training: 72 batch 299 batch_loss: 0.21353283524513245\n",
      "training: 72 batch 300 batch_loss: 0.21330419182777405\n",
      "training: 72 batch 301 batch_loss: 0.21202978491783142\n",
      "training: 72 batch 302 batch_loss: 0.21047267317771912\n",
      "training: 72 batch 303 batch_loss: 0.21571025252342224\n",
      "training: 72 batch 304 batch_loss: 0.2191888988018036\n",
      "training: 72 batch 305 batch_loss: 0.21527737379074097\n",
      "training: 72 batch 306 batch_loss: 0.21641692519187927\n",
      "training: 72 batch 307 batch_loss: 0.21758055686950684\n",
      "training: 72 batch 308 batch_loss: 0.2159988284111023\n",
      "training: 72 batch 309 batch_loss: 0.21638065576553345\n",
      "training: 72 batch 310 batch_loss: 0.21350502967834473\n",
      "training: 72 batch 311 batch_loss: 0.21282371878623962\n",
      "training: 72 batch 312 batch_loss: 0.21355336904525757\n",
      "training: 72 batch 313 batch_loss: 0.2105751633644104\n",
      "training: 72 batch 314 batch_loss: 0.21382290124893188\n",
      "training: 72 batch 315 batch_loss: 0.21342623233795166\n",
      "training: 72 batch 316 batch_loss: 0.21082425117492676\n",
      "training: 72 batch 317 batch_loss: 0.21502819657325745\n",
      "training: 72 batch 318 batch_loss: 0.21186786890029907\n",
      "training: 72 batch 319 batch_loss: 0.21614235639572144\n",
      "training: 72 batch 320 batch_loss: 0.21401125192642212\n",
      "training: 72 batch 321 batch_loss: 0.22010517120361328\n",
      "training: 72 batch 322 batch_loss: 0.21389234066009521\n",
      "training: 72 batch 323 batch_loss: 0.214755117893219\n",
      "training: 72 batch 324 batch_loss: 0.21412348747253418\n",
      "training: 72 batch 325 batch_loss: 0.21763181686401367\n",
      "training: 72 batch 326 batch_loss: 0.2139689326286316\n",
      "training: 72 batch 327 batch_loss: 0.21559953689575195\n",
      "training: 72 batch 328 batch_loss: 0.21572762727737427\n",
      "training: 72 batch 329 batch_loss: 0.21400460600852966\n",
      "training: 72 batch 330 batch_loss: 0.2097204327583313\n",
      "training: 72 batch 331 batch_loss: 0.21329981088638306\n",
      "training: 72 batch 332 batch_loss: 0.20948871970176697\n",
      "training: 72 batch 333 batch_loss: 0.21631428599357605\n",
      "training: 72 batch 334 batch_loss: 0.2106340229511261\n",
      "training: 72 batch 335 batch_loss: 0.21423551440238953\n",
      "training: 72 batch 336 batch_loss: 0.21413391828536987\n",
      "training: 72 batch 337 batch_loss: 0.21473661065101624\n",
      "training: 72 batch 338 batch_loss: 0.2114894986152649\n",
      "training: 72 batch 339 batch_loss: 0.21951910853385925\n",
      "training: 72 batch 340 batch_loss: 0.2162429690361023\n",
      "training: 72 batch 341 batch_loss: 0.21284064650535583\n",
      "training: 72 batch 342 batch_loss: 0.21869146823883057\n",
      "training: 72 batch 343 batch_loss: 0.213841050863266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 72 batch 344 batch_loss: 0.21650516986846924\n",
      "training: 72 batch 345 batch_loss: 0.21269527077674866\n",
      "training: 72 batch 346 batch_loss: 0.21439945697784424\n",
      "training: 72 batch 347 batch_loss: 0.21104183793067932\n",
      "training: 72 batch 348 batch_loss: 0.21337395906448364\n",
      "training: 72 batch 349 batch_loss: 0.21678385138511658\n",
      "training: 72 batch 350 batch_loss: 0.22095581889152527\n",
      "training: 72 batch 351 batch_loss: 0.21964296698570251\n",
      "training: 72 batch 352 batch_loss: 0.21753978729248047\n",
      "training: 72 batch 353 batch_loss: 0.21257182955741882\n",
      "training: 72 batch 354 batch_loss: 0.2142026126384735\n",
      "training: 72 batch 355 batch_loss: 0.21228563785552979\n",
      "training: 72 batch 356 batch_loss: 0.21916452050209045\n",
      "training: 72 batch 357 batch_loss: 0.2155395746231079\n",
      "training: 72 batch 358 batch_loss: 0.2185496687889099\n",
      "training: 72 batch 359 batch_loss: 0.21716049313545227\n",
      "training: 72 batch 360 batch_loss: 0.218926340341568\n",
      "training: 72 batch 361 batch_loss: 0.21453961730003357\n",
      "training: 72 batch 362 batch_loss: 0.21507778763771057\n",
      "training: 72 batch 363 batch_loss: 0.2167256474494934\n",
      "training: 72 batch 364 batch_loss: 0.2165173888206482\n",
      "training: 72 batch 365 batch_loss: 0.21434947848320007\n",
      "training: 72 batch 366 batch_loss: 0.2168993353843689\n",
      "training: 72 batch 367 batch_loss: 0.21668526530265808\n",
      "training: 72 batch 368 batch_loss: 0.21294057369232178\n",
      "training: 72 batch 369 batch_loss: 0.21458101272583008\n",
      "training: 72 batch 370 batch_loss: 0.21293237805366516\n",
      "training: 72 batch 371 batch_loss: 0.21237358450889587\n",
      "training: 72 batch 372 batch_loss: 0.21486344933509827\n",
      "training: 72 batch 373 batch_loss: 0.20889616012573242\n",
      "training: 72 batch 374 batch_loss: 0.21793007850646973\n",
      "training: 72 batch 375 batch_loss: 0.21421286463737488\n",
      "training: 72 batch 376 batch_loss: 0.2116137444972992\n",
      "training: 72 batch 377 batch_loss: 0.21219205856323242\n",
      "training: 72 batch 378 batch_loss: 0.21159809827804565\n",
      "training: 72 batch 379 batch_loss: 0.2126471996307373\n",
      "training: 72 batch 380 batch_loss: 0.21623843908309937\n",
      "training: 72 batch 381 batch_loss: 0.21415135264396667\n",
      "training: 72 batch 382 batch_loss: 0.21257129311561584\n",
      "training: 72 batch 383 batch_loss: 0.21640455722808838\n",
      "training: 72 batch 384 batch_loss: 0.21816310286521912\n",
      "training: 72 batch 385 batch_loss: 0.212086021900177\n",
      "training: 72 batch 386 batch_loss: 0.2120768129825592\n",
      "training: 72 batch 387 batch_loss: 0.2143416404724121\n",
      "training: 72 batch 388 batch_loss: 0.2119164764881134\n",
      "training: 72 batch 389 batch_loss: 0.21177929639816284\n",
      "training: 72 batch 390 batch_loss: 0.21343880891799927\n",
      "training: 72 batch 391 batch_loss: 0.21235111355781555\n",
      "training: 72 batch 392 batch_loss: 0.21745270490646362\n",
      "training: 72 batch 393 batch_loss: 0.21188300848007202\n",
      "training: 72 batch 394 batch_loss: 0.21644964814186096\n",
      "training: 72 batch 395 batch_loss: 0.21564671397209167\n",
      "training: 72 batch 396 batch_loss: 0.2219013273715973\n",
      "training: 72 batch 397 batch_loss: 0.21675360202789307\n",
      "training: 72 batch 398 batch_loss: 0.21725448966026306\n",
      "training: 72 batch 399 batch_loss: 0.2169334590435028\n",
      "training: 72 batch 400 batch_loss: 0.21696117520332336\n",
      "training: 72 batch 401 batch_loss: 0.21739289164543152\n",
      "training: 72 batch 402 batch_loss: 0.2141520380973816\n",
      "training: 72 batch 403 batch_loss: 0.21255725622177124\n",
      "training: 72 batch 404 batch_loss: 0.21780171990394592\n",
      "training: 72 batch 405 batch_loss: 0.2149232029914856\n",
      "training: 72 batch 406 batch_loss: 0.21344423294067383\n",
      "training: 72 batch 407 batch_loss: 0.21586790680885315\n",
      "training: 72 batch 408 batch_loss: 0.21824020147323608\n",
      "training: 72 batch 409 batch_loss: 0.21421587467193604\n",
      "training: 72 batch 410 batch_loss: 0.21410006284713745\n",
      "training: 72 batch 411 batch_loss: 0.21322393417358398\n",
      "training: 72 batch 412 batch_loss: 0.21228322386741638\n",
      "training: 72 batch 413 batch_loss: 0.2177567481994629\n",
      "training: 72 batch 414 batch_loss: 0.21292871236801147\n",
      "training: 72 batch 415 batch_loss: 0.2146642506122589\n",
      "training: 72 batch 416 batch_loss: 0.2136283814907074\n",
      "training: 72 batch 417 batch_loss: 0.215573251247406\n",
      "training: 72 batch 418 batch_loss: 0.20968306064605713\n",
      "training: 72 batch 419 batch_loss: 0.21363425254821777\n",
      "training: 72 batch 420 batch_loss: 0.21498268842697144\n",
      "training: 72 batch 421 batch_loss: 0.21170806884765625\n",
      "training: 72 batch 422 batch_loss: 0.22076106071472168\n",
      "training: 72 batch 423 batch_loss: 0.22016975283622742\n",
      "training: 72 batch 424 batch_loss: 0.21763882040977478\n",
      "training: 72 batch 425 batch_loss: 0.21856123208999634\n",
      "training: 72 batch 426 batch_loss: 0.21551668643951416\n",
      "training: 72 batch 427 batch_loss: 0.2117534875869751\n",
      "training: 72 batch 428 batch_loss: 0.22266092896461487\n",
      "training: 72 batch 429 batch_loss: 0.21196335554122925\n",
      "training: 72 batch 430 batch_loss: 0.21858429908752441\n",
      "training: 72 batch 431 batch_loss: 0.21669328212738037\n",
      "training: 72 batch 432 batch_loss: 0.21495166420936584\n",
      "training: 72 batch 433 batch_loss: 0.21192589402198792\n",
      "training: 72 batch 434 batch_loss: 0.21522438526153564\n",
      "training: 72 batch 435 batch_loss: 0.21427232027053833\n",
      "training: 72 batch 436 batch_loss: 0.21677878499031067\n",
      "training: 72 batch 437 batch_loss: 0.21617108583450317\n",
      "training: 72 batch 438 batch_loss: 0.21848362684249878\n",
      "training: 72 batch 439 batch_loss: 0.21204107999801636\n",
      "training: 72 batch 440 batch_loss: 0.21110549569129944\n",
      "training: 72 batch 441 batch_loss: 0.21644532680511475\n",
      "training: 72 batch 442 batch_loss: 0.21110692620277405\n",
      "training: 72 batch 443 batch_loss: 0.21652424335479736\n",
      "training: 72 batch 444 batch_loss: 0.21432968974113464\n",
      "training: 72 batch 445 batch_loss: 0.21257227659225464\n",
      "training: 72 batch 446 batch_loss: 0.21643105149269104\n",
      "training: 72 batch 447 batch_loss: 0.2173018753528595\n",
      "training: 72 batch 448 batch_loss: 0.21711158752441406\n",
      "training: 72 batch 449 batch_loss: 0.21401280164718628\n",
      "training: 72 batch 450 batch_loss: 0.22055411338806152\n",
      "training: 72 batch 451 batch_loss: 0.2097109854221344\n",
      "training: 72 batch 452 batch_loss: 0.21427664160728455\n",
      "training: 72 batch 453 batch_loss: 0.21442866325378418\n",
      "training: 72 batch 454 batch_loss: 0.21590310335159302\n",
      "training: 72 batch 455 batch_loss: 0.21329200267791748\n",
      "training: 72 batch 456 batch_loss: 0.2125113606452942\n",
      "training: 72 batch 457 batch_loss: 0.2164342999458313\n",
      "training: 72 batch 458 batch_loss: 0.2159329056739807\n",
      "training: 72 batch 459 batch_loss: 0.21269795298576355\n",
      "training: 72 batch 460 batch_loss: 0.2144761085510254\n",
      "training: 72 batch 461 batch_loss: 0.21559682488441467\n",
      "training: 72 batch 462 batch_loss: 0.21197721362113953\n",
      "training: 72 batch 463 batch_loss: 0.21504458785057068\n",
      "training: 72 batch 464 batch_loss: 0.21645593643188477\n",
      "training: 72 batch 465 batch_loss: 0.21575072407722473\n",
      "training: 72 batch 466 batch_loss: 0.21455147862434387\n",
      "training: 72 batch 467 batch_loss: 0.21868863701820374\n",
      "training: 72 batch 468 batch_loss: 0.21336516737937927\n",
      "training: 72 batch 469 batch_loss: 0.21759828925132751\n",
      "training: 72 batch 470 batch_loss: 0.21630707383155823\n",
      "training: 72 batch 471 batch_loss: 0.2136838138103485\n",
      "training: 72 batch 472 batch_loss: 0.2189566195011139\n",
      "training: 72 batch 473 batch_loss: 0.21321320533752441\n",
      "training: 72 batch 474 batch_loss: 0.21265381574630737\n",
      "training: 72 batch 475 batch_loss: 0.21885403990745544\n",
      "training: 72 batch 476 batch_loss: 0.21655312180519104\n",
      "training: 72 batch 477 batch_loss: 0.220086932182312\n",
      "training: 72 batch 478 batch_loss: 0.21903324127197266\n",
      "training: 72 batch 479 batch_loss: 0.2157621681690216\n",
      "training: 72 batch 480 batch_loss: 0.21856313943862915\n",
      "training: 72 batch 481 batch_loss: 0.21851959824562073\n",
      "training: 72 batch 482 batch_loss: 0.21636033058166504\n",
      "training: 72 batch 483 batch_loss: 0.2148951292037964\n",
      "training: 72 batch 484 batch_loss: 0.212416410446167\n",
      "training: 72 batch 485 batch_loss: 0.21656781435012817\n",
      "training: 72 batch 486 batch_loss: 0.21719512343406677\n",
      "training: 72 batch 487 batch_loss: 0.2123759388923645\n",
      "training: 72 batch 488 batch_loss: 0.21583965420722961\n",
      "training: 72 batch 489 batch_loss: 0.2179153859615326\n",
      "training: 72 batch 490 batch_loss: 0.2158539891242981\n",
      "training: 72 batch 491 batch_loss: 0.21339505910873413\n",
      "training: 72 batch 492 batch_loss: 0.21385741233825684\n",
      "training: 72 batch 493 batch_loss: 0.21650534868240356\n",
      "training: 72 batch 494 batch_loss: 0.21356582641601562\n",
      "training: 72 batch 495 batch_loss: 0.21340656280517578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 72 batch 496 batch_loss: 0.2110345959663391\n",
      "training: 72 batch 497 batch_loss: 0.21645575761795044\n",
      "training: 72 batch 498 batch_loss: 0.2157626450061798\n",
      "training: 72 batch 499 batch_loss: 0.21320760250091553\n",
      "training: 72 batch 500 batch_loss: 0.21731314063072205\n",
      "training: 72 batch 501 batch_loss: 0.21529918909072876\n",
      "training: 72 batch 502 batch_loss: 0.2152966856956482\n",
      "training: 72 batch 503 batch_loss: 0.21638324856758118\n",
      "training: 72 batch 504 batch_loss: 0.22175031900405884\n",
      "training: 72 batch 505 batch_loss: 0.21648409962654114\n",
      "training: 72 batch 506 batch_loss: 0.2154957354068756\n",
      "training: 72 batch 507 batch_loss: 0.21957498788833618\n",
      "training: 72 batch 508 batch_loss: 0.21612447500228882\n",
      "training: 72 batch 509 batch_loss: 0.21503496170043945\n",
      "training: 72 batch 510 batch_loss: 0.21922075748443604\n",
      "training: 72 batch 511 batch_loss: 0.21052756905555725\n",
      "training: 72 batch 512 batch_loss: 0.2150266468524933\n",
      "training: 72 batch 513 batch_loss: 0.21061959862709045\n",
      "training: 72 batch 514 batch_loss: 0.21775680780410767\n",
      "training: 72 batch 515 batch_loss: 0.2219531536102295\n",
      "training: 72 batch 516 batch_loss: 0.21226635575294495\n",
      "training: 72 batch 517 batch_loss: 0.21976560354232788\n",
      "training: 72 batch 518 batch_loss: 0.2112542986869812\n",
      "training: 72 batch 519 batch_loss: 0.22035181522369385\n",
      "training: 72 batch 520 batch_loss: 0.2147228717803955\n",
      "training: 72 batch 521 batch_loss: 0.2151760756969452\n",
      "training: 72 batch 522 batch_loss: 0.21365147829055786\n",
      "training: 72 batch 523 batch_loss: 0.21069711446762085\n",
      "training: 72 batch 524 batch_loss: 0.2163112461566925\n",
      "training: 72 batch 525 batch_loss: 0.21586549282073975\n",
      "training: 72 batch 526 batch_loss: 0.21259847283363342\n",
      "training: 72 batch 527 batch_loss: 0.2123374044895172\n",
      "training: 72 batch 528 batch_loss: 0.21104657649993896\n",
      "training: 72 batch 529 batch_loss: 0.21602976322174072\n",
      "training: 72 batch 530 batch_loss: 0.21454283595085144\n",
      "training: 72 batch 531 batch_loss: 0.2180679738521576\n",
      "training: 72 batch 532 batch_loss: 0.21905535459518433\n",
      "training: 72 batch 533 batch_loss: 0.21461111307144165\n",
      "training: 72 batch 534 batch_loss: 0.21659815311431885\n",
      "training: 72 batch 535 batch_loss: 0.21489942073822021\n",
      "training: 72 batch 536 batch_loss: 0.2161669135093689\n",
      "training: 72 batch 537 batch_loss: 0.21094870567321777\n",
      "training: 72 batch 538 batch_loss: 0.2106713056564331\n",
      "training: 72 batch 539 batch_loss: 0.2159755825996399\n",
      "training: 72 batch 540 batch_loss: 0.22019028663635254\n",
      "training: 72 batch 541 batch_loss: 0.2154501974582672\n",
      "training: 72 batch 542 batch_loss: 0.215023010969162\n",
      "training: 72 batch 543 batch_loss: 0.21240907907485962\n",
      "training: 72 batch 544 batch_loss: 0.21308094263076782\n",
      "training: 72 batch 545 batch_loss: 0.215859055519104\n",
      "training: 72 batch 546 batch_loss: 0.21745705604553223\n",
      "training: 72 batch 547 batch_loss: 0.2113354504108429\n",
      "training: 72 batch 548 batch_loss: 0.21976229548454285\n",
      "training: 72 batch 549 batch_loss: 0.21638116240501404\n",
      "training: 72 batch 550 batch_loss: 0.215433269739151\n",
      "training: 72 batch 551 batch_loss: 0.2135125994682312\n",
      "training: 72 batch 552 batch_loss: 0.2161446213722229\n",
      "training: 72 batch 553 batch_loss: 0.2161760926246643\n",
      "training: 72 batch 554 batch_loss: 0.21543902158737183\n",
      "training: 72 batch 555 batch_loss: 0.21402686834335327\n",
      "training: 72 batch 556 batch_loss: 0.2134263813495636\n",
      "training: 72 batch 557 batch_loss: 0.21838891506195068\n",
      "training: 72 batch 558 batch_loss: 0.21692955493927002\n",
      "training: 72 batch 559 batch_loss: 0.21570640802383423\n",
      "training: 72 batch 560 batch_loss: 0.2155035436153412\n",
      "training: 72 batch 561 batch_loss: 0.21979516744613647\n",
      "training: 72 batch 562 batch_loss: 0.21679967641830444\n",
      "training: 72 batch 563 batch_loss: 0.2210911214351654\n",
      "training: 72 batch 564 batch_loss: 0.21788343787193298\n",
      "training: 72 batch 565 batch_loss: 0.21537867188453674\n",
      "training: 72 batch 566 batch_loss: 0.21189191937446594\n",
      "training: 72 batch 567 batch_loss: 0.2152787744998932\n",
      "training: 72 batch 568 batch_loss: 0.21567028760910034\n",
      "training: 72 batch 569 batch_loss: 0.2187141478061676\n",
      "training: 72 batch 570 batch_loss: 0.21635106205940247\n",
      "training: 72 batch 571 batch_loss: 0.2181694209575653\n",
      "training: 72 batch 572 batch_loss: 0.21079778671264648\n",
      "training: 72 batch 573 batch_loss: 0.2123734951019287\n",
      "training: 72 batch 574 batch_loss: 0.2134752869606018\n",
      "training: 72 batch 575 batch_loss: 0.2179865837097168\n",
      "training: 72 batch 576 batch_loss: 0.21299785375595093\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | NDCG:0.06528136987177953\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 73 batch 0 batch_loss: 0.21366682648658752\n",
      "training: 73 batch 1 batch_loss: 0.21661782264709473\n",
      "training: 73 batch 2 batch_loss: 0.21356570720672607\n",
      "training: 73 batch 3 batch_loss: 0.2139318287372589\n",
      "training: 73 batch 4 batch_loss: 0.21611475944519043\n",
      "training: 73 batch 5 batch_loss: 0.21057593822479248\n",
      "training: 73 batch 6 batch_loss: 0.21274518966674805\n",
      "training: 73 batch 7 batch_loss: 0.21320098638534546\n",
      "training: 73 batch 8 batch_loss: 0.2124531865119934\n",
      "training: 73 batch 9 batch_loss: 0.21510916948318481\n",
      "training: 73 batch 10 batch_loss: 0.21242690086364746\n",
      "training: 73 batch 11 batch_loss: 0.21377936005592346\n",
      "training: 73 batch 12 batch_loss: 0.21058908104896545\n",
      "training: 73 batch 13 batch_loss: 0.21627125144004822\n",
      "training: 73 batch 14 batch_loss: 0.21074506640434265\n",
      "training: 73 batch 15 batch_loss: 0.21331006288528442\n",
      "training: 73 batch 16 batch_loss: 0.21230179071426392\n",
      "training: 73 batch 17 batch_loss: 0.211867094039917\n",
      "training: 73 batch 18 batch_loss: 0.2128513753414154\n",
      "training: 73 batch 19 batch_loss: 0.21436473727226257\n",
      "training: 73 batch 20 batch_loss: 0.2128056287765503\n",
      "training: 73 batch 21 batch_loss: 0.22066059708595276\n",
      "training: 73 batch 22 batch_loss: 0.21511012315750122\n",
      "training: 73 batch 23 batch_loss: 0.215971440076828\n",
      "training: 73 batch 24 batch_loss: 0.21554780006408691\n",
      "training: 73 batch 25 batch_loss: 0.2137812376022339\n",
      "training: 73 batch 26 batch_loss: 0.21824327111244202\n",
      "training: 73 batch 27 batch_loss: 0.208743155002594\n",
      "training: 73 batch 28 batch_loss: 0.2131338119506836\n",
      "training: 73 batch 29 batch_loss: 0.21647769212722778\n",
      "training: 73 batch 30 batch_loss: 0.21837902069091797\n",
      "training: 73 batch 31 batch_loss: 0.21526893973350525\n",
      "training: 73 batch 32 batch_loss: 0.21381551027297974\n",
      "training: 73 batch 33 batch_loss: 0.21748340129852295\n",
      "training: 73 batch 34 batch_loss: 0.2119496464729309\n",
      "training: 73 batch 35 batch_loss: 0.21636685729026794\n",
      "training: 73 batch 36 batch_loss: 0.2174309492111206\n",
      "training: 73 batch 37 batch_loss: 0.21006852388381958\n",
      "training: 73 batch 38 batch_loss: 0.21234449744224548\n",
      "training: 73 batch 39 batch_loss: 0.21251988410949707\n",
      "training: 73 batch 40 batch_loss: 0.21371519565582275\n",
      "training: 73 batch 41 batch_loss: 0.21567872166633606\n",
      "training: 73 batch 42 batch_loss: 0.21481645107269287\n",
      "training: 73 batch 43 batch_loss: 0.21788442134857178\n",
      "training: 73 batch 44 batch_loss: 0.21725061535835266\n",
      "training: 73 batch 45 batch_loss: 0.2106759250164032\n",
      "training: 73 batch 46 batch_loss: 0.2129065990447998\n",
      "training: 73 batch 47 batch_loss: 0.212485671043396\n",
      "training: 73 batch 48 batch_loss: 0.2146434783935547\n",
      "training: 73 batch 49 batch_loss: 0.2128584384918213\n",
      "training: 73 batch 50 batch_loss: 0.2121850550174713\n",
      "training: 73 batch 51 batch_loss: 0.21745675802230835\n",
      "training: 73 batch 52 batch_loss: 0.21070170402526855\n",
      "training: 73 batch 53 batch_loss: 0.210757315158844\n",
      "training: 73 batch 54 batch_loss: 0.21157541871070862\n",
      "training: 73 batch 55 batch_loss: 0.21302804350852966\n",
      "training: 73 batch 56 batch_loss: 0.21273866295814514\n",
      "training: 73 batch 57 batch_loss: 0.21616744995117188\n",
      "training: 73 batch 58 batch_loss: 0.21540120244026184\n",
      "training: 73 batch 59 batch_loss: 0.21411103010177612\n",
      "training: 73 batch 60 batch_loss: 0.21362808346748352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 73 batch 61 batch_loss: 0.2125290334224701\n",
      "training: 73 batch 62 batch_loss: 0.21813446283340454\n",
      "training: 73 batch 63 batch_loss: 0.21629226207733154\n",
      "training: 73 batch 64 batch_loss: 0.2162759006023407\n",
      "training: 73 batch 65 batch_loss: 0.2156936228275299\n",
      "training: 73 batch 66 batch_loss: 0.21624338626861572\n",
      "training: 73 batch 67 batch_loss: 0.20936626195907593\n",
      "training: 73 batch 68 batch_loss: 0.217246413230896\n",
      "training: 73 batch 69 batch_loss: 0.21586069464683533\n",
      "training: 73 batch 70 batch_loss: 0.2142218053340912\n",
      "training: 73 batch 71 batch_loss: 0.2128857970237732\n",
      "training: 73 batch 72 batch_loss: 0.2193901538848877\n",
      "training: 73 batch 73 batch_loss: 0.2132924199104309\n",
      "training: 73 batch 74 batch_loss: 0.21491152048110962\n",
      "training: 73 batch 75 batch_loss: 0.21142929792404175\n",
      "training: 73 batch 76 batch_loss: 0.21050041913986206\n",
      "training: 73 batch 77 batch_loss: 0.213795006275177\n",
      "training: 73 batch 78 batch_loss: 0.21375849843025208\n",
      "training: 73 batch 79 batch_loss: 0.21199309825897217\n",
      "training: 73 batch 80 batch_loss: 0.21865206956863403\n",
      "training: 73 batch 81 batch_loss: 0.2140401303768158\n",
      "training: 73 batch 82 batch_loss: 0.21773609519004822\n",
      "training: 73 batch 83 batch_loss: 0.21669495105743408\n",
      "training: 73 batch 84 batch_loss: 0.20933863520622253\n",
      "training: 73 batch 85 batch_loss: 0.21311458945274353\n",
      "training: 73 batch 86 batch_loss: 0.21184715628623962\n",
      "training: 73 batch 87 batch_loss: 0.21532613039016724\n",
      "training: 73 batch 88 batch_loss: 0.21222898364067078\n",
      "training: 73 batch 89 batch_loss: 0.2108878493309021\n",
      "training: 73 batch 90 batch_loss: 0.21394741535186768\n",
      "training: 73 batch 91 batch_loss: 0.2171797752380371\n",
      "training: 73 batch 92 batch_loss: 0.214268296957016\n",
      "training: 73 batch 93 batch_loss: 0.21533778309822083\n",
      "training: 73 batch 94 batch_loss: 0.21491917967796326\n",
      "training: 73 batch 95 batch_loss: 0.21424877643585205\n",
      "training: 73 batch 96 batch_loss: 0.2162947952747345\n",
      "training: 73 batch 97 batch_loss: 0.21621638536453247\n",
      "training: 73 batch 98 batch_loss: 0.21102085709571838\n",
      "training: 73 batch 99 batch_loss: 0.20970582962036133\n",
      "training: 73 batch 100 batch_loss: 0.21462056040763855\n",
      "training: 73 batch 101 batch_loss: 0.21309134364128113\n",
      "training: 73 batch 102 batch_loss: 0.2151271104812622\n",
      "training: 73 batch 103 batch_loss: 0.20998692512512207\n",
      "training: 73 batch 104 batch_loss: 0.21588465571403503\n",
      "training: 73 batch 105 batch_loss: 0.21537932753562927\n",
      "training: 73 batch 106 batch_loss: 0.2106301486492157\n",
      "training: 73 batch 107 batch_loss: 0.21644729375839233\n",
      "training: 73 batch 108 batch_loss: 0.21352726221084595\n",
      "training: 73 batch 109 batch_loss: 0.21151664853096008\n",
      "training: 73 batch 110 batch_loss: 0.2139301896095276\n",
      "training: 73 batch 111 batch_loss: 0.2146604061126709\n",
      "training: 73 batch 112 batch_loss: 0.21139603853225708\n",
      "training: 73 batch 113 batch_loss: 0.21340274810791016\n",
      "training: 73 batch 114 batch_loss: 0.21899619698524475\n",
      "training: 73 batch 115 batch_loss: 0.21454769372940063\n",
      "training: 73 batch 116 batch_loss: 0.21670418977737427\n",
      "training: 73 batch 117 batch_loss: 0.219522625207901\n",
      "training: 73 batch 118 batch_loss: 0.2128124237060547\n",
      "training: 73 batch 119 batch_loss: 0.21386578679084778\n",
      "training: 73 batch 120 batch_loss: 0.21547871828079224\n",
      "training: 73 batch 121 batch_loss: 0.21805572509765625\n",
      "training: 73 batch 122 batch_loss: 0.21505513787269592\n",
      "training: 73 batch 123 batch_loss: 0.2137673795223236\n",
      "training: 73 batch 124 batch_loss: 0.21501177549362183\n",
      "training: 73 batch 125 batch_loss: 0.21028468012809753\n",
      "training: 73 batch 126 batch_loss: 0.21164777874946594\n",
      "training: 73 batch 127 batch_loss: 0.21687299013137817\n",
      "training: 73 batch 128 batch_loss: 0.2140728235244751\n",
      "training: 73 batch 129 batch_loss: 0.2166725993156433\n",
      "training: 73 batch 130 batch_loss: 0.21737411618232727\n",
      "training: 73 batch 131 batch_loss: 0.21730422973632812\n",
      "training: 73 batch 132 batch_loss: 0.2127850353717804\n",
      "training: 73 batch 133 batch_loss: 0.21235284209251404\n",
      "training: 73 batch 134 batch_loss: 0.2158103585243225\n",
      "training: 73 batch 135 batch_loss: 0.21496519446372986\n",
      "training: 73 batch 136 batch_loss: 0.21421310305595398\n",
      "training: 73 batch 137 batch_loss: 0.21572524309158325\n",
      "training: 73 batch 138 batch_loss: 0.21027767658233643\n",
      "training: 73 batch 139 batch_loss: 0.2150583267211914\n",
      "training: 73 batch 140 batch_loss: 0.21712622046470642\n",
      "training: 73 batch 141 batch_loss: 0.21262630820274353\n",
      "training: 73 batch 142 batch_loss: 0.21364760398864746\n",
      "training: 73 batch 143 batch_loss: 0.2150084376335144\n",
      "training: 73 batch 144 batch_loss: 0.21523433923721313\n",
      "training: 73 batch 145 batch_loss: 0.2173026204109192\n",
      "training: 73 batch 146 batch_loss: 0.21439239382743835\n",
      "training: 73 batch 147 batch_loss: 0.21555551886558533\n",
      "training: 73 batch 148 batch_loss: 0.21362027525901794\n",
      "training: 73 batch 149 batch_loss: 0.21754497289657593\n",
      "training: 73 batch 150 batch_loss: 0.2149653136730194\n",
      "training: 73 batch 151 batch_loss: 0.2154492735862732\n",
      "training: 73 batch 152 batch_loss: 0.2130630910396576\n",
      "training: 73 batch 153 batch_loss: 0.21685630083084106\n",
      "training: 73 batch 154 batch_loss: 0.21564993262290955\n",
      "training: 73 batch 155 batch_loss: 0.21358966827392578\n",
      "training: 73 batch 156 batch_loss: 0.21587002277374268\n",
      "training: 73 batch 157 batch_loss: 0.2161954641342163\n",
      "training: 73 batch 158 batch_loss: 0.2141954004764557\n",
      "training: 73 batch 159 batch_loss: 0.2161628007888794\n",
      "training: 73 batch 160 batch_loss: 0.21226298809051514\n",
      "training: 73 batch 161 batch_loss: 0.21421509981155396\n",
      "training: 73 batch 162 batch_loss: 0.21811702847480774\n",
      "training: 73 batch 163 batch_loss: 0.21909815073013306\n",
      "training: 73 batch 164 batch_loss: 0.21214261651039124\n",
      "training: 73 batch 165 batch_loss: 0.21619224548339844\n",
      "training: 73 batch 166 batch_loss: 0.21524274349212646\n",
      "training: 73 batch 167 batch_loss: 0.21324831247329712\n",
      "training: 73 batch 168 batch_loss: 0.21594968438148499\n",
      "training: 73 batch 169 batch_loss: 0.21273785829544067\n",
      "training: 73 batch 170 batch_loss: 0.21235686540603638\n",
      "training: 73 batch 171 batch_loss: 0.21164190769195557\n",
      "training: 73 batch 172 batch_loss: 0.2154727578163147\n",
      "training: 73 batch 173 batch_loss: 0.21758025884628296\n",
      "training: 73 batch 174 batch_loss: 0.21721526980400085\n",
      "training: 73 batch 175 batch_loss: 0.21256029605865479\n",
      "training: 73 batch 176 batch_loss: 0.21525338292121887\n",
      "training: 73 batch 177 batch_loss: 0.21367305517196655\n",
      "training: 73 batch 178 batch_loss: 0.21678969264030457\n",
      "training: 73 batch 179 batch_loss: 0.21315103769302368\n",
      "training: 73 batch 180 batch_loss: 0.21353143453598022\n",
      "training: 73 batch 181 batch_loss: 0.21787038445472717\n",
      "training: 73 batch 182 batch_loss: 0.21522852778434753\n",
      "training: 73 batch 183 batch_loss: 0.21665716171264648\n",
      "training: 73 batch 184 batch_loss: 0.2119562029838562\n",
      "training: 73 batch 185 batch_loss: 0.2115148901939392\n",
      "training: 73 batch 186 batch_loss: 0.21094143390655518\n",
      "training: 73 batch 187 batch_loss: 0.21284526586532593\n",
      "training: 73 batch 188 batch_loss: 0.21985256671905518\n",
      "training: 73 batch 189 batch_loss: 0.2147819995880127\n",
      "training: 73 batch 190 batch_loss: 0.21619760990142822\n",
      "training: 73 batch 191 batch_loss: 0.21586233377456665\n",
      "training: 73 batch 192 batch_loss: 0.21234920620918274\n",
      "training: 73 batch 193 batch_loss: 0.21495187282562256\n",
      "training: 73 batch 194 batch_loss: 0.20982810854911804\n",
      "training: 73 batch 195 batch_loss: 0.21237263083457947\n",
      "training: 73 batch 196 batch_loss: 0.21388506889343262\n",
      "training: 73 batch 197 batch_loss: 0.21605059504508972\n",
      "training: 73 batch 198 batch_loss: 0.20934656262397766\n",
      "training: 73 batch 199 batch_loss: 0.2148161232471466\n",
      "training: 73 batch 200 batch_loss: 0.21381545066833496\n",
      "training: 73 batch 201 batch_loss: 0.2118341028690338\n",
      "training: 73 batch 202 batch_loss: 0.21674156188964844\n",
      "training: 73 batch 203 batch_loss: 0.21138983964920044\n",
      "training: 73 batch 204 batch_loss: 0.2140357792377472\n",
      "training: 73 batch 205 batch_loss: 0.21342825889587402\n",
      "training: 73 batch 206 batch_loss: 0.21610325574874878\n",
      "training: 73 batch 207 batch_loss: 0.21673652529716492\n",
      "training: 73 batch 208 batch_loss: 0.2122483253479004\n",
      "training: 73 batch 209 batch_loss: 0.21704566478729248\n",
      "training: 73 batch 210 batch_loss: 0.21262961626052856\n",
      "training: 73 batch 211 batch_loss: 0.21514946222305298\n",
      "training: 73 batch 212 batch_loss: 0.21824553608894348\n",
      "training: 73 batch 213 batch_loss: 0.21299010515213013\n",
      "training: 73 batch 214 batch_loss: 0.2132110893726349\n",
      "training: 73 batch 215 batch_loss: 0.2158706784248352\n",
      "training: 73 batch 216 batch_loss: 0.21279186010360718\n",
      "training: 73 batch 217 batch_loss: 0.2147054672241211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 73 batch 218 batch_loss: 0.21809712052345276\n",
      "training: 73 batch 219 batch_loss: 0.2180020809173584\n",
      "training: 73 batch 220 batch_loss: 0.21512284874916077\n",
      "training: 73 batch 221 batch_loss: 0.21199768781661987\n",
      "training: 73 batch 222 batch_loss: 0.21859917044639587\n",
      "training: 73 batch 223 batch_loss: 0.21121397614479065\n",
      "training: 73 batch 224 batch_loss: 0.21415239572525024\n",
      "training: 73 batch 225 batch_loss: 0.2166500985622406\n",
      "training: 73 batch 226 batch_loss: 0.21417757868766785\n",
      "training: 73 batch 227 batch_loss: 0.21950626373291016\n",
      "training: 73 batch 228 batch_loss: 0.21400588750839233\n",
      "training: 73 batch 229 batch_loss: 0.21684998273849487\n",
      "training: 73 batch 230 batch_loss: 0.21433097124099731\n",
      "training: 73 batch 231 batch_loss: 0.21658122539520264\n",
      "training: 73 batch 232 batch_loss: 0.21347743272781372\n",
      "training: 73 batch 233 batch_loss: 0.21719959378242493\n",
      "training: 73 batch 234 batch_loss: 0.21349409222602844\n",
      "training: 73 batch 235 batch_loss: 0.21677172183990479\n",
      "training: 73 batch 236 batch_loss: 0.2157931923866272\n",
      "training: 73 batch 237 batch_loss: 0.21615347266197205\n",
      "training: 73 batch 238 batch_loss: 0.21296927332878113\n",
      "training: 73 batch 239 batch_loss: 0.2131449580192566\n",
      "training: 73 batch 240 batch_loss: 0.21369972825050354\n",
      "training: 73 batch 241 batch_loss: 0.2138986587524414\n",
      "training: 73 batch 242 batch_loss: 0.21460413932800293\n",
      "training: 73 batch 243 batch_loss: 0.21704643964767456\n",
      "training: 73 batch 244 batch_loss: 0.2203996777534485\n",
      "training: 73 batch 245 batch_loss: 0.21414434909820557\n",
      "training: 73 batch 246 batch_loss: 0.21567213535308838\n",
      "training: 73 batch 247 batch_loss: 0.21510180830955505\n",
      "training: 73 batch 248 batch_loss: 0.21547731757164001\n",
      "training: 73 batch 249 batch_loss: 0.21547961235046387\n",
      "training: 73 batch 250 batch_loss: 0.21068495512008667\n",
      "training: 73 batch 251 batch_loss: 0.21267104148864746\n",
      "training: 73 batch 252 batch_loss: 0.2181098759174347\n",
      "training: 73 batch 253 batch_loss: 0.21636632084846497\n",
      "training: 73 batch 254 batch_loss: 0.21414655447006226\n",
      "training: 73 batch 255 batch_loss: 0.21273601055145264\n",
      "training: 73 batch 256 batch_loss: 0.21733683347702026\n",
      "training: 73 batch 257 batch_loss: 0.21673282980918884\n",
      "training: 73 batch 258 batch_loss: 0.21500885486602783\n",
      "training: 73 batch 259 batch_loss: 0.2222653329372406\n",
      "training: 73 batch 260 batch_loss: 0.2148938775062561\n",
      "training: 73 batch 261 batch_loss: 0.2115488350391388\n",
      "training: 73 batch 262 batch_loss: 0.21467605233192444\n",
      "training: 73 batch 263 batch_loss: 0.2150936722755432\n",
      "training: 73 batch 264 batch_loss: 0.21569553017616272\n",
      "training: 73 batch 265 batch_loss: 0.22282066941261292\n",
      "training: 73 batch 266 batch_loss: 0.217149019241333\n",
      "training: 73 batch 267 batch_loss: 0.21334227919578552\n",
      "training: 73 batch 268 batch_loss: 0.21029987931251526\n",
      "training: 73 batch 269 batch_loss: 0.21762055158615112\n",
      "training: 73 batch 270 batch_loss: 0.21415400505065918\n",
      "training: 73 batch 271 batch_loss: 0.21559441089630127\n",
      "training: 73 batch 272 batch_loss: 0.21286696195602417\n",
      "training: 73 batch 273 batch_loss: 0.2175898551940918\n",
      "training: 73 batch 274 batch_loss: 0.21286755800247192\n",
      "training: 73 batch 275 batch_loss: 0.2128014862537384\n",
      "training: 73 batch 276 batch_loss: 0.21840423345565796\n",
      "training: 73 batch 277 batch_loss: 0.21518254280090332\n",
      "training: 73 batch 278 batch_loss: 0.21962958574295044\n",
      "training: 73 batch 279 batch_loss: 0.21606549620628357\n",
      "training: 73 batch 280 batch_loss: 0.2121087610721588\n",
      "training: 73 batch 281 batch_loss: 0.21802407503128052\n",
      "training: 73 batch 282 batch_loss: 0.2150987982749939\n",
      "training: 73 batch 283 batch_loss: 0.20941320061683655\n",
      "training: 73 batch 284 batch_loss: 0.21531355381011963\n",
      "training: 73 batch 285 batch_loss: 0.21341070532798767\n",
      "training: 73 batch 286 batch_loss: 0.2132495641708374\n",
      "training: 73 batch 287 batch_loss: 0.2208632230758667\n",
      "training: 73 batch 288 batch_loss: 0.21685421466827393\n",
      "training: 73 batch 289 batch_loss: 0.21304485201835632\n",
      "training: 73 batch 290 batch_loss: 0.21453678607940674\n",
      "training: 73 batch 291 batch_loss: 0.21309426426887512\n",
      "training: 73 batch 292 batch_loss: 0.21635699272155762\n",
      "training: 73 batch 293 batch_loss: 0.21224460005760193\n",
      "training: 73 batch 294 batch_loss: 0.2178228199481964\n",
      "training: 73 batch 295 batch_loss: 0.2157442569732666\n",
      "training: 73 batch 296 batch_loss: 0.21408876776695251\n",
      "training: 73 batch 297 batch_loss: 0.21349745988845825\n",
      "training: 73 batch 298 batch_loss: 0.21622982621192932\n",
      "training: 73 batch 299 batch_loss: 0.21320268511772156\n",
      "training: 73 batch 300 batch_loss: 0.2175963819026947\n",
      "training: 73 batch 301 batch_loss: 0.21327906847000122\n",
      "training: 73 batch 302 batch_loss: 0.22008422017097473\n",
      "training: 73 batch 303 batch_loss: 0.21490272879600525\n",
      "training: 73 batch 304 batch_loss: 0.212013840675354\n",
      "training: 73 batch 305 batch_loss: 0.21453088521957397\n",
      "training: 73 batch 306 batch_loss: 0.21702620387077332\n",
      "training: 73 batch 307 batch_loss: 0.2166564166545868\n",
      "training: 73 batch 308 batch_loss: 0.21239489316940308\n",
      "training: 73 batch 309 batch_loss: 0.2131681740283966\n",
      "training: 73 batch 310 batch_loss: 0.21567517518997192\n",
      "training: 73 batch 311 batch_loss: 0.21438029408454895\n",
      "training: 73 batch 312 batch_loss: 0.21687978506088257\n",
      "training: 73 batch 313 batch_loss: 0.21509480476379395\n",
      "training: 73 batch 314 batch_loss: 0.2202359437942505\n",
      "training: 73 batch 315 batch_loss: 0.21845877170562744\n",
      "training: 73 batch 316 batch_loss: 0.21283197402954102\n",
      "training: 73 batch 317 batch_loss: 0.21669861674308777\n",
      "training: 73 batch 318 batch_loss: 0.21636831760406494\n",
      "training: 73 batch 319 batch_loss: 0.21377915143966675\n",
      "training: 73 batch 320 batch_loss: 0.21757936477661133\n",
      "training: 73 batch 321 batch_loss: 0.2132059931755066\n",
      "training: 73 batch 322 batch_loss: 0.21517515182495117\n",
      "training: 73 batch 323 batch_loss: 0.2150675654411316\n",
      "training: 73 batch 324 batch_loss: 0.21644291281700134\n",
      "training: 73 batch 325 batch_loss: 0.2166750431060791\n",
      "training: 73 batch 326 batch_loss: 0.21322733163833618\n",
      "training: 73 batch 327 batch_loss: 0.21487665176391602\n",
      "training: 73 batch 328 batch_loss: 0.2159717082977295\n",
      "training: 73 batch 329 batch_loss: 0.21660760045051575\n",
      "training: 73 batch 330 batch_loss: 0.2130734920501709\n",
      "training: 73 batch 331 batch_loss: 0.218995600938797\n",
      "training: 73 batch 332 batch_loss: 0.2170417308807373\n",
      "training: 73 batch 333 batch_loss: 0.21676620841026306\n",
      "training: 73 batch 334 batch_loss: 0.2130439281463623\n",
      "training: 73 batch 335 batch_loss: 0.2161823809146881\n",
      "training: 73 batch 336 batch_loss: 0.21779614686965942\n",
      "training: 73 batch 337 batch_loss: 0.21556979417800903\n",
      "training: 73 batch 338 batch_loss: 0.21789467334747314\n",
      "training: 73 batch 339 batch_loss: 0.2170950472354889\n",
      "training: 73 batch 340 batch_loss: 0.22102922201156616\n",
      "training: 73 batch 341 batch_loss: 0.21177396178245544\n",
      "training: 73 batch 342 batch_loss: 0.21453607082366943\n",
      "training: 73 batch 343 batch_loss: 0.21458429098129272\n",
      "training: 73 batch 344 batch_loss: 0.21318143606185913\n",
      "training: 73 batch 345 batch_loss: 0.21615469455718994\n",
      "training: 73 batch 346 batch_loss: 0.21387168765068054\n",
      "training: 73 batch 347 batch_loss: 0.2159382700920105\n",
      "training: 73 batch 348 batch_loss: 0.2212514877319336\n",
      "training: 73 batch 349 batch_loss: 0.21773937344551086\n",
      "training: 73 batch 350 batch_loss: 0.21750706434249878\n",
      "training: 73 batch 351 batch_loss: 0.21855193376541138\n",
      "training: 73 batch 352 batch_loss: 0.21608221530914307\n",
      "training: 73 batch 353 batch_loss: 0.2122853398323059\n",
      "training: 73 batch 354 batch_loss: 0.21673226356506348\n",
      "training: 73 batch 355 batch_loss: 0.21080857515335083\n",
      "training: 73 batch 356 batch_loss: 0.22142678499221802\n",
      "training: 73 batch 357 batch_loss: 0.21483451128005981\n",
      "training: 73 batch 358 batch_loss: 0.2171434462070465\n",
      "training: 73 batch 359 batch_loss: 0.21437546610832214\n",
      "training: 73 batch 360 batch_loss: 0.2126399576663971\n",
      "training: 73 batch 361 batch_loss: 0.21840602159500122\n",
      "training: 73 batch 362 batch_loss: 0.21163421869277954\n",
      "training: 73 batch 363 batch_loss: 0.2179623246192932\n",
      "training: 73 batch 364 batch_loss: 0.21544751524925232\n",
      "training: 73 batch 365 batch_loss: 0.21576368808746338\n",
      "training: 73 batch 366 batch_loss: 0.21727579832077026\n",
      "training: 73 batch 367 batch_loss: 0.21766656637191772\n",
      "training: 73 batch 368 batch_loss: 0.21581503748893738\n",
      "training: 73 batch 369 batch_loss: 0.21372905373573303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 73 batch 370 batch_loss: 0.22045555710792542\n",
      "training: 73 batch 371 batch_loss: 0.22048982977867126\n",
      "training: 73 batch 372 batch_loss: 0.21186116337776184\n",
      "training: 73 batch 373 batch_loss: 0.21952831745147705\n",
      "training: 73 batch 374 batch_loss: 0.2191861867904663\n",
      "training: 73 batch 375 batch_loss: 0.21899786591529846\n",
      "training: 73 batch 376 batch_loss: 0.21576890349388123\n",
      "training: 73 batch 377 batch_loss: 0.2163652777671814\n",
      "training: 73 batch 378 batch_loss: 0.21996760368347168\n",
      "training: 73 batch 379 batch_loss: 0.218963623046875\n",
      "training: 73 batch 380 batch_loss: 0.21665075421333313\n",
      "training: 73 batch 381 batch_loss: 0.2153553068637848\n",
      "training: 73 batch 382 batch_loss: 0.22034144401550293\n",
      "training: 73 batch 383 batch_loss: 0.2147999405860901\n",
      "training: 73 batch 384 batch_loss: 0.21402525901794434\n",
      "training: 73 batch 385 batch_loss: 0.21421101689338684\n",
      "training: 73 batch 386 batch_loss: 0.2146684229373932\n",
      "training: 73 batch 387 batch_loss: 0.21695315837860107\n",
      "training: 73 batch 388 batch_loss: 0.2164318859577179\n",
      "training: 73 batch 389 batch_loss: 0.213463693857193\n",
      "training: 73 batch 390 batch_loss: 0.21840861439704895\n",
      "training: 73 batch 391 batch_loss: 0.21213409304618835\n",
      "training: 73 batch 392 batch_loss: 0.21821600198745728\n",
      "training: 73 batch 393 batch_loss: 0.21545898914337158\n",
      "training: 73 batch 394 batch_loss: 0.2164498269557953\n",
      "training: 73 batch 395 batch_loss: 0.21739646792411804\n",
      "training: 73 batch 396 batch_loss: 0.21464228630065918\n",
      "training: 73 batch 397 batch_loss: 0.21867245435714722\n",
      "training: 73 batch 398 batch_loss: 0.21313738822937012\n",
      "training: 73 batch 399 batch_loss: 0.21241825819015503\n",
      "training: 73 batch 400 batch_loss: 0.2163466215133667\n",
      "training: 73 batch 401 batch_loss: 0.21915221214294434\n",
      "training: 73 batch 402 batch_loss: 0.21960243582725525\n",
      "training: 73 batch 403 batch_loss: 0.21426111459732056\n",
      "training: 73 batch 404 batch_loss: 0.21505296230316162\n",
      "training: 73 batch 405 batch_loss: 0.21627098321914673\n",
      "training: 73 batch 406 batch_loss: 0.20735621452331543\n",
      "training: 73 batch 407 batch_loss: 0.214725524187088\n",
      "training: 73 batch 408 batch_loss: 0.2164861559867859\n",
      "training: 73 batch 409 batch_loss: 0.21262425184249878\n",
      "training: 73 batch 410 batch_loss: 0.21902969479560852\n",
      "training: 73 batch 411 batch_loss: 0.21724286675453186\n",
      "training: 73 batch 412 batch_loss: 0.21633240580558777\n",
      "training: 73 batch 413 batch_loss: 0.2095874547958374\n",
      "training: 73 batch 414 batch_loss: 0.21824049949645996\n",
      "training: 73 batch 415 batch_loss: 0.21403416991233826\n",
      "training: 73 batch 416 batch_loss: 0.21694684028625488\n",
      "training: 73 batch 417 batch_loss: 0.2180461287498474\n",
      "training: 73 batch 418 batch_loss: 0.22027325630187988\n",
      "training: 73 batch 419 batch_loss: 0.21286219358444214\n",
      "training: 73 batch 420 batch_loss: 0.2156393826007843\n",
      "training: 73 batch 421 batch_loss: 0.21423116326332092\n",
      "training: 73 batch 422 batch_loss: 0.2138071358203888\n",
      "training: 73 batch 423 batch_loss: 0.21547061204910278\n",
      "training: 73 batch 424 batch_loss: 0.21444177627563477\n",
      "training: 73 batch 425 batch_loss: 0.20986920595169067\n",
      "training: 73 batch 426 batch_loss: 0.2094493806362152\n",
      "training: 73 batch 427 batch_loss: 0.21384182572364807\n",
      "training: 73 batch 428 batch_loss: 0.21938687562942505\n",
      "training: 73 batch 429 batch_loss: 0.21838515996932983\n",
      "training: 73 batch 430 batch_loss: 0.21400126814842224\n",
      "training: 73 batch 431 batch_loss: 0.212728351354599\n",
      "training: 73 batch 432 batch_loss: 0.2162632942199707\n",
      "training: 73 batch 433 batch_loss: 0.2144910991191864\n",
      "training: 73 batch 434 batch_loss: 0.2138359248638153\n",
      "training: 73 batch 435 batch_loss: 0.21570655703544617\n",
      "training: 73 batch 436 batch_loss: 0.21302741765975952\n",
      "training: 73 batch 437 batch_loss: 0.21349012851715088\n",
      "training: 73 batch 438 batch_loss: 0.21556922793388367\n",
      "training: 73 batch 439 batch_loss: 0.216586172580719\n",
      "training: 73 batch 440 batch_loss: 0.21196198463439941\n",
      "training: 73 batch 441 batch_loss: 0.2163795530796051\n",
      "training: 73 batch 442 batch_loss: 0.21264493465423584\n",
      "training: 73 batch 443 batch_loss: 0.21838292479515076\n",
      "training: 73 batch 444 batch_loss: 0.21747490763664246\n",
      "training: 73 batch 445 batch_loss: 0.2153497338294983\n",
      "training: 73 batch 446 batch_loss: 0.21788084506988525\n",
      "training: 73 batch 447 batch_loss: 0.21754860877990723\n",
      "training: 73 batch 448 batch_loss: 0.21696481108665466\n",
      "training: 73 batch 449 batch_loss: 0.21896770596504211\n",
      "training: 73 batch 450 batch_loss: 0.2164905071258545\n",
      "training: 73 batch 451 batch_loss: 0.22074416279792786\n",
      "training: 73 batch 452 batch_loss: 0.21721196174621582\n",
      "training: 73 batch 453 batch_loss: 0.213830828666687\n",
      "training: 73 batch 454 batch_loss: 0.21953153610229492\n",
      "training: 73 batch 455 batch_loss: 0.21642610430717468\n",
      "training: 73 batch 456 batch_loss: 0.2179727554321289\n",
      "training: 73 batch 457 batch_loss: 0.21921253204345703\n",
      "training: 73 batch 458 batch_loss: 0.21731781959533691\n",
      "training: 73 batch 459 batch_loss: 0.21534383296966553\n",
      "training: 73 batch 460 batch_loss: 0.21222275495529175\n",
      "training: 73 batch 461 batch_loss: 0.21354889869689941\n",
      "training: 73 batch 462 batch_loss: 0.21981170773506165\n",
      "training: 73 batch 463 batch_loss: 0.21510106325149536\n",
      "training: 73 batch 464 batch_loss: 0.21781978011131287\n",
      "training: 73 batch 465 batch_loss: 0.21956774592399597\n",
      "training: 73 batch 466 batch_loss: 0.21432307362556458\n",
      "training: 73 batch 467 batch_loss: 0.21695321798324585\n",
      "training: 73 batch 468 batch_loss: 0.2161836326122284\n",
      "training: 73 batch 469 batch_loss: 0.2173411250114441\n",
      "training: 73 batch 470 batch_loss: 0.21512341499328613\n",
      "training: 73 batch 471 batch_loss: 0.21412041783332825\n",
      "training: 73 batch 472 batch_loss: 0.21751832962036133\n",
      "training: 73 batch 473 batch_loss: 0.21711447834968567\n",
      "training: 73 batch 474 batch_loss: 0.2171095311641693\n",
      "training: 73 batch 475 batch_loss: 0.21528246998786926\n",
      "training: 73 batch 476 batch_loss: 0.216973215341568\n",
      "training: 73 batch 477 batch_loss: 0.21315935254096985\n",
      "training: 73 batch 478 batch_loss: 0.22084665298461914\n",
      "training: 73 batch 479 batch_loss: 0.21832573413848877\n",
      "training: 73 batch 480 batch_loss: 0.2171703577041626\n",
      "training: 73 batch 481 batch_loss: 0.21513190865516663\n",
      "training: 73 batch 482 batch_loss: 0.21839672327041626\n",
      "training: 73 batch 483 batch_loss: 0.2143409550189972\n",
      "training: 73 batch 484 batch_loss: 0.2182602882385254\n",
      "training: 73 batch 485 batch_loss: 0.21614080667495728\n",
      "training: 73 batch 486 batch_loss: 0.21471628546714783\n",
      "training: 73 batch 487 batch_loss: 0.2181137204170227\n",
      "training: 73 batch 488 batch_loss: 0.21583786606788635\n",
      "training: 73 batch 489 batch_loss: 0.21910324692726135\n",
      "training: 73 batch 490 batch_loss: 0.2151406705379486\n",
      "training: 73 batch 491 batch_loss: 0.21665602922439575\n",
      "training: 73 batch 492 batch_loss: 0.21991786360740662\n",
      "training: 73 batch 493 batch_loss: 0.21493899822235107\n",
      "training: 73 batch 494 batch_loss: 0.21572798490524292\n",
      "training: 73 batch 495 batch_loss: 0.2139788269996643\n",
      "training: 73 batch 496 batch_loss: 0.21201074123382568\n",
      "training: 73 batch 497 batch_loss: 0.21750283241271973\n",
      "training: 73 batch 498 batch_loss: 0.21569004654884338\n",
      "training: 73 batch 499 batch_loss: 0.21868202090263367\n",
      "training: 73 batch 500 batch_loss: 0.21297025680541992\n",
      "training: 73 batch 501 batch_loss: 0.22253915667533875\n",
      "training: 73 batch 502 batch_loss: 0.21461057662963867\n",
      "training: 73 batch 503 batch_loss: 0.21544420719146729\n",
      "training: 73 batch 504 batch_loss: 0.216806560754776\n",
      "training: 73 batch 505 batch_loss: 0.21181294322013855\n",
      "training: 73 batch 506 batch_loss: 0.21314966678619385\n",
      "training: 73 batch 507 batch_loss: 0.21656706929206848\n",
      "training: 73 batch 508 batch_loss: 0.21519696712493896\n",
      "training: 73 batch 509 batch_loss: 0.21793153882026672\n",
      "training: 73 batch 510 batch_loss: 0.21862846612930298\n",
      "training: 73 batch 511 batch_loss: 0.21838271617889404\n",
      "training: 73 batch 512 batch_loss: 0.2213849425315857\n",
      "training: 73 batch 513 batch_loss: 0.21170365810394287\n",
      "training: 73 batch 514 batch_loss: 0.21465665102005005\n",
      "training: 73 batch 515 batch_loss: 0.2164156436920166\n",
      "training: 73 batch 516 batch_loss: 0.2166185975074768\n",
      "training: 73 batch 517 batch_loss: 0.21661841869354248\n",
      "training: 73 batch 518 batch_loss: 0.21545925736427307\n",
      "training: 73 batch 519 batch_loss: 0.21667274832725525\n",
      "training: 73 batch 520 batch_loss: 0.2174663245677948\n",
      "training: 73 batch 521 batch_loss: 0.21951252222061157\n",
      "training: 73 batch 522 batch_loss: 0.21523559093475342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 73 batch 523 batch_loss: 0.2154708206653595\n",
      "training: 73 batch 524 batch_loss: 0.213769793510437\n",
      "training: 73 batch 525 batch_loss: 0.21734896302223206\n",
      "training: 73 batch 526 batch_loss: 0.21716269850730896\n",
      "training: 73 batch 527 batch_loss: 0.2210475504398346\n",
      "training: 73 batch 528 batch_loss: 0.21313348412513733\n",
      "training: 73 batch 529 batch_loss: 0.21736761927604675\n",
      "training: 73 batch 530 batch_loss: 0.21587777137756348\n",
      "training: 73 batch 531 batch_loss: 0.21711528301239014\n",
      "training: 73 batch 532 batch_loss: 0.21822905540466309\n",
      "training: 73 batch 533 batch_loss: 0.2195989489555359\n",
      "training: 73 batch 534 batch_loss: 0.22160395979881287\n",
      "training: 73 batch 535 batch_loss: 0.2153635323047638\n",
      "training: 73 batch 536 batch_loss: 0.21763938665390015\n",
      "training: 73 batch 537 batch_loss: 0.21685832738876343\n",
      "training: 73 batch 538 batch_loss: 0.2139829397201538\n",
      "training: 73 batch 539 batch_loss: 0.21887892484664917\n",
      "training: 73 batch 540 batch_loss: 0.2177642583847046\n",
      "training: 73 batch 541 batch_loss: 0.21848374605178833\n",
      "training: 73 batch 542 batch_loss: 0.21833091974258423\n",
      "training: 73 batch 543 batch_loss: 0.21650195121765137\n",
      "training: 73 batch 544 batch_loss: 0.21491271257400513\n",
      "training: 73 batch 545 batch_loss: 0.2173798382282257\n",
      "training: 73 batch 546 batch_loss: 0.22015467286109924\n",
      "training: 73 batch 547 batch_loss: 0.21857976913452148\n",
      "training: 73 batch 548 batch_loss: 0.21628692746162415\n",
      "training: 73 batch 549 batch_loss: 0.2191697657108307\n",
      "training: 73 batch 550 batch_loss: 0.2145257592201233\n",
      "training: 73 batch 551 batch_loss: 0.21616560220718384\n",
      "training: 73 batch 552 batch_loss: 0.2168085277080536\n",
      "training: 73 batch 553 batch_loss: 0.2180517017841339\n",
      "training: 73 batch 554 batch_loss: 0.21621105074882507\n",
      "training: 73 batch 555 batch_loss: 0.21328753232955933\n",
      "training: 73 batch 556 batch_loss: 0.21716919541358948\n",
      "training: 73 batch 557 batch_loss: 0.222825288772583\n",
      "training: 73 batch 558 batch_loss: 0.22018936276435852\n",
      "training: 73 batch 559 batch_loss: 0.21287432312965393\n",
      "training: 73 batch 560 batch_loss: 0.21709826588630676\n",
      "training: 73 batch 561 batch_loss: 0.2158534824848175\n",
      "training: 73 batch 562 batch_loss: 0.21475562453269958\n",
      "training: 73 batch 563 batch_loss: 0.2209608256816864\n",
      "training: 73 batch 564 batch_loss: 0.21644183993339539\n",
      "training: 73 batch 565 batch_loss: 0.22087138891220093\n",
      "training: 73 batch 566 batch_loss: 0.21334877610206604\n",
      "training: 73 batch 567 batch_loss: 0.2166537642478943\n",
      "training: 73 batch 568 batch_loss: 0.22042745351791382\n",
      "training: 73 batch 569 batch_loss: 0.21908631920814514\n",
      "training: 73 batch 570 batch_loss: 0.2124352753162384\n",
      "training: 73 batch 571 batch_loss: 0.22027820348739624\n",
      "training: 73 batch 572 batch_loss: 0.21386897563934326\n",
      "training: 73 batch 573 batch_loss: 0.21152693033218384\n",
      "training: 73 batch 574 batch_loss: 0.21761161088943481\n",
      "training: 73 batch 575 batch_loss: 0.21669286489486694\n",
      "training: 73 batch 576 batch_loss: 0.2137400209903717\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 73, Hit Ratio:0.033498449330930435 | Precision:0.04942494839280448 | Recall:0.06515242000333213 | NDCG:0.06427275360589159\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 74 batch 0 batch_loss: 0.20934727787971497\n",
      "training: 74 batch 1 batch_loss: 0.21793591976165771\n",
      "training: 74 batch 2 batch_loss: 0.21606677770614624\n",
      "training: 74 batch 3 batch_loss: 0.2134353220462799\n",
      "training: 74 batch 4 batch_loss: 0.2161378264427185\n",
      "training: 74 batch 5 batch_loss: 0.21523058414459229\n",
      "training: 74 batch 6 batch_loss: 0.21641004085540771\n",
      "training: 74 batch 7 batch_loss: 0.22029584646224976\n",
      "training: 74 batch 8 batch_loss: 0.21153461933135986\n",
      "training: 74 batch 9 batch_loss: 0.21753886342048645\n",
      "training: 74 batch 10 batch_loss: 0.21057024598121643\n",
      "training: 74 batch 11 batch_loss: 0.2174226939678192\n",
      "training: 74 batch 12 batch_loss: 0.21062695980072021\n",
      "training: 74 batch 13 batch_loss: 0.21408823132514954\n",
      "training: 74 batch 14 batch_loss: 0.21214213967323303\n",
      "training: 74 batch 15 batch_loss: 0.21406280994415283\n",
      "training: 74 batch 16 batch_loss: 0.2120191752910614\n",
      "training: 74 batch 17 batch_loss: 0.21416398882865906\n",
      "training: 74 batch 18 batch_loss: 0.21571367979049683\n",
      "training: 74 batch 19 batch_loss: 0.21414616703987122\n",
      "training: 74 batch 20 batch_loss: 0.21492493152618408\n",
      "training: 74 batch 21 batch_loss: 0.21436798572540283\n",
      "training: 74 batch 22 batch_loss: 0.21614274382591248\n",
      "training: 74 batch 23 batch_loss: 0.21510076522827148\n",
      "training: 74 batch 24 batch_loss: 0.21173828840255737\n",
      "training: 74 batch 25 batch_loss: 0.21847355365753174\n",
      "training: 74 batch 26 batch_loss: 0.2190125584602356\n",
      "training: 74 batch 27 batch_loss: 0.21661120653152466\n",
      "training: 74 batch 28 batch_loss: 0.2182309627532959\n",
      "training: 74 batch 29 batch_loss: 0.21266862750053406\n",
      "training: 74 batch 30 batch_loss: 0.21385449171066284\n",
      "training: 74 batch 31 batch_loss: 0.21402105689048767\n",
      "training: 74 batch 32 batch_loss: 0.21378910541534424\n",
      "training: 74 batch 33 batch_loss: 0.2139548659324646\n",
      "training: 74 batch 34 batch_loss: 0.22032105922698975\n",
      "training: 74 batch 35 batch_loss: 0.21282759308815002\n",
      "training: 74 batch 36 batch_loss: 0.2170354127883911\n",
      "training: 74 batch 37 batch_loss: 0.2125493288040161\n",
      "training: 74 batch 38 batch_loss: 0.21017634868621826\n",
      "training: 74 batch 39 batch_loss: 0.208431214094162\n",
      "training: 74 batch 40 batch_loss: 0.21527054905891418\n",
      "training: 74 batch 41 batch_loss: 0.21118003129959106\n",
      "training: 74 batch 42 batch_loss: 0.21161919832229614\n",
      "training: 74 batch 43 batch_loss: 0.20821541547775269\n",
      "training: 74 batch 44 batch_loss: 0.21613982319831848\n",
      "training: 74 batch 45 batch_loss: 0.21525725722312927\n",
      "training: 74 batch 46 batch_loss: 0.21576860547065735\n",
      "training: 74 batch 47 batch_loss: 0.21617761254310608\n",
      "training: 74 batch 48 batch_loss: 0.21248209476470947\n",
      "training: 74 batch 49 batch_loss: 0.21458390355110168\n",
      "training: 74 batch 50 batch_loss: 0.2178311049938202\n",
      "training: 74 batch 51 batch_loss: 0.21213191747665405\n",
      "training: 74 batch 52 batch_loss: 0.21039637923240662\n",
      "training: 74 batch 53 batch_loss: 0.2141726016998291\n",
      "training: 74 batch 54 batch_loss: 0.2136356234550476\n",
      "training: 74 batch 55 batch_loss: 0.21803992986679077\n",
      "training: 74 batch 56 batch_loss: 0.2180710732936859\n",
      "training: 74 batch 57 batch_loss: 0.21862679719924927\n",
      "training: 74 batch 58 batch_loss: 0.2150660753250122\n",
      "training: 74 batch 59 batch_loss: 0.21674823760986328\n",
      "training: 74 batch 60 batch_loss: 0.21963495016098022\n",
      "training: 74 batch 61 batch_loss: 0.2167036235332489\n",
      "training: 74 batch 62 batch_loss: 0.21884816884994507\n",
      "training: 74 batch 63 batch_loss: 0.21241554617881775\n",
      "training: 74 batch 64 batch_loss: 0.21901744604110718\n",
      "training: 74 batch 65 batch_loss: 0.21612441539764404\n",
      "training: 74 batch 66 batch_loss: 0.21591439843177795\n",
      "training: 74 batch 67 batch_loss: 0.21322399377822876\n",
      "training: 74 batch 68 batch_loss: 0.21589329838752747\n",
      "training: 74 batch 69 batch_loss: 0.21470826864242554\n",
      "training: 74 batch 70 batch_loss: 0.2107824981212616\n",
      "training: 74 batch 71 batch_loss: 0.21454113721847534\n",
      "training: 74 batch 72 batch_loss: 0.21375420689582825\n",
      "training: 74 batch 73 batch_loss: 0.21255618333816528\n",
      "training: 74 batch 74 batch_loss: 0.21637287735939026\n",
      "training: 74 batch 75 batch_loss: 0.21702194213867188\n",
      "training: 74 batch 76 batch_loss: 0.22159209847450256\n",
      "training: 74 batch 77 batch_loss: 0.21285071969032288\n",
      "training: 74 batch 78 batch_loss: 0.21369805932044983\n",
      "training: 74 batch 79 batch_loss: 0.21118459105491638\n",
      "training: 74 batch 80 batch_loss: 0.21727782487869263\n",
      "training: 74 batch 81 batch_loss: 0.2187572717666626\n",
      "training: 74 batch 82 batch_loss: 0.21475133299827576\n",
      "training: 74 batch 83 batch_loss: 0.21195757389068604\n",
      "training: 74 batch 84 batch_loss: 0.21629977226257324\n",
      "training: 74 batch 85 batch_loss: 0.21276116371154785\n",
      "training: 74 batch 86 batch_loss: 0.21292313933372498\n",
      "training: 74 batch 87 batch_loss: 0.2109643518924713\n",
      "training: 74 batch 88 batch_loss: 0.213442862033844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 74 batch 89 batch_loss: 0.215433269739151\n",
      "training: 74 batch 90 batch_loss: 0.21940597891807556\n",
      "training: 74 batch 91 batch_loss: 0.21811017394065857\n",
      "training: 74 batch 92 batch_loss: 0.21984118223190308\n",
      "training: 74 batch 93 batch_loss: 0.21662834286689758\n",
      "training: 74 batch 94 batch_loss: 0.2148546576499939\n",
      "training: 74 batch 95 batch_loss: 0.21387508511543274\n",
      "training: 74 batch 96 batch_loss: 0.21615850925445557\n",
      "training: 74 batch 97 batch_loss: 0.20975342392921448\n",
      "training: 74 batch 98 batch_loss: 0.2153172791004181\n",
      "training: 74 batch 99 batch_loss: 0.20973795652389526\n",
      "training: 74 batch 100 batch_loss: 0.21358132362365723\n",
      "training: 74 batch 101 batch_loss: 0.21448162198066711\n",
      "training: 74 batch 102 batch_loss: 0.2141198217868805\n",
      "training: 74 batch 103 batch_loss: 0.21421664953231812\n",
      "training: 74 batch 104 batch_loss: 0.21786409616470337\n",
      "training: 74 batch 105 batch_loss: 0.21812474727630615\n",
      "training: 74 batch 106 batch_loss: 0.21082442998886108\n",
      "training: 74 batch 107 batch_loss: 0.21645420789718628\n",
      "training: 74 batch 108 batch_loss: 0.2169073224067688\n",
      "training: 74 batch 109 batch_loss: 0.21701717376708984\n",
      "training: 74 batch 110 batch_loss: 0.21188706159591675\n",
      "training: 74 batch 111 batch_loss: 0.21636253595352173\n",
      "training: 74 batch 112 batch_loss: 0.21304863691329956\n",
      "training: 74 batch 113 batch_loss: 0.21201318502426147\n",
      "training: 74 batch 114 batch_loss: 0.21347573399543762\n",
      "training: 74 batch 115 batch_loss: 0.21656590700149536\n",
      "training: 74 batch 116 batch_loss: 0.21649092435836792\n",
      "training: 74 batch 117 batch_loss: 0.21351367235183716\n",
      "training: 74 batch 118 batch_loss: 0.2125851809978485\n",
      "training: 74 batch 119 batch_loss: 0.21798107028007507\n",
      "training: 74 batch 120 batch_loss: 0.21128147840499878\n",
      "training: 74 batch 121 batch_loss: 0.21359598636627197\n",
      "training: 74 batch 122 batch_loss: 0.2140289545059204\n",
      "training: 74 batch 123 batch_loss: 0.21811333298683167\n",
      "training: 74 batch 124 batch_loss: 0.21737241744995117\n",
      "training: 74 batch 125 batch_loss: 0.21216797828674316\n",
      "training: 74 batch 126 batch_loss: 0.2179323434829712\n",
      "training: 74 batch 127 batch_loss: 0.22147059440612793\n",
      "training: 74 batch 128 batch_loss: 0.21422189474105835\n",
      "training: 74 batch 129 batch_loss: 0.21134066581726074\n",
      "training: 74 batch 130 batch_loss: 0.21454808115959167\n",
      "training: 74 batch 131 batch_loss: 0.21563464403152466\n",
      "training: 74 batch 132 batch_loss: 0.21373501420021057\n",
      "training: 74 batch 133 batch_loss: 0.213679701089859\n",
      "training: 74 batch 134 batch_loss: 0.21493849158287048\n",
      "training: 74 batch 135 batch_loss: 0.21920382976531982\n",
      "training: 74 batch 136 batch_loss: 0.21113547682762146\n",
      "training: 74 batch 137 batch_loss: 0.2175895869731903\n",
      "training: 74 batch 138 batch_loss: 0.2166508436203003\n",
      "training: 74 batch 139 batch_loss: 0.2175980806350708\n",
      "training: 74 batch 140 batch_loss: 0.21445494890213013\n",
      "training: 74 batch 141 batch_loss: 0.21582287549972534\n",
      "training: 74 batch 142 batch_loss: 0.21740204095840454\n",
      "training: 74 batch 143 batch_loss: 0.21343857049942017\n",
      "training: 74 batch 144 batch_loss: 0.21273747086524963\n",
      "training: 74 batch 145 batch_loss: 0.21494290232658386\n",
      "training: 74 batch 146 batch_loss: 0.21710437536239624\n",
      "training: 74 batch 147 batch_loss: 0.2177526354789734\n",
      "training: 74 batch 148 batch_loss: 0.22045239806175232\n",
      "training: 74 batch 149 batch_loss: 0.2179117500782013\n",
      "training: 74 batch 150 batch_loss: 0.21316230297088623\n",
      "training: 74 batch 151 batch_loss: 0.21617311239242554\n",
      "training: 74 batch 152 batch_loss: 0.21474766731262207\n",
      "training: 74 batch 153 batch_loss: 0.2114216387271881\n",
      "training: 74 batch 154 batch_loss: 0.21790751814842224\n",
      "training: 74 batch 155 batch_loss: 0.2181512713432312\n",
      "training: 74 batch 156 batch_loss: 0.2206743359565735\n",
      "training: 74 batch 157 batch_loss: 0.21419420838356018\n",
      "training: 74 batch 158 batch_loss: 0.2185511589050293\n",
      "training: 74 batch 159 batch_loss: 0.21823927760124207\n",
      "training: 74 batch 160 batch_loss: 0.2168102264404297\n",
      "training: 74 batch 161 batch_loss: 0.2163349688053131\n",
      "training: 74 batch 162 batch_loss: 0.21263766288757324\n",
      "training: 74 batch 163 batch_loss: 0.2132376730442047\n",
      "training: 74 batch 164 batch_loss: 0.2202051281929016\n",
      "training: 74 batch 165 batch_loss: 0.21271482110023499\n",
      "training: 74 batch 166 batch_loss: 0.21382084488868713\n",
      "training: 74 batch 167 batch_loss: 0.21653994917869568\n",
      "training: 74 batch 168 batch_loss: 0.21444234251976013\n",
      "training: 74 batch 169 batch_loss: 0.21753162145614624\n",
      "training: 74 batch 170 batch_loss: 0.2160947620868683\n",
      "training: 74 batch 171 batch_loss: 0.21568956971168518\n",
      "training: 74 batch 172 batch_loss: 0.21708980202674866\n",
      "training: 74 batch 173 batch_loss: 0.2170865833759308\n",
      "training: 74 batch 174 batch_loss: 0.22373297810554504\n",
      "training: 74 batch 175 batch_loss: 0.21828508377075195\n",
      "training: 74 batch 176 batch_loss: 0.21675118803977966\n",
      "training: 74 batch 177 batch_loss: 0.21443718671798706\n",
      "training: 74 batch 178 batch_loss: 0.21383622288703918\n",
      "training: 74 batch 179 batch_loss: 0.21753451228141785\n",
      "training: 74 batch 180 batch_loss: 0.21778053045272827\n",
      "training: 74 batch 181 batch_loss: 0.21770143508911133\n",
      "training: 74 batch 182 batch_loss: 0.21377608180046082\n",
      "training: 74 batch 183 batch_loss: 0.2193286120891571\n",
      "training: 74 batch 184 batch_loss: 0.2146955132484436\n",
      "training: 74 batch 185 batch_loss: 0.21741604804992676\n",
      "training: 74 batch 186 batch_loss: 0.2176954746246338\n",
      "training: 74 batch 187 batch_loss: 0.21984830498695374\n",
      "training: 74 batch 188 batch_loss: 0.21350765228271484\n",
      "training: 74 batch 189 batch_loss: 0.21152335405349731\n",
      "training: 74 batch 190 batch_loss: 0.21396946907043457\n",
      "training: 74 batch 191 batch_loss: 0.21829766035079956\n",
      "training: 74 batch 192 batch_loss: 0.21810442209243774\n",
      "training: 74 batch 193 batch_loss: 0.21112006902694702\n",
      "training: 74 batch 194 batch_loss: 0.215757817029953\n",
      "training: 74 batch 195 batch_loss: 0.21693062782287598\n",
      "training: 74 batch 196 batch_loss: 0.21735751628875732\n",
      "training: 74 batch 197 batch_loss: 0.21894603967666626\n",
      "training: 74 batch 198 batch_loss: 0.21556749939918518\n",
      "training: 74 batch 199 batch_loss: 0.21295911073684692\n",
      "training: 74 batch 200 batch_loss: 0.21737977862358093\n",
      "training: 74 batch 201 batch_loss: 0.2165624499320984\n",
      "training: 74 batch 202 batch_loss: 0.219483882188797\n",
      "training: 74 batch 203 batch_loss: 0.22205442190170288\n",
      "training: 74 batch 204 batch_loss: 0.2143590748310089\n",
      "training: 74 batch 205 batch_loss: 0.22003310918807983\n",
      "training: 74 batch 206 batch_loss: 0.21662375330924988\n",
      "training: 74 batch 207 batch_loss: 0.21559447050094604\n",
      "training: 74 batch 208 batch_loss: 0.2152605652809143\n",
      "training: 74 batch 209 batch_loss: 0.2181551456451416\n",
      "training: 74 batch 210 batch_loss: 0.2172522246837616\n",
      "training: 74 batch 211 batch_loss: 0.21276572346687317\n",
      "training: 74 batch 212 batch_loss: 0.21732747554779053\n",
      "training: 74 batch 213 batch_loss: 0.21443861722946167\n",
      "training: 74 batch 214 batch_loss: 0.2157827913761139\n",
      "training: 74 batch 215 batch_loss: 0.21016758680343628\n",
      "training: 74 batch 216 batch_loss: 0.21457329392433167\n",
      "training: 74 batch 217 batch_loss: 0.21295714378356934\n",
      "training: 74 batch 218 batch_loss: 0.21532070636749268\n",
      "training: 74 batch 219 batch_loss: 0.21559953689575195\n",
      "training: 74 batch 220 batch_loss: 0.21404656767845154\n",
      "training: 74 batch 221 batch_loss: 0.21155571937561035\n",
      "training: 74 batch 222 batch_loss: 0.2178954780101776\n",
      "training: 74 batch 223 batch_loss: 0.21574246883392334\n",
      "training: 74 batch 224 batch_loss: 0.21418866515159607\n",
      "training: 74 batch 225 batch_loss: 0.21753820776939392\n",
      "training: 74 batch 226 batch_loss: 0.21729370951652527\n",
      "training: 74 batch 227 batch_loss: 0.21511316299438477\n",
      "training: 74 batch 228 batch_loss: 0.21431642770767212\n",
      "training: 74 batch 229 batch_loss: 0.2155272364616394\n",
      "training: 74 batch 230 batch_loss: 0.2153400182723999\n",
      "training: 74 batch 231 batch_loss: 0.21527644991874695\n",
      "training: 74 batch 232 batch_loss: 0.21635249257087708\n",
      "training: 74 batch 233 batch_loss: 0.21728485822677612\n",
      "training: 74 batch 234 batch_loss: 0.21677029132843018\n",
      "training: 74 batch 235 batch_loss: 0.2156515121459961\n",
      "training: 74 batch 236 batch_loss: 0.219028502702713\n",
      "training: 74 batch 237 batch_loss: 0.21581146121025085\n",
      "training: 74 batch 238 batch_loss: 0.21560701727867126\n",
      "training: 74 batch 239 batch_loss: 0.2143033742904663\n",
      "training: 74 batch 240 batch_loss: 0.2159038782119751\n",
      "training: 74 batch 241 batch_loss: 0.2117604911327362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 74 batch 242 batch_loss: 0.21695935726165771\n",
      "training: 74 batch 243 batch_loss: 0.2134551703929901\n",
      "training: 74 batch 244 batch_loss: 0.21640318632125854\n",
      "training: 74 batch 245 batch_loss: 0.21394634246826172\n",
      "training: 74 batch 246 batch_loss: 0.21456149220466614\n",
      "training: 74 batch 247 batch_loss: 0.21699020266532898\n",
      "training: 74 batch 248 batch_loss: 0.21524491906166077\n",
      "training: 74 batch 249 batch_loss: 0.21445906162261963\n",
      "training: 74 batch 250 batch_loss: 0.21655288338661194\n",
      "training: 74 batch 251 batch_loss: 0.21227192878723145\n",
      "training: 74 batch 252 batch_loss: 0.21157848834991455\n",
      "training: 74 batch 253 batch_loss: 0.21616816520690918\n",
      "training: 74 batch 254 batch_loss: 0.2160225510597229\n",
      "training: 74 batch 255 batch_loss: 0.2169198989868164\n",
      "training: 74 batch 256 batch_loss: 0.2132108509540558\n",
      "training: 74 batch 257 batch_loss: 0.21379417181015015\n",
      "training: 74 batch 258 batch_loss: 0.2152157425880432\n",
      "training: 74 batch 259 batch_loss: 0.21580517292022705\n",
      "training: 74 batch 260 batch_loss: 0.21828404068946838\n",
      "training: 74 batch 261 batch_loss: 0.2145373821258545\n",
      "training: 74 batch 262 batch_loss: 0.22009998559951782\n",
      "training: 74 batch 263 batch_loss: 0.2146483063697815\n",
      "training: 74 batch 264 batch_loss: 0.2180670201778412\n",
      "training: 74 batch 265 batch_loss: 0.2174040675163269\n",
      "training: 74 batch 266 batch_loss: 0.21282950043678284\n",
      "training: 74 batch 267 batch_loss: 0.21839630603790283\n",
      "training: 74 batch 268 batch_loss: 0.21802246570587158\n",
      "training: 74 batch 269 batch_loss: 0.21560996770858765\n",
      "training: 74 batch 270 batch_loss: 0.21567565202713013\n",
      "training: 74 batch 271 batch_loss: 0.21557965874671936\n",
      "training: 74 batch 272 batch_loss: 0.21301192045211792\n",
      "training: 74 batch 273 batch_loss: 0.21732309460639954\n",
      "training: 74 batch 274 batch_loss: 0.21845033764839172\n",
      "training: 74 batch 275 batch_loss: 0.22032326459884644\n",
      "training: 74 batch 276 batch_loss: 0.21315690875053406\n",
      "training: 74 batch 277 batch_loss: 0.21754422783851624\n",
      "training: 74 batch 278 batch_loss: 0.22014454007148743\n",
      "training: 74 batch 279 batch_loss: 0.21652042865753174\n",
      "training: 74 batch 280 batch_loss: 0.2211131751537323\n",
      "training: 74 batch 281 batch_loss: 0.2183237373828888\n",
      "training: 74 batch 282 batch_loss: 0.21509668231010437\n",
      "training: 74 batch 283 batch_loss: 0.21711179614067078\n",
      "training: 74 batch 284 batch_loss: 0.21623903512954712\n",
      "training: 74 batch 285 batch_loss: 0.21632486581802368\n",
      "training: 74 batch 286 batch_loss: 0.21350744366645813\n",
      "training: 74 batch 287 batch_loss: 0.21684518456459045\n",
      "training: 74 batch 288 batch_loss: 0.21402135491371155\n",
      "training: 74 batch 289 batch_loss: 0.2136722207069397\n",
      "training: 74 batch 290 batch_loss: 0.2119063436985016\n",
      "training: 74 batch 291 batch_loss: 0.21476638317108154\n",
      "training: 74 batch 292 batch_loss: 0.21631762385368347\n",
      "training: 74 batch 293 batch_loss: 0.21775776147842407\n",
      "training: 74 batch 294 batch_loss: 0.2176327109336853\n",
      "training: 74 batch 295 batch_loss: 0.21844875812530518\n",
      "training: 74 batch 296 batch_loss: 0.21706658601760864\n",
      "training: 74 batch 297 batch_loss: 0.21461841464042664\n",
      "training: 74 batch 298 batch_loss: 0.21790874004364014\n",
      "training: 74 batch 299 batch_loss: 0.21394142508506775\n",
      "training: 74 batch 300 batch_loss: 0.2190191149711609\n",
      "training: 74 batch 301 batch_loss: 0.2161233127117157\n",
      "training: 74 batch 302 batch_loss: 0.2180943787097931\n",
      "training: 74 batch 303 batch_loss: 0.21213766932487488\n",
      "training: 74 batch 304 batch_loss: 0.21800467371940613\n",
      "training: 74 batch 305 batch_loss: 0.21489492058753967\n",
      "training: 74 batch 306 batch_loss: 0.21524900197982788\n",
      "training: 74 batch 307 batch_loss: 0.21580782532691956\n",
      "training: 74 batch 308 batch_loss: 0.21611818671226501\n",
      "training: 74 batch 309 batch_loss: 0.21827378869056702\n",
      "training: 74 batch 310 batch_loss: 0.21107995510101318\n",
      "training: 74 batch 311 batch_loss: 0.2137899100780487\n",
      "training: 74 batch 312 batch_loss: 0.2164950668811798\n",
      "training: 74 batch 313 batch_loss: 0.2181990146636963\n",
      "training: 74 batch 314 batch_loss: 0.21260374784469604\n",
      "training: 74 batch 315 batch_loss: 0.21815669536590576\n",
      "training: 74 batch 316 batch_loss: 0.21074801683425903\n",
      "training: 74 batch 317 batch_loss: 0.21552854776382446\n",
      "training: 74 batch 318 batch_loss: 0.21920138597488403\n",
      "training: 74 batch 319 batch_loss: 0.2169302999973297\n",
      "training: 74 batch 320 batch_loss: 0.2157198190689087\n",
      "training: 74 batch 321 batch_loss: 0.2127619981765747\n",
      "training: 74 batch 322 batch_loss: 0.2141074538230896\n",
      "training: 74 batch 323 batch_loss: 0.2190464735031128\n",
      "training: 74 batch 324 batch_loss: 0.2126975655555725\n",
      "training: 74 batch 325 batch_loss: 0.21770071983337402\n",
      "training: 74 batch 326 batch_loss: 0.2177603542804718\n",
      "training: 74 batch 327 batch_loss: 0.2176935374736786\n",
      "training: 74 batch 328 batch_loss: 0.22162076830863953\n",
      "training: 74 batch 329 batch_loss: 0.21999043226242065\n",
      "training: 74 batch 330 batch_loss: 0.2185947597026825\n",
      "training: 74 batch 331 batch_loss: 0.2154243290424347\n",
      "training: 74 batch 332 batch_loss: 0.21750250458717346\n",
      "training: 74 batch 333 batch_loss: 0.21313825249671936\n",
      "training: 74 batch 334 batch_loss: 0.2158670425415039\n",
      "training: 74 batch 335 batch_loss: 0.21805861592292786\n",
      "training: 74 batch 336 batch_loss: 0.21943399310112\n",
      "training: 74 batch 337 batch_loss: 0.21632003784179688\n",
      "training: 74 batch 338 batch_loss: 0.22033369541168213\n",
      "training: 74 batch 339 batch_loss: 0.21420955657958984\n",
      "training: 74 batch 340 batch_loss: 0.21864986419677734\n",
      "training: 74 batch 341 batch_loss: 0.21655848622322083\n",
      "training: 74 batch 342 batch_loss: 0.2178826928138733\n",
      "training: 74 batch 343 batch_loss: 0.21809479594230652\n",
      "training: 74 batch 344 batch_loss: 0.2211506962776184\n",
      "training: 74 batch 345 batch_loss: 0.21547606587409973\n",
      "training: 74 batch 346 batch_loss: 0.21394193172454834\n",
      "training: 74 batch 347 batch_loss: 0.21612966060638428\n",
      "training: 74 batch 348 batch_loss: 0.2141154408454895\n",
      "training: 74 batch 349 batch_loss: 0.2168492078781128\n",
      "training: 74 batch 350 batch_loss: 0.21980488300323486\n",
      "training: 74 batch 351 batch_loss: 0.22053414583206177\n",
      "training: 74 batch 352 batch_loss: 0.21920576691627502\n",
      "training: 74 batch 353 batch_loss: 0.21668577194213867\n",
      "training: 74 batch 354 batch_loss: 0.21880030632019043\n",
      "training: 74 batch 355 batch_loss: 0.2169705629348755\n",
      "training: 74 batch 356 batch_loss: 0.21602290868759155\n",
      "training: 74 batch 357 batch_loss: 0.21654140949249268\n",
      "training: 74 batch 358 batch_loss: 0.21762382984161377\n",
      "training: 74 batch 359 batch_loss: 0.21037673950195312\n",
      "training: 74 batch 360 batch_loss: 0.2155379056930542\n",
      "training: 74 batch 361 batch_loss: 0.21728715300559998\n",
      "training: 74 batch 362 batch_loss: 0.21775907278060913\n",
      "training: 74 batch 363 batch_loss: 0.21609872579574585\n",
      "training: 74 batch 364 batch_loss: 0.22147884964942932\n",
      "training: 74 batch 365 batch_loss: 0.21259033679962158\n",
      "training: 74 batch 366 batch_loss: 0.21611207723617554\n",
      "training: 74 batch 367 batch_loss: 0.21726474165916443\n",
      "training: 74 batch 368 batch_loss: 0.21848729252815247\n",
      "training: 74 batch 369 batch_loss: 0.21143168210983276\n",
      "training: 74 batch 370 batch_loss: 0.213742196559906\n",
      "training: 74 batch 371 batch_loss: 0.2172495424747467\n",
      "training: 74 batch 372 batch_loss: 0.21649831533432007\n",
      "training: 74 batch 373 batch_loss: 0.22052878141403198\n",
      "training: 74 batch 374 batch_loss: 0.21519383788108826\n",
      "training: 74 batch 375 batch_loss: 0.22056081891059875\n",
      "training: 74 batch 376 batch_loss: 0.2170221209526062\n",
      "training: 74 batch 377 batch_loss: 0.21411579847335815\n",
      "training: 74 batch 378 batch_loss: 0.2158079445362091\n",
      "training: 74 batch 379 batch_loss: 0.21906858682632446\n",
      "training: 74 batch 380 batch_loss: 0.2143022119998932\n",
      "training: 74 batch 381 batch_loss: 0.21528533101081848\n",
      "training: 74 batch 382 batch_loss: 0.21649053692817688\n",
      "training: 74 batch 383 batch_loss: 0.2148178219795227\n",
      "training: 74 batch 384 batch_loss: 0.21837040781974792\n",
      "training: 74 batch 385 batch_loss: 0.21611076593399048\n",
      "training: 74 batch 386 batch_loss: 0.22077465057373047\n",
      "training: 74 batch 387 batch_loss: 0.21631938219070435\n",
      "training: 74 batch 388 batch_loss: 0.2176920771598816\n",
      "training: 74 batch 389 batch_loss: 0.21716204285621643\n",
      "training: 74 batch 390 batch_loss: 0.21856364607810974\n",
      "training: 74 batch 391 batch_loss: 0.21665430068969727\n",
      "training: 74 batch 392 batch_loss: 0.21209469437599182\n",
      "training: 74 batch 393 batch_loss: 0.21595710515975952\n",
      "training: 74 batch 394 batch_loss: 0.2207544445991516\n",
      "training: 74 batch 395 batch_loss: 0.21456894278526306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 74 batch 396 batch_loss: 0.212532639503479\n",
      "training: 74 batch 397 batch_loss: 0.21974870562553406\n",
      "training: 74 batch 398 batch_loss: 0.21675556898117065\n",
      "training: 74 batch 399 batch_loss: 0.22376984357833862\n",
      "training: 74 batch 400 batch_loss: 0.21471485495567322\n",
      "training: 74 batch 401 batch_loss: 0.21720251441001892\n",
      "training: 74 batch 402 batch_loss: 0.21513798832893372\n",
      "training: 74 batch 403 batch_loss: 0.22111281752586365\n",
      "training: 74 batch 404 batch_loss: 0.21562236547470093\n",
      "training: 74 batch 405 batch_loss: 0.2147279977798462\n",
      "training: 74 batch 406 batch_loss: 0.22119027376174927\n",
      "training: 74 batch 407 batch_loss: 0.2157793939113617\n",
      "training: 74 batch 408 batch_loss: 0.21580511331558228\n",
      "training: 74 batch 409 batch_loss: 0.21978282928466797\n",
      "training: 74 batch 410 batch_loss: 0.21656811237335205\n",
      "training: 74 batch 411 batch_loss: 0.2159036099910736\n",
      "training: 74 batch 412 batch_loss: 0.21447589993476868\n",
      "training: 74 batch 413 batch_loss: 0.21345877647399902\n",
      "training: 74 batch 414 batch_loss: 0.21791112422943115\n",
      "training: 74 batch 415 batch_loss: 0.21937322616577148\n",
      "training: 74 batch 416 batch_loss: 0.21495336294174194\n",
      "training: 74 batch 417 batch_loss: 0.21502244472503662\n",
      "training: 74 batch 418 batch_loss: 0.2212221324443817\n",
      "training: 74 batch 419 batch_loss: 0.22222930192947388\n",
      "training: 74 batch 420 batch_loss: 0.22138196229934692\n",
      "training: 74 batch 421 batch_loss: 0.214219331741333\n",
      "training: 74 batch 422 batch_loss: 0.2180560827255249\n",
      "training: 74 batch 423 batch_loss: 0.2199709713459015\n",
      "training: 74 batch 424 batch_loss: 0.22107356786727905\n",
      "training: 74 batch 425 batch_loss: 0.21826684474945068\n",
      "training: 74 batch 426 batch_loss: 0.2152285873889923\n",
      "training: 74 batch 427 batch_loss: 0.21942207217216492\n",
      "training: 74 batch 428 batch_loss: 0.2206515371799469\n",
      "training: 74 batch 429 batch_loss: 0.2199912667274475\n",
      "training: 74 batch 430 batch_loss: 0.21747201681137085\n",
      "training: 74 batch 431 batch_loss: 0.21683833003044128\n",
      "training: 74 batch 432 batch_loss: 0.21800851821899414\n",
      "training: 74 batch 433 batch_loss: 0.22016552090644836\n",
      "training: 74 batch 434 batch_loss: 0.21634948253631592\n",
      "training: 74 batch 435 batch_loss: 0.2149081826210022\n",
      "training: 74 batch 436 batch_loss: 0.21564507484436035\n",
      "training: 74 batch 437 batch_loss: 0.21767345070838928\n",
      "training: 74 batch 438 batch_loss: 0.21346178650856018\n",
      "training: 74 batch 439 batch_loss: 0.21685874462127686\n",
      "training: 74 batch 440 batch_loss: 0.22131994366645813\n",
      "training: 74 batch 441 batch_loss: 0.21646839380264282\n",
      "training: 74 batch 442 batch_loss: 0.21537530422210693\n",
      "training: 74 batch 443 batch_loss: 0.21976548433303833\n",
      "training: 74 batch 444 batch_loss: 0.22140055894851685\n",
      "training: 74 batch 445 batch_loss: 0.21870839595794678\n",
      "training: 74 batch 446 batch_loss: 0.21822649240493774\n",
      "training: 74 batch 447 batch_loss: 0.22220224142074585\n",
      "training: 74 batch 448 batch_loss: 0.2217627763748169\n",
      "training: 74 batch 449 batch_loss: 0.21454784274101257\n",
      "training: 74 batch 450 batch_loss: 0.21921271085739136\n",
      "training: 74 batch 451 batch_loss: 0.21720269322395325\n",
      "training: 74 batch 452 batch_loss: 0.2154463231563568\n",
      "training: 74 batch 453 batch_loss: 0.21896374225616455\n",
      "training: 74 batch 454 batch_loss: 0.2221672534942627\n",
      "training: 74 batch 455 batch_loss: 0.21959847211837769\n",
      "training: 74 batch 456 batch_loss: 0.21733394265174866\n",
      "training: 74 batch 457 batch_loss: 0.2185368835926056\n",
      "training: 74 batch 458 batch_loss: 0.21856725215911865\n",
      "training: 74 batch 459 batch_loss: 0.21592727303504944\n",
      "training: 74 batch 460 batch_loss: 0.21614497900009155\n",
      "training: 74 batch 461 batch_loss: 0.21921175718307495\n",
      "training: 74 batch 462 batch_loss: 0.21930775046348572\n",
      "training: 74 batch 463 batch_loss: 0.21506443619728088\n",
      "training: 74 batch 464 batch_loss: 0.21734285354614258\n",
      "training: 74 batch 465 batch_loss: 0.21619769930839539\n",
      "training: 74 batch 466 batch_loss: 0.21985316276550293\n",
      "training: 74 batch 467 batch_loss: 0.21650353074073792\n",
      "training: 74 batch 468 batch_loss: 0.21601468324661255\n",
      "training: 74 batch 469 batch_loss: 0.21401309967041016\n",
      "training: 74 batch 470 batch_loss: 0.21546804904937744\n",
      "training: 74 batch 471 batch_loss: 0.2182881236076355\n",
      "training: 74 batch 472 batch_loss: 0.21484756469726562\n",
      "training: 74 batch 473 batch_loss: 0.21444034576416016\n",
      "training: 74 batch 474 batch_loss: 0.21265220642089844\n",
      "training: 74 batch 475 batch_loss: 0.2187521755695343\n",
      "training: 74 batch 476 batch_loss: 0.2122214138507843\n",
      "training: 74 batch 477 batch_loss: 0.21479934453964233\n",
      "training: 74 batch 478 batch_loss: 0.21598803997039795\n",
      "training: 74 batch 479 batch_loss: 0.21394959092140198\n",
      "training: 74 batch 480 batch_loss: 0.21656060218811035\n",
      "training: 74 batch 481 batch_loss: 0.21477848291397095\n",
      "training: 74 batch 482 batch_loss: 0.21945762634277344\n",
      "training: 74 batch 483 batch_loss: 0.2182568907737732\n",
      "training: 74 batch 484 batch_loss: 0.2173866629600525\n",
      "training: 74 batch 485 batch_loss: 0.21924912929534912\n",
      "training: 74 batch 486 batch_loss: 0.21417281031608582\n",
      "training: 74 batch 487 batch_loss: 0.21509763598442078\n",
      "training: 74 batch 488 batch_loss: 0.21574825048446655\n",
      "training: 74 batch 489 batch_loss: 0.21880897879600525\n",
      "training: 74 batch 490 batch_loss: 0.21893632411956787\n",
      "training: 74 batch 491 batch_loss: 0.21350473165512085\n",
      "training: 74 batch 492 batch_loss: 0.21907764673233032\n",
      "training: 74 batch 493 batch_loss: 0.21841320395469666\n",
      "training: 74 batch 494 batch_loss: 0.21954143047332764\n",
      "training: 74 batch 495 batch_loss: 0.218034029006958\n",
      "training: 74 batch 496 batch_loss: 0.21460312604904175\n",
      "training: 74 batch 497 batch_loss: 0.21637344360351562\n",
      "training: 74 batch 498 batch_loss: 0.2144889235496521\n",
      "training: 74 batch 499 batch_loss: 0.21411952376365662\n",
      "training: 74 batch 500 batch_loss: 0.21822935342788696\n",
      "training: 74 batch 501 batch_loss: 0.21586820483207703\n",
      "training: 74 batch 502 batch_loss: 0.21324363350868225\n",
      "training: 74 batch 503 batch_loss: 0.21879279613494873\n",
      "training: 74 batch 504 batch_loss: 0.2205037772655487\n",
      "training: 74 batch 505 batch_loss: 0.21506589651107788\n",
      "training: 74 batch 506 batch_loss: 0.21510595083236694\n",
      "training: 74 batch 507 batch_loss: 0.21566331386566162\n",
      "training: 74 batch 508 batch_loss: 0.2168062925338745\n",
      "training: 74 batch 509 batch_loss: 0.2145494818687439\n",
      "training: 74 batch 510 batch_loss: 0.21713709831237793\n",
      "training: 74 batch 511 batch_loss: 0.21415936946868896\n",
      "training: 74 batch 512 batch_loss: 0.21580755710601807\n",
      "training: 74 batch 513 batch_loss: 0.216000497341156\n",
      "training: 74 batch 514 batch_loss: 0.22122794389724731\n",
      "training: 74 batch 515 batch_loss: 0.2198522686958313\n",
      "training: 74 batch 516 batch_loss: 0.21820068359375\n",
      "training: 74 batch 517 batch_loss: 0.2124304473400116\n",
      "training: 74 batch 518 batch_loss: 0.21455851197242737\n",
      "training: 74 batch 519 batch_loss: 0.21630382537841797\n",
      "training: 74 batch 520 batch_loss: 0.21673116087913513\n",
      "training: 74 batch 521 batch_loss: 0.21482431888580322\n",
      "training: 74 batch 522 batch_loss: 0.2227659821510315\n",
      "training: 74 batch 523 batch_loss: 0.21865302324295044\n",
      "training: 74 batch 524 batch_loss: 0.2182559072971344\n",
      "training: 74 batch 525 batch_loss: 0.21826273202896118\n",
      "training: 74 batch 526 batch_loss: 0.2170034945011139\n",
      "training: 74 batch 527 batch_loss: 0.21699035167694092\n",
      "training: 74 batch 528 batch_loss: 0.22008472681045532\n",
      "training: 74 batch 529 batch_loss: 0.21538329124450684\n",
      "training: 74 batch 530 batch_loss: 0.21770736575126648\n",
      "training: 74 batch 531 batch_loss: 0.2135767936706543\n",
      "training: 74 batch 532 batch_loss: 0.21696597337722778\n",
      "training: 74 batch 533 batch_loss: 0.2178739309310913\n",
      "training: 74 batch 534 batch_loss: 0.2178383767604828\n",
      "training: 74 batch 535 batch_loss: 0.21851393580436707\n",
      "training: 74 batch 536 batch_loss: 0.21897834539413452\n",
      "training: 74 batch 537 batch_loss: 0.21873235702514648\n",
      "training: 74 batch 538 batch_loss: 0.219264954328537\n",
      "training: 74 batch 539 batch_loss: 0.21270602941513062\n",
      "training: 74 batch 540 batch_loss: 0.21518173813819885\n",
      "training: 74 batch 541 batch_loss: 0.21591365337371826\n",
      "training: 74 batch 542 batch_loss: 0.21393731236457825\n",
      "training: 74 batch 543 batch_loss: 0.21767783164978027\n",
      "training: 74 batch 544 batch_loss: 0.2173318862915039\n",
      "training: 74 batch 545 batch_loss: 0.21675974130630493\n",
      "training: 74 batch 546 batch_loss: 0.21489721536636353\n",
      "training: 74 batch 547 batch_loss: 0.2153685986995697\n",
      "training: 74 batch 548 batch_loss: 0.2146584689617157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 74 batch 549 batch_loss: 0.2174099087715149\n",
      "training: 74 batch 550 batch_loss: 0.2113991379737854\n",
      "training: 74 batch 551 batch_loss: 0.21975073218345642\n",
      "training: 74 batch 552 batch_loss: 0.214768648147583\n",
      "training: 74 batch 553 batch_loss: 0.21631166338920593\n",
      "training: 74 batch 554 batch_loss: 0.21614721417427063\n",
      "training: 74 batch 555 batch_loss: 0.2198537290096283\n",
      "training: 74 batch 556 batch_loss: 0.21984976530075073\n",
      "training: 74 batch 557 batch_loss: 0.21304824948310852\n",
      "training: 74 batch 558 batch_loss: 0.2194541096687317\n",
      "training: 74 batch 559 batch_loss: 0.2199864387512207\n",
      "training: 74 batch 560 batch_loss: 0.21710306406021118\n",
      "training: 74 batch 561 batch_loss: 0.21924865245819092\n",
      "training: 74 batch 562 batch_loss: 0.21850359439849854\n",
      "training: 74 batch 563 batch_loss: 0.22011446952819824\n",
      "training: 74 batch 564 batch_loss: 0.214313805103302\n",
      "training: 74 batch 565 batch_loss: 0.21721196174621582\n",
      "training: 74 batch 566 batch_loss: 0.21662229299545288\n",
      "training: 74 batch 567 batch_loss: 0.21435174345970154\n",
      "training: 74 batch 568 batch_loss: 0.21515536308288574\n",
      "training: 74 batch 569 batch_loss: 0.2197040319442749\n",
      "training: 74 batch 570 batch_loss: 0.2167840600013733\n",
      "training: 74 batch 571 batch_loss: 0.21927639842033386\n",
      "training: 74 batch 572 batch_loss: 0.21642979979515076\n",
      "training: 74 batch 573 batch_loss: 0.21875691413879395\n",
      "training: 74 batch 574 batch_loss: 0.21866965293884277\n",
      "training: 74 batch 575 batch_loss: 0.21617484092712402\n",
      "training: 74 batch 576 batch_loss: 0.2128579020500183\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 74, Hit Ratio:0.033508442901733215 | Precision:0.049439693305809496 | Recall:0.0653853459629658 | NDCG:0.06439129810512087\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 75 batch 0 batch_loss: 0.21485906839370728\n",
      "training: 75 batch 1 batch_loss: 0.21445760130882263\n",
      "training: 75 batch 2 batch_loss: 0.21767458319664001\n",
      "training: 75 batch 3 batch_loss: 0.21559610962867737\n",
      "training: 75 batch 4 batch_loss: 0.2148403823375702\n",
      "training: 75 batch 5 batch_loss: 0.2134307622909546\n",
      "training: 75 batch 6 batch_loss: 0.2133989930152893\n",
      "training: 75 batch 7 batch_loss: 0.21691465377807617\n",
      "training: 75 batch 8 batch_loss: 0.21641623973846436\n",
      "training: 75 batch 9 batch_loss: 0.21130219101905823\n",
      "training: 75 batch 10 batch_loss: 0.217859148979187\n",
      "training: 75 batch 11 batch_loss: 0.21665215492248535\n",
      "training: 75 batch 12 batch_loss: 0.21493405103683472\n",
      "training: 75 batch 13 batch_loss: 0.21387618780136108\n",
      "training: 75 batch 14 batch_loss: 0.21466264128684998\n",
      "training: 75 batch 15 batch_loss: 0.21874111890792847\n",
      "training: 75 batch 16 batch_loss: 0.2173599898815155\n",
      "training: 75 batch 17 batch_loss: 0.2105621099472046\n",
      "training: 75 batch 18 batch_loss: 0.21827766299247742\n",
      "training: 75 batch 19 batch_loss: 0.2135581374168396\n",
      "training: 75 batch 20 batch_loss: 0.21522310376167297\n",
      "training: 75 batch 21 batch_loss: 0.2148052453994751\n",
      "training: 75 batch 22 batch_loss: 0.220289945602417\n",
      "training: 75 batch 23 batch_loss: 0.21365979313850403\n",
      "training: 75 batch 24 batch_loss: 0.212110698223114\n",
      "training: 75 batch 25 batch_loss: 0.21278893947601318\n",
      "training: 75 batch 26 batch_loss: 0.21590492129325867\n",
      "training: 75 batch 27 batch_loss: 0.21715372800827026\n",
      "training: 75 batch 28 batch_loss: 0.21366876363754272\n",
      "training: 75 batch 29 batch_loss: 0.21326974034309387\n",
      "training: 75 batch 30 batch_loss: 0.21602991223335266\n",
      "training: 75 batch 31 batch_loss: 0.2160465121269226\n",
      "training: 75 batch 32 batch_loss: 0.21105971932411194\n",
      "training: 75 batch 33 batch_loss: 0.2124568223953247\n",
      "training: 75 batch 34 batch_loss: 0.21513652801513672\n",
      "training: 75 batch 35 batch_loss: 0.216984361410141\n",
      "training: 75 batch 36 batch_loss: 0.21878919005393982\n",
      "training: 75 batch 37 batch_loss: 0.21161800622940063\n",
      "training: 75 batch 38 batch_loss: 0.21051740646362305\n",
      "training: 75 batch 39 batch_loss: 0.2180594503879547\n",
      "training: 75 batch 40 batch_loss: 0.21402502059936523\n",
      "training: 75 batch 41 batch_loss: 0.21845743060112\n",
      "training: 75 batch 42 batch_loss: 0.21465054154396057\n",
      "training: 75 batch 43 batch_loss: 0.21846681833267212\n",
      "training: 75 batch 44 batch_loss: 0.21371334791183472\n",
      "training: 75 batch 45 batch_loss: 0.22076144814491272\n",
      "training: 75 batch 46 batch_loss: 0.21340978145599365\n",
      "training: 75 batch 47 batch_loss: 0.2191137671470642\n",
      "training: 75 batch 48 batch_loss: 0.2166641652584076\n",
      "training: 75 batch 49 batch_loss: 0.2151525914669037\n",
      "training: 75 batch 50 batch_loss: 0.21777287125587463\n",
      "training: 75 batch 51 batch_loss: 0.21538928151130676\n",
      "training: 75 batch 52 batch_loss: 0.21202701330184937\n",
      "training: 75 batch 53 batch_loss: 0.22004973888397217\n",
      "training: 75 batch 54 batch_loss: 0.21345344185829163\n",
      "training: 75 batch 55 batch_loss: 0.216633141040802\n",
      "training: 75 batch 56 batch_loss: 0.21408891677856445\n",
      "training: 75 batch 57 batch_loss: 0.21624678373336792\n",
      "training: 75 batch 58 batch_loss: 0.21534156799316406\n",
      "training: 75 batch 59 batch_loss: 0.21807906031608582\n",
      "training: 75 batch 60 batch_loss: 0.21212610602378845\n",
      "training: 75 batch 61 batch_loss: 0.21980422735214233\n",
      "training: 75 batch 62 batch_loss: 0.2189961075782776\n",
      "training: 75 batch 63 batch_loss: 0.22008371353149414\n",
      "training: 75 batch 64 batch_loss: 0.2181416153907776\n",
      "training: 75 batch 65 batch_loss: 0.21155008673667908\n",
      "training: 75 batch 66 batch_loss: 0.21823352575302124\n",
      "training: 75 batch 67 batch_loss: 0.2196473479270935\n",
      "training: 75 batch 68 batch_loss: 0.21720224618911743\n",
      "training: 75 batch 69 batch_loss: 0.21334987878799438\n",
      "training: 75 batch 70 batch_loss: 0.21247005462646484\n",
      "training: 75 batch 71 batch_loss: 0.21612966060638428\n",
      "training: 75 batch 72 batch_loss: 0.21600523591041565\n",
      "training: 75 batch 73 batch_loss: 0.21702656149864197\n",
      "training: 75 batch 74 batch_loss: 0.2142239809036255\n",
      "training: 75 batch 75 batch_loss: 0.2159171998500824\n",
      "training: 75 batch 76 batch_loss: 0.216950923204422\n",
      "training: 75 batch 77 batch_loss: 0.21070122718811035\n",
      "training: 75 batch 78 batch_loss: 0.21387144923210144\n",
      "training: 75 batch 79 batch_loss: 0.2189306616783142\n",
      "training: 75 batch 80 batch_loss: 0.2179986834526062\n",
      "training: 75 batch 81 batch_loss: 0.21425724029541016\n",
      "training: 75 batch 82 batch_loss: 0.2189202904701233\n",
      "training: 75 batch 83 batch_loss: 0.21634435653686523\n",
      "training: 75 batch 84 batch_loss: 0.2193145751953125\n",
      "training: 75 batch 85 batch_loss: 0.21105355024337769\n",
      "training: 75 batch 86 batch_loss: 0.21697944402694702\n",
      "training: 75 batch 87 batch_loss: 0.21714502573013306\n",
      "training: 75 batch 88 batch_loss: 0.22489795088768005\n",
      "training: 75 batch 89 batch_loss: 0.214219331741333\n",
      "training: 75 batch 90 batch_loss: 0.21915191411972046\n",
      "training: 75 batch 91 batch_loss: 0.21420502662658691\n",
      "training: 75 batch 92 batch_loss: 0.21461734175682068\n",
      "training: 75 batch 93 batch_loss: 0.2140599489212036\n",
      "training: 75 batch 94 batch_loss: 0.2168366014957428\n",
      "training: 75 batch 95 batch_loss: 0.2168337106704712\n",
      "training: 75 batch 96 batch_loss: 0.21601733565330505\n",
      "training: 75 batch 97 batch_loss: 0.21237903833389282\n",
      "training: 75 batch 98 batch_loss: 0.21686962246894836\n",
      "training: 75 batch 99 batch_loss: 0.2151281237602234\n",
      "training: 75 batch 100 batch_loss: 0.21629473567008972\n",
      "training: 75 batch 101 batch_loss: 0.21436548233032227\n",
      "training: 75 batch 102 batch_loss: 0.2164647877216339\n",
      "training: 75 batch 103 batch_loss: 0.2116754949092865\n",
      "training: 75 batch 104 batch_loss: 0.22105079889297485\n",
      "training: 75 batch 105 batch_loss: 0.21408772468566895\n",
      "training: 75 batch 106 batch_loss: 0.22328317165374756\n",
      "training: 75 batch 107 batch_loss: 0.21163690090179443\n",
      "training: 75 batch 108 batch_loss: 0.21553963422775269\n",
      "training: 75 batch 109 batch_loss: 0.21771526336669922\n",
      "training: 75 batch 110 batch_loss: 0.2169194221496582\n",
      "training: 75 batch 111 batch_loss: 0.21492880582809448\n",
      "training: 75 batch 112 batch_loss: 0.21576935052871704\n",
      "training: 75 batch 113 batch_loss: 0.21910995244979858\n",
      "training: 75 batch 114 batch_loss: 0.21579581499099731\n",
      "training: 75 batch 115 batch_loss: 0.2177332043647766\n",
      "training: 75 batch 116 batch_loss: 0.21324196457862854\n",
      "training: 75 batch 117 batch_loss: 0.21349391341209412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 75 batch 118 batch_loss: 0.21643707156181335\n",
      "training: 75 batch 119 batch_loss: 0.21797028183937073\n",
      "training: 75 batch 120 batch_loss: 0.2143615484237671\n",
      "training: 75 batch 121 batch_loss: 0.21604102849960327\n",
      "training: 75 batch 122 batch_loss: 0.21857473254203796\n",
      "training: 75 batch 123 batch_loss: 0.21190553903579712\n",
      "training: 75 batch 124 batch_loss: 0.21817293763160706\n",
      "training: 75 batch 125 batch_loss: 0.21872106194496155\n",
      "training: 75 batch 126 batch_loss: 0.21980175375938416\n",
      "training: 75 batch 127 batch_loss: 0.22025203704833984\n",
      "training: 75 batch 128 batch_loss: 0.2164793312549591\n",
      "training: 75 batch 129 batch_loss: 0.21899861097335815\n",
      "training: 75 batch 130 batch_loss: 0.2151736617088318\n",
      "training: 75 batch 131 batch_loss: 0.21814978122711182\n",
      "training: 75 batch 132 batch_loss: 0.2160910964012146\n",
      "training: 75 batch 133 batch_loss: 0.21697068214416504\n",
      "training: 75 batch 134 batch_loss: 0.2138635814189911\n",
      "training: 75 batch 135 batch_loss: 0.21639510989189148\n",
      "training: 75 batch 136 batch_loss: 0.21107059717178345\n",
      "training: 75 batch 137 batch_loss: 0.22078505158424377\n",
      "training: 75 batch 138 batch_loss: 0.21529152989387512\n",
      "training: 75 batch 139 batch_loss: 0.21476438641548157\n",
      "training: 75 batch 140 batch_loss: 0.21363094449043274\n",
      "training: 75 batch 141 batch_loss: 0.21781820058822632\n",
      "training: 75 batch 142 batch_loss: 0.21409118175506592\n",
      "training: 75 batch 143 batch_loss: 0.2160748541355133\n",
      "training: 75 batch 144 batch_loss: 0.2190287709236145\n",
      "training: 75 batch 145 batch_loss: 0.21640396118164062\n",
      "training: 75 batch 146 batch_loss: 0.21893265843391418\n",
      "training: 75 batch 147 batch_loss: 0.21277868747711182\n",
      "training: 75 batch 148 batch_loss: 0.2145754098892212\n",
      "training: 75 batch 149 batch_loss: 0.21403571963310242\n",
      "training: 75 batch 150 batch_loss: 0.21727663278579712\n",
      "training: 75 batch 151 batch_loss: 0.21845802664756775\n",
      "training: 75 batch 152 batch_loss: 0.21632292866706848\n",
      "training: 75 batch 153 batch_loss: 0.21499913930892944\n",
      "training: 75 batch 154 batch_loss: 0.2205401062965393\n",
      "training: 75 batch 155 batch_loss: 0.21934294700622559\n",
      "training: 75 batch 156 batch_loss: 0.21484851837158203\n",
      "training: 75 batch 157 batch_loss: 0.21497565507888794\n",
      "training: 75 batch 158 batch_loss: 0.21734285354614258\n",
      "training: 75 batch 159 batch_loss: 0.2145630419254303\n",
      "training: 75 batch 160 batch_loss: 0.21439608931541443\n",
      "training: 75 batch 161 batch_loss: 0.21418535709381104\n",
      "training: 75 batch 162 batch_loss: 0.21428978443145752\n",
      "training: 75 batch 163 batch_loss: 0.21465381979942322\n",
      "training: 75 batch 164 batch_loss: 0.21922338008880615\n",
      "training: 75 batch 165 batch_loss: 0.21390825510025024\n",
      "training: 75 batch 166 batch_loss: 0.2159995138645172\n",
      "training: 75 batch 167 batch_loss: 0.21559453010559082\n",
      "training: 75 batch 168 batch_loss: 0.21514979004859924\n",
      "training: 75 batch 169 batch_loss: 0.217884361743927\n",
      "training: 75 batch 170 batch_loss: 0.216269850730896\n",
      "training: 75 batch 171 batch_loss: 0.21862739324569702\n",
      "training: 75 batch 172 batch_loss: 0.21780851483345032\n",
      "training: 75 batch 173 batch_loss: 0.21459543704986572\n",
      "training: 75 batch 174 batch_loss: 0.21589422225952148\n",
      "training: 75 batch 175 batch_loss: 0.2122061550617218\n",
      "training: 75 batch 176 batch_loss: 0.22007524967193604\n",
      "training: 75 batch 177 batch_loss: 0.21841368079185486\n",
      "training: 75 batch 178 batch_loss: 0.21207523345947266\n",
      "training: 75 batch 179 batch_loss: 0.2196328341960907\n",
      "training: 75 batch 180 batch_loss: 0.22023004293441772\n",
      "training: 75 batch 181 batch_loss: 0.21498364210128784\n",
      "training: 75 batch 182 batch_loss: 0.22128590941429138\n",
      "training: 75 batch 183 batch_loss: 0.21595197916030884\n",
      "training: 75 batch 184 batch_loss: 0.22077298164367676\n",
      "training: 75 batch 185 batch_loss: 0.21777868270874023\n",
      "training: 75 batch 186 batch_loss: 0.21722057461738586\n",
      "training: 75 batch 187 batch_loss: 0.2156044840812683\n",
      "training: 75 batch 188 batch_loss: 0.21098420023918152\n",
      "training: 75 batch 189 batch_loss: 0.2174389362335205\n",
      "training: 75 batch 190 batch_loss: 0.21372586488723755\n",
      "training: 75 batch 191 batch_loss: 0.21464326977729797\n",
      "training: 75 batch 192 batch_loss: 0.21465528011322021\n",
      "training: 75 batch 193 batch_loss: 0.21893209218978882\n",
      "training: 75 batch 194 batch_loss: 0.21812224388122559\n",
      "training: 75 batch 195 batch_loss: 0.21657851338386536\n",
      "training: 75 batch 196 batch_loss: 0.2169276475906372\n",
      "training: 75 batch 197 batch_loss: 0.21061301231384277\n",
      "training: 75 batch 198 batch_loss: 0.21737879514694214\n",
      "training: 75 batch 199 batch_loss: 0.21721112728118896\n",
      "training: 75 batch 200 batch_loss: 0.21510308980941772\n",
      "training: 75 batch 201 batch_loss: 0.21829694509506226\n",
      "training: 75 batch 202 batch_loss: 0.21659749746322632\n",
      "training: 75 batch 203 batch_loss: 0.21805384755134583\n",
      "training: 75 batch 204 batch_loss: 0.21492689847946167\n",
      "training: 75 batch 205 batch_loss: 0.21042820811271667\n",
      "training: 75 batch 206 batch_loss: 0.21143442392349243\n",
      "training: 75 batch 207 batch_loss: 0.2140992283821106\n",
      "training: 75 batch 208 batch_loss: 0.21816307306289673\n",
      "training: 75 batch 209 batch_loss: 0.21693146228790283\n",
      "training: 75 batch 210 batch_loss: 0.21719807386398315\n",
      "training: 75 batch 211 batch_loss: 0.21638309955596924\n",
      "training: 75 batch 212 batch_loss: 0.21747618913650513\n",
      "training: 75 batch 213 batch_loss: 0.214394211769104\n",
      "training: 75 batch 214 batch_loss: 0.21547088027000427\n",
      "training: 75 batch 215 batch_loss: 0.217422753572464\n",
      "training: 75 batch 216 batch_loss: 0.2197769582271576\n",
      "training: 75 batch 217 batch_loss: 0.21722382307052612\n",
      "training: 75 batch 218 batch_loss: 0.22319847345352173\n",
      "training: 75 batch 219 batch_loss: 0.21921193599700928\n",
      "training: 75 batch 220 batch_loss: 0.22342029213905334\n",
      "training: 75 batch 221 batch_loss: 0.21997222304344177\n",
      "training: 75 batch 222 batch_loss: 0.22058376669883728\n",
      "training: 75 batch 223 batch_loss: 0.2161678969860077\n",
      "training: 75 batch 224 batch_loss: 0.21916106343269348\n",
      "training: 75 batch 225 batch_loss: 0.215818852186203\n",
      "training: 75 batch 226 batch_loss: 0.21696603298187256\n",
      "training: 75 batch 227 batch_loss: 0.21615108847618103\n",
      "training: 75 batch 228 batch_loss: 0.2193543016910553\n",
      "training: 75 batch 229 batch_loss: 0.21552658081054688\n",
      "training: 75 batch 230 batch_loss: 0.2156158685684204\n",
      "training: 75 batch 231 batch_loss: 0.2172047197818756\n",
      "training: 75 batch 232 batch_loss: 0.21409133076667786\n",
      "training: 75 batch 233 batch_loss: 0.21506169438362122\n",
      "training: 75 batch 234 batch_loss: 0.21773633360862732\n",
      "training: 75 batch 235 batch_loss: 0.2189648449420929\n",
      "training: 75 batch 236 batch_loss: 0.21835047006607056\n",
      "training: 75 batch 237 batch_loss: 0.21522235870361328\n",
      "training: 75 batch 238 batch_loss: 0.21713349223136902\n",
      "training: 75 batch 239 batch_loss: 0.21355414390563965\n",
      "training: 75 batch 240 batch_loss: 0.2196323275566101\n",
      "training: 75 batch 241 batch_loss: 0.21820437908172607\n",
      "training: 75 batch 242 batch_loss: 0.21529802680015564\n",
      "training: 75 batch 243 batch_loss: 0.21600663661956787\n",
      "training: 75 batch 244 batch_loss: 0.2191876471042633\n",
      "training: 75 batch 245 batch_loss: 0.2188614308834076\n",
      "training: 75 batch 246 batch_loss: 0.21581542491912842\n",
      "training: 75 batch 247 batch_loss: 0.2147078514099121\n",
      "training: 75 batch 248 batch_loss: 0.21296799182891846\n",
      "training: 75 batch 249 batch_loss: 0.2155320644378662\n",
      "training: 75 batch 250 batch_loss: 0.21597769856452942\n",
      "training: 75 batch 251 batch_loss: 0.21839290857315063\n",
      "training: 75 batch 252 batch_loss: 0.21586847305297852\n",
      "training: 75 batch 253 batch_loss: 0.2106373906135559\n",
      "training: 75 batch 254 batch_loss: 0.21465158462524414\n",
      "training: 75 batch 255 batch_loss: 0.21756231784820557\n",
      "training: 75 batch 256 batch_loss: 0.2145305573940277\n",
      "training: 75 batch 257 batch_loss: 0.21576446294784546\n",
      "training: 75 batch 258 batch_loss: 0.2165287733078003\n",
      "training: 75 batch 259 batch_loss: 0.21789824962615967\n",
      "training: 75 batch 260 batch_loss: 0.21656808257102966\n",
      "training: 75 batch 261 batch_loss: 0.21554908156394958\n",
      "training: 75 batch 262 batch_loss: 0.21484029293060303\n",
      "training: 75 batch 263 batch_loss: 0.22014102339744568\n",
      "training: 75 batch 264 batch_loss: 0.21833515167236328\n",
      "training: 75 batch 265 batch_loss: 0.21546140313148499\n",
      "training: 75 batch 266 batch_loss: 0.21521687507629395\n",
      "training: 75 batch 267 batch_loss: 0.21541711688041687\n",
      "training: 75 batch 268 batch_loss: 0.21910908818244934\n",
      "training: 75 batch 269 batch_loss: 0.21465212106704712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 75 batch 270 batch_loss: 0.22047501802444458\n",
      "training: 75 batch 271 batch_loss: 0.2136518359184265\n",
      "training: 75 batch 272 batch_loss: 0.2156965434551239\n",
      "training: 75 batch 273 batch_loss: 0.21472275257110596\n",
      "training: 75 batch 274 batch_loss: 0.2174346148967743\n",
      "training: 75 batch 275 batch_loss: 0.21708369255065918\n",
      "training: 75 batch 276 batch_loss: 0.2184619903564453\n",
      "training: 75 batch 277 batch_loss: 0.21558868885040283\n",
      "training: 75 batch 278 batch_loss: 0.2203330099582672\n",
      "training: 75 batch 279 batch_loss: 0.21927422285079956\n",
      "training: 75 batch 280 batch_loss: 0.22111091017723083\n",
      "training: 75 batch 281 batch_loss: 0.2209210991859436\n",
      "training: 75 batch 282 batch_loss: 0.21353018283843994\n",
      "training: 75 batch 283 batch_loss: 0.21724984049797058\n",
      "training: 75 batch 284 batch_loss: 0.2147466540336609\n",
      "training: 75 batch 285 batch_loss: 0.2184317409992218\n",
      "training: 75 batch 286 batch_loss: 0.21979764103889465\n",
      "training: 75 batch 287 batch_loss: 0.21534472703933716\n",
      "training: 75 batch 288 batch_loss: 0.2221304178237915\n",
      "training: 75 batch 289 batch_loss: 0.21691367030143738\n",
      "training: 75 batch 290 batch_loss: 0.21698465943336487\n",
      "training: 75 batch 291 batch_loss: 0.21943768858909607\n",
      "training: 75 batch 292 batch_loss: 0.2218180000782013\n",
      "training: 75 batch 293 batch_loss: 0.21686825156211853\n",
      "training: 75 batch 294 batch_loss: 0.2173331379890442\n",
      "training: 75 batch 295 batch_loss: 0.216913104057312\n",
      "training: 75 batch 296 batch_loss: 0.21622446179389954\n",
      "training: 75 batch 297 batch_loss: 0.21339914202690125\n",
      "training: 75 batch 298 batch_loss: 0.21964913606643677\n",
      "training: 75 batch 299 batch_loss: 0.21094289422035217\n",
      "training: 75 batch 300 batch_loss: 0.2144564986228943\n",
      "training: 75 batch 301 batch_loss: 0.209980309009552\n",
      "training: 75 batch 302 batch_loss: 0.22109505534172058\n",
      "training: 75 batch 303 batch_loss: 0.21915003657341003\n",
      "training: 75 batch 304 batch_loss: 0.2133159041404724\n",
      "training: 75 batch 305 batch_loss: 0.21442323923110962\n",
      "training: 75 batch 306 batch_loss: 0.21558567881584167\n",
      "training: 75 batch 307 batch_loss: 0.22128835320472717\n",
      "training: 75 batch 308 batch_loss: 0.21672466397285461\n",
      "training: 75 batch 309 batch_loss: 0.21818965673446655\n",
      "training: 75 batch 310 batch_loss: 0.213534414768219\n",
      "training: 75 batch 311 batch_loss: 0.22055861353874207\n",
      "training: 75 batch 312 batch_loss: 0.21446895599365234\n",
      "training: 75 batch 313 batch_loss: 0.21552759408950806\n",
      "training: 75 batch 314 batch_loss: 0.21546483039855957\n",
      "training: 75 batch 315 batch_loss: 0.21569585800170898\n",
      "training: 75 batch 316 batch_loss: 0.21598783135414124\n",
      "training: 75 batch 317 batch_loss: 0.21559417247772217\n",
      "training: 75 batch 318 batch_loss: 0.21496516466140747\n",
      "training: 75 batch 319 batch_loss: 0.21621352434158325\n",
      "training: 75 batch 320 batch_loss: 0.21770304441452026\n",
      "training: 75 batch 321 batch_loss: 0.21747994422912598\n",
      "training: 75 batch 322 batch_loss: 0.2155747413635254\n",
      "training: 75 batch 323 batch_loss: 0.22106027603149414\n",
      "training: 75 batch 324 batch_loss: 0.22089505195617676\n",
      "training: 75 batch 325 batch_loss: 0.21915170550346375\n",
      "training: 75 batch 326 batch_loss: 0.2209862470626831\n",
      "training: 75 batch 327 batch_loss: 0.21362704038619995\n",
      "training: 75 batch 328 batch_loss: 0.2183804214000702\n",
      "training: 75 batch 329 batch_loss: 0.21698489785194397\n",
      "training: 75 batch 330 batch_loss: 0.21608996391296387\n",
      "training: 75 batch 331 batch_loss: 0.22082743048667908\n",
      "training: 75 batch 332 batch_loss: 0.22009363770484924\n",
      "training: 75 batch 333 batch_loss: 0.21657419204711914\n",
      "training: 75 batch 334 batch_loss: 0.21636700630187988\n",
      "training: 75 batch 335 batch_loss: 0.2133444845676422\n",
      "training: 75 batch 336 batch_loss: 0.2177567481994629\n",
      "training: 75 batch 337 batch_loss: 0.2184993028640747\n",
      "training: 75 batch 338 batch_loss: 0.21671366691589355\n",
      "training: 75 batch 339 batch_loss: 0.21566706895828247\n",
      "training: 75 batch 340 batch_loss: 0.21931782364845276\n",
      "training: 75 batch 341 batch_loss: 0.21733975410461426\n",
      "training: 75 batch 342 batch_loss: 0.216843843460083\n",
      "training: 75 batch 343 batch_loss: 0.213881254196167\n",
      "training: 75 batch 344 batch_loss: 0.22221121191978455\n",
      "training: 75 batch 345 batch_loss: 0.21875017881393433\n",
      "training: 75 batch 346 batch_loss: 0.21756809949874878\n",
      "training: 75 batch 347 batch_loss: 0.2200777232646942\n",
      "training: 75 batch 348 batch_loss: 0.21532902121543884\n",
      "training: 75 batch 349 batch_loss: 0.2207089066505432\n",
      "training: 75 batch 350 batch_loss: 0.21582788228988647\n",
      "training: 75 batch 351 batch_loss: 0.21947747468948364\n",
      "training: 75 batch 352 batch_loss: 0.22284162044525146\n",
      "training: 75 batch 353 batch_loss: 0.217328280210495\n",
      "training: 75 batch 354 batch_loss: 0.21572566032409668\n",
      "training: 75 batch 355 batch_loss: 0.21398919820785522\n",
      "training: 75 batch 356 batch_loss: 0.21843615174293518\n",
      "training: 75 batch 357 batch_loss: 0.21927589178085327\n",
      "training: 75 batch 358 batch_loss: 0.21580559015274048\n",
      "training: 75 batch 359 batch_loss: 0.21493881940841675\n",
      "training: 75 batch 360 batch_loss: 0.21543729305267334\n",
      "training: 75 batch 361 batch_loss: 0.21722263097763062\n",
      "training: 75 batch 362 batch_loss: 0.21356365084648132\n",
      "training: 75 batch 363 batch_loss: 0.21786341071128845\n",
      "training: 75 batch 364 batch_loss: 0.21658915281295776\n",
      "training: 75 batch 365 batch_loss: 0.21846133470535278\n",
      "training: 75 batch 366 batch_loss: 0.2201230525970459\n",
      "training: 75 batch 367 batch_loss: 0.21652084589004517\n",
      "training: 75 batch 368 batch_loss: 0.22080695629119873\n",
      "training: 75 batch 369 batch_loss: 0.2163274884223938\n",
      "training: 75 batch 370 batch_loss: 0.21517300605773926\n",
      "training: 75 batch 371 batch_loss: 0.2203138768672943\n",
      "training: 75 batch 372 batch_loss: 0.21190786361694336\n",
      "training: 75 batch 373 batch_loss: 0.2144431173801422\n",
      "training: 75 batch 374 batch_loss: 0.2199346423149109\n",
      "training: 75 batch 375 batch_loss: 0.2192879617214203\n",
      "training: 75 batch 376 batch_loss: 0.2154260277748108\n",
      "training: 75 batch 377 batch_loss: 0.2148236632347107\n",
      "training: 75 batch 378 batch_loss: 0.2207261323928833\n",
      "training: 75 batch 379 batch_loss: 0.21414262056350708\n",
      "training: 75 batch 380 batch_loss: 0.21743902564048767\n",
      "training: 75 batch 381 batch_loss: 0.21603268384933472\n",
      "training: 75 batch 382 batch_loss: 0.21997913718223572\n",
      "training: 75 batch 383 batch_loss: 0.2168547511100769\n",
      "training: 75 batch 384 batch_loss: 0.21816366910934448\n",
      "training: 75 batch 385 batch_loss: 0.2161213755607605\n",
      "training: 75 batch 386 batch_loss: 0.21874287724494934\n",
      "training: 75 batch 387 batch_loss: 0.219997376203537\n",
      "training: 75 batch 388 batch_loss: 0.21944165229797363\n",
      "training: 75 batch 389 batch_loss: 0.21180567145347595\n",
      "training: 75 batch 390 batch_loss: 0.21329236030578613\n",
      "training: 75 batch 391 batch_loss: 0.2172858715057373\n",
      "training: 75 batch 392 batch_loss: 0.21984046697616577\n",
      "training: 75 batch 393 batch_loss: 0.21366801857948303\n",
      "training: 75 batch 394 batch_loss: 0.21219319105148315\n",
      "training: 75 batch 395 batch_loss: 0.2206002175807953\n",
      "training: 75 batch 396 batch_loss: 0.21676498651504517\n",
      "training: 75 batch 397 batch_loss: 0.22257006168365479\n",
      "training: 75 batch 398 batch_loss: 0.21463096141815186\n",
      "training: 75 batch 399 batch_loss: 0.2199309766292572\n",
      "training: 75 batch 400 batch_loss: 0.2197299599647522\n",
      "training: 75 batch 401 batch_loss: 0.21777787804603577\n",
      "training: 75 batch 402 batch_loss: 0.21645605564117432\n",
      "training: 75 batch 403 batch_loss: 0.21694505214691162\n",
      "training: 75 batch 404 batch_loss: 0.21810922026634216\n",
      "training: 75 batch 405 batch_loss: 0.21484306454658508\n",
      "training: 75 batch 406 batch_loss: 0.216470867395401\n",
      "training: 75 batch 407 batch_loss: 0.21736013889312744\n",
      "training: 75 batch 408 batch_loss: 0.21662816405296326\n",
      "training: 75 batch 409 batch_loss: 0.21463477611541748\n",
      "training: 75 batch 410 batch_loss: 0.21628811955451965\n",
      "training: 75 batch 411 batch_loss: 0.21750766038894653\n",
      "training: 75 batch 412 batch_loss: 0.22156646847724915\n",
      "training: 75 batch 413 batch_loss: 0.2127663493156433\n",
      "training: 75 batch 414 batch_loss: 0.21620219945907593\n",
      "training: 75 batch 415 batch_loss: 0.21860235929489136\n",
      "training: 75 batch 416 batch_loss: 0.22010588645935059\n",
      "training: 75 batch 417 batch_loss: 0.21682536602020264\n",
      "training: 75 batch 418 batch_loss: 0.21920901536941528\n",
      "training: 75 batch 419 batch_loss: 0.2153204381465912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 75 batch 420 batch_loss: 0.21901994943618774\n",
      "training: 75 batch 421 batch_loss: 0.2167677879333496\n",
      "training: 75 batch 422 batch_loss: 0.21545052528381348\n",
      "training: 75 batch 423 batch_loss: 0.21978724002838135\n",
      "training: 75 batch 424 batch_loss: 0.2189044952392578\n",
      "training: 75 batch 425 batch_loss: 0.2210778295993805\n",
      "training: 75 batch 426 batch_loss: 0.2155386507511139\n",
      "training: 75 batch 427 batch_loss: 0.2226792573928833\n",
      "training: 75 batch 428 batch_loss: 0.2114553451538086\n",
      "training: 75 batch 429 batch_loss: 0.21447432041168213\n",
      "training: 75 batch 430 batch_loss: 0.21748778223991394\n",
      "training: 75 batch 431 batch_loss: 0.22060436010360718\n",
      "training: 75 batch 432 batch_loss: 0.21870601177215576\n",
      "training: 75 batch 433 batch_loss: 0.21525278687477112\n",
      "training: 75 batch 434 batch_loss: 0.21792501211166382\n",
      "training: 75 batch 435 batch_loss: 0.22036263346672058\n",
      "training: 75 batch 436 batch_loss: 0.22138303518295288\n",
      "training: 75 batch 437 batch_loss: 0.2197456657886505\n",
      "training: 75 batch 438 batch_loss: 0.22101646661758423\n",
      "training: 75 batch 439 batch_loss: 0.21366912126541138\n",
      "training: 75 batch 440 batch_loss: 0.21524280309677124\n",
      "training: 75 batch 441 batch_loss: 0.2200535237789154\n",
      "training: 75 batch 442 batch_loss: 0.2186037003993988\n",
      "training: 75 batch 443 batch_loss: 0.2134246528148651\n",
      "training: 75 batch 444 batch_loss: 0.22034019231796265\n",
      "training: 75 batch 445 batch_loss: 0.21941456198692322\n",
      "training: 75 batch 446 batch_loss: 0.2186846137046814\n",
      "training: 75 batch 447 batch_loss: 0.2164287567138672\n",
      "training: 75 batch 448 batch_loss: 0.21833008527755737\n",
      "training: 75 batch 449 batch_loss: 0.21984323859214783\n",
      "training: 75 batch 450 batch_loss: 0.21167346835136414\n",
      "training: 75 batch 451 batch_loss: 0.21857008337974548\n",
      "training: 75 batch 452 batch_loss: 0.22094041109085083\n",
      "training: 75 batch 453 batch_loss: 0.22045910358428955\n",
      "training: 75 batch 454 batch_loss: 0.2209334373474121\n",
      "training: 75 batch 455 batch_loss: 0.21549555659294128\n",
      "training: 75 batch 456 batch_loss: 0.2171238660812378\n",
      "training: 75 batch 457 batch_loss: 0.21904289722442627\n",
      "training: 75 batch 458 batch_loss: 0.2221718430519104\n",
      "training: 75 batch 459 batch_loss: 0.21396800875663757\n",
      "training: 75 batch 460 batch_loss: 0.21702390909194946\n",
      "training: 75 batch 461 batch_loss: 0.2206915318965912\n",
      "training: 75 batch 462 batch_loss: 0.21658629179000854\n",
      "training: 75 batch 463 batch_loss: 0.2151949405670166\n",
      "training: 75 batch 464 batch_loss: 0.21493834257125854\n",
      "training: 75 batch 465 batch_loss: 0.21492686867713928\n",
      "training: 75 batch 466 batch_loss: 0.21548596024513245\n",
      "training: 75 batch 467 batch_loss: 0.2186737060546875\n",
      "training: 75 batch 468 batch_loss: 0.21317699551582336\n",
      "training: 75 batch 469 batch_loss: 0.21898308396339417\n",
      "training: 75 batch 470 batch_loss: 0.21670931577682495\n",
      "training: 75 batch 471 batch_loss: 0.21264132857322693\n",
      "training: 75 batch 472 batch_loss: 0.21760696172714233\n",
      "training: 75 batch 473 batch_loss: 0.21752750873565674\n",
      "training: 75 batch 474 batch_loss: 0.2194691300392151\n",
      "training: 75 batch 475 batch_loss: 0.2197384536266327\n",
      "training: 75 batch 476 batch_loss: 0.2200843095779419\n",
      "training: 75 batch 477 batch_loss: 0.21849465370178223\n",
      "training: 75 batch 478 batch_loss: 0.2206311821937561\n",
      "training: 75 batch 479 batch_loss: 0.2201816439628601\n",
      "training: 75 batch 480 batch_loss: 0.21797192096710205\n",
      "training: 75 batch 481 batch_loss: 0.2179965376853943\n",
      "training: 75 batch 482 batch_loss: 0.21974390745162964\n",
      "training: 75 batch 483 batch_loss: 0.2155178189277649\n",
      "training: 75 batch 484 batch_loss: 0.22266709804534912\n",
      "training: 75 batch 485 batch_loss: 0.2185543179512024\n",
      "training: 75 batch 486 batch_loss: 0.21745601296424866\n",
      "training: 75 batch 487 batch_loss: 0.22082626819610596\n",
      "training: 75 batch 488 batch_loss: 0.2192300260066986\n",
      "training: 75 batch 489 batch_loss: 0.2188301682472229\n",
      "training: 75 batch 490 batch_loss: 0.22028407454490662\n",
      "training: 75 batch 491 batch_loss: 0.21727201342582703\n",
      "training: 75 batch 492 batch_loss: 0.21914684772491455\n",
      "training: 75 batch 493 batch_loss: 0.21754801273345947\n",
      "training: 75 batch 494 batch_loss: 0.2134810984134674\n",
      "training: 75 batch 495 batch_loss: 0.21151289343833923\n",
      "training: 75 batch 496 batch_loss: 0.21659228205680847\n",
      "training: 75 batch 497 batch_loss: 0.2189587652683258\n",
      "training: 75 batch 498 batch_loss: 0.2199246883392334\n",
      "training: 75 batch 499 batch_loss: 0.21510255336761475\n",
      "training: 75 batch 500 batch_loss: 0.21708223223686218\n",
      "training: 75 batch 501 batch_loss: 0.21829640865325928\n",
      "training: 75 batch 502 batch_loss: 0.21428170800209045\n",
      "training: 75 batch 503 batch_loss: 0.2146540880203247\n",
      "training: 75 batch 504 batch_loss: 0.2142922282218933\n",
      "training: 75 batch 505 batch_loss: 0.21754157543182373\n",
      "training: 75 batch 506 batch_loss: 0.21649694442749023\n",
      "training: 75 batch 507 batch_loss: 0.22054067254066467\n",
      "training: 75 batch 508 batch_loss: 0.21833905577659607\n",
      "training: 75 batch 509 batch_loss: 0.21685782074928284\n",
      "training: 75 batch 510 batch_loss: 0.21659362316131592\n",
      "training: 75 batch 511 batch_loss: 0.2185964584350586\n",
      "training: 75 batch 512 batch_loss: 0.2257712185382843\n",
      "training: 75 batch 513 batch_loss: 0.21404853463172913\n",
      "training: 75 batch 514 batch_loss: 0.22023281455039978\n",
      "training: 75 batch 515 batch_loss: 0.22153449058532715\n",
      "training: 75 batch 516 batch_loss: 0.2168063223361969\n",
      "training: 75 batch 517 batch_loss: 0.2168116271495819\n",
      "training: 75 batch 518 batch_loss: 0.2187848687171936\n",
      "training: 75 batch 519 batch_loss: 0.21116149425506592\n",
      "training: 75 batch 520 batch_loss: 0.21736469864845276\n",
      "training: 75 batch 521 batch_loss: 0.2156849205493927\n",
      "training: 75 batch 522 batch_loss: 0.21670633554458618\n",
      "training: 75 batch 523 batch_loss: 0.21564427018165588\n",
      "training: 75 batch 524 batch_loss: 0.21943819522857666\n",
      "training: 75 batch 525 batch_loss: 0.21706777811050415\n",
      "training: 75 batch 526 batch_loss: 0.21877476572990417\n",
      "training: 75 batch 527 batch_loss: 0.21865099668502808\n",
      "training: 75 batch 528 batch_loss: 0.21891635656356812\n",
      "training: 75 batch 529 batch_loss: 0.2216191291809082\n",
      "training: 75 batch 530 batch_loss: 0.21156418323516846\n",
      "training: 75 batch 531 batch_loss: 0.21606633067131042\n",
      "training: 75 batch 532 batch_loss: 0.21736395359039307\n",
      "training: 75 batch 533 batch_loss: 0.21519985795021057\n",
      "training: 75 batch 534 batch_loss: 0.2196054458618164\n",
      "training: 75 batch 535 batch_loss: 0.22088682651519775\n",
      "training: 75 batch 536 batch_loss: 0.21506398916244507\n",
      "training: 75 batch 537 batch_loss: 0.21816304326057434\n",
      "training: 75 batch 538 batch_loss: 0.22005760669708252\n",
      "training: 75 batch 539 batch_loss: 0.2161175012588501\n",
      "training: 75 batch 540 batch_loss: 0.21972525119781494\n",
      "training: 75 batch 541 batch_loss: 0.222831130027771\n",
      "training: 75 batch 542 batch_loss: 0.21620357036590576\n",
      "training: 75 batch 543 batch_loss: 0.22097057104110718\n",
      "training: 75 batch 544 batch_loss: 0.2174675464630127\n",
      "training: 75 batch 545 batch_loss: 0.21982115507125854\n",
      "training: 75 batch 546 batch_loss: 0.21617606282234192\n",
      "training: 75 batch 547 batch_loss: 0.21747151017189026\n",
      "training: 75 batch 548 batch_loss: 0.21714144945144653\n",
      "training: 75 batch 549 batch_loss: 0.21923011541366577\n",
      "training: 75 batch 550 batch_loss: 0.21915537118911743\n",
      "training: 75 batch 551 batch_loss: 0.22053471207618713\n",
      "training: 75 batch 552 batch_loss: 0.22178059816360474\n",
      "training: 75 batch 553 batch_loss: 0.21978601813316345\n",
      "training: 75 batch 554 batch_loss: 0.21659883856773376\n",
      "training: 75 batch 555 batch_loss: 0.2199607789516449\n",
      "training: 75 batch 556 batch_loss: 0.2185913324356079\n",
      "training: 75 batch 557 batch_loss: 0.21933764219284058\n",
      "training: 75 batch 558 batch_loss: 0.21957463026046753\n",
      "training: 75 batch 559 batch_loss: 0.2165849804878235\n",
      "training: 75 batch 560 batch_loss: 0.22037607431411743\n",
      "training: 75 batch 561 batch_loss: 0.2195621132850647\n",
      "training: 75 batch 562 batch_loss: 0.225885808467865\n",
      "training: 75 batch 563 batch_loss: 0.2174639105796814\n",
      "training: 75 batch 564 batch_loss: 0.21702754497528076\n",
      "training: 75 batch 565 batch_loss: 0.2114720642566681\n",
      "training: 75 batch 566 batch_loss: 0.2176438570022583\n",
      "training: 75 batch 567 batch_loss: 0.21660453081130981\n",
      "training: 75 batch 568 batch_loss: 0.21955996751785278\n",
      "training: 75 batch 569 batch_loss: 0.21692824363708496\n",
      "training: 75 batch 570 batch_loss: 0.21713364124298096\n",
      "training: 75 batch 571 batch_loss: 0.21956709027290344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 75 batch 572 batch_loss: 0.2154141664505005\n",
      "training: 75 batch 573 batch_loss: 0.21349331736564636\n",
      "training: 75 batch 574 batch_loss: 0.21744075417518616\n",
      "training: 75 batch 575 batch_loss: 0.21446135640144348\n",
      "training: 75 batch 576 batch_loss: 0.2249058187007904\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 75, Hit Ratio:0.03386821145063343 | Precision:0.04997051017398997 | Recall:0.0657460148025706 | NDCG:0.06483557245872083\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 76 batch 0 batch_loss: 0.21755996346473694\n",
      "training: 76 batch 1 batch_loss: 0.21575438976287842\n",
      "training: 76 batch 2 batch_loss: 0.21710994839668274\n",
      "training: 76 batch 3 batch_loss: 0.21779486536979675\n",
      "training: 76 batch 4 batch_loss: 0.21287637948989868\n",
      "training: 76 batch 5 batch_loss: 0.21303462982177734\n",
      "training: 76 batch 6 batch_loss: 0.22001338005065918\n",
      "training: 76 batch 7 batch_loss: 0.2183554470539093\n",
      "training: 76 batch 8 batch_loss: 0.21707448363304138\n",
      "training: 76 batch 9 batch_loss: 0.21058505773544312\n",
      "training: 76 batch 10 batch_loss: 0.2162071168422699\n",
      "training: 76 batch 11 batch_loss: 0.2180725336074829\n",
      "training: 76 batch 12 batch_loss: 0.21613585948944092\n",
      "training: 76 batch 13 batch_loss: 0.21372666954994202\n",
      "training: 76 batch 14 batch_loss: 0.21584412455558777\n",
      "training: 76 batch 15 batch_loss: 0.21549618244171143\n",
      "training: 76 batch 16 batch_loss: 0.21347391605377197\n",
      "training: 76 batch 17 batch_loss: 0.21853595972061157\n",
      "training: 76 batch 18 batch_loss: 0.2209044098854065\n",
      "training: 76 batch 19 batch_loss: 0.2142033874988556\n",
      "training: 76 batch 20 batch_loss: 0.21415302157402039\n",
      "training: 76 batch 21 batch_loss: 0.21660003066062927\n",
      "training: 76 batch 22 batch_loss: 0.21755778789520264\n",
      "training: 76 batch 23 batch_loss: 0.21708828210830688\n",
      "training: 76 batch 24 batch_loss: 0.21400225162506104\n",
      "training: 76 batch 25 batch_loss: 0.2166878581047058\n",
      "training: 76 batch 26 batch_loss: 0.21694719791412354\n",
      "training: 76 batch 27 batch_loss: 0.21990448236465454\n",
      "training: 76 batch 28 batch_loss: 0.21413260698318481\n",
      "training: 76 batch 29 batch_loss: 0.21616590023040771\n",
      "training: 76 batch 30 batch_loss: 0.2168026864528656\n",
      "training: 76 batch 31 batch_loss: 0.21526101231575012\n",
      "training: 76 batch 32 batch_loss: 0.2157706320285797\n",
      "training: 76 batch 33 batch_loss: 0.21744248270988464\n",
      "training: 76 batch 34 batch_loss: 0.22185349464416504\n",
      "training: 76 batch 35 batch_loss: 0.21783944964408875\n",
      "training: 76 batch 36 batch_loss: 0.21171259880065918\n",
      "training: 76 batch 37 batch_loss: 0.2221946120262146\n",
      "training: 76 batch 38 batch_loss: 0.21384400129318237\n",
      "training: 76 batch 39 batch_loss: 0.22164201736450195\n",
      "training: 76 batch 40 batch_loss: 0.21969884634017944\n",
      "training: 76 batch 41 batch_loss: 0.21722248196601868\n",
      "training: 76 batch 42 batch_loss: 0.21884891390800476\n",
      "training: 76 batch 43 batch_loss: 0.2190709114074707\n",
      "training: 76 batch 44 batch_loss: 0.21924138069152832\n",
      "training: 76 batch 45 batch_loss: 0.21410495042800903\n",
      "training: 76 batch 46 batch_loss: 0.21313238143920898\n",
      "training: 76 batch 47 batch_loss: 0.21730318665504456\n",
      "training: 76 batch 48 batch_loss: 0.21203339099884033\n",
      "training: 76 batch 49 batch_loss: 0.22028034925460815\n",
      "training: 76 batch 50 batch_loss: 0.22292137145996094\n",
      "training: 76 batch 51 batch_loss: 0.2163066565990448\n",
      "training: 76 batch 52 batch_loss: 0.21683162450790405\n",
      "training: 76 batch 53 batch_loss: 0.2153947651386261\n",
      "training: 76 batch 54 batch_loss: 0.21760225296020508\n",
      "training: 76 batch 55 batch_loss: 0.21724405884742737\n",
      "training: 76 batch 56 batch_loss: 0.2161760926246643\n",
      "training: 76 batch 57 batch_loss: 0.21859848499298096\n",
      "training: 76 batch 58 batch_loss: 0.2212071716785431\n",
      "training: 76 batch 59 batch_loss: 0.21783453226089478\n",
      "training: 76 batch 60 batch_loss: 0.21847715973854065\n",
      "training: 76 batch 61 batch_loss: 0.2175920009613037\n",
      "training: 76 batch 62 batch_loss: 0.21199935674667358\n",
      "training: 76 batch 63 batch_loss: 0.2137318253517151\n",
      "training: 76 batch 64 batch_loss: 0.21408694982528687\n",
      "training: 76 batch 65 batch_loss: 0.21440276503562927\n",
      "training: 76 batch 66 batch_loss: 0.22059062123298645\n",
      "training: 76 batch 67 batch_loss: 0.21578776836395264\n",
      "training: 76 batch 68 batch_loss: 0.21952509880065918\n",
      "training: 76 batch 69 batch_loss: 0.2187231183052063\n",
      "training: 76 batch 70 batch_loss: 0.21249699592590332\n",
      "training: 76 batch 71 batch_loss: 0.2180081009864807\n",
      "training: 76 batch 72 batch_loss: 0.21145564317703247\n",
      "training: 76 batch 73 batch_loss: 0.21715152263641357\n",
      "training: 76 batch 74 batch_loss: 0.22148361802101135\n",
      "training: 76 batch 75 batch_loss: 0.2155693769454956\n",
      "training: 76 batch 76 batch_loss: 0.21760058403015137\n",
      "training: 76 batch 77 batch_loss: 0.21798139810562134\n",
      "training: 76 batch 78 batch_loss: 0.21969658136367798\n",
      "training: 76 batch 79 batch_loss: 0.21717065572738647\n",
      "training: 76 batch 80 batch_loss: 0.21133747696876526\n",
      "training: 76 batch 81 batch_loss: 0.21649780869483948\n",
      "training: 76 batch 82 batch_loss: 0.2171241044998169\n",
      "training: 76 batch 83 batch_loss: 0.2154645323753357\n",
      "training: 76 batch 84 batch_loss: 0.21657976508140564\n",
      "training: 76 batch 85 batch_loss: 0.21178770065307617\n",
      "training: 76 batch 86 batch_loss: 0.2165534794330597\n",
      "training: 76 batch 87 batch_loss: 0.21385273337364197\n",
      "training: 76 batch 88 batch_loss: 0.21408966183662415\n",
      "training: 76 batch 89 batch_loss: 0.21569302678108215\n",
      "training: 76 batch 90 batch_loss: 0.21737423539161682\n",
      "training: 76 batch 91 batch_loss: 0.21996012330055237\n",
      "training: 76 batch 92 batch_loss: 0.21587583422660828\n",
      "training: 76 batch 93 batch_loss: 0.22011637687683105\n",
      "training: 76 batch 94 batch_loss: 0.2178601324558258\n",
      "training: 76 batch 95 batch_loss: 0.21737921237945557\n",
      "training: 76 batch 96 batch_loss: 0.217242032289505\n",
      "training: 76 batch 97 batch_loss: 0.2137770652770996\n",
      "training: 76 batch 98 batch_loss: 0.21425840258598328\n",
      "training: 76 batch 99 batch_loss: 0.21524816751480103\n",
      "training: 76 batch 100 batch_loss: 0.2166234850883484\n",
      "training: 76 batch 101 batch_loss: 0.2177242636680603\n",
      "training: 76 batch 102 batch_loss: 0.21833005547523499\n",
      "training: 76 batch 103 batch_loss: 0.21533948183059692\n",
      "training: 76 batch 104 batch_loss: 0.21914908289909363\n",
      "training: 76 batch 105 batch_loss: 0.21373388171195984\n",
      "training: 76 batch 106 batch_loss: 0.2199140191078186\n",
      "training: 76 batch 107 batch_loss: 0.2165186107158661\n",
      "training: 76 batch 108 batch_loss: 0.2176453173160553\n",
      "training: 76 batch 109 batch_loss: 0.214933842420578\n",
      "training: 76 batch 110 batch_loss: 0.21818095445632935\n",
      "training: 76 batch 111 batch_loss: 0.2153269350528717\n",
      "training: 76 batch 112 batch_loss: 0.21790602803230286\n",
      "training: 76 batch 113 batch_loss: 0.2163701355457306\n",
      "training: 76 batch 114 batch_loss: 0.21584159135818481\n",
      "training: 76 batch 115 batch_loss: 0.21849524974822998\n",
      "training: 76 batch 116 batch_loss: 0.21590113639831543\n",
      "training: 76 batch 117 batch_loss: 0.21660837531089783\n",
      "training: 76 batch 118 batch_loss: 0.2174428105354309\n",
      "training: 76 batch 119 batch_loss: 0.21264058351516724\n",
      "training: 76 batch 120 batch_loss: 0.2175447940826416\n",
      "training: 76 batch 121 batch_loss: 0.21904653310775757\n",
      "training: 76 batch 122 batch_loss: 0.21298784017562866\n",
      "training: 76 batch 123 batch_loss: 0.21496641635894775\n",
      "training: 76 batch 124 batch_loss: 0.21735262870788574\n",
      "training: 76 batch 125 batch_loss: 0.21915453672409058\n",
      "training: 76 batch 126 batch_loss: 0.21635153889656067\n",
      "training: 76 batch 127 batch_loss: 0.21893548965454102\n",
      "training: 76 batch 128 batch_loss: 0.21845358610153198\n",
      "training: 76 batch 129 batch_loss: 0.2192043662071228\n",
      "training: 76 batch 130 batch_loss: 0.21614301204681396\n",
      "training: 76 batch 131 batch_loss: 0.21702983975410461\n",
      "training: 76 batch 132 batch_loss: 0.21557092666625977\n",
      "training: 76 batch 133 batch_loss: 0.21331119537353516\n",
      "training: 76 batch 134 batch_loss: 0.21666240692138672\n",
      "training: 76 batch 135 batch_loss: 0.21433025598526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 76 batch 136 batch_loss: 0.21679118275642395\n",
      "training: 76 batch 137 batch_loss: 0.2140015959739685\n",
      "training: 76 batch 138 batch_loss: 0.2179310917854309\n",
      "training: 76 batch 139 batch_loss: 0.21904891729354858\n",
      "training: 76 batch 140 batch_loss: 0.21561962366104126\n",
      "training: 76 batch 141 batch_loss: 0.2168288230895996\n",
      "training: 76 batch 142 batch_loss: 0.21983644366264343\n",
      "training: 76 batch 143 batch_loss: 0.21773627400398254\n",
      "training: 76 batch 144 batch_loss: 0.22173342108726501\n",
      "training: 76 batch 145 batch_loss: 0.2187730073928833\n",
      "training: 76 batch 146 batch_loss: 0.21837958693504333\n",
      "training: 76 batch 147 batch_loss: 0.21706819534301758\n",
      "training: 76 batch 148 batch_loss: 0.21310898661613464\n",
      "training: 76 batch 149 batch_loss: 0.21581211686134338\n",
      "training: 76 batch 150 batch_loss: 0.21808090806007385\n",
      "training: 76 batch 151 batch_loss: 0.2140870988368988\n",
      "training: 76 batch 152 batch_loss: 0.21857258677482605\n",
      "training: 76 batch 153 batch_loss: 0.21704977750778198\n",
      "training: 76 batch 154 batch_loss: 0.21826070547103882\n",
      "training: 76 batch 155 batch_loss: 0.21362310647964478\n",
      "training: 76 batch 156 batch_loss: 0.21540850400924683\n",
      "training: 76 batch 157 batch_loss: 0.21463137865066528\n",
      "training: 76 batch 158 batch_loss: 0.22127920389175415\n",
      "training: 76 batch 159 batch_loss: 0.21148443222045898\n",
      "training: 76 batch 160 batch_loss: 0.2136843502521515\n",
      "training: 76 batch 161 batch_loss: 0.21755367517471313\n",
      "training: 76 batch 162 batch_loss: 0.21866020560264587\n",
      "training: 76 batch 163 batch_loss: 0.21852001547813416\n",
      "training: 76 batch 164 batch_loss: 0.21890389919281006\n",
      "training: 76 batch 165 batch_loss: 0.2133985161781311\n",
      "training: 76 batch 166 batch_loss: 0.2220931351184845\n",
      "training: 76 batch 167 batch_loss: 0.21667695045471191\n",
      "training: 76 batch 168 batch_loss: 0.21614357829093933\n",
      "training: 76 batch 169 batch_loss: 0.21963587403297424\n",
      "training: 76 batch 170 batch_loss: 0.22490951418876648\n",
      "training: 76 batch 171 batch_loss: 0.21348124742507935\n",
      "training: 76 batch 172 batch_loss: 0.21806550025939941\n",
      "training: 76 batch 173 batch_loss: 0.21689242124557495\n",
      "training: 76 batch 174 batch_loss: 0.21861478686332703\n",
      "training: 76 batch 175 batch_loss: 0.21395635604858398\n",
      "training: 76 batch 176 batch_loss: 0.21536722779273987\n",
      "training: 76 batch 177 batch_loss: 0.21996727585792542\n",
      "training: 76 batch 178 batch_loss: 0.2178867757320404\n",
      "training: 76 batch 179 batch_loss: 0.21873411536216736\n",
      "training: 76 batch 180 batch_loss: 0.21597158908843994\n",
      "training: 76 batch 181 batch_loss: 0.2173595428466797\n",
      "training: 76 batch 182 batch_loss: 0.21958434581756592\n",
      "training: 76 batch 183 batch_loss: 0.2132132649421692\n",
      "training: 76 batch 184 batch_loss: 0.21872779726982117\n",
      "training: 76 batch 185 batch_loss: 0.21705672144889832\n",
      "training: 76 batch 186 batch_loss: 0.21693426370620728\n",
      "training: 76 batch 187 batch_loss: 0.21788230538368225\n",
      "training: 76 batch 188 batch_loss: 0.21210134029388428\n",
      "training: 76 batch 189 batch_loss: 0.21458667516708374\n",
      "training: 76 batch 190 batch_loss: 0.21554237604141235\n",
      "training: 76 batch 191 batch_loss: 0.21826666593551636\n",
      "training: 76 batch 192 batch_loss: 0.21845194697380066\n",
      "training: 76 batch 193 batch_loss: 0.21409273147583008\n",
      "training: 76 batch 194 batch_loss: 0.21684318780899048\n",
      "training: 76 batch 195 batch_loss: 0.2185271978378296\n",
      "training: 76 batch 196 batch_loss: 0.21641704440116882\n",
      "training: 76 batch 197 batch_loss: 0.214295893907547\n",
      "training: 76 batch 198 batch_loss: 0.21701595187187195\n",
      "training: 76 batch 199 batch_loss: 0.2167244851589203\n",
      "training: 76 batch 200 batch_loss: 0.21228492259979248\n",
      "training: 76 batch 201 batch_loss: 0.22034403681755066\n",
      "training: 76 batch 202 batch_loss: 0.22052007913589478\n",
      "training: 76 batch 203 batch_loss: 0.22074320912361145\n",
      "training: 76 batch 204 batch_loss: 0.21851366758346558\n",
      "training: 76 batch 205 batch_loss: 0.21513444185256958\n",
      "training: 76 batch 206 batch_loss: 0.21925562620162964\n",
      "training: 76 batch 207 batch_loss: 0.21808230876922607\n",
      "training: 76 batch 208 batch_loss: 0.21761870384216309\n",
      "training: 76 batch 209 batch_loss: 0.2219054102897644\n",
      "training: 76 batch 210 batch_loss: 0.21755284070968628\n",
      "training: 76 batch 211 batch_loss: 0.21477219462394714\n",
      "training: 76 batch 212 batch_loss: 0.2160906195640564\n",
      "training: 76 batch 213 batch_loss: 0.21857503056526184\n",
      "training: 76 batch 214 batch_loss: 0.22220557928085327\n",
      "training: 76 batch 215 batch_loss: 0.21464014053344727\n",
      "training: 76 batch 216 batch_loss: 0.2178158462047577\n",
      "training: 76 batch 217 batch_loss: 0.2217777669429779\n",
      "training: 76 batch 218 batch_loss: 0.21704328060150146\n",
      "training: 76 batch 219 batch_loss: 0.21369102597236633\n",
      "training: 76 batch 220 batch_loss: 0.21809208393096924\n",
      "training: 76 batch 221 batch_loss: 0.21547064185142517\n",
      "training: 76 batch 222 batch_loss: 0.21780166029930115\n",
      "training: 76 batch 223 batch_loss: 0.22145193815231323\n",
      "training: 76 batch 224 batch_loss: 0.211432546377182\n",
      "training: 76 batch 225 batch_loss: 0.2162298560142517\n",
      "training: 76 batch 226 batch_loss: 0.21507224440574646\n",
      "training: 76 batch 227 batch_loss: 0.21594682335853577\n",
      "training: 76 batch 228 batch_loss: 0.21887603402137756\n",
      "training: 76 batch 229 batch_loss: 0.21968421339988708\n",
      "training: 76 batch 230 batch_loss: 0.216619610786438\n",
      "training: 76 batch 231 batch_loss: 0.22248348593711853\n",
      "training: 76 batch 232 batch_loss: 0.21429696679115295\n",
      "training: 76 batch 233 batch_loss: 0.21853813529014587\n",
      "training: 76 batch 234 batch_loss: 0.2210829257965088\n",
      "training: 76 batch 235 batch_loss: 0.21778792142868042\n",
      "training: 76 batch 236 batch_loss: 0.21985173225402832\n",
      "training: 76 batch 237 batch_loss: 0.2178964614868164\n",
      "training: 76 batch 238 batch_loss: 0.21902647614479065\n",
      "training: 76 batch 239 batch_loss: 0.219448983669281\n",
      "training: 76 batch 240 batch_loss: 0.21994319558143616\n",
      "training: 76 batch 241 batch_loss: 0.21590188145637512\n",
      "training: 76 batch 242 batch_loss: 0.21720260381698608\n",
      "training: 76 batch 243 batch_loss: 0.21988123655319214\n",
      "training: 76 batch 244 batch_loss: 0.21875768899917603\n",
      "training: 76 batch 245 batch_loss: 0.2165004312992096\n",
      "training: 76 batch 246 batch_loss: 0.21604567766189575\n",
      "training: 76 batch 247 batch_loss: 0.2167912721633911\n",
      "training: 76 batch 248 batch_loss: 0.21526062488555908\n",
      "training: 76 batch 249 batch_loss: 0.21617883443832397\n",
      "training: 76 batch 250 batch_loss: 0.21475479006767273\n",
      "training: 76 batch 251 batch_loss: 0.21872687339782715\n",
      "training: 76 batch 252 batch_loss: 0.21777674555778503\n",
      "training: 76 batch 253 batch_loss: 0.21747276186943054\n",
      "training: 76 batch 254 batch_loss: 0.2196613848209381\n",
      "training: 76 batch 255 batch_loss: 0.22116410732269287\n",
      "training: 76 batch 256 batch_loss: 0.21982932090759277\n",
      "training: 76 batch 257 batch_loss: 0.21357247233390808\n",
      "training: 76 batch 258 batch_loss: 0.22112655639648438\n",
      "training: 76 batch 259 batch_loss: 0.21994319558143616\n",
      "training: 76 batch 260 batch_loss: 0.22117316722869873\n",
      "training: 76 batch 261 batch_loss: 0.21545368432998657\n",
      "training: 76 batch 262 batch_loss: 0.21450123190879822\n",
      "training: 76 batch 263 batch_loss: 0.21617823839187622\n",
      "training: 76 batch 264 batch_loss: 0.21998906135559082\n",
      "training: 76 batch 265 batch_loss: 0.21618184447288513\n",
      "training: 76 batch 266 batch_loss: 0.2205996811389923\n",
      "training: 76 batch 267 batch_loss: 0.22321146726608276\n",
      "training: 76 batch 268 batch_loss: 0.21707755327224731\n",
      "training: 76 batch 269 batch_loss: 0.21432572603225708\n",
      "training: 76 batch 270 batch_loss: 0.2170153260231018\n",
      "training: 76 batch 271 batch_loss: 0.2172951102256775\n",
      "training: 76 batch 272 batch_loss: 0.21385368704795837\n",
      "training: 76 batch 273 batch_loss: 0.21558153629302979\n",
      "training: 76 batch 274 batch_loss: 0.2215956747531891\n",
      "training: 76 batch 275 batch_loss: 0.22133880853652954\n",
      "training: 76 batch 276 batch_loss: 0.2190825641155243\n",
      "training: 76 batch 277 batch_loss: 0.2196843922138214\n",
      "training: 76 batch 278 batch_loss: 0.22063300013542175\n",
      "training: 76 batch 279 batch_loss: 0.2131398618221283\n",
      "training: 76 batch 280 batch_loss: 0.21862933039665222\n",
      "training: 76 batch 281 batch_loss: 0.21680670976638794\n",
      "training: 76 batch 282 batch_loss: 0.22157242894172668\n",
      "training: 76 batch 283 batch_loss: 0.21635377407073975\n",
      "training: 76 batch 284 batch_loss: 0.2183907926082611\n",
      "training: 76 batch 285 batch_loss: 0.2152222990989685\n",
      "training: 76 batch 286 batch_loss: 0.21947506070137024\n",
      "training: 76 batch 287 batch_loss: 0.2182726263999939\n",
      "training: 76 batch 288 batch_loss: 0.21566227078437805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 76 batch 289 batch_loss: 0.21591082215309143\n",
      "training: 76 batch 290 batch_loss: 0.21371138095855713\n",
      "training: 76 batch 291 batch_loss: 0.22012826800346375\n",
      "training: 76 batch 292 batch_loss: 0.21696871519088745\n",
      "training: 76 batch 293 batch_loss: 0.2202797532081604\n",
      "training: 76 batch 294 batch_loss: 0.21613532304763794\n",
      "training: 76 batch 295 batch_loss: 0.21463540196418762\n",
      "training: 76 batch 296 batch_loss: 0.2152455449104309\n",
      "training: 76 batch 297 batch_loss: 0.21428483724594116\n",
      "training: 76 batch 298 batch_loss: 0.21807408332824707\n",
      "training: 76 batch 299 batch_loss: 0.21599751710891724\n",
      "training: 76 batch 300 batch_loss: 0.22071728110313416\n",
      "training: 76 batch 301 batch_loss: 0.21657618880271912\n",
      "training: 76 batch 302 batch_loss: 0.22075623273849487\n",
      "training: 76 batch 303 batch_loss: 0.21820276975631714\n",
      "training: 76 batch 304 batch_loss: 0.22238940000534058\n",
      "training: 76 batch 305 batch_loss: 0.22116026282310486\n",
      "training: 76 batch 306 batch_loss: 0.22092700004577637\n",
      "training: 76 batch 307 batch_loss: 0.2197033166885376\n",
      "training: 76 batch 308 batch_loss: 0.21862339973449707\n",
      "training: 76 batch 309 batch_loss: 0.21767786145210266\n",
      "training: 76 batch 310 batch_loss: 0.2174333930015564\n",
      "training: 76 batch 311 batch_loss: 0.21948233246803284\n",
      "training: 76 batch 312 batch_loss: 0.21429160237312317\n",
      "training: 76 batch 313 batch_loss: 0.21899724006652832\n",
      "training: 76 batch 314 batch_loss: 0.21918082237243652\n",
      "training: 76 batch 315 batch_loss: 0.2172325849533081\n",
      "training: 76 batch 316 batch_loss: 0.22138404846191406\n",
      "training: 76 batch 317 batch_loss: 0.21591144800186157\n",
      "training: 76 batch 318 batch_loss: 0.21229854226112366\n",
      "training: 76 batch 319 batch_loss: 0.21905186772346497\n",
      "training: 76 batch 320 batch_loss: 0.21175062656402588\n",
      "training: 76 batch 321 batch_loss: 0.21720948815345764\n",
      "training: 76 batch 322 batch_loss: 0.21280154585838318\n",
      "training: 76 batch 323 batch_loss: 0.2179090678691864\n",
      "training: 76 batch 324 batch_loss: 0.22352302074432373\n",
      "training: 76 batch 325 batch_loss: 0.22182106971740723\n",
      "training: 76 batch 326 batch_loss: 0.2191758155822754\n",
      "training: 76 batch 327 batch_loss: 0.21792113780975342\n",
      "training: 76 batch 328 batch_loss: 0.2176663875579834\n",
      "training: 76 batch 329 batch_loss: 0.2171858549118042\n",
      "training: 76 batch 330 batch_loss: 0.21836280822753906\n",
      "training: 76 batch 331 batch_loss: 0.21912631392478943\n",
      "training: 76 batch 332 batch_loss: 0.21639060974121094\n",
      "training: 76 batch 333 batch_loss: 0.21992799639701843\n",
      "training: 76 batch 334 batch_loss: 0.21921271085739136\n",
      "training: 76 batch 335 batch_loss: 0.21921059489250183\n",
      "training: 76 batch 336 batch_loss: 0.21533086895942688\n",
      "training: 76 batch 337 batch_loss: 0.22032585740089417\n",
      "training: 76 batch 338 batch_loss: 0.21620488166809082\n",
      "training: 76 batch 339 batch_loss: 0.22431331872940063\n",
      "training: 76 batch 340 batch_loss: 0.21423205733299255\n",
      "training: 76 batch 341 batch_loss: 0.21929442882537842\n",
      "training: 76 batch 342 batch_loss: 0.21496722102165222\n",
      "training: 76 batch 343 batch_loss: 0.21467232704162598\n",
      "training: 76 batch 344 batch_loss: 0.21718814969062805\n",
      "training: 76 batch 345 batch_loss: 0.21801382303237915\n",
      "training: 76 batch 346 batch_loss: 0.21311500668525696\n",
      "training: 76 batch 347 batch_loss: 0.21565431356430054\n",
      "training: 76 batch 348 batch_loss: 0.22023090720176697\n",
      "training: 76 batch 349 batch_loss: 0.21694180369377136\n",
      "training: 76 batch 350 batch_loss: 0.220345139503479\n",
      "training: 76 batch 351 batch_loss: 0.21916595101356506\n",
      "training: 76 batch 352 batch_loss: 0.21562126278877258\n",
      "training: 76 batch 353 batch_loss: 0.2211676836013794\n",
      "training: 76 batch 354 batch_loss: 0.2169155478477478\n",
      "training: 76 batch 355 batch_loss: 0.22010645270347595\n",
      "training: 76 batch 356 batch_loss: 0.21599674224853516\n",
      "training: 76 batch 357 batch_loss: 0.22001460194587708\n",
      "training: 76 batch 358 batch_loss: 0.21856221556663513\n",
      "training: 76 batch 359 batch_loss: 0.21549832820892334\n",
      "training: 76 batch 360 batch_loss: 0.21766075491905212\n",
      "training: 76 batch 361 batch_loss: 0.2155573070049286\n",
      "training: 76 batch 362 batch_loss: 0.2205740511417389\n",
      "training: 76 batch 363 batch_loss: 0.2159084975719452\n",
      "training: 76 batch 364 batch_loss: 0.2166961431503296\n",
      "training: 76 batch 365 batch_loss: 0.21484196186065674\n",
      "training: 76 batch 366 batch_loss: 0.21873173117637634\n",
      "training: 76 batch 367 batch_loss: 0.21851250529289246\n",
      "training: 76 batch 368 batch_loss: 0.22065064311027527\n",
      "training: 76 batch 369 batch_loss: 0.21560239791870117\n",
      "training: 76 batch 370 batch_loss: 0.2157800793647766\n",
      "training: 76 batch 371 batch_loss: 0.2176961600780487\n",
      "training: 76 batch 372 batch_loss: 0.2177702784538269\n",
      "training: 76 batch 373 batch_loss: 0.22041568160057068\n",
      "training: 76 batch 374 batch_loss: 0.2179100513458252\n",
      "training: 76 batch 375 batch_loss: 0.22407424449920654\n",
      "training: 76 batch 376 batch_loss: 0.21296125650405884\n",
      "training: 76 batch 377 batch_loss: 0.21614935994148254\n",
      "training: 76 batch 378 batch_loss: 0.2186516523361206\n",
      "training: 76 batch 379 batch_loss: 0.2199331820011139\n",
      "training: 76 batch 380 batch_loss: 0.2224821150302887\n",
      "training: 76 batch 381 batch_loss: 0.21999803185462952\n",
      "training: 76 batch 382 batch_loss: 0.21920806169509888\n",
      "training: 76 batch 383 batch_loss: 0.2177131474018097\n",
      "training: 76 batch 384 batch_loss: 0.21942538022994995\n",
      "training: 76 batch 385 batch_loss: 0.21762770414352417\n",
      "training: 76 batch 386 batch_loss: 0.2157476246356964\n",
      "training: 76 batch 387 batch_loss: 0.22223231196403503\n",
      "training: 76 batch 388 batch_loss: 0.21628540754318237\n",
      "training: 76 batch 389 batch_loss: 0.2151786983013153\n",
      "training: 76 batch 390 batch_loss: 0.21739506721496582\n",
      "training: 76 batch 391 batch_loss: 0.22077885270118713\n",
      "training: 76 batch 392 batch_loss: 0.22058042883872986\n",
      "training: 76 batch 393 batch_loss: 0.21769145131111145\n",
      "training: 76 batch 394 batch_loss: 0.2245451807975769\n",
      "training: 76 batch 395 batch_loss: 0.22053837776184082\n",
      "training: 76 batch 396 batch_loss: 0.21791473031044006\n",
      "training: 76 batch 397 batch_loss: 0.2211982011795044\n",
      "training: 76 batch 398 batch_loss: 0.22139978408813477\n",
      "training: 76 batch 399 batch_loss: 0.22017252445220947\n",
      "training: 76 batch 400 batch_loss: 0.21691972017288208\n",
      "training: 76 batch 401 batch_loss: 0.21966740489006042\n",
      "training: 76 batch 402 batch_loss: 0.2175973653793335\n",
      "training: 76 batch 403 batch_loss: 0.22324979305267334\n",
      "training: 76 batch 404 batch_loss: 0.2164897620677948\n",
      "training: 76 batch 405 batch_loss: 0.2168406844139099\n",
      "training: 76 batch 406 batch_loss: 0.21782046556472778\n",
      "training: 76 batch 407 batch_loss: 0.2184988558292389\n",
      "training: 76 batch 408 batch_loss: 0.21549981832504272\n",
      "training: 76 batch 409 batch_loss: 0.21484392881393433\n",
      "training: 76 batch 410 batch_loss: 0.21771550178527832\n",
      "training: 76 batch 411 batch_loss: 0.21821698546409607\n",
      "training: 76 batch 412 batch_loss: 0.21982473134994507\n",
      "training: 76 batch 413 batch_loss: 0.218784362077713\n",
      "training: 76 batch 414 batch_loss: 0.2164599597454071\n",
      "training: 76 batch 415 batch_loss: 0.2219967544078827\n",
      "training: 76 batch 416 batch_loss: 0.21808403730392456\n",
      "training: 76 batch 417 batch_loss: 0.2156960368156433\n",
      "training: 76 batch 418 batch_loss: 0.2193201184272766\n",
      "training: 76 batch 419 batch_loss: 0.21852105855941772\n",
      "training: 76 batch 420 batch_loss: 0.2192370891571045\n",
      "training: 76 batch 421 batch_loss: 0.2213897705078125\n",
      "training: 76 batch 422 batch_loss: 0.22079694271087646\n",
      "training: 76 batch 423 batch_loss: 0.21993589401245117\n",
      "training: 76 batch 424 batch_loss: 0.21437901258468628\n",
      "training: 76 batch 425 batch_loss: 0.21634218096733093\n",
      "training: 76 batch 426 batch_loss: 0.21744439005851746\n",
      "training: 76 batch 427 batch_loss: 0.22005096077919006\n",
      "training: 76 batch 428 batch_loss: 0.21683862805366516\n",
      "training: 76 batch 429 batch_loss: 0.22166582942008972\n",
      "training: 76 batch 430 batch_loss: 0.2186891734600067\n",
      "training: 76 batch 431 batch_loss: 0.21998131275177002\n",
      "training: 76 batch 432 batch_loss: 0.22227993607521057\n",
      "training: 76 batch 433 batch_loss: 0.22120967507362366\n",
      "training: 76 batch 434 batch_loss: 0.21953675150871277\n",
      "training: 76 batch 435 batch_loss: 0.219823956489563\n",
      "training: 76 batch 436 batch_loss: 0.21873432397842407\n",
      "training: 76 batch 437 batch_loss: 0.21962875127792358\n",
      "training: 76 batch 438 batch_loss: 0.2157362699508667\n",
      "training: 76 batch 439 batch_loss: 0.2258954644203186\n",
      "training: 76 batch 440 batch_loss: 0.21581411361694336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 76 batch 441 batch_loss: 0.21476474404335022\n",
      "training: 76 batch 442 batch_loss: 0.22108951210975647\n",
      "training: 76 batch 443 batch_loss: 0.22105860710144043\n",
      "training: 76 batch 444 batch_loss: 0.2187897264957428\n",
      "training: 76 batch 445 batch_loss: 0.22137662768363953\n",
      "training: 76 batch 446 batch_loss: 0.21853673458099365\n",
      "training: 76 batch 447 batch_loss: 0.21848726272583008\n",
      "training: 76 batch 448 batch_loss: 0.21577772498130798\n",
      "training: 76 batch 449 batch_loss: 0.21510371565818787\n",
      "training: 76 batch 450 batch_loss: 0.2205287516117096\n",
      "training: 76 batch 451 batch_loss: 0.21814939379692078\n",
      "training: 76 batch 452 batch_loss: 0.2167958915233612\n",
      "training: 76 batch 453 batch_loss: 0.21886613965034485\n",
      "training: 76 batch 454 batch_loss: 0.22015339136123657\n",
      "training: 76 batch 455 batch_loss: 0.22219747304916382\n",
      "training: 76 batch 456 batch_loss: 0.22053131461143494\n",
      "training: 76 batch 457 batch_loss: 0.21971142292022705\n",
      "training: 76 batch 458 batch_loss: 0.2202073335647583\n",
      "training: 76 batch 459 batch_loss: 0.21568429470062256\n",
      "training: 76 batch 460 batch_loss: 0.2219054102897644\n",
      "training: 76 batch 461 batch_loss: 0.2162366509437561\n",
      "training: 76 batch 462 batch_loss: 0.22130143642425537\n",
      "training: 76 batch 463 batch_loss: 0.21807274222373962\n",
      "training: 76 batch 464 batch_loss: 0.21548229455947876\n",
      "training: 76 batch 465 batch_loss: 0.22047075629234314\n",
      "training: 76 batch 466 batch_loss: 0.21986466646194458\n",
      "training: 76 batch 467 batch_loss: 0.22025945782661438\n",
      "training: 76 batch 468 batch_loss: 0.2216712236404419\n",
      "training: 76 batch 469 batch_loss: 0.2202092409133911\n",
      "training: 76 batch 470 batch_loss: 0.21772688627243042\n",
      "training: 76 batch 471 batch_loss: 0.2180626094341278\n",
      "training: 76 batch 472 batch_loss: 0.21300283074378967\n",
      "training: 76 batch 473 batch_loss: 0.21922928094863892\n",
      "training: 76 batch 474 batch_loss: 0.21381929516792297\n",
      "training: 76 batch 475 batch_loss: 0.21873879432678223\n",
      "training: 76 batch 476 batch_loss: 0.21579232811927795\n",
      "training: 76 batch 477 batch_loss: 0.21493908762931824\n",
      "training: 76 batch 478 batch_loss: 0.21632003784179688\n",
      "training: 76 batch 479 batch_loss: 0.21679604053497314\n",
      "training: 76 batch 480 batch_loss: 0.21521621942520142\n",
      "training: 76 batch 481 batch_loss: 0.21833345293998718\n",
      "training: 76 batch 482 batch_loss: 0.21778565645217896\n",
      "training: 76 batch 483 batch_loss: 0.21709033846855164\n",
      "training: 76 batch 484 batch_loss: 0.21716520190238953\n",
      "training: 76 batch 485 batch_loss: 0.2219131588935852\n",
      "training: 76 batch 486 batch_loss: 0.21950486302375793\n",
      "training: 76 batch 487 batch_loss: 0.2177133560180664\n",
      "training: 76 batch 488 batch_loss: 0.2192767858505249\n",
      "training: 76 batch 489 batch_loss: 0.22095754742622375\n",
      "training: 76 batch 490 batch_loss: 0.21898391842842102\n",
      "training: 76 batch 491 batch_loss: 0.21456661820411682\n",
      "training: 76 batch 492 batch_loss: 0.21948030591011047\n",
      "training: 76 batch 493 batch_loss: 0.21827369928359985\n",
      "training: 76 batch 494 batch_loss: 0.21262794733047485\n",
      "training: 76 batch 495 batch_loss: 0.21565228700637817\n",
      "training: 76 batch 496 batch_loss: 0.2163582146167755\n",
      "training: 76 batch 497 batch_loss: 0.21609526872634888\n",
      "training: 76 batch 498 batch_loss: 0.21431034803390503\n",
      "training: 76 batch 499 batch_loss: 0.21842744946479797\n",
      "training: 76 batch 500 batch_loss: 0.22127848863601685\n",
      "training: 76 batch 501 batch_loss: 0.21880319714546204\n",
      "training: 76 batch 502 batch_loss: 0.2202858328819275\n",
      "training: 76 batch 503 batch_loss: 0.22047719359397888\n",
      "training: 76 batch 504 batch_loss: 0.22028237581253052\n",
      "training: 76 batch 505 batch_loss: 0.2177790403366089\n",
      "training: 76 batch 506 batch_loss: 0.21868029236793518\n",
      "training: 76 batch 507 batch_loss: 0.22123414278030396\n",
      "training: 76 batch 508 batch_loss: 0.21527627110481262\n",
      "training: 76 batch 509 batch_loss: 0.218794584274292\n",
      "training: 76 batch 510 batch_loss: 0.22203081846237183\n",
      "training: 76 batch 511 batch_loss: 0.21884119510650635\n",
      "training: 76 batch 512 batch_loss: 0.21824881434440613\n",
      "training: 76 batch 513 batch_loss: 0.21753934025764465\n",
      "training: 76 batch 514 batch_loss: 0.217504620552063\n",
      "training: 76 batch 515 batch_loss: 0.21778768301010132\n",
      "training: 76 batch 516 batch_loss: 0.21310877799987793\n",
      "training: 76 batch 517 batch_loss: 0.21810847520828247\n",
      "training: 76 batch 518 batch_loss: 0.21541082859039307\n",
      "training: 76 batch 519 batch_loss: 0.21980291604995728\n",
      "training: 76 batch 520 batch_loss: 0.22312477231025696\n",
      "training: 76 batch 521 batch_loss: 0.2205193042755127\n",
      "training: 76 batch 522 batch_loss: 0.21354159712791443\n",
      "training: 76 batch 523 batch_loss: 0.21810096502304077\n",
      "training: 76 batch 524 batch_loss: 0.2191857099533081\n",
      "training: 76 batch 525 batch_loss: 0.21805304288864136\n",
      "training: 76 batch 526 batch_loss: 0.21870002150535583\n",
      "training: 76 batch 527 batch_loss: 0.22279807925224304\n",
      "training: 76 batch 528 batch_loss: 0.2142139971256256\n",
      "training: 76 batch 529 batch_loss: 0.21756917238235474\n",
      "training: 76 batch 530 batch_loss: 0.21634528040885925\n",
      "training: 76 batch 531 batch_loss: 0.2188323736190796\n",
      "training: 76 batch 532 batch_loss: 0.21691080927848816\n",
      "training: 76 batch 533 batch_loss: 0.22079795598983765\n",
      "training: 76 batch 534 batch_loss: 0.21893727779388428\n",
      "training: 76 batch 535 batch_loss: 0.21992281079292297\n",
      "training: 76 batch 536 batch_loss: 0.21864980459213257\n",
      "training: 76 batch 537 batch_loss: 0.2232389748096466\n",
      "training: 76 batch 538 batch_loss: 0.22102954983711243\n",
      "training: 76 batch 539 batch_loss: 0.2231311798095703\n",
      "training: 76 batch 540 batch_loss: 0.21848437190055847\n",
      "training: 76 batch 541 batch_loss: 0.21634674072265625\n",
      "training: 76 batch 542 batch_loss: 0.21996641159057617\n",
      "training: 76 batch 543 batch_loss: 0.21952202916145325\n",
      "training: 76 batch 544 batch_loss: 0.21900016069412231\n",
      "training: 76 batch 545 batch_loss: 0.21750476956367493\n",
      "training: 76 batch 546 batch_loss: 0.21709129214286804\n",
      "training: 76 batch 547 batch_loss: 0.21829447150230408\n",
      "training: 76 batch 548 batch_loss: 0.2175646424293518\n",
      "training: 76 batch 549 batch_loss: 0.21493679285049438\n",
      "training: 76 batch 550 batch_loss: 0.211282879114151\n",
      "training: 76 batch 551 batch_loss: 0.21847498416900635\n",
      "training: 76 batch 552 batch_loss: 0.21784362196922302\n",
      "training: 76 batch 553 batch_loss: 0.2225176990032196\n",
      "training: 76 batch 554 batch_loss: 0.21873870491981506\n",
      "training: 76 batch 555 batch_loss: 0.2199089527130127\n",
      "training: 76 batch 556 batch_loss: 0.21905377507209778\n",
      "training: 76 batch 557 batch_loss: 0.22065570950508118\n",
      "training: 76 batch 558 batch_loss: 0.21874168515205383\n",
      "training: 76 batch 559 batch_loss: 0.21998301148414612\n",
      "training: 76 batch 560 batch_loss: 0.22107213735580444\n",
      "training: 76 batch 561 batch_loss: 0.21775317192077637\n",
      "training: 76 batch 562 batch_loss: 0.21943187713623047\n",
      "training: 76 batch 563 batch_loss: 0.22022616863250732\n",
      "training: 76 batch 564 batch_loss: 0.21875214576721191\n",
      "training: 76 batch 565 batch_loss: 0.21431708335876465\n",
      "training: 76 batch 566 batch_loss: 0.2176303267478943\n",
      "training: 76 batch 567 batch_loss: 0.22238677740097046\n",
      "training: 76 batch 568 batch_loss: 0.21676534414291382\n",
      "training: 76 batch 569 batch_loss: 0.2164612114429474\n",
      "training: 76 batch 570 batch_loss: 0.2150149643421173\n",
      "training: 76 batch 571 batch_loss: 0.21614840626716614\n",
      "training: 76 batch 572 batch_loss: 0.22184494137763977\n",
      "training: 76 batch 573 batch_loss: 0.2181316316127777\n",
      "training: 76 batch 574 batch_loss: 0.22185653448104858\n",
      "training: 76 batch 575 batch_loss: 0.2197754681110382\n",
      "training: 76 batch 576 batch_loss: 0.21787258982658386\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 76, Hit Ratio:0.0336383593221694 | Precision:0.04963137717487467 | Recall:0.06556606707055572 | NDCG:0.06457137851235445\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 77 batch 0 batch_loss: 0.21450850367546082\n",
      "training: 77 batch 1 batch_loss: 0.21786299347877502\n",
      "training: 77 batch 2 batch_loss: 0.21626710891723633\n",
      "training: 77 batch 3 batch_loss: 0.21908989548683167\n",
      "training: 77 batch 4 batch_loss: 0.21291688084602356\n",
      "training: 77 batch 5 batch_loss: 0.21843457221984863\n",
      "training: 77 batch 6 batch_loss: 0.21724316477775574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 77 batch 7 batch_loss: 0.21771946549415588\n",
      "training: 77 batch 8 batch_loss: 0.22008419036865234\n",
      "training: 77 batch 9 batch_loss: 0.2182409167289734\n",
      "training: 77 batch 10 batch_loss: 0.21887537837028503\n",
      "training: 77 batch 11 batch_loss: 0.2213730812072754\n",
      "training: 77 batch 12 batch_loss: 0.21683108806610107\n",
      "training: 77 batch 13 batch_loss: 0.21769607067108154\n",
      "training: 77 batch 14 batch_loss: 0.22195428609848022\n",
      "training: 77 batch 15 batch_loss: 0.21725574135780334\n",
      "training: 77 batch 16 batch_loss: 0.21694135665893555\n",
      "training: 77 batch 17 batch_loss: 0.21464836597442627\n",
      "training: 77 batch 18 batch_loss: 0.21576637029647827\n",
      "training: 77 batch 19 batch_loss: 0.22027403116226196\n",
      "training: 77 batch 20 batch_loss: 0.2170502245426178\n",
      "training: 77 batch 21 batch_loss: 0.2174082100391388\n",
      "training: 77 batch 22 batch_loss: 0.21930548548698425\n",
      "training: 77 batch 23 batch_loss: 0.21633636951446533\n",
      "training: 77 batch 24 batch_loss: 0.22230106592178345\n",
      "training: 77 batch 25 batch_loss: 0.21657314896583557\n",
      "training: 77 batch 26 batch_loss: 0.2225951850414276\n",
      "training: 77 batch 27 batch_loss: 0.21363580226898193\n",
      "training: 77 batch 28 batch_loss: 0.2174665629863739\n",
      "training: 77 batch 29 batch_loss: 0.2151174545288086\n",
      "training: 77 batch 30 batch_loss: 0.21838858723640442\n",
      "training: 77 batch 31 batch_loss: 0.2155924141407013\n",
      "training: 77 batch 32 batch_loss: 0.22049260139465332\n",
      "training: 77 batch 33 batch_loss: 0.21307170391082764\n",
      "training: 77 batch 34 batch_loss: 0.22047579288482666\n",
      "training: 77 batch 35 batch_loss: 0.21744811534881592\n",
      "training: 77 batch 36 batch_loss: 0.2188408374786377\n",
      "training: 77 batch 37 batch_loss: 0.2134832739830017\n",
      "training: 77 batch 38 batch_loss: 0.21631106734275818\n",
      "training: 77 batch 39 batch_loss: 0.21630921959877014\n",
      "training: 77 batch 40 batch_loss: 0.21827742457389832\n",
      "training: 77 batch 41 batch_loss: 0.21682953834533691\n",
      "training: 77 batch 42 batch_loss: 0.21615570783615112\n",
      "training: 77 batch 43 batch_loss: 0.20893654227256775\n",
      "training: 77 batch 44 batch_loss: 0.22046753764152527\n",
      "training: 77 batch 45 batch_loss: 0.21477210521697998\n",
      "training: 77 batch 46 batch_loss: 0.21658194065093994\n",
      "training: 77 batch 47 batch_loss: 0.21815744042396545\n",
      "training: 77 batch 48 batch_loss: 0.22070437669754028\n",
      "training: 77 batch 49 batch_loss: 0.21560394763946533\n",
      "training: 77 batch 50 batch_loss: 0.21728652715682983\n",
      "training: 77 batch 51 batch_loss: 0.21799048781394958\n",
      "training: 77 batch 52 batch_loss: 0.21997076272964478\n",
      "training: 77 batch 53 batch_loss: 0.21694830060005188\n",
      "training: 77 batch 54 batch_loss: 0.21430036425590515\n",
      "training: 77 batch 55 batch_loss: 0.21386554837226868\n",
      "training: 77 batch 56 batch_loss: 0.221005380153656\n",
      "training: 77 batch 57 batch_loss: 0.21393471956253052\n",
      "training: 77 batch 58 batch_loss: 0.2192901074886322\n",
      "training: 77 batch 59 batch_loss: 0.21755465865135193\n",
      "training: 77 batch 60 batch_loss: 0.21720966696739197\n",
      "training: 77 batch 61 batch_loss: 0.2146841287612915\n",
      "training: 77 batch 62 batch_loss: 0.21462252736091614\n",
      "training: 77 batch 63 batch_loss: 0.2173779308795929\n",
      "training: 77 batch 64 batch_loss: 0.21837511658668518\n",
      "training: 77 batch 65 batch_loss: 0.21831849217414856\n",
      "training: 77 batch 66 batch_loss: 0.21587255597114563\n",
      "training: 77 batch 67 batch_loss: 0.21646413207054138\n",
      "training: 77 batch 68 batch_loss: 0.21910753846168518\n",
      "training: 77 batch 69 batch_loss: 0.2183174192905426\n",
      "training: 77 batch 70 batch_loss: 0.21939867734909058\n",
      "training: 77 batch 71 batch_loss: 0.2237362265586853\n",
      "training: 77 batch 72 batch_loss: 0.2168346345424652\n",
      "training: 77 batch 73 batch_loss: 0.21814608573913574\n",
      "training: 77 batch 74 batch_loss: 0.21497252583503723\n",
      "training: 77 batch 75 batch_loss: 0.2147320806980133\n",
      "training: 77 batch 76 batch_loss: 0.21519696712493896\n",
      "training: 77 batch 77 batch_loss: 0.21638113260269165\n",
      "training: 77 batch 78 batch_loss: 0.22390595078468323\n",
      "training: 77 batch 79 batch_loss: 0.21689313650131226\n",
      "training: 77 batch 80 batch_loss: 0.21803495287895203\n",
      "training: 77 batch 81 batch_loss: 0.21141815185546875\n",
      "training: 77 batch 82 batch_loss: 0.21970444917678833\n",
      "training: 77 batch 83 batch_loss: 0.21865570545196533\n",
      "training: 77 batch 84 batch_loss: 0.2153167724609375\n",
      "training: 77 batch 85 batch_loss: 0.22101080417633057\n",
      "training: 77 batch 86 batch_loss: 0.21589142084121704\n",
      "training: 77 batch 87 batch_loss: 0.21824383735656738\n",
      "training: 77 batch 88 batch_loss: 0.21562886238098145\n",
      "training: 77 batch 89 batch_loss: 0.21703854203224182\n",
      "training: 77 batch 90 batch_loss: 0.21518409252166748\n",
      "training: 77 batch 91 batch_loss: 0.21519264578819275\n",
      "training: 77 batch 92 batch_loss: 0.21650034189224243\n",
      "training: 77 batch 93 batch_loss: 0.21811413764953613\n",
      "training: 77 batch 94 batch_loss: 0.2184470295906067\n",
      "training: 77 batch 95 batch_loss: 0.2174716293811798\n",
      "training: 77 batch 96 batch_loss: 0.21022066473960876\n",
      "training: 77 batch 97 batch_loss: 0.22013187408447266\n",
      "training: 77 batch 98 batch_loss: 0.21918988227844238\n",
      "training: 77 batch 99 batch_loss: 0.2204967737197876\n",
      "training: 77 batch 100 batch_loss: 0.21987351775169373\n",
      "training: 77 batch 101 batch_loss: 0.21929070353507996\n",
      "training: 77 batch 102 batch_loss: 0.22042301297187805\n",
      "training: 77 batch 103 batch_loss: 0.21962404251098633\n",
      "training: 77 batch 104 batch_loss: 0.21753093600273132\n",
      "training: 77 batch 105 batch_loss: 0.22167998552322388\n",
      "training: 77 batch 106 batch_loss: 0.21787601709365845\n",
      "training: 77 batch 107 batch_loss: 0.21649044752120972\n",
      "training: 77 batch 108 batch_loss: 0.21566343307495117\n",
      "training: 77 batch 109 batch_loss: 0.21684721112251282\n",
      "training: 77 batch 110 batch_loss: 0.21650072932243347\n",
      "training: 77 batch 111 batch_loss: 0.21466314792633057\n",
      "training: 77 batch 112 batch_loss: 0.2169545292854309\n",
      "training: 77 batch 113 batch_loss: 0.21690815687179565\n",
      "training: 77 batch 114 batch_loss: 0.21851325035095215\n",
      "training: 77 batch 115 batch_loss: 0.21917223930358887\n",
      "training: 77 batch 116 batch_loss: 0.2169283628463745\n",
      "training: 77 batch 117 batch_loss: 0.21709007024765015\n",
      "training: 77 batch 118 batch_loss: 0.219686359167099\n",
      "training: 77 batch 119 batch_loss: 0.21546655893325806\n",
      "training: 77 batch 120 batch_loss: 0.2154352068901062\n",
      "training: 77 batch 121 batch_loss: 0.21700862050056458\n",
      "training: 77 batch 122 batch_loss: 0.2207534909248352\n",
      "training: 77 batch 123 batch_loss: 0.21823284029960632\n",
      "training: 77 batch 124 batch_loss: 0.21360069513320923\n",
      "training: 77 batch 125 batch_loss: 0.21459144353866577\n",
      "training: 77 batch 126 batch_loss: 0.21537721157073975\n",
      "training: 77 batch 127 batch_loss: 0.22001636028289795\n",
      "training: 77 batch 128 batch_loss: 0.21504154801368713\n",
      "training: 77 batch 129 batch_loss: 0.2182731330394745\n",
      "training: 77 batch 130 batch_loss: 0.21766799688339233\n",
      "training: 77 batch 131 batch_loss: 0.21985077857971191\n",
      "training: 77 batch 132 batch_loss: 0.2157396674156189\n",
      "training: 77 batch 133 batch_loss: 0.22275090217590332\n",
      "training: 77 batch 134 batch_loss: 0.2212616503238678\n",
      "training: 77 batch 135 batch_loss: 0.21738123893737793\n",
      "training: 77 batch 136 batch_loss: 0.21987873315811157\n",
      "training: 77 batch 137 batch_loss: 0.22211521863937378\n",
      "training: 77 batch 138 batch_loss: 0.21594005823135376\n",
      "training: 77 batch 139 batch_loss: 0.21397703886032104\n",
      "training: 77 batch 140 batch_loss: 0.21602651476860046\n",
      "training: 77 batch 141 batch_loss: 0.2187170386314392\n",
      "training: 77 batch 142 batch_loss: 0.21826285123825073\n",
      "training: 77 batch 143 batch_loss: 0.21897923946380615\n",
      "training: 77 batch 144 batch_loss: 0.21579080820083618\n",
      "training: 77 batch 145 batch_loss: 0.2174820899963379\n",
      "training: 77 batch 146 batch_loss: 0.2193514108657837\n",
      "training: 77 batch 147 batch_loss: 0.2166648507118225\n",
      "training: 77 batch 148 batch_loss: 0.2172011137008667\n",
      "training: 77 batch 149 batch_loss: 0.2180478572845459\n",
      "training: 77 batch 150 batch_loss: 0.22082683444023132\n",
      "training: 77 batch 151 batch_loss: 0.217200368642807\n",
      "training: 77 batch 152 batch_loss: 0.21858954429626465\n",
      "training: 77 batch 153 batch_loss: 0.21376562118530273\n",
      "training: 77 batch 154 batch_loss: 0.2201855182647705\n",
      "training: 77 batch 155 batch_loss: 0.2215222418308258\n",
      "training: 77 batch 156 batch_loss: 0.21905860304832458\n",
      "training: 77 batch 157 batch_loss: 0.21461936831474304\n",
      "training: 77 batch 158 batch_loss: 0.21812671422958374\n",
      "training: 77 batch 159 batch_loss: 0.21835815906524658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 77 batch 160 batch_loss: 0.21587222814559937\n",
      "training: 77 batch 161 batch_loss: 0.2168412208557129\n",
      "training: 77 batch 162 batch_loss: 0.21996253728866577\n",
      "training: 77 batch 163 batch_loss: 0.21379607915878296\n",
      "training: 77 batch 164 batch_loss: 0.2197115421295166\n",
      "training: 77 batch 165 batch_loss: 0.2188257873058319\n",
      "training: 77 batch 166 batch_loss: 0.21425673365592957\n",
      "training: 77 batch 167 batch_loss: 0.22036734223365784\n",
      "training: 77 batch 168 batch_loss: 0.21836984157562256\n",
      "training: 77 batch 169 batch_loss: 0.2189091444015503\n",
      "training: 77 batch 170 batch_loss: 0.21944555640220642\n",
      "training: 77 batch 171 batch_loss: 0.21824312210083008\n",
      "training: 77 batch 172 batch_loss: 0.2181498110294342\n",
      "training: 77 batch 173 batch_loss: 0.2148456573486328\n",
      "training: 77 batch 174 batch_loss: 0.21239206194877625\n",
      "training: 77 batch 175 batch_loss: 0.21811506152153015\n",
      "training: 77 batch 176 batch_loss: 0.21637314558029175\n",
      "training: 77 batch 177 batch_loss: 0.22289007902145386\n",
      "training: 77 batch 178 batch_loss: 0.21658778190612793\n",
      "training: 77 batch 179 batch_loss: 0.22132471203804016\n",
      "training: 77 batch 180 batch_loss: 0.21596893668174744\n",
      "training: 77 batch 181 batch_loss: 0.21675324440002441\n",
      "training: 77 batch 182 batch_loss: 0.2242867350578308\n",
      "training: 77 batch 183 batch_loss: 0.21473848819732666\n",
      "training: 77 batch 184 batch_loss: 0.21534398198127747\n",
      "training: 77 batch 185 batch_loss: 0.2150324285030365\n",
      "training: 77 batch 186 batch_loss: 0.21911951899528503\n",
      "training: 77 batch 187 batch_loss: 0.2197255790233612\n",
      "training: 77 batch 188 batch_loss: 0.22232282161712646\n",
      "training: 77 batch 189 batch_loss: 0.21614891290664673\n",
      "training: 77 batch 190 batch_loss: 0.21975302696228027\n",
      "training: 77 batch 191 batch_loss: 0.2173963189125061\n",
      "training: 77 batch 192 batch_loss: 0.217013418674469\n",
      "training: 77 batch 193 batch_loss: 0.21618583798408508\n",
      "training: 77 batch 194 batch_loss: 0.2189505696296692\n",
      "training: 77 batch 195 batch_loss: 0.2211829423904419\n",
      "training: 77 batch 196 batch_loss: 0.21951845288276672\n",
      "training: 77 batch 197 batch_loss: 0.21668455004692078\n",
      "training: 77 batch 198 batch_loss: 0.21663343906402588\n",
      "training: 77 batch 199 batch_loss: 0.21960154175758362\n",
      "training: 77 batch 200 batch_loss: 0.21898439526557922\n",
      "training: 77 batch 201 batch_loss: 0.21928167343139648\n",
      "training: 77 batch 202 batch_loss: 0.22103306651115417\n",
      "training: 77 batch 203 batch_loss: 0.2187943458557129\n",
      "training: 77 batch 204 batch_loss: 0.21702849864959717\n",
      "training: 77 batch 205 batch_loss: 0.21827149391174316\n",
      "training: 77 batch 206 batch_loss: 0.2209816575050354\n",
      "training: 77 batch 207 batch_loss: 0.22055071592330933\n",
      "training: 77 batch 208 batch_loss: 0.2179579734802246\n",
      "training: 77 batch 209 batch_loss: 0.218729168176651\n",
      "training: 77 batch 210 batch_loss: 0.2175440490245819\n",
      "training: 77 batch 211 batch_loss: 0.21785512566566467\n",
      "training: 77 batch 212 batch_loss: 0.21557191014289856\n",
      "training: 77 batch 213 batch_loss: 0.21695870161056519\n",
      "training: 77 batch 214 batch_loss: 0.21822410821914673\n",
      "training: 77 batch 215 batch_loss: 0.21835389733314514\n",
      "training: 77 batch 216 batch_loss: 0.21624478697776794\n",
      "training: 77 batch 217 batch_loss: 0.21886521577835083\n",
      "training: 77 batch 218 batch_loss: 0.22212320566177368\n",
      "training: 77 batch 219 batch_loss: 0.2211618423461914\n",
      "training: 77 batch 220 batch_loss: 0.22181487083435059\n",
      "training: 77 batch 221 batch_loss: 0.2206888496875763\n",
      "training: 77 batch 222 batch_loss: 0.21796467900276184\n",
      "training: 77 batch 223 batch_loss: 0.2154959738254547\n",
      "training: 77 batch 224 batch_loss: 0.21811610460281372\n",
      "training: 77 batch 225 batch_loss: 0.2188577950000763\n",
      "training: 77 batch 226 batch_loss: 0.21858325600624084\n",
      "training: 77 batch 227 batch_loss: 0.21619001030921936\n",
      "training: 77 batch 228 batch_loss: 0.22053804993629456\n",
      "training: 77 batch 229 batch_loss: 0.21724820137023926\n",
      "training: 77 batch 230 batch_loss: 0.22275084257125854\n",
      "training: 77 batch 231 batch_loss: 0.2215196192264557\n",
      "training: 77 batch 232 batch_loss: 0.21913579106330872\n",
      "training: 77 batch 233 batch_loss: 0.2168104350566864\n",
      "training: 77 batch 234 batch_loss: 0.21584156155586243\n",
      "training: 77 batch 235 batch_loss: 0.21639999747276306\n",
      "training: 77 batch 236 batch_loss: 0.21672192215919495\n",
      "training: 77 batch 237 batch_loss: 0.21535849571228027\n",
      "training: 77 batch 238 batch_loss: 0.21689695119857788\n",
      "training: 77 batch 239 batch_loss: 0.2194560170173645\n",
      "training: 77 batch 240 batch_loss: 0.21173739433288574\n",
      "training: 77 batch 241 batch_loss: 0.22123149037361145\n",
      "training: 77 batch 242 batch_loss: 0.21950048208236694\n",
      "training: 77 batch 243 batch_loss: 0.22462686896324158\n",
      "training: 77 batch 244 batch_loss: 0.215908944606781\n",
      "training: 77 batch 245 batch_loss: 0.2177182137966156\n",
      "training: 77 batch 246 batch_loss: 0.21907690167427063\n",
      "training: 77 batch 247 batch_loss: 0.21598482131958008\n",
      "training: 77 batch 248 batch_loss: 0.21742039918899536\n",
      "training: 77 batch 249 batch_loss: 0.2196619212627411\n",
      "training: 77 batch 250 batch_loss: 0.2155379056930542\n",
      "training: 77 batch 251 batch_loss: 0.2185051441192627\n",
      "training: 77 batch 252 batch_loss: 0.21888166666030884\n",
      "training: 77 batch 253 batch_loss: 0.21997737884521484\n",
      "training: 77 batch 254 batch_loss: 0.21612021327018738\n",
      "training: 77 batch 255 batch_loss: 0.22369763255119324\n",
      "training: 77 batch 256 batch_loss: 0.21627578139305115\n",
      "training: 77 batch 257 batch_loss: 0.2207142412662506\n",
      "training: 77 batch 258 batch_loss: 0.216802716255188\n",
      "training: 77 batch 259 batch_loss: 0.22268113493919373\n",
      "training: 77 batch 260 batch_loss: 0.21266430616378784\n",
      "training: 77 batch 261 batch_loss: 0.2214256227016449\n",
      "training: 77 batch 262 batch_loss: 0.21685737371444702\n",
      "training: 77 batch 263 batch_loss: 0.21892282366752625\n",
      "training: 77 batch 264 batch_loss: 0.21823909878730774\n",
      "training: 77 batch 265 batch_loss: 0.2194283902645111\n",
      "training: 77 batch 266 batch_loss: 0.21717846393585205\n",
      "training: 77 batch 267 batch_loss: 0.21705833077430725\n",
      "training: 77 batch 268 batch_loss: 0.2199639081954956\n",
      "training: 77 batch 269 batch_loss: 0.22079521417617798\n",
      "training: 77 batch 270 batch_loss: 0.21966728568077087\n",
      "training: 77 batch 271 batch_loss: 0.22072488069534302\n",
      "training: 77 batch 272 batch_loss: 0.22115430235862732\n",
      "training: 77 batch 273 batch_loss: 0.21960294246673584\n",
      "training: 77 batch 274 batch_loss: 0.21763142943382263\n",
      "training: 77 batch 275 batch_loss: 0.21643409132957458\n",
      "training: 77 batch 276 batch_loss: 0.22037449479103088\n",
      "training: 77 batch 277 batch_loss: 0.21987688541412354\n",
      "training: 77 batch 278 batch_loss: 0.21853220462799072\n",
      "training: 77 batch 279 batch_loss: 0.22214791178703308\n",
      "training: 77 batch 280 batch_loss: 0.2179529368877411\n",
      "training: 77 batch 281 batch_loss: 0.21995556354522705\n",
      "training: 77 batch 282 batch_loss: 0.2216283679008484\n",
      "training: 77 batch 283 batch_loss: 0.2170119285583496\n",
      "training: 77 batch 284 batch_loss: 0.21998542547225952\n",
      "training: 77 batch 285 batch_loss: 0.21709385514259338\n",
      "training: 77 batch 286 batch_loss: 0.21393007040023804\n",
      "training: 77 batch 287 batch_loss: 0.21906855702400208\n",
      "training: 77 batch 288 batch_loss: 0.22108545899391174\n",
      "training: 77 batch 289 batch_loss: 0.2146419882774353\n",
      "training: 77 batch 290 batch_loss: 0.21666720509529114\n",
      "training: 77 batch 291 batch_loss: 0.2184562087059021\n",
      "training: 77 batch 292 batch_loss: 0.22066423296928406\n",
      "training: 77 batch 293 batch_loss: 0.21945339441299438\n",
      "training: 77 batch 294 batch_loss: 0.220997154712677\n",
      "training: 77 batch 295 batch_loss: 0.21607381105422974\n",
      "training: 77 batch 296 batch_loss: 0.2215171754360199\n",
      "training: 77 batch 297 batch_loss: 0.21234577894210815\n",
      "training: 77 batch 298 batch_loss: 0.21848928928375244\n",
      "training: 77 batch 299 batch_loss: 0.21180623769760132\n",
      "training: 77 batch 300 batch_loss: 0.21927082538604736\n",
      "training: 77 batch 301 batch_loss: 0.218290776014328\n",
      "training: 77 batch 302 batch_loss: 0.21614670753479004\n",
      "training: 77 batch 303 batch_loss: 0.21841081976890564\n",
      "training: 77 batch 304 batch_loss: 0.21722188591957092\n",
      "training: 77 batch 305 batch_loss: 0.21982049942016602\n",
      "training: 77 batch 306 batch_loss: 0.21341681480407715\n",
      "training: 77 batch 307 batch_loss: 0.2117646336555481\n",
      "training: 77 batch 308 batch_loss: 0.22308778762817383\n",
      "training: 77 batch 309 batch_loss: 0.2193458080291748\n",
      "training: 77 batch 310 batch_loss: 0.22309786081314087\n",
      "training: 77 batch 311 batch_loss: 0.21921420097351074\n",
      "training: 77 batch 312 batch_loss: 0.21911701560020447\n",
      "training: 77 batch 313 batch_loss: 0.21769598126411438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 77 batch 314 batch_loss: 0.21968862414360046\n",
      "training: 77 batch 315 batch_loss: 0.22318363189697266\n",
      "training: 77 batch 316 batch_loss: 0.21768295764923096\n",
      "training: 77 batch 317 batch_loss: 0.21765810251235962\n",
      "training: 77 batch 318 batch_loss: 0.2209303379058838\n",
      "training: 77 batch 319 batch_loss: 0.21511897444725037\n",
      "training: 77 batch 320 batch_loss: 0.2196703553199768\n",
      "training: 77 batch 321 batch_loss: 0.2218533754348755\n",
      "training: 77 batch 322 batch_loss: 0.22077441215515137\n",
      "training: 77 batch 323 batch_loss: 0.22080588340759277\n",
      "training: 77 batch 324 batch_loss: 0.21816158294677734\n",
      "training: 77 batch 325 batch_loss: 0.22383692860603333\n",
      "training: 77 batch 326 batch_loss: 0.2176039218902588\n",
      "training: 77 batch 327 batch_loss: 0.21866530179977417\n",
      "training: 77 batch 328 batch_loss: 0.21346992254257202\n",
      "training: 77 batch 329 batch_loss: 0.21832966804504395\n",
      "training: 77 batch 330 batch_loss: 0.21828165650367737\n",
      "training: 77 batch 331 batch_loss: 0.21639758348464966\n",
      "training: 77 batch 332 batch_loss: 0.21849384903907776\n",
      "training: 77 batch 333 batch_loss: 0.2158031463623047\n",
      "training: 77 batch 334 batch_loss: 0.21841531991958618\n",
      "training: 77 batch 335 batch_loss: 0.2234611213207245\n",
      "training: 77 batch 336 batch_loss: 0.21939429640769958\n",
      "training: 77 batch 337 batch_loss: 0.22038772702217102\n",
      "training: 77 batch 338 batch_loss: 0.2117687463760376\n",
      "training: 77 batch 339 batch_loss: 0.22015991806983948\n",
      "training: 77 batch 340 batch_loss: 0.21912431716918945\n",
      "training: 77 batch 341 batch_loss: 0.21863949298858643\n",
      "training: 77 batch 342 batch_loss: 0.222770094871521\n",
      "training: 77 batch 343 batch_loss: 0.21679583191871643\n",
      "training: 77 batch 344 batch_loss: 0.21880707144737244\n",
      "training: 77 batch 345 batch_loss: 0.217288076877594\n",
      "training: 77 batch 346 batch_loss: 0.21751528978347778\n",
      "training: 77 batch 347 batch_loss: 0.21960598230361938\n",
      "training: 77 batch 348 batch_loss: 0.22170037031173706\n",
      "training: 77 batch 349 batch_loss: 0.22534412145614624\n",
      "training: 77 batch 350 batch_loss: 0.21909186244010925\n",
      "training: 77 batch 351 batch_loss: 0.21416118741035461\n",
      "training: 77 batch 352 batch_loss: 0.21847712993621826\n",
      "training: 77 batch 353 batch_loss: 0.22050917148590088\n",
      "training: 77 batch 354 batch_loss: 0.22285205125808716\n",
      "training: 77 batch 355 batch_loss: 0.21800822019577026\n",
      "training: 77 batch 356 batch_loss: 0.2204907238483429\n",
      "training: 77 batch 357 batch_loss: 0.21498480439186096\n",
      "training: 77 batch 358 batch_loss: 0.21493268013000488\n",
      "training: 77 batch 359 batch_loss: 0.21675646305084229\n",
      "training: 77 batch 360 batch_loss: 0.21524512767791748\n",
      "training: 77 batch 361 batch_loss: 0.22107738256454468\n",
      "training: 77 batch 362 batch_loss: 0.220927894115448\n",
      "training: 77 batch 363 batch_loss: 0.218002587556839\n",
      "training: 77 batch 364 batch_loss: 0.2207161784172058\n",
      "training: 77 batch 365 batch_loss: 0.2196008265018463\n",
      "training: 77 batch 366 batch_loss: 0.21787679195404053\n",
      "training: 77 batch 367 batch_loss: 0.2200060486793518\n",
      "training: 77 batch 368 batch_loss: 0.21696555614471436\n",
      "training: 77 batch 369 batch_loss: 0.2246815264225006\n",
      "training: 77 batch 370 batch_loss: 0.21980899572372437\n",
      "training: 77 batch 371 batch_loss: 0.21637436747550964\n",
      "training: 77 batch 372 batch_loss: 0.21527081727981567\n",
      "training: 77 batch 373 batch_loss: 0.21892818808555603\n",
      "training: 77 batch 374 batch_loss: 0.22144147753715515\n",
      "training: 77 batch 375 batch_loss: 0.21798491477966309\n",
      "training: 77 batch 376 batch_loss: 0.22096014022827148\n",
      "training: 77 batch 377 batch_loss: 0.21940061450004578\n",
      "training: 77 batch 378 batch_loss: 0.21846574544906616\n",
      "training: 77 batch 379 batch_loss: 0.21746692061424255\n",
      "training: 77 batch 380 batch_loss: 0.2219138741493225\n",
      "training: 77 batch 381 batch_loss: 0.2181335687637329\n",
      "training: 77 batch 382 batch_loss: 0.21842050552368164\n",
      "training: 77 batch 383 batch_loss: 0.21374619007110596\n",
      "training: 77 batch 384 batch_loss: 0.21685034036636353\n",
      "training: 77 batch 385 batch_loss: 0.21973589062690735\n",
      "training: 77 batch 386 batch_loss: 0.21809756755828857\n",
      "training: 77 batch 387 batch_loss: 0.2170705497264862\n",
      "training: 77 batch 388 batch_loss: 0.21700626611709595\n",
      "training: 77 batch 389 batch_loss: 0.21792945265769958\n",
      "training: 77 batch 390 batch_loss: 0.2219703495502472\n",
      "training: 77 batch 391 batch_loss: 0.2199951410293579\n",
      "training: 77 batch 392 batch_loss: 0.21555998921394348\n",
      "training: 77 batch 393 batch_loss: 0.21645519137382507\n",
      "training: 77 batch 394 batch_loss: 0.21902421116828918\n",
      "training: 77 batch 395 batch_loss: 0.21722766757011414\n",
      "training: 77 batch 396 batch_loss: 0.2191772758960724\n",
      "training: 77 batch 397 batch_loss: 0.2187400758266449\n",
      "training: 77 batch 398 batch_loss: 0.22183102369308472\n",
      "training: 77 batch 399 batch_loss: 0.21855849027633667\n",
      "training: 77 batch 400 batch_loss: 0.21429795026779175\n",
      "training: 77 batch 401 batch_loss: 0.21998286247253418\n",
      "training: 77 batch 402 batch_loss: 0.21733009815216064\n",
      "training: 77 batch 403 batch_loss: 0.21839922666549683\n",
      "training: 77 batch 404 batch_loss: 0.22007596492767334\n",
      "training: 77 batch 405 batch_loss: 0.21425428986549377\n",
      "training: 77 batch 406 batch_loss: 0.21819230914115906\n",
      "training: 77 batch 407 batch_loss: 0.21765947341918945\n",
      "training: 77 batch 408 batch_loss: 0.2235308587551117\n",
      "training: 77 batch 409 batch_loss: 0.21729081869125366\n",
      "training: 77 batch 410 batch_loss: 0.22250452637672424\n",
      "training: 77 batch 411 batch_loss: 0.21880421042442322\n",
      "training: 77 batch 412 batch_loss: 0.21935436129570007\n",
      "training: 77 batch 413 batch_loss: 0.218835711479187\n",
      "training: 77 batch 414 batch_loss: 0.22351232171058655\n",
      "training: 77 batch 415 batch_loss: 0.216083824634552\n",
      "training: 77 batch 416 batch_loss: 0.22191151976585388\n",
      "training: 77 batch 417 batch_loss: 0.22615641355514526\n",
      "training: 77 batch 418 batch_loss: 0.2196330428123474\n",
      "training: 77 batch 419 batch_loss: 0.22046947479248047\n",
      "training: 77 batch 420 batch_loss: 0.2189030945301056\n",
      "training: 77 batch 421 batch_loss: 0.2205759882926941\n",
      "training: 77 batch 422 batch_loss: 0.22140544652938843\n",
      "training: 77 batch 423 batch_loss: 0.2162785530090332\n",
      "training: 77 batch 424 batch_loss: 0.21988868713378906\n",
      "training: 77 batch 425 batch_loss: 0.21931564807891846\n",
      "training: 77 batch 426 batch_loss: 0.2196744680404663\n",
      "training: 77 batch 427 batch_loss: 0.2213379442691803\n",
      "training: 77 batch 428 batch_loss: 0.22022739052772522\n",
      "training: 77 batch 429 batch_loss: 0.2164630889892578\n",
      "training: 77 batch 430 batch_loss: 0.21722260117530823\n",
      "training: 77 batch 431 batch_loss: 0.22009027004241943\n",
      "training: 77 batch 432 batch_loss: 0.21702396869659424\n",
      "training: 77 batch 433 batch_loss: 0.22012728452682495\n",
      "training: 77 batch 434 batch_loss: 0.22112423181533813\n",
      "training: 77 batch 435 batch_loss: 0.21956393122673035\n",
      "training: 77 batch 436 batch_loss: 0.22047913074493408\n",
      "training: 77 batch 437 batch_loss: 0.218658447265625\n",
      "training: 77 batch 438 batch_loss: 0.21955165266990662\n",
      "training: 77 batch 439 batch_loss: 0.2205566167831421\n",
      "training: 77 batch 440 batch_loss: 0.2165965437889099\n",
      "training: 77 batch 441 batch_loss: 0.2174447774887085\n",
      "training: 77 batch 442 batch_loss: 0.21895578503608704\n",
      "training: 77 batch 443 batch_loss: 0.21899035573005676\n",
      "training: 77 batch 444 batch_loss: 0.2213970124721527\n",
      "training: 77 batch 445 batch_loss: 0.21792086958885193\n",
      "training: 77 batch 446 batch_loss: 0.21936815977096558\n",
      "training: 77 batch 447 batch_loss: 0.2206830084323883\n",
      "training: 77 batch 448 batch_loss: 0.21721825003623962\n",
      "training: 77 batch 449 batch_loss: 0.21804025769233704\n",
      "training: 77 batch 450 batch_loss: 0.2237461507320404\n",
      "training: 77 batch 451 batch_loss: 0.21771806478500366\n",
      "training: 77 batch 452 batch_loss: 0.21443766355514526\n",
      "training: 77 batch 453 batch_loss: 0.22001007199287415\n",
      "training: 77 batch 454 batch_loss: 0.22080999612808228\n",
      "training: 77 batch 455 batch_loss: 0.2157982885837555\n",
      "training: 77 batch 456 batch_loss: 0.21812936663627625\n",
      "training: 77 batch 457 batch_loss: 0.2220696210861206\n",
      "training: 77 batch 458 batch_loss: 0.21859663724899292\n",
      "training: 77 batch 459 batch_loss: 0.22079741954803467\n",
      "training: 77 batch 460 batch_loss: 0.21611779928207397\n",
      "training: 77 batch 461 batch_loss: 0.22071638703346252\n",
      "training: 77 batch 462 batch_loss: 0.22386059165000916\n",
      "training: 77 batch 463 batch_loss: 0.22150284051895142\n",
      "training: 77 batch 464 batch_loss: 0.2195674479007721\n",
      "training: 77 batch 465 batch_loss: 0.21916994452476501\n",
      "training: 77 batch 466 batch_loss: 0.2178819179534912\n",
      "training: 77 batch 467 batch_loss: 0.21782773733139038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 77 batch 468 batch_loss: 0.22185426950454712\n",
      "training: 77 batch 469 batch_loss: 0.21829360723495483\n",
      "training: 77 batch 470 batch_loss: 0.22146081924438477\n",
      "training: 77 batch 471 batch_loss: 0.21774393320083618\n",
      "training: 77 batch 472 batch_loss: 0.2209809124469757\n",
      "training: 77 batch 473 batch_loss: 0.21659955382347107\n",
      "training: 77 batch 474 batch_loss: 0.21786105632781982\n",
      "training: 77 batch 475 batch_loss: 0.22758179903030396\n",
      "training: 77 batch 476 batch_loss: 0.22008708119392395\n",
      "training: 77 batch 477 batch_loss: 0.22070014476776123\n",
      "training: 77 batch 478 batch_loss: 0.21943092346191406\n",
      "training: 77 batch 479 batch_loss: 0.2225669026374817\n",
      "training: 77 batch 480 batch_loss: 0.2217017412185669\n",
      "training: 77 batch 481 batch_loss: 0.21805432438850403\n",
      "training: 77 batch 482 batch_loss: 0.21612924337387085\n",
      "training: 77 batch 483 batch_loss: 0.2208077311515808\n",
      "training: 77 batch 484 batch_loss: 0.21773475408554077\n",
      "training: 77 batch 485 batch_loss: 0.21810752153396606\n",
      "training: 77 batch 486 batch_loss: 0.2144402265548706\n",
      "training: 77 batch 487 batch_loss: 0.21680760383605957\n",
      "training: 77 batch 488 batch_loss: 0.21617242693901062\n",
      "training: 77 batch 489 batch_loss: 0.22023773193359375\n",
      "training: 77 batch 490 batch_loss: 0.2207818627357483\n",
      "training: 77 batch 491 batch_loss: 0.22102826833724976\n",
      "training: 77 batch 492 batch_loss: 0.21791201829910278\n",
      "training: 77 batch 493 batch_loss: 0.2250487506389618\n",
      "training: 77 batch 494 batch_loss: 0.21533972024917603\n",
      "training: 77 batch 495 batch_loss: 0.21608248353004456\n",
      "training: 77 batch 496 batch_loss: 0.21847158670425415\n",
      "training: 77 batch 497 batch_loss: 0.22432070970535278\n",
      "training: 77 batch 498 batch_loss: 0.21819329261779785\n",
      "training: 77 batch 499 batch_loss: 0.2190757393836975\n",
      "training: 77 batch 500 batch_loss: 0.2225542962551117\n",
      "training: 77 batch 501 batch_loss: 0.22270172834396362\n",
      "training: 77 batch 502 batch_loss: 0.21567568182945251\n",
      "training: 77 batch 503 batch_loss: 0.22027114033699036\n",
      "training: 77 batch 504 batch_loss: 0.22063001990318298\n",
      "training: 77 batch 505 batch_loss: 0.22351306676864624\n",
      "training: 77 batch 506 batch_loss: 0.2211073935031891\n",
      "training: 77 batch 507 batch_loss: 0.21732613444328308\n",
      "training: 77 batch 508 batch_loss: 0.21540671586990356\n",
      "training: 77 batch 509 batch_loss: 0.22025710344314575\n",
      "training: 77 batch 510 batch_loss: 0.22232875227928162\n",
      "training: 77 batch 511 batch_loss: 0.2210201621055603\n",
      "training: 77 batch 512 batch_loss: 0.21748003363609314\n",
      "training: 77 batch 513 batch_loss: 0.21366623044013977\n",
      "training: 77 batch 514 batch_loss: 0.21280038356781006\n",
      "training: 77 batch 515 batch_loss: 0.22280335426330566\n",
      "training: 77 batch 516 batch_loss: 0.2185078263282776\n",
      "training: 77 batch 517 batch_loss: 0.22390496730804443\n",
      "training: 77 batch 518 batch_loss: 0.21784693002700806\n",
      "training: 77 batch 519 batch_loss: 0.21971330046653748\n",
      "training: 77 batch 520 batch_loss: 0.21418234705924988\n",
      "training: 77 batch 521 batch_loss: 0.2197180688381195\n",
      "training: 77 batch 522 batch_loss: 0.21789979934692383\n",
      "training: 77 batch 523 batch_loss: 0.21499407291412354\n",
      "training: 77 batch 524 batch_loss: 0.2164168655872345\n",
      "training: 77 batch 525 batch_loss: 0.2174907624721527\n",
      "training: 77 batch 526 batch_loss: 0.22248110175132751\n",
      "training: 77 batch 527 batch_loss: 0.21677446365356445\n",
      "training: 77 batch 528 batch_loss: 0.22480598092079163\n",
      "training: 77 batch 529 batch_loss: 0.2215060591697693\n",
      "training: 77 batch 530 batch_loss: 0.21750003099441528\n",
      "training: 77 batch 531 batch_loss: 0.21775048971176147\n",
      "training: 77 batch 532 batch_loss: 0.21937817335128784\n",
      "training: 77 batch 533 batch_loss: 0.21826201677322388\n",
      "training: 77 batch 534 batch_loss: 0.21783781051635742\n",
      "training: 77 batch 535 batch_loss: 0.2220703661441803\n",
      "training: 77 batch 536 batch_loss: 0.21532446146011353\n",
      "training: 77 batch 537 batch_loss: 0.2204519808292389\n",
      "training: 77 batch 538 batch_loss: 0.21897828578948975\n",
      "training: 77 batch 539 batch_loss: 0.21902251243591309\n",
      "training: 77 batch 540 batch_loss: 0.2191809117794037\n",
      "training: 77 batch 541 batch_loss: 0.2207789421081543\n",
      "training: 77 batch 542 batch_loss: 0.2172975242137909\n",
      "training: 77 batch 543 batch_loss: 0.2233830988407135\n",
      "training: 77 batch 544 batch_loss: 0.21738725900650024\n",
      "training: 77 batch 545 batch_loss: 0.21569085121154785\n",
      "training: 77 batch 546 batch_loss: 0.21779197454452515\n",
      "training: 77 batch 547 batch_loss: 0.2213883399963379\n",
      "training: 77 batch 548 batch_loss: 0.21887657046318054\n",
      "training: 77 batch 549 batch_loss: 0.22314125299453735\n",
      "training: 77 batch 550 batch_loss: 0.21778368949890137\n",
      "training: 77 batch 551 batch_loss: 0.22314125299453735\n",
      "training: 77 batch 552 batch_loss: 0.21835783123970032\n",
      "training: 77 batch 553 batch_loss: 0.2158353328704834\n",
      "training: 77 batch 554 batch_loss: 0.2219105064868927\n",
      "training: 77 batch 555 batch_loss: 0.21822616457939148\n",
      "training: 77 batch 556 batch_loss: 0.22050073742866516\n",
      "training: 77 batch 557 batch_loss: 0.2181839644908905\n",
      "training: 77 batch 558 batch_loss: 0.21960973739624023\n",
      "training: 77 batch 559 batch_loss: 0.2213941514492035\n",
      "training: 77 batch 560 batch_loss: 0.22060728073120117\n",
      "training: 77 batch 561 batch_loss: 0.22349336743354797\n",
      "training: 77 batch 562 batch_loss: 0.21850734949111938\n",
      "training: 77 batch 563 batch_loss: 0.21994179487228394\n",
      "training: 77 batch 564 batch_loss: 0.22390317916870117\n",
      "training: 77 batch 565 batch_loss: 0.22073319554328918\n",
      "training: 77 batch 566 batch_loss: 0.2179889976978302\n",
      "training: 77 batch 567 batch_loss: 0.2179480791091919\n",
      "training: 77 batch 568 batch_loss: 0.2179376184940338\n",
      "training: 77 batch 569 batch_loss: 0.2199426293373108\n",
      "training: 77 batch 570 batch_loss: 0.21681421995162964\n",
      "training: 77 batch 571 batch_loss: 0.21963655948638916\n",
      "training: 77 batch 572 batch_loss: 0.2179323434829712\n",
      "training: 77 batch 573 batch_loss: 0.22069865465164185\n",
      "training: 77 batch 574 batch_loss: 0.21858173608779907\n",
      "training: 77 batch 575 batch_loss: 0.22650131583213806\n",
      "training: 77 batch 576 batch_loss: 0.21761438250541687\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 77, Hit Ratio:0.033728301459394455 | Precision:0.04976408139191979 | Recall:0.06553510953860928 | NDCG:0.06471823804311189\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 78 batch 0 batch_loss: 0.21574091911315918\n",
      "training: 78 batch 1 batch_loss: 0.2145949900150299\n",
      "training: 78 batch 2 batch_loss: 0.21297001838684082\n",
      "training: 78 batch 3 batch_loss: 0.21906355023384094\n",
      "training: 78 batch 4 batch_loss: 0.2174903154373169\n",
      "training: 78 batch 5 batch_loss: 0.2187199592590332\n",
      "training: 78 batch 6 batch_loss: 0.21887168288230896\n",
      "training: 78 batch 7 batch_loss: 0.21942931413650513\n",
      "training: 78 batch 8 batch_loss: 0.2151108682155609\n",
      "training: 78 batch 9 batch_loss: 0.21439850330352783\n",
      "training: 78 batch 10 batch_loss: 0.2196304202079773\n",
      "training: 78 batch 11 batch_loss: 0.21803927421569824\n",
      "training: 78 batch 12 batch_loss: 0.2182847559452057\n",
      "training: 78 batch 13 batch_loss: 0.21872574090957642\n",
      "training: 78 batch 14 batch_loss: 0.21165287494659424\n",
      "training: 78 batch 15 batch_loss: 0.21859759092330933\n",
      "training: 78 batch 16 batch_loss: 0.220221608877182\n",
      "training: 78 batch 17 batch_loss: 0.21708527207374573\n",
      "training: 78 batch 18 batch_loss: 0.21487396955490112\n",
      "training: 78 batch 19 batch_loss: 0.21672579646110535\n",
      "training: 78 batch 20 batch_loss: 0.21616944670677185\n",
      "training: 78 batch 21 batch_loss: 0.22269019484519958\n",
      "training: 78 batch 22 batch_loss: 0.21765604615211487\n",
      "training: 78 batch 23 batch_loss: 0.21718961000442505\n",
      "training: 78 batch 24 batch_loss: 0.22231394052505493\n",
      "training: 78 batch 25 batch_loss: 0.22073471546173096\n",
      "training: 78 batch 26 batch_loss: 0.2209109365940094\n",
      "training: 78 batch 27 batch_loss: 0.21983101963996887\n",
      "training: 78 batch 28 batch_loss: 0.22096627950668335\n",
      "training: 78 batch 29 batch_loss: 0.22104740142822266\n",
      "training: 78 batch 30 batch_loss: 0.22113242745399475\n",
      "training: 78 batch 31 batch_loss: 0.21871879696846008\n",
      "training: 78 batch 32 batch_loss: 0.21779704093933105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 78 batch 33 batch_loss: 0.21602803468704224\n",
      "training: 78 batch 34 batch_loss: 0.2182886004447937\n",
      "training: 78 batch 35 batch_loss: 0.21326100826263428\n",
      "training: 78 batch 36 batch_loss: 0.22045865654945374\n",
      "training: 78 batch 37 batch_loss: 0.21939659118652344\n",
      "training: 78 batch 38 batch_loss: 0.21805799007415771\n",
      "training: 78 batch 39 batch_loss: 0.22347432374954224\n",
      "training: 78 batch 40 batch_loss: 0.21513313055038452\n",
      "training: 78 batch 41 batch_loss: 0.21725249290466309\n",
      "training: 78 batch 42 batch_loss: 0.22038674354553223\n",
      "training: 78 batch 43 batch_loss: 0.21680307388305664\n",
      "training: 78 batch 44 batch_loss: 0.2163364589214325\n",
      "training: 78 batch 45 batch_loss: 0.21392273902893066\n",
      "training: 78 batch 46 batch_loss: 0.2208811640739441\n",
      "training: 78 batch 47 batch_loss: 0.20893019437789917\n",
      "training: 78 batch 48 batch_loss: 0.2175857424736023\n",
      "training: 78 batch 49 batch_loss: 0.21389958262443542\n",
      "training: 78 batch 50 batch_loss: 0.2175275981426239\n",
      "training: 78 batch 51 batch_loss: 0.21796658635139465\n",
      "training: 78 batch 52 batch_loss: 0.21976736187934875\n",
      "training: 78 batch 53 batch_loss: 0.21510744094848633\n",
      "training: 78 batch 54 batch_loss: 0.21950280666351318\n",
      "training: 78 batch 55 batch_loss: 0.2139202058315277\n",
      "training: 78 batch 56 batch_loss: 0.2173404097557068\n",
      "training: 78 batch 57 batch_loss: 0.21799981594085693\n",
      "training: 78 batch 58 batch_loss: 0.21376371383666992\n",
      "training: 78 batch 59 batch_loss: 0.22514387965202332\n",
      "training: 78 batch 60 batch_loss: 0.21795564889907837\n",
      "training: 78 batch 61 batch_loss: 0.21686479449272156\n",
      "training: 78 batch 62 batch_loss: 0.2208482027053833\n",
      "training: 78 batch 63 batch_loss: 0.22064608335494995\n",
      "training: 78 batch 64 batch_loss: 0.21375420689582825\n",
      "training: 78 batch 65 batch_loss: 0.2159786820411682\n",
      "training: 78 batch 66 batch_loss: 0.2223483920097351\n",
      "training: 78 batch 67 batch_loss: 0.21737155318260193\n",
      "training: 78 batch 68 batch_loss: 0.21391555666923523\n",
      "training: 78 batch 69 batch_loss: 0.21634817123413086\n",
      "training: 78 batch 70 batch_loss: 0.21475625038146973\n",
      "training: 78 batch 71 batch_loss: 0.21617725491523743\n",
      "training: 78 batch 72 batch_loss: 0.21933019161224365\n",
      "training: 78 batch 73 batch_loss: 0.2122548222541809\n",
      "training: 78 batch 74 batch_loss: 0.214397132396698\n",
      "training: 78 batch 75 batch_loss: 0.21713611483573914\n",
      "training: 78 batch 76 batch_loss: 0.22076520323753357\n",
      "training: 78 batch 77 batch_loss: 0.22259336709976196\n",
      "training: 78 batch 78 batch_loss: 0.21769088506698608\n",
      "training: 78 batch 79 batch_loss: 0.22146514058113098\n",
      "training: 78 batch 80 batch_loss: 0.22312462329864502\n",
      "training: 78 batch 81 batch_loss: 0.2179361879825592\n",
      "training: 78 batch 82 batch_loss: 0.21226298809051514\n",
      "training: 78 batch 83 batch_loss: 0.2171970009803772\n",
      "training: 78 batch 84 batch_loss: 0.21554943919181824\n",
      "training: 78 batch 85 batch_loss: 0.22003987431526184\n",
      "training: 78 batch 86 batch_loss: 0.21406424045562744\n",
      "training: 78 batch 87 batch_loss: 0.21895021200180054\n",
      "training: 78 batch 88 batch_loss: 0.2225467562675476\n",
      "training: 78 batch 89 batch_loss: 0.21648132801055908\n",
      "training: 78 batch 90 batch_loss: 0.21404868364334106\n",
      "training: 78 batch 91 batch_loss: 0.21717765927314758\n",
      "training: 78 batch 92 batch_loss: 0.2181534469127655\n",
      "training: 78 batch 93 batch_loss: 0.21985501050949097\n",
      "training: 78 batch 94 batch_loss: 0.2186884880065918\n",
      "training: 78 batch 95 batch_loss: 0.22111690044403076\n",
      "training: 78 batch 96 batch_loss: 0.2184571623802185\n",
      "training: 78 batch 97 batch_loss: 0.21782630681991577\n",
      "training: 78 batch 98 batch_loss: 0.2184034287929535\n",
      "training: 78 batch 99 batch_loss: 0.21931910514831543\n",
      "training: 78 batch 100 batch_loss: 0.2206745445728302\n",
      "training: 78 batch 101 batch_loss: 0.21837195754051208\n",
      "training: 78 batch 102 batch_loss: 0.2221362292766571\n",
      "training: 78 batch 103 batch_loss: 0.21553927659988403\n",
      "training: 78 batch 104 batch_loss: 0.21593144536018372\n",
      "training: 78 batch 105 batch_loss: 0.21998724341392517\n",
      "training: 78 batch 106 batch_loss: 0.21799352765083313\n",
      "training: 78 batch 107 batch_loss: 0.2213907241821289\n",
      "training: 78 batch 108 batch_loss: 0.2168395221233368\n",
      "training: 78 batch 109 batch_loss: 0.2193075716495514\n",
      "training: 78 batch 110 batch_loss: 0.21640965342521667\n",
      "training: 78 batch 111 batch_loss: 0.21919935941696167\n",
      "training: 78 batch 112 batch_loss: 0.21888816356658936\n",
      "training: 78 batch 113 batch_loss: 0.21639075875282288\n",
      "training: 78 batch 114 batch_loss: 0.21910786628723145\n",
      "training: 78 batch 115 batch_loss: 0.2204628884792328\n",
      "training: 78 batch 116 batch_loss: 0.22033748030662537\n",
      "training: 78 batch 117 batch_loss: 0.216517835855484\n",
      "training: 78 batch 118 batch_loss: 0.21854498982429504\n",
      "training: 78 batch 119 batch_loss: 0.21725600957870483\n",
      "training: 78 batch 120 batch_loss: 0.21729940176010132\n",
      "training: 78 batch 121 batch_loss: 0.217694491147995\n",
      "training: 78 batch 122 batch_loss: 0.21286940574645996\n",
      "training: 78 batch 123 batch_loss: 0.21832957863807678\n",
      "training: 78 batch 124 batch_loss: 0.2202080488204956\n",
      "training: 78 batch 125 batch_loss: 0.22080299258232117\n",
      "training: 78 batch 126 batch_loss: 0.21486544609069824\n",
      "training: 78 batch 127 batch_loss: 0.21717935800552368\n",
      "training: 78 batch 128 batch_loss: 0.21891078352928162\n",
      "training: 78 batch 129 batch_loss: 0.21554088592529297\n",
      "training: 78 batch 130 batch_loss: 0.22040581703186035\n",
      "training: 78 batch 131 batch_loss: 0.21955260634422302\n",
      "training: 78 batch 132 batch_loss: 0.22151565551757812\n",
      "training: 78 batch 133 batch_loss: 0.2246013581752777\n",
      "training: 78 batch 134 batch_loss: 0.21653610467910767\n",
      "training: 78 batch 135 batch_loss: 0.21859806776046753\n",
      "training: 78 batch 136 batch_loss: 0.22274741530418396\n",
      "training: 78 batch 137 batch_loss: 0.22034424543380737\n",
      "training: 78 batch 138 batch_loss: 0.22138401865959167\n",
      "training: 78 batch 139 batch_loss: 0.2176770567893982\n",
      "training: 78 batch 140 batch_loss: 0.21791142225265503\n",
      "training: 78 batch 141 batch_loss: 0.21740946173667908\n",
      "training: 78 batch 142 batch_loss: 0.22067934274673462\n",
      "training: 78 batch 143 batch_loss: 0.22061389684677124\n",
      "training: 78 batch 144 batch_loss: 0.22012364864349365\n",
      "training: 78 batch 145 batch_loss: 0.22099852561950684\n",
      "training: 78 batch 146 batch_loss: 0.22237765789031982\n",
      "training: 78 batch 147 batch_loss: 0.21449816226959229\n",
      "training: 78 batch 148 batch_loss: 0.2164667844772339\n",
      "training: 78 batch 149 batch_loss: 0.21767467260360718\n",
      "training: 78 batch 150 batch_loss: 0.21589237451553345\n",
      "training: 78 batch 151 batch_loss: 0.22192814946174622\n",
      "training: 78 batch 152 batch_loss: 0.21992012858390808\n",
      "training: 78 batch 153 batch_loss: 0.22028005123138428\n",
      "training: 78 batch 154 batch_loss: 0.21694457530975342\n",
      "training: 78 batch 155 batch_loss: 0.21986687183380127\n",
      "training: 78 batch 156 batch_loss: 0.2142307162284851\n",
      "training: 78 batch 157 batch_loss: 0.21436923742294312\n",
      "training: 78 batch 158 batch_loss: 0.2188991904258728\n",
      "training: 78 batch 159 batch_loss: 0.21953225135803223\n",
      "training: 78 batch 160 batch_loss: 0.21448737382888794\n",
      "training: 78 batch 161 batch_loss: 0.21889829635620117\n",
      "training: 78 batch 162 batch_loss: 0.2178315818309784\n",
      "training: 78 batch 163 batch_loss: 0.21538466215133667\n",
      "training: 78 batch 164 batch_loss: 0.21996736526489258\n",
      "training: 78 batch 165 batch_loss: 0.22087672352790833\n",
      "training: 78 batch 166 batch_loss: 0.218348890542984\n",
      "training: 78 batch 167 batch_loss: 0.2225327491760254\n",
      "training: 78 batch 168 batch_loss: 0.21432971954345703\n",
      "training: 78 batch 169 batch_loss: 0.21894419193267822\n",
      "training: 78 batch 170 batch_loss: 0.2203679084777832\n",
      "training: 78 batch 171 batch_loss: 0.21434271335601807\n",
      "training: 78 batch 172 batch_loss: 0.21683204174041748\n",
      "training: 78 batch 173 batch_loss: 0.21719631552696228\n",
      "training: 78 batch 174 batch_loss: 0.22301241755485535\n",
      "training: 78 batch 175 batch_loss: 0.21904316544532776\n",
      "training: 78 batch 176 batch_loss: 0.21679264307022095\n",
      "training: 78 batch 177 batch_loss: 0.2189367413520813\n",
      "training: 78 batch 178 batch_loss: 0.21577298641204834\n",
      "training: 78 batch 179 batch_loss: 0.2217172384262085\n",
      "training: 78 batch 180 batch_loss: 0.21870845556259155\n",
      "training: 78 batch 181 batch_loss: 0.213424414396286\n",
      "training: 78 batch 182 batch_loss: 0.21726486086845398\n",
      "training: 78 batch 183 batch_loss: 0.21842807531356812\n",
      "training: 78 batch 184 batch_loss: 0.22201579809188843\n",
      "training: 78 batch 185 batch_loss: 0.2154158651828766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 78 batch 186 batch_loss: 0.21922454237937927\n",
      "training: 78 batch 187 batch_loss: 0.22040033340454102\n",
      "training: 78 batch 188 batch_loss: 0.2177402377128601\n",
      "training: 78 batch 189 batch_loss: 0.21338915824890137\n",
      "training: 78 batch 190 batch_loss: 0.22081276774406433\n",
      "training: 78 batch 191 batch_loss: 0.2202233374118805\n",
      "training: 78 batch 192 batch_loss: 0.21691617369651794\n",
      "training: 78 batch 193 batch_loss: 0.21741661429405212\n",
      "training: 78 batch 194 batch_loss: 0.22141635417938232\n",
      "training: 78 batch 195 batch_loss: 0.22129568457603455\n",
      "training: 78 batch 196 batch_loss: 0.22117885947227478\n",
      "training: 78 batch 197 batch_loss: 0.21253547072410583\n",
      "training: 78 batch 198 batch_loss: 0.21679088473320007\n",
      "training: 78 batch 199 batch_loss: 0.21631863713264465\n",
      "training: 78 batch 200 batch_loss: 0.21986740827560425\n",
      "training: 78 batch 201 batch_loss: 0.21905887126922607\n",
      "training: 78 batch 202 batch_loss: 0.22025072574615479\n",
      "training: 78 batch 203 batch_loss: 0.22051241993904114\n",
      "training: 78 batch 204 batch_loss: 0.21928828954696655\n",
      "training: 78 batch 205 batch_loss: 0.21843522787094116\n",
      "training: 78 batch 206 batch_loss: 0.2242998480796814\n",
      "training: 78 batch 207 batch_loss: 0.22113502025604248\n",
      "training: 78 batch 208 batch_loss: 0.21588090062141418\n",
      "training: 78 batch 209 batch_loss: 0.22496521472930908\n",
      "training: 78 batch 210 batch_loss: 0.22214153409004211\n",
      "training: 78 batch 211 batch_loss: 0.2183237075805664\n",
      "training: 78 batch 212 batch_loss: 0.21380716562271118\n",
      "training: 78 batch 213 batch_loss: 0.21558183431625366\n",
      "training: 78 batch 214 batch_loss: 0.21801188588142395\n",
      "training: 78 batch 215 batch_loss: 0.21671655774116516\n",
      "training: 78 batch 216 batch_loss: 0.21887966990470886\n",
      "training: 78 batch 217 batch_loss: 0.21668502688407898\n",
      "training: 78 batch 218 batch_loss: 0.22186094522476196\n",
      "training: 78 batch 219 batch_loss: 0.21788376569747925\n",
      "training: 78 batch 220 batch_loss: 0.2214515507221222\n",
      "training: 78 batch 221 batch_loss: 0.21498659253120422\n",
      "training: 78 batch 222 batch_loss: 0.22024765610694885\n",
      "training: 78 batch 223 batch_loss: 0.21920743584632874\n",
      "training: 78 batch 224 batch_loss: 0.22320738434791565\n",
      "training: 78 batch 225 batch_loss: 0.22230854630470276\n",
      "training: 78 batch 226 batch_loss: 0.21771487593650818\n",
      "training: 78 batch 227 batch_loss: 0.21894341707229614\n",
      "training: 78 batch 228 batch_loss: 0.21672937273979187\n",
      "training: 78 batch 229 batch_loss: 0.2228699028491974\n",
      "training: 78 batch 230 batch_loss: 0.2164137363433838\n",
      "training: 78 batch 231 batch_loss: 0.22390666604042053\n",
      "training: 78 batch 232 batch_loss: 0.21850943565368652\n",
      "training: 78 batch 233 batch_loss: 0.2219754159450531\n",
      "training: 78 batch 234 batch_loss: 0.2211921513080597\n",
      "training: 78 batch 235 batch_loss: 0.22210684418678284\n",
      "training: 78 batch 236 batch_loss: 0.2172546684741974\n",
      "training: 78 batch 237 batch_loss: 0.2264428734779358\n",
      "training: 78 batch 238 batch_loss: 0.21952486038208008\n",
      "training: 78 batch 239 batch_loss: 0.21439647674560547\n",
      "training: 78 batch 240 batch_loss: 0.2183070182800293\n",
      "training: 78 batch 241 batch_loss: 0.21980604529380798\n",
      "training: 78 batch 242 batch_loss: 0.21828344464302063\n",
      "training: 78 batch 243 batch_loss: 0.2196016013622284\n",
      "training: 78 batch 244 batch_loss: 0.2188422679901123\n",
      "training: 78 batch 245 batch_loss: 0.2213411033153534\n",
      "training: 78 batch 246 batch_loss: 0.22027722001075745\n",
      "training: 78 batch 247 batch_loss: 0.2177799940109253\n",
      "training: 78 batch 248 batch_loss: 0.2210535705089569\n",
      "training: 78 batch 249 batch_loss: 0.21838471293449402\n",
      "training: 78 batch 250 batch_loss: 0.21420693397521973\n",
      "training: 78 batch 251 batch_loss: 0.21972209215164185\n",
      "training: 78 batch 252 batch_loss: 0.21818578243255615\n",
      "training: 78 batch 253 batch_loss: 0.22181212902069092\n",
      "training: 78 batch 254 batch_loss: 0.22257286310195923\n",
      "training: 78 batch 255 batch_loss: 0.2185608446598053\n",
      "training: 78 batch 256 batch_loss: 0.22155392169952393\n",
      "training: 78 batch 257 batch_loss: 0.21571287512779236\n",
      "training: 78 batch 258 batch_loss: 0.22286418080329895\n",
      "training: 78 batch 259 batch_loss: 0.22001761198043823\n",
      "training: 78 batch 260 batch_loss: 0.21761676669120789\n",
      "training: 78 batch 261 batch_loss: 0.21775227785110474\n",
      "training: 78 batch 262 batch_loss: 0.22117555141448975\n",
      "training: 78 batch 263 batch_loss: 0.218189537525177\n",
      "training: 78 batch 264 batch_loss: 0.21895134449005127\n",
      "training: 78 batch 265 batch_loss: 0.21787038445472717\n",
      "training: 78 batch 266 batch_loss: 0.2205415666103363\n",
      "training: 78 batch 267 batch_loss: 0.21769672632217407\n",
      "training: 78 batch 268 batch_loss: 0.2161949872970581\n",
      "training: 78 batch 269 batch_loss: 0.216334730386734\n",
      "training: 78 batch 270 batch_loss: 0.22036999464035034\n",
      "training: 78 batch 271 batch_loss: 0.2195950150489807\n",
      "training: 78 batch 272 batch_loss: 0.22090613842010498\n",
      "training: 78 batch 273 batch_loss: 0.2124813199043274\n",
      "training: 78 batch 274 batch_loss: 0.21987199783325195\n",
      "training: 78 batch 275 batch_loss: 0.21897834539413452\n",
      "training: 78 batch 276 batch_loss: 0.2175249457359314\n",
      "training: 78 batch 277 batch_loss: 0.21635094285011292\n",
      "training: 78 batch 278 batch_loss: 0.21647462248802185\n",
      "training: 78 batch 279 batch_loss: 0.2168685793876648\n",
      "training: 78 batch 280 batch_loss: 0.2172444760799408\n",
      "training: 78 batch 281 batch_loss: 0.21723663806915283\n",
      "training: 78 batch 282 batch_loss: 0.22230201959609985\n",
      "training: 78 batch 283 batch_loss: 0.21908116340637207\n",
      "training: 78 batch 284 batch_loss: 0.22201839089393616\n",
      "training: 78 batch 285 batch_loss: 0.2171543836593628\n",
      "training: 78 batch 286 batch_loss: 0.21616610884666443\n",
      "training: 78 batch 287 batch_loss: 0.21967735886573792\n",
      "training: 78 batch 288 batch_loss: 0.21839821338653564\n",
      "training: 78 batch 289 batch_loss: 0.2197096347808838\n",
      "training: 78 batch 290 batch_loss: 0.2192341387271881\n",
      "training: 78 batch 291 batch_loss: 0.22541239857673645\n",
      "training: 78 batch 292 batch_loss: 0.22021785378456116\n",
      "training: 78 batch 293 batch_loss: 0.22525763511657715\n",
      "training: 78 batch 294 batch_loss: 0.21323838829994202\n",
      "training: 78 batch 295 batch_loss: 0.21840524673461914\n",
      "training: 78 batch 296 batch_loss: 0.21966317296028137\n",
      "training: 78 batch 297 batch_loss: 0.220356285572052\n",
      "training: 78 batch 298 batch_loss: 0.2189566195011139\n",
      "training: 78 batch 299 batch_loss: 0.21581393480300903\n",
      "training: 78 batch 300 batch_loss: 0.21880465745925903\n",
      "training: 78 batch 301 batch_loss: 0.21749109029769897\n",
      "training: 78 batch 302 batch_loss: 0.2236434817314148\n",
      "training: 78 batch 303 batch_loss: 0.22350701689720154\n",
      "training: 78 batch 304 batch_loss: 0.22545278072357178\n",
      "training: 78 batch 305 batch_loss: 0.22397738695144653\n",
      "training: 78 batch 306 batch_loss: 0.2161899209022522\n",
      "training: 78 batch 307 batch_loss: 0.21779072284698486\n",
      "training: 78 batch 308 batch_loss: 0.2168135941028595\n",
      "training: 78 batch 309 batch_loss: 0.22394439578056335\n",
      "training: 78 batch 310 batch_loss: 0.22231397032737732\n",
      "training: 78 batch 311 batch_loss: 0.22116440534591675\n",
      "training: 78 batch 312 batch_loss: 0.2205566167831421\n",
      "training: 78 batch 313 batch_loss: 0.21950608491897583\n",
      "training: 78 batch 314 batch_loss: 0.21585974097251892\n",
      "training: 78 batch 315 batch_loss: 0.22430118918418884\n",
      "training: 78 batch 316 batch_loss: 0.2188662886619568\n",
      "training: 78 batch 317 batch_loss: 0.2198716104030609\n",
      "training: 78 batch 318 batch_loss: 0.22246167063713074\n",
      "training: 78 batch 319 batch_loss: 0.21996372938156128\n",
      "training: 78 batch 320 batch_loss: 0.21775424480438232\n",
      "training: 78 batch 321 batch_loss: 0.21852093935012817\n",
      "training: 78 batch 322 batch_loss: 0.2247525453567505\n",
      "training: 78 batch 323 batch_loss: 0.2230696678161621\n",
      "training: 78 batch 324 batch_loss: 0.22236371040344238\n",
      "training: 78 batch 325 batch_loss: 0.22252517938613892\n",
      "training: 78 batch 326 batch_loss: 0.2183995246887207\n",
      "training: 78 batch 327 batch_loss: 0.2173173427581787\n",
      "training: 78 batch 328 batch_loss: 0.22054940462112427\n",
      "training: 78 batch 329 batch_loss: 0.22140178084373474\n",
      "training: 78 batch 330 batch_loss: 0.22453898191452026\n",
      "training: 78 batch 331 batch_loss: 0.21762216091156006\n",
      "training: 78 batch 332 batch_loss: 0.22173762321472168\n",
      "training: 78 batch 333 batch_loss: 0.2176835536956787\n",
      "training: 78 batch 334 batch_loss: 0.21714231371879578\n",
      "training: 78 batch 335 batch_loss: 0.22531428933143616\n",
      "training: 78 batch 336 batch_loss: 0.2189106047153473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 78 batch 337 batch_loss: 0.21534490585327148\n",
      "training: 78 batch 338 batch_loss: 0.21830996870994568\n",
      "training: 78 batch 339 batch_loss: 0.21911686658859253\n",
      "training: 78 batch 340 batch_loss: 0.2199702262878418\n",
      "training: 78 batch 341 batch_loss: 0.21919319033622742\n",
      "training: 78 batch 342 batch_loss: 0.2196255624294281\n",
      "training: 78 batch 343 batch_loss: 0.21924257278442383\n",
      "training: 78 batch 344 batch_loss: 0.21855700016021729\n",
      "training: 78 batch 345 batch_loss: 0.21948185563087463\n",
      "training: 78 batch 346 batch_loss: 0.22082951664924622\n",
      "training: 78 batch 347 batch_loss: 0.2177092432975769\n",
      "training: 78 batch 348 batch_loss: 0.21828314661979675\n",
      "training: 78 batch 349 batch_loss: 0.21980303525924683\n",
      "training: 78 batch 350 batch_loss: 0.22004300355911255\n",
      "training: 78 batch 351 batch_loss: 0.21779921650886536\n",
      "training: 78 batch 352 batch_loss: 0.2207222580909729\n",
      "training: 78 batch 353 batch_loss: 0.22457623481750488\n",
      "training: 78 batch 354 batch_loss: 0.22299951314926147\n",
      "training: 78 batch 355 batch_loss: 0.219169020652771\n",
      "training: 78 batch 356 batch_loss: 0.21974796056747437\n",
      "training: 78 batch 357 batch_loss: 0.2219710648059845\n",
      "training: 78 batch 358 batch_loss: 0.215010404586792\n",
      "training: 78 batch 359 batch_loss: 0.21973848342895508\n",
      "training: 78 batch 360 batch_loss: 0.21998852491378784\n",
      "training: 78 batch 361 batch_loss: 0.22135445475578308\n",
      "training: 78 batch 362 batch_loss: 0.21988284587860107\n",
      "training: 78 batch 363 batch_loss: 0.22050043940544128\n",
      "training: 78 batch 364 batch_loss: 0.21729522943496704\n",
      "training: 78 batch 365 batch_loss: 0.22274786233901978\n",
      "training: 78 batch 366 batch_loss: 0.22046533226966858\n",
      "training: 78 batch 367 batch_loss: 0.21657702326774597\n",
      "training: 78 batch 368 batch_loss: 0.21668726205825806\n",
      "training: 78 batch 369 batch_loss: 0.21728730201721191\n",
      "training: 78 batch 370 batch_loss: 0.21829459071159363\n",
      "training: 78 batch 371 batch_loss: 0.21773836016654968\n",
      "training: 78 batch 372 batch_loss: 0.21528029441833496\n",
      "training: 78 batch 373 batch_loss: 0.21614155173301697\n",
      "training: 78 batch 374 batch_loss: 0.22045129537582397\n",
      "training: 78 batch 375 batch_loss: 0.21968919038772583\n",
      "training: 78 batch 376 batch_loss: 0.22058424353599548\n",
      "training: 78 batch 377 batch_loss: 0.2193160355091095\n",
      "training: 78 batch 378 batch_loss: 0.22244465351104736\n",
      "training: 78 batch 379 batch_loss: 0.22087696194648743\n",
      "training: 78 batch 380 batch_loss: 0.22491836547851562\n",
      "training: 78 batch 381 batch_loss: 0.22086086869239807\n",
      "training: 78 batch 382 batch_loss: 0.220636785030365\n",
      "training: 78 batch 383 batch_loss: 0.21487128734588623\n",
      "training: 78 batch 384 batch_loss: 0.22523945569992065\n",
      "training: 78 batch 385 batch_loss: 0.2182653546333313\n",
      "training: 78 batch 386 batch_loss: 0.21968531608581543\n",
      "training: 78 batch 387 batch_loss: 0.2175946831703186\n",
      "training: 78 batch 388 batch_loss: 0.2171129584312439\n",
      "training: 78 batch 389 batch_loss: 0.22111746668815613\n",
      "training: 78 batch 390 batch_loss: 0.22233685851097107\n",
      "training: 78 batch 391 batch_loss: 0.21771687269210815\n",
      "training: 78 batch 392 batch_loss: 0.21518898010253906\n",
      "training: 78 batch 393 batch_loss: 0.2206047773361206\n",
      "training: 78 batch 394 batch_loss: 0.21889206767082214\n",
      "training: 78 batch 395 batch_loss: 0.21933412551879883\n",
      "training: 78 batch 396 batch_loss: 0.21926307678222656\n",
      "training: 78 batch 397 batch_loss: 0.21830201148986816\n",
      "training: 78 batch 398 batch_loss: 0.21500536799430847\n",
      "training: 78 batch 399 batch_loss: 0.2212180197238922\n",
      "training: 78 batch 400 batch_loss: 0.22216865420341492\n",
      "training: 78 batch 401 batch_loss: 0.21948498487472534\n",
      "training: 78 batch 402 batch_loss: 0.22079813480377197\n",
      "training: 78 batch 403 batch_loss: 0.21612629294395447\n",
      "training: 78 batch 404 batch_loss: 0.2200559675693512\n",
      "training: 78 batch 405 batch_loss: 0.22143355011940002\n",
      "training: 78 batch 406 batch_loss: 0.22122937440872192\n",
      "training: 78 batch 407 batch_loss: 0.224686861038208\n",
      "training: 78 batch 408 batch_loss: 0.21986165642738342\n",
      "training: 78 batch 409 batch_loss: 0.22096997499465942\n",
      "training: 78 batch 410 batch_loss: 0.2249029576778412\n",
      "training: 78 batch 411 batch_loss: 0.22406768798828125\n",
      "training: 78 batch 412 batch_loss: 0.21907028555870056\n",
      "training: 78 batch 413 batch_loss: 0.22232353687286377\n",
      "training: 78 batch 414 batch_loss: 0.2196803092956543\n",
      "training: 78 batch 415 batch_loss: 0.2219373881816864\n",
      "training: 78 batch 416 batch_loss: 0.21778222918510437\n",
      "training: 78 batch 417 batch_loss: 0.21862810850143433\n",
      "training: 78 batch 418 batch_loss: 0.22148019075393677\n",
      "training: 78 batch 419 batch_loss: 0.2223515808582306\n",
      "training: 78 batch 420 batch_loss: 0.2169683873653412\n",
      "training: 78 batch 421 batch_loss: 0.2191714644432068\n",
      "training: 78 batch 422 batch_loss: 0.2157122790813446\n",
      "training: 78 batch 423 batch_loss: 0.21329110860824585\n",
      "training: 78 batch 424 batch_loss: 0.22075510025024414\n",
      "training: 78 batch 425 batch_loss: 0.22318097949028015\n",
      "training: 78 batch 426 batch_loss: 0.22100943326950073\n",
      "training: 78 batch 427 batch_loss: 0.21531614661216736\n",
      "training: 78 batch 428 batch_loss: 0.21976512670516968\n",
      "training: 78 batch 429 batch_loss: 0.21810674667358398\n",
      "training: 78 batch 430 batch_loss: 0.2171112596988678\n",
      "training: 78 batch 431 batch_loss: 0.21889755129814148\n",
      "training: 78 batch 432 batch_loss: 0.21761208772659302\n",
      "training: 78 batch 433 batch_loss: 0.2207125723361969\n",
      "training: 78 batch 434 batch_loss: 0.2174120843410492\n",
      "training: 78 batch 435 batch_loss: 0.22170868515968323\n",
      "training: 78 batch 436 batch_loss: 0.22277972102165222\n",
      "training: 78 batch 437 batch_loss: 0.22186917066574097\n",
      "training: 78 batch 438 batch_loss: 0.2163245975971222\n",
      "training: 78 batch 439 batch_loss: 0.219536691904068\n",
      "training: 78 batch 440 batch_loss: 0.22129887342453003\n",
      "training: 78 batch 441 batch_loss: 0.21473544836044312\n",
      "training: 78 batch 442 batch_loss: 0.22380217909812927\n",
      "training: 78 batch 443 batch_loss: 0.2163301706314087\n",
      "training: 78 batch 444 batch_loss: 0.21882891654968262\n",
      "training: 78 batch 445 batch_loss: 0.2210370898246765\n",
      "training: 78 batch 446 batch_loss: 0.22116190195083618\n",
      "training: 78 batch 447 batch_loss: 0.2202029526233673\n",
      "training: 78 batch 448 batch_loss: 0.221451997756958\n",
      "training: 78 batch 449 batch_loss: 0.22189611196517944\n",
      "training: 78 batch 450 batch_loss: 0.22257179021835327\n",
      "training: 78 batch 451 batch_loss: 0.21565887331962585\n",
      "training: 78 batch 452 batch_loss: 0.22200095653533936\n",
      "training: 78 batch 453 batch_loss: 0.22163918614387512\n",
      "training: 78 batch 454 batch_loss: 0.22313052415847778\n",
      "training: 78 batch 455 batch_loss: 0.2187759280204773\n",
      "training: 78 batch 456 batch_loss: 0.22337427735328674\n",
      "training: 78 batch 457 batch_loss: 0.21567916870117188\n",
      "training: 78 batch 458 batch_loss: 0.22349989414215088\n",
      "training: 78 batch 459 batch_loss: 0.22163724899291992\n",
      "training: 78 batch 460 batch_loss: 0.21963432431221008\n",
      "training: 78 batch 461 batch_loss: 0.22398453950881958\n",
      "training: 78 batch 462 batch_loss: 0.21935701370239258\n",
      "training: 78 batch 463 batch_loss: 0.21775028109550476\n",
      "training: 78 batch 464 batch_loss: 0.22171670198440552\n",
      "training: 78 batch 465 batch_loss: 0.21701979637145996\n",
      "training: 78 batch 466 batch_loss: 0.21860888600349426\n",
      "training: 78 batch 467 batch_loss: 0.22025737166404724\n",
      "training: 78 batch 468 batch_loss: 0.21832385659217834\n",
      "training: 78 batch 469 batch_loss: 0.21405094861984253\n",
      "training: 78 batch 470 batch_loss: 0.21996265649795532\n",
      "training: 78 batch 471 batch_loss: 0.22223755717277527\n",
      "training: 78 batch 472 batch_loss: 0.21962273120880127\n",
      "training: 78 batch 473 batch_loss: 0.21905368566513062\n",
      "training: 78 batch 474 batch_loss: 0.22249430418014526\n",
      "training: 78 batch 475 batch_loss: 0.21725893020629883\n",
      "training: 78 batch 476 batch_loss: 0.2215944230556488\n",
      "training: 78 batch 477 batch_loss: 0.21820011734962463\n",
      "training: 78 batch 478 batch_loss: 0.2179594337940216\n",
      "training: 78 batch 479 batch_loss: 0.21740901470184326\n",
      "training: 78 batch 480 batch_loss: 0.21928775310516357\n",
      "training: 78 batch 481 batch_loss: 0.22030973434448242\n",
      "training: 78 batch 482 batch_loss: 0.22051793336868286\n",
      "training: 78 batch 483 batch_loss: 0.22139635682106018\n",
      "training: 78 batch 484 batch_loss: 0.22065457701683044\n",
      "training: 78 batch 485 batch_loss: 0.21895140409469604\n",
      "training: 78 batch 486 batch_loss: 0.2218513786792755\n",
      "training: 78 batch 487 batch_loss: 0.22143307328224182\n",
      "training: 78 batch 488 batch_loss: 0.21884918212890625\n",
      "training: 78 batch 489 batch_loss: 0.22369295358657837\n",
      "training: 78 batch 490 batch_loss: 0.22292867302894592\n",
      "training: 78 batch 491 batch_loss: 0.21715649962425232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 78 batch 492 batch_loss: 0.21984583139419556\n",
      "training: 78 batch 493 batch_loss: 0.21982920169830322\n",
      "training: 78 batch 494 batch_loss: 0.21985897421836853\n",
      "training: 78 batch 495 batch_loss: 0.22291186451911926\n",
      "training: 78 batch 496 batch_loss: 0.22201600670814514\n",
      "training: 78 batch 497 batch_loss: 0.22166123986244202\n",
      "training: 78 batch 498 batch_loss: 0.22380536794662476\n",
      "training: 78 batch 499 batch_loss: 0.22161418199539185\n",
      "training: 78 batch 500 batch_loss: 0.21893560886383057\n",
      "training: 78 batch 501 batch_loss: 0.2171509563922882\n",
      "training: 78 batch 502 batch_loss: 0.22042402625083923\n",
      "training: 78 batch 503 batch_loss: 0.22191306948661804\n",
      "training: 78 batch 504 batch_loss: 0.22029703855514526\n",
      "training: 78 batch 505 batch_loss: 0.21827954053878784\n",
      "training: 78 batch 506 batch_loss: 0.21948951482772827\n",
      "training: 78 batch 507 batch_loss: 0.2178122103214264\n",
      "training: 78 batch 508 batch_loss: 0.22036734223365784\n",
      "training: 78 batch 509 batch_loss: 0.21761196851730347\n",
      "training: 78 batch 510 batch_loss: 0.21791604161262512\n",
      "training: 78 batch 511 batch_loss: 0.2174343764781952\n",
      "training: 78 batch 512 batch_loss: 0.22049066424369812\n",
      "training: 78 batch 513 batch_loss: 0.21839788556098938\n",
      "training: 78 batch 514 batch_loss: 0.21878084540367126\n",
      "training: 78 batch 515 batch_loss: 0.21725547313690186\n",
      "training: 78 batch 516 batch_loss: 0.2174164354801178\n",
      "training: 78 batch 517 batch_loss: 0.22613951563835144\n",
      "training: 78 batch 518 batch_loss: 0.2225639522075653\n",
      "training: 78 batch 519 batch_loss: 0.22272631525993347\n",
      "training: 78 batch 520 batch_loss: 0.22057929635047913\n",
      "training: 78 batch 521 batch_loss: 0.2244560718536377\n",
      "training: 78 batch 522 batch_loss: 0.22185197472572327\n",
      "training: 78 batch 523 batch_loss: 0.21460193395614624\n",
      "training: 78 batch 524 batch_loss: 0.2232663929462433\n",
      "training: 78 batch 525 batch_loss: 0.22591862082481384\n",
      "training: 78 batch 526 batch_loss: 0.21908164024353027\n",
      "training: 78 batch 527 batch_loss: 0.22334840893745422\n",
      "training: 78 batch 528 batch_loss: 0.22010254859924316\n",
      "training: 78 batch 529 batch_loss: 0.2219184935092926\n",
      "training: 78 batch 530 batch_loss: 0.22266939282417297\n",
      "training: 78 batch 531 batch_loss: 0.2225494384765625\n",
      "training: 78 batch 532 batch_loss: 0.2249639630317688\n",
      "training: 78 batch 533 batch_loss: 0.21947306394577026\n",
      "training: 78 batch 534 batch_loss: 0.22356164455413818\n",
      "training: 78 batch 535 batch_loss: 0.22586491703987122\n",
      "training: 78 batch 536 batch_loss: 0.2154863476753235\n",
      "training: 78 batch 537 batch_loss: 0.2209947407245636\n",
      "training: 78 batch 538 batch_loss: 0.21596524119377136\n",
      "training: 78 batch 539 batch_loss: 0.21969819068908691\n",
      "training: 78 batch 540 batch_loss: 0.22164684534072876\n",
      "training: 78 batch 541 batch_loss: 0.2200709581375122\n",
      "training: 78 batch 542 batch_loss: 0.21592053771018982\n",
      "training: 78 batch 543 batch_loss: 0.22481077909469604\n",
      "training: 78 batch 544 batch_loss: 0.21970903873443604\n",
      "training: 78 batch 545 batch_loss: 0.22221839427947998\n",
      "training: 78 batch 546 batch_loss: 0.21730929613113403\n",
      "training: 78 batch 547 batch_loss: 0.21979036927223206\n",
      "training: 78 batch 548 batch_loss: 0.2179575264453888\n",
      "training: 78 batch 549 batch_loss: 0.2199367880821228\n",
      "training: 78 batch 550 batch_loss: 0.2197110950946808\n",
      "training: 78 batch 551 batch_loss: 0.2228553593158722\n",
      "training: 78 batch 552 batch_loss: 0.22202655673027039\n",
      "training: 78 batch 553 batch_loss: 0.21993902325630188\n",
      "training: 78 batch 554 batch_loss: 0.21796810626983643\n",
      "training: 78 batch 555 batch_loss: 0.22432520985603333\n",
      "training: 78 batch 556 batch_loss: 0.21647709608078003\n",
      "training: 78 batch 557 batch_loss: 0.22064417600631714\n",
      "training: 78 batch 558 batch_loss: 0.22487321496009827\n",
      "training: 78 batch 559 batch_loss: 0.21816888451576233\n",
      "training: 78 batch 560 batch_loss: 0.21709802746772766\n",
      "training: 78 batch 561 batch_loss: 0.22240814566612244\n",
      "training: 78 batch 562 batch_loss: 0.2185194492340088\n",
      "training: 78 batch 563 batch_loss: 0.21850603818893433\n",
      "training: 78 batch 564 batch_loss: 0.21971318125724792\n",
      "training: 78 batch 565 batch_loss: 0.21976715326309204\n",
      "training: 78 batch 566 batch_loss: 0.22081011533737183\n",
      "training: 78 batch 567 batch_loss: 0.22022917866706848\n",
      "training: 78 batch 568 batch_loss: 0.21825039386749268\n",
      "training: 78 batch 569 batch_loss: 0.21916741132736206\n",
      "training: 78 batch 570 batch_loss: 0.22078919410705566\n",
      "training: 78 batch 571 batch_loss: 0.22129923105239868\n",
      "training: 78 batch 572 batch_loss: 0.21731358766555786\n",
      "training: 78 batch 573 batch_loss: 0.22371724247932434\n",
      "training: 78 batch 574 batch_loss: 0.21978497505187988\n",
      "training: 78 batch 575 batch_loss: 0.2217596173286438\n",
      "training: 78 batch 576 batch_loss: 0.22098422050476074\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 78, Hit Ratio:0.03369498955671851 | Precision:0.04971493168190308 | Recall:0.06574352412058034 | NDCG:0.06480309596836732\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 79 batch 0 batch_loss: 0.21766066551208496\n",
      "training: 79 batch 1 batch_loss: 0.21729090809822083\n",
      "training: 79 batch 2 batch_loss: 0.2139560878276825\n",
      "training: 79 batch 3 batch_loss: 0.22356224060058594\n",
      "training: 79 batch 4 batch_loss: 0.2198410928249359\n",
      "training: 79 batch 5 batch_loss: 0.22074615955352783\n",
      "training: 79 batch 6 batch_loss: 0.21524718403816223\n",
      "training: 79 batch 7 batch_loss: 0.21747976541519165\n",
      "training: 79 batch 8 batch_loss: 0.2177761197090149\n",
      "training: 79 batch 9 batch_loss: 0.21512511372566223\n",
      "training: 79 batch 10 batch_loss: 0.21890366077423096\n",
      "training: 79 batch 11 batch_loss: 0.21794408559799194\n",
      "training: 79 batch 12 batch_loss: 0.21707358956336975\n",
      "training: 79 batch 13 batch_loss: 0.2183576226234436\n",
      "training: 79 batch 14 batch_loss: 0.21567150950431824\n",
      "training: 79 batch 15 batch_loss: 0.21925151348114014\n",
      "training: 79 batch 16 batch_loss: 0.21875590085983276\n",
      "training: 79 batch 17 batch_loss: 0.22032037377357483\n",
      "training: 79 batch 18 batch_loss: 0.21911627054214478\n",
      "training: 79 batch 19 batch_loss: 0.21700149774551392\n",
      "training: 79 batch 20 batch_loss: 0.2233409881591797\n",
      "training: 79 batch 21 batch_loss: 0.21771514415740967\n",
      "training: 79 batch 22 batch_loss: 0.21848776936531067\n",
      "training: 79 batch 23 batch_loss: 0.2228434979915619\n",
      "training: 79 batch 24 batch_loss: 0.22003787755966187\n",
      "training: 79 batch 25 batch_loss: 0.21748846769332886\n",
      "training: 79 batch 26 batch_loss: 0.21372240781784058\n",
      "training: 79 batch 27 batch_loss: 0.2230345606803894\n",
      "training: 79 batch 28 batch_loss: 0.21684664487838745\n",
      "training: 79 batch 29 batch_loss: 0.22101041674613953\n",
      "training: 79 batch 30 batch_loss: 0.21795007586479187\n",
      "training: 79 batch 31 batch_loss: 0.22089973092079163\n",
      "training: 79 batch 32 batch_loss: 0.22006836533546448\n",
      "training: 79 batch 33 batch_loss: 0.21798402070999146\n",
      "training: 79 batch 34 batch_loss: 0.21786314249038696\n",
      "training: 79 batch 35 batch_loss: 0.21522507071495056\n",
      "training: 79 batch 36 batch_loss: 0.21231159567832947\n",
      "training: 79 batch 37 batch_loss: 0.21761319041252136\n",
      "training: 79 batch 38 batch_loss: 0.21755623817443848\n",
      "training: 79 batch 39 batch_loss: 0.21669641137123108\n",
      "training: 79 batch 40 batch_loss: 0.22064048051834106\n",
      "training: 79 batch 41 batch_loss: 0.2164447009563446\n",
      "training: 79 batch 42 batch_loss: 0.2168993353843689\n",
      "training: 79 batch 43 batch_loss: 0.22166848182678223\n",
      "training: 79 batch 44 batch_loss: 0.21894899010658264\n",
      "training: 79 batch 45 batch_loss: 0.219529926776886\n",
      "training: 79 batch 46 batch_loss: 0.2157173454761505\n",
      "training: 79 batch 47 batch_loss: 0.21909472346305847\n",
      "training: 79 batch 48 batch_loss: 0.22007966041564941\n",
      "training: 79 batch 49 batch_loss: 0.22039729356765747\n",
      "training: 79 batch 50 batch_loss: 0.22206056118011475\n",
      "training: 79 batch 51 batch_loss: 0.21905744075775146\n",
      "training: 79 batch 52 batch_loss: 0.21687409281730652\n",
      "training: 79 batch 53 batch_loss: 0.2183176875114441\n",
      "training: 79 batch 54 batch_loss: 0.21939057111740112\n",
      "training: 79 batch 55 batch_loss: 0.21816730499267578\n",
      "training: 79 batch 56 batch_loss: 0.2233193814754486\n",
      "training: 79 batch 57 batch_loss: 0.2188318967819214\n",
      "training: 79 batch 58 batch_loss: 0.21777456998825073\n",
      "training: 79 batch 59 batch_loss: 0.2179080843925476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 79 batch 60 batch_loss: 0.21994394063949585\n",
      "training: 79 batch 61 batch_loss: 0.2210260033607483\n",
      "training: 79 batch 62 batch_loss: 0.21717271208763123\n",
      "training: 79 batch 63 batch_loss: 0.21821731328964233\n",
      "training: 79 batch 64 batch_loss: 0.21997156739234924\n",
      "training: 79 batch 65 batch_loss: 0.21723908185958862\n",
      "training: 79 batch 66 batch_loss: 0.22004187107086182\n",
      "training: 79 batch 67 batch_loss: 0.21385717391967773\n",
      "training: 79 batch 68 batch_loss: 0.22126799821853638\n",
      "training: 79 batch 69 batch_loss: 0.21920493245124817\n",
      "training: 79 batch 70 batch_loss: 0.21822738647460938\n",
      "training: 79 batch 71 batch_loss: 0.2193107008934021\n",
      "training: 79 batch 72 batch_loss: 0.2191971242427826\n",
      "training: 79 batch 73 batch_loss: 0.22069209814071655\n",
      "training: 79 batch 74 batch_loss: 0.22003966569900513\n",
      "training: 79 batch 75 batch_loss: 0.2196163833141327\n",
      "training: 79 batch 76 batch_loss: 0.22062626481056213\n",
      "training: 79 batch 77 batch_loss: 0.2229394018650055\n",
      "training: 79 batch 78 batch_loss: 0.21302282810211182\n",
      "training: 79 batch 79 batch_loss: 0.2244308590888977\n",
      "training: 79 batch 80 batch_loss: 0.21873345971107483\n",
      "training: 79 batch 81 batch_loss: 0.21924453973770142\n",
      "training: 79 batch 82 batch_loss: 0.21786868572235107\n",
      "training: 79 batch 83 batch_loss: 0.22468748688697815\n",
      "training: 79 batch 84 batch_loss: 0.219620943069458\n",
      "training: 79 batch 85 batch_loss: 0.2216305434703827\n",
      "training: 79 batch 86 batch_loss: 0.22453385591506958\n",
      "training: 79 batch 87 batch_loss: 0.22021058201789856\n",
      "training: 79 batch 88 batch_loss: 0.21900829672813416\n",
      "training: 79 batch 89 batch_loss: 0.21664249897003174\n",
      "training: 79 batch 90 batch_loss: 0.21738743782043457\n",
      "training: 79 batch 91 batch_loss: 0.21832996606826782\n",
      "training: 79 batch 92 batch_loss: 0.2239275574684143\n",
      "training: 79 batch 93 batch_loss: 0.21610665321350098\n",
      "training: 79 batch 94 batch_loss: 0.21791797876358032\n",
      "training: 79 batch 95 batch_loss: 0.219295471906662\n",
      "training: 79 batch 96 batch_loss: 0.21859663724899292\n",
      "training: 79 batch 97 batch_loss: 0.2211374044418335\n",
      "training: 79 batch 98 batch_loss: 0.22022360563278198\n",
      "training: 79 batch 99 batch_loss: 0.2216222584247589\n",
      "training: 79 batch 100 batch_loss: 0.2234799563884735\n",
      "training: 79 batch 101 batch_loss: 0.22121107578277588\n",
      "training: 79 batch 102 batch_loss: 0.2199643850326538\n",
      "training: 79 batch 103 batch_loss: 0.2190552055835724\n",
      "training: 79 batch 104 batch_loss: 0.2205030620098114\n",
      "training: 79 batch 105 batch_loss: 0.21932071447372437\n",
      "training: 79 batch 106 batch_loss: 0.21435970067977905\n",
      "training: 79 batch 107 batch_loss: 0.21731987595558167\n",
      "training: 79 batch 108 batch_loss: 0.21978884935379028\n",
      "training: 79 batch 109 batch_loss: 0.21941184997558594\n",
      "training: 79 batch 110 batch_loss: 0.22365054488182068\n",
      "training: 79 batch 111 batch_loss: 0.2232079803943634\n",
      "training: 79 batch 112 batch_loss: 0.22283756732940674\n",
      "training: 79 batch 113 batch_loss: 0.21597880125045776\n",
      "training: 79 batch 114 batch_loss: 0.21945002675056458\n",
      "training: 79 batch 115 batch_loss: 0.21645352244377136\n",
      "training: 79 batch 116 batch_loss: 0.21892023086547852\n",
      "training: 79 batch 117 batch_loss: 0.22112199664115906\n",
      "training: 79 batch 118 batch_loss: 0.21751385927200317\n",
      "training: 79 batch 119 batch_loss: 0.21984678506851196\n",
      "training: 79 batch 120 batch_loss: 0.2190956473350525\n",
      "training: 79 batch 121 batch_loss: 0.21780192852020264\n",
      "training: 79 batch 122 batch_loss: 0.21664360165596008\n",
      "training: 79 batch 123 batch_loss: 0.21676763892173767\n",
      "training: 79 batch 124 batch_loss: 0.21775802969932556\n",
      "training: 79 batch 125 batch_loss: 0.22199076414108276\n",
      "training: 79 batch 126 batch_loss: 0.2191016972064972\n",
      "training: 79 batch 127 batch_loss: 0.22031673789024353\n",
      "training: 79 batch 128 batch_loss: 0.2206125557422638\n",
      "training: 79 batch 129 batch_loss: 0.21822667121887207\n",
      "training: 79 batch 130 batch_loss: 0.2194555699825287\n",
      "training: 79 batch 131 batch_loss: 0.2181709110736847\n",
      "training: 79 batch 132 batch_loss: 0.21472060680389404\n",
      "training: 79 batch 133 batch_loss: 0.2143992781639099\n",
      "training: 79 batch 134 batch_loss: 0.21898359060287476\n",
      "training: 79 batch 135 batch_loss: 0.21865975856781006\n",
      "training: 79 batch 136 batch_loss: 0.21770533919334412\n",
      "training: 79 batch 137 batch_loss: 0.22218146920204163\n",
      "training: 79 batch 138 batch_loss: 0.21446657180786133\n",
      "training: 79 batch 139 batch_loss: 0.21701499819755554\n",
      "training: 79 batch 140 batch_loss: 0.21706309914588928\n",
      "training: 79 batch 141 batch_loss: 0.22471573948860168\n",
      "training: 79 batch 142 batch_loss: 0.21619942784309387\n",
      "training: 79 batch 143 batch_loss: 0.21671470999717712\n",
      "training: 79 batch 144 batch_loss: 0.2152060568332672\n",
      "training: 79 batch 145 batch_loss: 0.22087833285331726\n",
      "training: 79 batch 146 batch_loss: 0.22055187821388245\n",
      "training: 79 batch 147 batch_loss: 0.22099292278289795\n",
      "training: 79 batch 148 batch_loss: 0.2206711769104004\n",
      "training: 79 batch 149 batch_loss: 0.21601539850234985\n",
      "training: 79 batch 150 batch_loss: 0.21688500046730042\n",
      "training: 79 batch 151 batch_loss: 0.22047361731529236\n",
      "training: 79 batch 152 batch_loss: 0.21886417269706726\n",
      "training: 79 batch 153 batch_loss: 0.21670475602149963\n",
      "training: 79 batch 154 batch_loss: 0.2153516411781311\n",
      "training: 79 batch 155 batch_loss: 0.22107160091400146\n",
      "training: 79 batch 156 batch_loss: 0.2184087634086609\n",
      "training: 79 batch 157 batch_loss: 0.21941083669662476\n",
      "training: 79 batch 158 batch_loss: 0.2142930030822754\n",
      "training: 79 batch 159 batch_loss: 0.2202605903148651\n",
      "training: 79 batch 160 batch_loss: 0.22002634406089783\n",
      "training: 79 batch 161 batch_loss: 0.22319316864013672\n",
      "training: 79 batch 162 batch_loss: 0.21735423803329468\n",
      "training: 79 batch 163 batch_loss: 0.21725255250930786\n",
      "training: 79 batch 164 batch_loss: 0.2210322618484497\n",
      "training: 79 batch 165 batch_loss: 0.21743062138557434\n",
      "training: 79 batch 166 batch_loss: 0.21804535388946533\n",
      "training: 79 batch 167 batch_loss: 0.22257158160209656\n",
      "training: 79 batch 168 batch_loss: 0.21559131145477295\n",
      "training: 79 batch 169 batch_loss: 0.2209974229335785\n",
      "training: 79 batch 170 batch_loss: 0.22047212719917297\n",
      "training: 79 batch 171 batch_loss: 0.22219014167785645\n",
      "training: 79 batch 172 batch_loss: 0.22079795598983765\n",
      "training: 79 batch 173 batch_loss: 0.21525025367736816\n",
      "training: 79 batch 174 batch_loss: 0.21982145309448242\n",
      "training: 79 batch 175 batch_loss: 0.21826279163360596\n",
      "training: 79 batch 176 batch_loss: 0.21789905428886414\n",
      "training: 79 batch 177 batch_loss: 0.2207348346710205\n",
      "training: 79 batch 178 batch_loss: 0.21796807646751404\n",
      "training: 79 batch 179 batch_loss: 0.2230207324028015\n",
      "training: 79 batch 180 batch_loss: 0.22702497243881226\n",
      "training: 79 batch 181 batch_loss: 0.21847882866859436\n",
      "training: 79 batch 182 batch_loss: 0.21735548973083496\n",
      "training: 79 batch 183 batch_loss: 0.2185525894165039\n",
      "training: 79 batch 184 batch_loss: 0.21742841601371765\n",
      "training: 79 batch 185 batch_loss: 0.21874591708183289\n",
      "training: 79 batch 186 batch_loss: 0.22070804238319397\n",
      "training: 79 batch 187 batch_loss: 0.22270923852920532\n",
      "training: 79 batch 188 batch_loss: 0.2245052456855774\n",
      "training: 79 batch 189 batch_loss: 0.21950417757034302\n",
      "training: 79 batch 190 batch_loss: 0.221646249294281\n",
      "training: 79 batch 191 batch_loss: 0.22324249148368835\n",
      "training: 79 batch 192 batch_loss: 0.2206612527370453\n",
      "training: 79 batch 193 batch_loss: 0.22085225582122803\n",
      "training: 79 batch 194 batch_loss: 0.22127529978752136\n",
      "training: 79 batch 195 batch_loss: 0.21713146567344666\n",
      "training: 79 batch 196 batch_loss: 0.21870070695877075\n",
      "training: 79 batch 197 batch_loss: 0.2215498685836792\n",
      "training: 79 batch 198 batch_loss: 0.22051864862442017\n",
      "training: 79 batch 199 batch_loss: 0.22058939933776855\n",
      "training: 79 batch 200 batch_loss: 0.2146649956703186\n",
      "training: 79 batch 201 batch_loss: 0.2169586718082428\n",
      "training: 79 batch 202 batch_loss: 0.2162458896636963\n",
      "training: 79 batch 203 batch_loss: 0.22220641374588013\n",
      "training: 79 batch 204 batch_loss: 0.22182786464691162\n",
      "training: 79 batch 205 batch_loss: 0.2173195779323578\n",
      "training: 79 batch 206 batch_loss: 0.21920040249824524\n",
      "training: 79 batch 207 batch_loss: 0.22070163488388062\n",
      "training: 79 batch 208 batch_loss: 0.2209855020046234\n",
      "training: 79 batch 209 batch_loss: 0.22002077102661133\n",
      "training: 79 batch 210 batch_loss: 0.22260749340057373\n",
      "training: 79 batch 211 batch_loss: 0.21624454855918884\n",
      "training: 79 batch 212 batch_loss: 0.21931874752044678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 79 batch 213 batch_loss: 0.22106319665908813\n",
      "training: 79 batch 214 batch_loss: 0.21974605321884155\n",
      "training: 79 batch 215 batch_loss: 0.21979203820228577\n",
      "training: 79 batch 216 batch_loss: 0.2204335331916809\n",
      "training: 79 batch 217 batch_loss: 0.21778690814971924\n",
      "training: 79 batch 218 batch_loss: 0.22136250138282776\n",
      "training: 79 batch 219 batch_loss: 0.2249453365802765\n",
      "training: 79 batch 220 batch_loss: 0.22068911790847778\n",
      "training: 79 batch 221 batch_loss: 0.22193828225135803\n",
      "training: 79 batch 222 batch_loss: 0.220375657081604\n",
      "training: 79 batch 223 batch_loss: 0.21555352210998535\n",
      "training: 79 batch 224 batch_loss: 0.22478744387626648\n",
      "training: 79 batch 225 batch_loss: 0.21964317560195923\n",
      "training: 79 batch 226 batch_loss: 0.22263789176940918\n",
      "training: 79 batch 227 batch_loss: 0.21974250674247742\n",
      "training: 79 batch 228 batch_loss: 0.22315365076065063\n",
      "training: 79 batch 229 batch_loss: 0.21532049775123596\n",
      "training: 79 batch 230 batch_loss: 0.2178720235824585\n",
      "training: 79 batch 231 batch_loss: 0.22008824348449707\n",
      "training: 79 batch 232 batch_loss: 0.22176408767700195\n",
      "training: 79 batch 233 batch_loss: 0.22146594524383545\n",
      "training: 79 batch 234 batch_loss: 0.22066965699195862\n",
      "training: 79 batch 235 batch_loss: 0.22255486249923706\n",
      "training: 79 batch 236 batch_loss: 0.21900463104248047\n",
      "training: 79 batch 237 batch_loss: 0.22185638546943665\n",
      "training: 79 batch 238 batch_loss: 0.2193847894668579\n",
      "training: 79 batch 239 batch_loss: 0.22121107578277588\n",
      "training: 79 batch 240 batch_loss: 0.22502008080482483\n",
      "training: 79 batch 241 batch_loss: 0.22103050351142883\n",
      "training: 79 batch 242 batch_loss: 0.21881037950515747\n",
      "training: 79 batch 243 batch_loss: 0.2176406979560852\n",
      "training: 79 batch 244 batch_loss: 0.21985429525375366\n",
      "training: 79 batch 245 batch_loss: 0.22228264808654785\n",
      "training: 79 batch 246 batch_loss: 0.22100725769996643\n",
      "training: 79 batch 247 batch_loss: 0.22214999794960022\n",
      "training: 79 batch 248 batch_loss: 0.22298935055732727\n",
      "training: 79 batch 249 batch_loss: 0.22163718938827515\n",
      "training: 79 batch 250 batch_loss: 0.22114339470863342\n",
      "training: 79 batch 251 batch_loss: 0.2221052050590515\n",
      "training: 79 batch 252 batch_loss: 0.22030454874038696\n",
      "training: 79 batch 253 batch_loss: 0.2223733365535736\n",
      "training: 79 batch 254 batch_loss: 0.22165906429290771\n",
      "training: 79 batch 255 batch_loss: 0.22242361307144165\n",
      "training: 79 batch 256 batch_loss: 0.21860754489898682\n",
      "training: 79 batch 257 batch_loss: 0.2177548110485077\n",
      "training: 79 batch 258 batch_loss: 0.22159209847450256\n",
      "training: 79 batch 259 batch_loss: 0.22381830215454102\n",
      "training: 79 batch 260 batch_loss: 0.2228715717792511\n",
      "training: 79 batch 261 batch_loss: 0.21381807327270508\n",
      "training: 79 batch 262 batch_loss: 0.2229624092578888\n",
      "training: 79 batch 263 batch_loss: 0.21948564052581787\n",
      "training: 79 batch 264 batch_loss: 0.2209600806236267\n",
      "training: 79 batch 265 batch_loss: 0.22555479407310486\n",
      "training: 79 batch 266 batch_loss: 0.2181195318698883\n",
      "training: 79 batch 267 batch_loss: 0.2206535041332245\n",
      "training: 79 batch 268 batch_loss: 0.22083944082260132\n",
      "training: 79 batch 269 batch_loss: 0.22205227613449097\n",
      "training: 79 batch 270 batch_loss: 0.22161170840263367\n",
      "training: 79 batch 271 batch_loss: 0.2216583490371704\n",
      "training: 79 batch 272 batch_loss: 0.21626484394073486\n",
      "training: 79 batch 273 batch_loss: 0.2214888632297516\n",
      "training: 79 batch 274 batch_loss: 0.22204339504241943\n",
      "training: 79 batch 275 batch_loss: 0.217323899269104\n",
      "training: 79 batch 276 batch_loss: 0.21874785423278809\n",
      "training: 79 batch 277 batch_loss: 0.2176138162612915\n",
      "training: 79 batch 278 batch_loss: 0.21857818961143494\n",
      "training: 79 batch 279 batch_loss: 0.22142291069030762\n",
      "training: 79 batch 280 batch_loss: 0.22007295489311218\n",
      "training: 79 batch 281 batch_loss: 0.21475735306739807\n",
      "training: 79 batch 282 batch_loss: 0.22281548380851746\n",
      "training: 79 batch 283 batch_loss: 0.21693581342697144\n",
      "training: 79 batch 284 batch_loss: 0.22038498520851135\n",
      "training: 79 batch 285 batch_loss: 0.22200635075569153\n",
      "training: 79 batch 286 batch_loss: 0.22265681624412537\n",
      "training: 79 batch 287 batch_loss: 0.21985405683517456\n",
      "training: 79 batch 288 batch_loss: 0.22359198331832886\n",
      "training: 79 batch 289 batch_loss: 0.220486581325531\n",
      "training: 79 batch 290 batch_loss: 0.2203679084777832\n",
      "training: 79 batch 291 batch_loss: 0.2190069556236267\n",
      "training: 79 batch 292 batch_loss: 0.2219940423965454\n",
      "training: 79 batch 293 batch_loss: 0.22045207023620605\n",
      "training: 79 batch 294 batch_loss: 0.2188996970653534\n",
      "training: 79 batch 295 batch_loss: 0.21434330940246582\n",
      "training: 79 batch 296 batch_loss: 0.22046858072280884\n",
      "training: 79 batch 297 batch_loss: 0.2185947597026825\n",
      "training: 79 batch 298 batch_loss: 0.2208445966243744\n",
      "training: 79 batch 299 batch_loss: 0.2174280285835266\n",
      "training: 79 batch 300 batch_loss: 0.21990624070167542\n",
      "training: 79 batch 301 batch_loss: 0.2218199372291565\n",
      "training: 79 batch 302 batch_loss: 0.21757963299751282\n",
      "training: 79 batch 303 batch_loss: 0.21962741017341614\n",
      "training: 79 batch 304 batch_loss: 0.2231607437133789\n",
      "training: 79 batch 305 batch_loss: 0.21386346220970154\n",
      "training: 79 batch 306 batch_loss: 0.2160302996635437\n",
      "training: 79 batch 307 batch_loss: 0.22159382700920105\n",
      "training: 79 batch 308 batch_loss: 0.21750903129577637\n",
      "training: 79 batch 309 batch_loss: 0.21775546669960022\n",
      "training: 79 batch 310 batch_loss: 0.21990469098091125\n",
      "training: 79 batch 311 batch_loss: 0.21803617477416992\n",
      "training: 79 batch 312 batch_loss: 0.218977153301239\n",
      "training: 79 batch 313 batch_loss: 0.22163403034210205\n",
      "training: 79 batch 314 batch_loss: 0.22153937816619873\n",
      "training: 79 batch 315 batch_loss: 0.22193679213523865\n",
      "training: 79 batch 316 batch_loss: 0.22172588109970093\n",
      "training: 79 batch 317 batch_loss: 0.22348719835281372\n",
      "training: 79 batch 318 batch_loss: 0.22015473246574402\n",
      "training: 79 batch 319 batch_loss: 0.21649375557899475\n",
      "training: 79 batch 320 batch_loss: 0.2220439612865448\n",
      "training: 79 batch 321 batch_loss: 0.22120410203933716\n",
      "training: 79 batch 322 batch_loss: 0.22092732787132263\n",
      "training: 79 batch 323 batch_loss: 0.21444356441497803\n",
      "training: 79 batch 324 batch_loss: 0.22273844480514526\n",
      "training: 79 batch 325 batch_loss: 0.220640629529953\n",
      "training: 79 batch 326 batch_loss: 0.2237001657485962\n",
      "training: 79 batch 327 batch_loss: 0.22161924839019775\n",
      "training: 79 batch 328 batch_loss: 0.2251853346824646\n",
      "training: 79 batch 329 batch_loss: 0.21415016055107117\n",
      "training: 79 batch 330 batch_loss: 0.22146311402320862\n",
      "training: 79 batch 331 batch_loss: 0.21965265274047852\n",
      "training: 79 batch 332 batch_loss: 0.2220383584499359\n",
      "training: 79 batch 333 batch_loss: 0.21887534856796265\n",
      "training: 79 batch 334 batch_loss: 0.2182270586490631\n",
      "training: 79 batch 335 batch_loss: 0.22008898854255676\n",
      "training: 79 batch 336 batch_loss: 0.21950006484985352\n",
      "training: 79 batch 337 batch_loss: 0.2199675440788269\n",
      "training: 79 batch 338 batch_loss: 0.22271397709846497\n",
      "training: 79 batch 339 batch_loss: 0.21805328130722046\n",
      "training: 79 batch 340 batch_loss: 0.21975594758987427\n",
      "training: 79 batch 341 batch_loss: 0.2224622666835785\n",
      "training: 79 batch 342 batch_loss: 0.22357600927352905\n",
      "training: 79 batch 343 batch_loss: 0.21721094846725464\n",
      "training: 79 batch 344 batch_loss: 0.2173047661781311\n",
      "training: 79 batch 345 batch_loss: 0.22045385837554932\n",
      "training: 79 batch 346 batch_loss: 0.2200559377670288\n",
      "training: 79 batch 347 batch_loss: 0.22652927041053772\n",
      "training: 79 batch 348 batch_loss: 0.21963301301002502\n",
      "training: 79 batch 349 batch_loss: 0.21970823407173157\n",
      "training: 79 batch 350 batch_loss: 0.2202017605304718\n",
      "training: 79 batch 351 batch_loss: 0.22028470039367676\n",
      "training: 79 batch 352 batch_loss: 0.21929237246513367\n",
      "training: 79 batch 353 batch_loss: 0.223355233669281\n",
      "training: 79 batch 354 batch_loss: 0.21913188695907593\n",
      "training: 79 batch 355 batch_loss: 0.2189030945301056\n",
      "training: 79 batch 356 batch_loss: 0.22145837545394897\n",
      "training: 79 batch 357 batch_loss: 0.22067397832870483\n",
      "training: 79 batch 358 batch_loss: 0.21854466199874878\n",
      "training: 79 batch 359 batch_loss: 0.22001367807388306\n",
      "training: 79 batch 360 batch_loss: 0.22352278232574463\n",
      "training: 79 batch 361 batch_loss: 0.22327658534049988\n",
      "training: 79 batch 362 batch_loss: 0.21685180068016052\n",
      "training: 79 batch 363 batch_loss: 0.22123992443084717\n",
      "training: 79 batch 364 batch_loss: 0.2197779417037964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 79 batch 365 batch_loss: 0.22015151381492615\n",
      "training: 79 batch 366 batch_loss: 0.2214638590812683\n",
      "training: 79 batch 367 batch_loss: 0.21673548221588135\n",
      "training: 79 batch 368 batch_loss: 0.21888679265975952\n",
      "training: 79 batch 369 batch_loss: 0.22202438116073608\n",
      "training: 79 batch 370 batch_loss: 0.22258049249649048\n",
      "training: 79 batch 371 batch_loss: 0.22097647190093994\n",
      "training: 79 batch 372 batch_loss: 0.2181464433670044\n",
      "training: 79 batch 373 batch_loss: 0.21888041496276855\n",
      "training: 79 batch 374 batch_loss: 0.22175395488739014\n",
      "training: 79 batch 375 batch_loss: 0.21820205450057983\n",
      "training: 79 batch 376 batch_loss: 0.2254173755645752\n",
      "training: 79 batch 377 batch_loss: 0.22223913669586182\n",
      "training: 79 batch 378 batch_loss: 0.22244009375572205\n",
      "training: 79 batch 379 batch_loss: 0.22323718667030334\n",
      "training: 79 batch 380 batch_loss: 0.22202974557876587\n",
      "training: 79 batch 381 batch_loss: 0.22090700268745422\n",
      "training: 79 batch 382 batch_loss: 0.21787217259407043\n",
      "training: 79 batch 383 batch_loss: 0.2260637879371643\n",
      "training: 79 batch 384 batch_loss: 0.21922573447227478\n",
      "training: 79 batch 385 batch_loss: 0.222896009683609\n",
      "training: 79 batch 386 batch_loss: 0.21990302205085754\n",
      "training: 79 batch 387 batch_loss: 0.21863144636154175\n",
      "training: 79 batch 388 batch_loss: 0.2224125862121582\n",
      "training: 79 batch 389 batch_loss: 0.221023291349411\n",
      "training: 79 batch 390 batch_loss: 0.21574807167053223\n",
      "training: 79 batch 391 batch_loss: 0.22134307026863098\n",
      "training: 79 batch 392 batch_loss: 0.21945276856422424\n",
      "training: 79 batch 393 batch_loss: 0.21752998232841492\n",
      "training: 79 batch 394 batch_loss: 0.2168392539024353\n",
      "training: 79 batch 395 batch_loss: 0.221868634223938\n",
      "training: 79 batch 396 batch_loss: 0.22243806719779968\n",
      "training: 79 batch 397 batch_loss: 0.22385039925575256\n",
      "training: 79 batch 398 batch_loss: 0.21939906477928162\n",
      "training: 79 batch 399 batch_loss: 0.21669089794158936\n",
      "training: 79 batch 400 batch_loss: 0.21752819418907166\n",
      "training: 79 batch 401 batch_loss: 0.21598494052886963\n",
      "training: 79 batch 402 batch_loss: 0.22129663825035095\n",
      "training: 79 batch 403 batch_loss: 0.2191685140132904\n",
      "training: 79 batch 404 batch_loss: 0.21489602327346802\n",
      "training: 79 batch 405 batch_loss: 0.2262885570526123\n",
      "training: 79 batch 406 batch_loss: 0.22049087285995483\n",
      "training: 79 batch 407 batch_loss: 0.2203335464000702\n",
      "training: 79 batch 408 batch_loss: 0.22389909625053406\n",
      "training: 79 batch 409 batch_loss: 0.2154560089111328\n",
      "training: 79 batch 410 batch_loss: 0.22243773937225342\n",
      "training: 79 batch 411 batch_loss: 0.22214075922966003\n",
      "training: 79 batch 412 batch_loss: 0.22287821769714355\n",
      "training: 79 batch 413 batch_loss: 0.21818730235099792\n",
      "training: 79 batch 414 batch_loss: 0.22512400150299072\n",
      "training: 79 batch 415 batch_loss: 0.22372391819953918\n",
      "training: 79 batch 416 batch_loss: 0.23113209009170532\n",
      "training: 79 batch 417 batch_loss: 0.22163018584251404\n",
      "training: 79 batch 418 batch_loss: 0.2239266037940979\n",
      "training: 79 batch 419 batch_loss: 0.21672186255455017\n",
      "training: 79 batch 420 batch_loss: 0.22214818000793457\n",
      "training: 79 batch 421 batch_loss: 0.2191617488861084\n",
      "training: 79 batch 422 batch_loss: 0.2210218608379364\n",
      "training: 79 batch 423 batch_loss: 0.2236137092113495\n",
      "training: 79 batch 424 batch_loss: 0.22177529335021973\n",
      "training: 79 batch 425 batch_loss: 0.22335851192474365\n",
      "training: 79 batch 426 batch_loss: 0.21999377012252808\n",
      "training: 79 batch 427 batch_loss: 0.22131884098052979\n",
      "training: 79 batch 428 batch_loss: 0.2256384789943695\n",
      "training: 79 batch 429 batch_loss: 0.21794378757476807\n",
      "training: 79 batch 430 batch_loss: 0.2199963629245758\n",
      "training: 79 batch 431 batch_loss: 0.2226477861404419\n",
      "training: 79 batch 432 batch_loss: 0.21938520669937134\n",
      "training: 79 batch 433 batch_loss: 0.21716907620429993\n",
      "training: 79 batch 434 batch_loss: 0.22152462601661682\n",
      "training: 79 batch 435 batch_loss: 0.21784928441047668\n",
      "training: 79 batch 436 batch_loss: 0.22479352355003357\n",
      "training: 79 batch 437 batch_loss: 0.2224806547164917\n",
      "training: 79 batch 438 batch_loss: 0.2214633822441101\n",
      "training: 79 batch 439 batch_loss: 0.2187645435333252\n",
      "training: 79 batch 440 batch_loss: 0.21966764330863953\n",
      "training: 79 batch 441 batch_loss: 0.2206871211528778\n",
      "training: 79 batch 442 batch_loss: 0.22674289345741272\n",
      "training: 79 batch 443 batch_loss: 0.218611478805542\n",
      "training: 79 batch 444 batch_loss: 0.22321397066116333\n",
      "training: 79 batch 445 batch_loss: 0.22081997990608215\n",
      "training: 79 batch 446 batch_loss: 0.21929898858070374\n",
      "training: 79 batch 447 batch_loss: 0.2210727334022522\n",
      "training: 79 batch 448 batch_loss: 0.22254550457000732\n",
      "training: 79 batch 449 batch_loss: 0.22929340600967407\n",
      "training: 79 batch 450 batch_loss: 0.22282364964485168\n",
      "training: 79 batch 451 batch_loss: 0.2227495014667511\n",
      "training: 79 batch 452 batch_loss: 0.2195691168308258\n",
      "training: 79 batch 453 batch_loss: 0.22094514966011047\n",
      "training: 79 batch 454 batch_loss: 0.22521620988845825\n",
      "training: 79 batch 455 batch_loss: 0.22281518578529358\n",
      "training: 79 batch 456 batch_loss: 0.21999123692512512\n",
      "training: 79 batch 457 batch_loss: 0.21463999152183533\n",
      "training: 79 batch 458 batch_loss: 0.22242271900177002\n",
      "training: 79 batch 459 batch_loss: 0.21940064430236816\n",
      "training: 79 batch 460 batch_loss: 0.21930572390556335\n",
      "training: 79 batch 461 batch_loss: 0.22132426500320435\n",
      "training: 79 batch 462 batch_loss: 0.21823427081108093\n",
      "training: 79 batch 463 batch_loss: 0.21388065814971924\n",
      "training: 79 batch 464 batch_loss: 0.2210301160812378\n",
      "training: 79 batch 465 batch_loss: 0.22301942110061646\n",
      "training: 79 batch 466 batch_loss: 0.22424548864364624\n",
      "training: 79 batch 467 batch_loss: 0.22218158841133118\n",
      "training: 79 batch 468 batch_loss: 0.2184753119945526\n",
      "training: 79 batch 469 batch_loss: 0.22271278500556946\n",
      "training: 79 batch 470 batch_loss: 0.22306841611862183\n",
      "training: 79 batch 471 batch_loss: 0.219292551279068\n",
      "training: 79 batch 472 batch_loss: 0.21655911207199097\n",
      "training: 79 batch 473 batch_loss: 0.22500073909759521\n",
      "training: 79 batch 474 batch_loss: 0.21991375088691711\n",
      "training: 79 batch 475 batch_loss: 0.21995970606803894\n",
      "training: 79 batch 476 batch_loss: 0.21979519724845886\n",
      "training: 79 batch 477 batch_loss: 0.22019141912460327\n",
      "training: 79 batch 478 batch_loss: 0.21757885813713074\n",
      "training: 79 batch 479 batch_loss: 0.21851542592048645\n",
      "training: 79 batch 480 batch_loss: 0.22172433137893677\n",
      "training: 79 batch 481 batch_loss: 0.22122636437416077\n",
      "training: 79 batch 482 batch_loss: 0.2177303433418274\n",
      "training: 79 batch 483 batch_loss: 0.22206684947013855\n",
      "training: 79 batch 484 batch_loss: 0.22022154927253723\n",
      "training: 79 batch 485 batch_loss: 0.21987321972846985\n",
      "training: 79 batch 486 batch_loss: 0.2209928035736084\n",
      "training: 79 batch 487 batch_loss: 0.22053733468055725\n",
      "training: 79 batch 488 batch_loss: 0.21884191036224365\n",
      "training: 79 batch 489 batch_loss: 0.21843361854553223\n",
      "training: 79 batch 490 batch_loss: 0.21629467606544495\n",
      "training: 79 batch 491 batch_loss: 0.22261929512023926\n",
      "training: 79 batch 492 batch_loss: 0.22032493352890015\n",
      "training: 79 batch 493 batch_loss: 0.22104349732398987\n",
      "training: 79 batch 494 batch_loss: 0.22086524963378906\n",
      "training: 79 batch 495 batch_loss: 0.22088360786437988\n",
      "training: 79 batch 496 batch_loss: 0.22095391154289246\n",
      "training: 79 batch 497 batch_loss: 0.2259443998336792\n",
      "training: 79 batch 498 batch_loss: 0.21760135889053345\n",
      "training: 79 batch 499 batch_loss: 0.22171151638031006\n",
      "training: 79 batch 500 batch_loss: 0.21974411606788635\n",
      "training: 79 batch 501 batch_loss: 0.22045069932937622\n",
      "training: 79 batch 502 batch_loss: 0.2200741171836853\n",
      "training: 79 batch 503 batch_loss: 0.21960210800170898\n",
      "training: 79 batch 504 batch_loss: 0.22527122497558594\n",
      "training: 79 batch 505 batch_loss: 0.2230657935142517\n",
      "training: 79 batch 506 batch_loss: 0.2190270721912384\n",
      "training: 79 batch 507 batch_loss: 0.22062787413597107\n",
      "training: 79 batch 508 batch_loss: 0.22084209322929382\n",
      "training: 79 batch 509 batch_loss: 0.21649909019470215\n",
      "training: 79 batch 510 batch_loss: 0.21996259689331055\n",
      "training: 79 batch 511 batch_loss: 0.2218991219997406\n",
      "training: 79 batch 512 batch_loss: 0.22116467356681824\n",
      "training: 79 batch 513 batch_loss: 0.2192506492137909\n",
      "training: 79 batch 514 batch_loss: 0.22054272890090942\n",
      "training: 79 batch 515 batch_loss: 0.2209424078464508\n",
      "training: 79 batch 516 batch_loss: 0.22152256965637207\n",
      "training: 79 batch 517 batch_loss: 0.22220155596733093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 79 batch 518 batch_loss: 0.22209662199020386\n",
      "training: 79 batch 519 batch_loss: 0.22162646055221558\n",
      "training: 79 batch 520 batch_loss: 0.21734249591827393\n",
      "training: 79 batch 521 batch_loss: 0.2216401994228363\n",
      "training: 79 batch 522 batch_loss: 0.2179100215435028\n",
      "training: 79 batch 523 batch_loss: 0.2244212031364441\n",
      "training: 79 batch 524 batch_loss: 0.2223092019557953\n",
      "training: 79 batch 525 batch_loss: 0.2254282534122467\n",
      "training: 79 batch 526 batch_loss: 0.2205708920955658\n",
      "training: 79 batch 527 batch_loss: 0.22138580679893494\n",
      "training: 79 batch 528 batch_loss: 0.22010290622711182\n",
      "training: 79 batch 529 batch_loss: 0.22300761938095093\n",
      "training: 79 batch 530 batch_loss: 0.22324711084365845\n",
      "training: 79 batch 531 batch_loss: 0.2197754979133606\n",
      "training: 79 batch 532 batch_loss: 0.22057947516441345\n",
      "training: 79 batch 533 batch_loss: 0.21998339891433716\n",
      "training: 79 batch 534 batch_loss: 0.21933603286743164\n",
      "training: 79 batch 535 batch_loss: 0.22006618976593018\n",
      "training: 79 batch 536 batch_loss: 0.22070404887199402\n",
      "training: 79 batch 537 batch_loss: 0.22265860438346863\n",
      "training: 79 batch 538 batch_loss: 0.2203170657157898\n",
      "training: 79 batch 539 batch_loss: 0.21574434638023376\n",
      "training: 79 batch 540 batch_loss: 0.22258496284484863\n",
      "training: 79 batch 541 batch_loss: 0.21599191427230835\n",
      "training: 79 batch 542 batch_loss: 0.21858292818069458\n",
      "training: 79 batch 543 batch_loss: 0.21983832120895386\n",
      "training: 79 batch 544 batch_loss: 0.22248801589012146\n",
      "training: 79 batch 545 batch_loss: 0.2198876440525055\n",
      "training: 79 batch 546 batch_loss: 0.22018858790397644\n",
      "training: 79 batch 547 batch_loss: 0.22177869081497192\n",
      "training: 79 batch 548 batch_loss: 0.2200077474117279\n",
      "training: 79 batch 549 batch_loss: 0.21960735321044922\n",
      "training: 79 batch 550 batch_loss: 0.22000890970230103\n",
      "training: 79 batch 551 batch_loss: 0.2200639247894287\n",
      "training: 79 batch 552 batch_loss: 0.2175009846687317\n",
      "training: 79 batch 553 batch_loss: 0.21772164106369019\n",
      "training: 79 batch 554 batch_loss: 0.21532195806503296\n",
      "training: 79 batch 555 batch_loss: 0.2194296419620514\n",
      "training: 79 batch 556 batch_loss: 0.22121486067771912\n",
      "training: 79 batch 557 batch_loss: 0.22330507636070251\n",
      "training: 79 batch 558 batch_loss: 0.21716994047164917\n",
      "training: 79 batch 559 batch_loss: 0.2197066843509674\n",
      "training: 79 batch 560 batch_loss: 0.22319933772087097\n",
      "training: 79 batch 561 batch_loss: 0.2240201234817505\n",
      "training: 79 batch 562 batch_loss: 0.21772116422653198\n",
      "training: 79 batch 563 batch_loss: 0.2226233184337616\n",
      "training: 79 batch 564 batch_loss: 0.22366777062416077\n",
      "training: 79 batch 565 batch_loss: 0.216008722782135\n",
      "training: 79 batch 566 batch_loss: 0.219816654920578\n",
      "training: 79 batch 567 batch_loss: 0.22150778770446777\n",
      "training: 79 batch 568 batch_loss: 0.22262689471244812\n",
      "training: 79 batch 569 batch_loss: 0.21848955750465393\n",
      "training: 79 batch 570 batch_loss: 0.22232073545455933\n",
      "training: 79 batch 571 batch_loss: 0.22240400314331055\n",
      "training: 79 batch 572 batch_loss: 0.22178307175636292\n",
      "training: 79 batch 573 batch_loss: 0.2226964235305786\n",
      "training: 79 batch 574 batch_loss: 0.22176635265350342\n",
      "training: 79 batch 575 batch_loss: 0.22092050313949585\n",
      "training: 79 batch 576 batch_loss: 0.21967115998268127\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 79, Hit Ratio:0.03378160050367597 | Precision:0.049842720927946525 | Recall:0.06576628292292858 | NDCG:0.06485163778297853\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 80 batch 0 batch_loss: 0.21774855256080627\n",
      "training: 80 batch 1 batch_loss: 0.21597135066986084\n",
      "training: 80 batch 2 batch_loss: 0.22149977087974548\n",
      "training: 80 batch 3 batch_loss: 0.2194676697254181\n",
      "training: 80 batch 4 batch_loss: 0.21887511014938354\n",
      "training: 80 batch 5 batch_loss: 0.2199811041355133\n",
      "training: 80 batch 6 batch_loss: 0.21795055270195007\n",
      "training: 80 batch 7 batch_loss: 0.21922007203102112\n",
      "training: 80 batch 8 batch_loss: 0.2176360785961151\n",
      "training: 80 batch 9 batch_loss: 0.2218528389930725\n",
      "training: 80 batch 10 batch_loss: 0.2198350429534912\n",
      "training: 80 batch 11 batch_loss: 0.22188469767570496\n",
      "training: 80 batch 12 batch_loss: 0.22001796960830688\n",
      "training: 80 batch 13 batch_loss: 0.2189686894416809\n",
      "training: 80 batch 14 batch_loss: 0.21442687511444092\n",
      "training: 80 batch 15 batch_loss: 0.22041934728622437\n",
      "training: 80 batch 16 batch_loss: 0.2228451669216156\n",
      "training: 80 batch 17 batch_loss: 0.2148612141609192\n",
      "training: 80 batch 18 batch_loss: 0.21614402532577515\n",
      "training: 80 batch 19 batch_loss: 0.22339898347854614\n",
      "training: 80 batch 20 batch_loss: 0.2193869948387146\n",
      "training: 80 batch 21 batch_loss: 0.22220471501350403\n",
      "training: 80 batch 22 batch_loss: 0.21756470203399658\n",
      "training: 80 batch 23 batch_loss: 0.2198093831539154\n",
      "training: 80 batch 24 batch_loss: 0.22034204006195068\n",
      "training: 80 batch 25 batch_loss: 0.22357013821601868\n",
      "training: 80 batch 26 batch_loss: 0.21624994277954102\n",
      "training: 80 batch 27 batch_loss: 0.21702522039413452\n",
      "training: 80 batch 28 batch_loss: 0.21790862083435059\n",
      "training: 80 batch 29 batch_loss: 0.2220049798488617\n",
      "training: 80 batch 30 batch_loss: 0.21859878301620483\n",
      "training: 80 batch 31 batch_loss: 0.2221716046333313\n",
      "training: 80 batch 32 batch_loss: 0.21794047951698303\n",
      "training: 80 batch 33 batch_loss: 0.22356122732162476\n",
      "training: 80 batch 34 batch_loss: 0.21830421686172485\n",
      "training: 80 batch 35 batch_loss: 0.21892565488815308\n",
      "training: 80 batch 36 batch_loss: 0.21691787242889404\n",
      "training: 80 batch 37 batch_loss: 0.22196242213249207\n",
      "training: 80 batch 38 batch_loss: 0.22150254249572754\n",
      "training: 80 batch 39 batch_loss: 0.21647986769676208\n",
      "training: 80 batch 40 batch_loss: 0.22146424651145935\n",
      "training: 80 batch 41 batch_loss: 0.21684199571609497\n",
      "training: 80 batch 42 batch_loss: 0.21684396266937256\n",
      "training: 80 batch 43 batch_loss: 0.22365707159042358\n",
      "training: 80 batch 44 batch_loss: 0.22067439556121826\n",
      "training: 80 batch 45 batch_loss: 0.22081109881401062\n",
      "training: 80 batch 46 batch_loss: 0.21707051992416382\n",
      "training: 80 batch 47 batch_loss: 0.21970990300178528\n",
      "training: 80 batch 48 batch_loss: 0.22250515222549438\n",
      "training: 80 batch 49 batch_loss: 0.22166943550109863\n",
      "training: 80 batch 50 batch_loss: 0.22102758288383484\n",
      "training: 80 batch 51 batch_loss: 0.21836182475090027\n",
      "training: 80 batch 52 batch_loss: 0.2172257900238037\n",
      "training: 80 batch 53 batch_loss: 0.21291276812553406\n",
      "training: 80 batch 54 batch_loss: 0.2240520417690277\n",
      "training: 80 batch 55 batch_loss: 0.216201514005661\n",
      "training: 80 batch 56 batch_loss: 0.21839237213134766\n",
      "training: 80 batch 57 batch_loss: 0.21854224801063538\n",
      "training: 80 batch 58 batch_loss: 0.2156282663345337\n",
      "training: 80 batch 59 batch_loss: 0.22230538725852966\n",
      "training: 80 batch 60 batch_loss: 0.22031402587890625\n",
      "training: 80 batch 61 batch_loss: 0.22355684638023376\n",
      "training: 80 batch 62 batch_loss: 0.21841827034950256\n",
      "training: 80 batch 63 batch_loss: 0.21912172436714172\n",
      "training: 80 batch 64 batch_loss: 0.21602997183799744\n",
      "training: 80 batch 65 batch_loss: 0.2172742486000061\n",
      "training: 80 batch 66 batch_loss: 0.21952682733535767\n",
      "training: 80 batch 67 batch_loss: 0.21922537684440613\n",
      "training: 80 batch 68 batch_loss: 0.21991461515426636\n",
      "training: 80 batch 69 batch_loss: 0.22052732110023499\n",
      "training: 80 batch 70 batch_loss: 0.21959081292152405\n",
      "training: 80 batch 71 batch_loss: 0.22288817167282104\n",
      "training: 80 batch 72 batch_loss: 0.218898743391037\n",
      "training: 80 batch 73 batch_loss: 0.21930450201034546\n",
      "training: 80 batch 74 batch_loss: 0.21616250276565552\n",
      "training: 80 batch 75 batch_loss: 0.2211368978023529\n",
      "training: 80 batch 76 batch_loss: 0.21952074766159058\n",
      "training: 80 batch 77 batch_loss: 0.2180619239807129\n",
      "training: 80 batch 78 batch_loss: 0.21864518523216248\n",
      "training: 80 batch 79 batch_loss: 0.22247260808944702\n",
      "training: 80 batch 80 batch_loss: 0.21841195225715637\n",
      "training: 80 batch 81 batch_loss: 0.2226978838443756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 80 batch 82 batch_loss: 0.22040662169456482\n",
      "training: 80 batch 83 batch_loss: 0.22232574224472046\n",
      "training: 80 batch 84 batch_loss: 0.21739807724952698\n",
      "training: 80 batch 85 batch_loss: 0.21760448813438416\n",
      "training: 80 batch 86 batch_loss: 0.2203168272972107\n",
      "training: 80 batch 87 batch_loss: 0.22126123309135437\n",
      "training: 80 batch 88 batch_loss: 0.22395309805870056\n",
      "training: 80 batch 89 batch_loss: 0.22380858659744263\n",
      "training: 80 batch 90 batch_loss: 0.21911263465881348\n",
      "training: 80 batch 91 batch_loss: 0.21615523099899292\n",
      "training: 80 batch 92 batch_loss: 0.21621543169021606\n",
      "training: 80 batch 93 batch_loss: 0.2193400263786316\n",
      "training: 80 batch 94 batch_loss: 0.22108891606330872\n",
      "training: 80 batch 95 batch_loss: 0.21865913271903992\n",
      "training: 80 batch 96 batch_loss: 0.21788263320922852\n",
      "training: 80 batch 97 batch_loss: 0.22136786580085754\n",
      "training: 80 batch 98 batch_loss: 0.2206622064113617\n",
      "training: 80 batch 99 batch_loss: 0.21936184167861938\n",
      "training: 80 batch 100 batch_loss: 0.2218865156173706\n",
      "training: 80 batch 101 batch_loss: 0.22080975770950317\n",
      "training: 80 batch 102 batch_loss: 0.22384697198867798\n",
      "training: 80 batch 103 batch_loss: 0.21608379483222961\n",
      "training: 80 batch 104 batch_loss: 0.2216334342956543\n",
      "training: 80 batch 105 batch_loss: 0.21845531463623047\n",
      "training: 80 batch 106 batch_loss: 0.22133567929267883\n",
      "training: 80 batch 107 batch_loss: 0.21893489360809326\n",
      "training: 80 batch 108 batch_loss: 0.2195987105369568\n",
      "training: 80 batch 109 batch_loss: 0.221676766872406\n",
      "training: 80 batch 110 batch_loss: 0.21468982100486755\n",
      "training: 80 batch 111 batch_loss: 0.2220400869846344\n",
      "training: 80 batch 112 batch_loss: 0.221779465675354\n",
      "training: 80 batch 113 batch_loss: 0.21858301758766174\n",
      "training: 80 batch 114 batch_loss: 0.21546590328216553\n",
      "training: 80 batch 115 batch_loss: 0.21836504340171814\n",
      "training: 80 batch 116 batch_loss: 0.2216067612171173\n",
      "training: 80 batch 117 batch_loss: 0.21841803193092346\n",
      "training: 80 batch 118 batch_loss: 0.22057297825813293\n",
      "training: 80 batch 119 batch_loss: 0.21997442841529846\n",
      "training: 80 batch 120 batch_loss: 0.21853256225585938\n",
      "training: 80 batch 121 batch_loss: 0.22324803471565247\n",
      "training: 80 batch 122 batch_loss: 0.21887749433517456\n",
      "training: 80 batch 123 batch_loss: 0.21894600987434387\n",
      "training: 80 batch 124 batch_loss: 0.22226673364639282\n",
      "training: 80 batch 125 batch_loss: 0.22343721985816956\n",
      "training: 80 batch 126 batch_loss: 0.2203693687915802\n",
      "training: 80 batch 127 batch_loss: 0.21872994303703308\n",
      "training: 80 batch 128 batch_loss: 0.22160449624061584\n",
      "training: 80 batch 129 batch_loss: 0.22115927934646606\n",
      "training: 80 batch 130 batch_loss: 0.21285244822502136\n",
      "training: 80 batch 131 batch_loss: 0.22103747725486755\n",
      "training: 80 batch 132 batch_loss: 0.21606838703155518\n",
      "training: 80 batch 133 batch_loss: 0.2254089117050171\n",
      "training: 80 batch 134 batch_loss: 0.22153106331825256\n",
      "training: 80 batch 135 batch_loss: 0.22281324863433838\n",
      "training: 80 batch 136 batch_loss: 0.2193400263786316\n",
      "training: 80 batch 137 batch_loss: 0.22319427132606506\n",
      "training: 80 batch 138 batch_loss: 0.22359296679496765\n",
      "training: 80 batch 139 batch_loss: 0.21849948167800903\n",
      "training: 80 batch 140 batch_loss: 0.219772607088089\n",
      "training: 80 batch 141 batch_loss: 0.22178658843040466\n",
      "training: 80 batch 142 batch_loss: 0.21642035245895386\n",
      "training: 80 batch 143 batch_loss: 0.21812471747398376\n",
      "training: 80 batch 144 batch_loss: 0.21702516078948975\n",
      "training: 80 batch 145 batch_loss: 0.2184249460697174\n",
      "training: 80 batch 146 batch_loss: 0.2240016758441925\n",
      "training: 80 batch 147 batch_loss: 0.22433432936668396\n",
      "training: 80 batch 148 batch_loss: 0.21962296962738037\n",
      "training: 80 batch 149 batch_loss: 0.21842226386070251\n",
      "training: 80 batch 150 batch_loss: 0.22058814764022827\n",
      "training: 80 batch 151 batch_loss: 0.2170172929763794\n",
      "training: 80 batch 152 batch_loss: 0.2230062484741211\n",
      "training: 80 batch 153 batch_loss: 0.2184542417526245\n",
      "training: 80 batch 154 batch_loss: 0.2197447419166565\n",
      "training: 80 batch 155 batch_loss: 0.2201354205608368\n",
      "training: 80 batch 156 batch_loss: 0.21928000450134277\n",
      "training: 80 batch 157 batch_loss: 0.21839195489883423\n",
      "training: 80 batch 158 batch_loss: 0.21733057498931885\n",
      "training: 80 batch 159 batch_loss: 0.2221546471118927\n",
      "training: 80 batch 160 batch_loss: 0.22069653868675232\n",
      "training: 80 batch 161 batch_loss: 0.2216910421848297\n",
      "training: 80 batch 162 batch_loss: 0.22266799211502075\n",
      "training: 80 batch 163 batch_loss: 0.21605736017227173\n",
      "training: 80 batch 164 batch_loss: 0.2208915650844574\n",
      "training: 80 batch 165 batch_loss: 0.22320371866226196\n",
      "training: 80 batch 166 batch_loss: 0.2197543978691101\n",
      "training: 80 batch 167 batch_loss: 0.22386708855628967\n",
      "training: 80 batch 168 batch_loss: 0.21809333562850952\n",
      "training: 80 batch 169 batch_loss: 0.22137302160263062\n",
      "training: 80 batch 170 batch_loss: 0.2201027274131775\n",
      "training: 80 batch 171 batch_loss: 0.22024551033973694\n",
      "training: 80 batch 172 batch_loss: 0.22111797332763672\n",
      "training: 80 batch 173 batch_loss: 0.21731126308441162\n",
      "training: 80 batch 174 batch_loss: 0.22022858262062073\n",
      "training: 80 batch 175 batch_loss: 0.2249375581741333\n",
      "training: 80 batch 176 batch_loss: 0.22148895263671875\n",
      "training: 80 batch 177 batch_loss: 0.21753334999084473\n",
      "training: 80 batch 178 batch_loss: 0.22154909372329712\n",
      "training: 80 batch 179 batch_loss: 0.22030487656593323\n",
      "training: 80 batch 180 batch_loss: 0.2212752103805542\n",
      "training: 80 batch 181 batch_loss: 0.22010958194732666\n",
      "training: 80 batch 182 batch_loss: 0.22262495756149292\n",
      "training: 80 batch 183 batch_loss: 0.21658533811569214\n",
      "training: 80 batch 184 batch_loss: 0.22131752967834473\n",
      "training: 80 batch 185 batch_loss: 0.22166195511817932\n",
      "training: 80 batch 186 batch_loss: 0.22300651669502258\n",
      "training: 80 batch 187 batch_loss: 0.22024109959602356\n",
      "training: 80 batch 188 batch_loss: 0.21898561716079712\n",
      "training: 80 batch 189 batch_loss: 0.22056645154953003\n",
      "training: 80 batch 190 batch_loss: 0.2218751609325409\n",
      "training: 80 batch 191 batch_loss: 0.21925294399261475\n",
      "training: 80 batch 192 batch_loss: 0.22112873196601868\n",
      "training: 80 batch 193 batch_loss: 0.2201339304447174\n",
      "training: 80 batch 194 batch_loss: 0.2157709002494812\n",
      "training: 80 batch 195 batch_loss: 0.22145426273345947\n",
      "training: 80 batch 196 batch_loss: 0.21712082624435425\n",
      "training: 80 batch 197 batch_loss: 0.22033631801605225\n",
      "training: 80 batch 198 batch_loss: 0.22265642881393433\n",
      "training: 80 batch 199 batch_loss: 0.22076430916786194\n",
      "training: 80 batch 200 batch_loss: 0.22191983461380005\n",
      "training: 80 batch 201 batch_loss: 0.21771392226219177\n",
      "training: 80 batch 202 batch_loss: 0.21940737962722778\n",
      "training: 80 batch 203 batch_loss: 0.222663015127182\n",
      "training: 80 batch 204 batch_loss: 0.2219555675983429\n",
      "training: 80 batch 205 batch_loss: 0.22206398844718933\n",
      "training: 80 batch 206 batch_loss: 0.22505754232406616\n",
      "training: 80 batch 207 batch_loss: 0.21903645992279053\n",
      "training: 80 batch 208 batch_loss: 0.2204272449016571\n",
      "training: 80 batch 209 batch_loss: 0.21706223487854004\n",
      "training: 80 batch 210 batch_loss: 0.219920814037323\n",
      "training: 80 batch 211 batch_loss: 0.22138270735740662\n",
      "training: 80 batch 212 batch_loss: 0.22203779220581055\n",
      "training: 80 batch 213 batch_loss: 0.2213832139968872\n",
      "training: 80 batch 214 batch_loss: 0.22363823652267456\n",
      "training: 80 batch 215 batch_loss: 0.21932369470596313\n",
      "training: 80 batch 216 batch_loss: 0.22395867109298706\n",
      "training: 80 batch 217 batch_loss: 0.22199854254722595\n",
      "training: 80 batch 218 batch_loss: 0.22301119565963745\n",
      "training: 80 batch 219 batch_loss: 0.21586987376213074\n",
      "training: 80 batch 220 batch_loss: 0.2232060730457306\n",
      "training: 80 batch 221 batch_loss: 0.2174803912639618\n",
      "training: 80 batch 222 batch_loss: 0.2211896777153015\n",
      "training: 80 batch 223 batch_loss: 0.22208642959594727\n",
      "training: 80 batch 224 batch_loss: 0.22076866030693054\n",
      "training: 80 batch 225 batch_loss: 0.22489333152770996\n",
      "training: 80 batch 226 batch_loss: 0.22265377640724182\n",
      "training: 80 batch 227 batch_loss: 0.21804362535476685\n",
      "training: 80 batch 228 batch_loss: 0.21957695484161377\n",
      "training: 80 batch 229 batch_loss: 0.2224738895893097\n",
      "training: 80 batch 230 batch_loss: 0.21647462248802185\n",
      "training: 80 batch 231 batch_loss: 0.22162893414497375\n",
      "training: 80 batch 232 batch_loss: 0.21810108423233032\n",
      "training: 80 batch 233 batch_loss: 0.22091853618621826\n",
      "training: 80 batch 234 batch_loss: 0.22144988179206848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 80 batch 235 batch_loss: 0.2189011573791504\n",
      "training: 80 batch 236 batch_loss: 0.2238175868988037\n",
      "training: 80 batch 237 batch_loss: 0.21735185384750366\n",
      "training: 80 batch 238 batch_loss: 0.22030743956565857\n",
      "training: 80 batch 239 batch_loss: 0.219582200050354\n",
      "training: 80 batch 240 batch_loss: 0.21751442551612854\n",
      "training: 80 batch 241 batch_loss: 0.218600332736969\n",
      "training: 80 batch 242 batch_loss: 0.21964570879936218\n",
      "training: 80 batch 243 batch_loss: 0.21560803055763245\n",
      "training: 80 batch 244 batch_loss: 0.2213001251220703\n",
      "training: 80 batch 245 batch_loss: 0.22095385193824768\n",
      "training: 80 batch 246 batch_loss: 0.21975955367088318\n",
      "training: 80 batch 247 batch_loss: 0.2223263382911682\n",
      "training: 80 batch 248 batch_loss: 0.226596862077713\n",
      "training: 80 batch 249 batch_loss: 0.22114071249961853\n",
      "training: 80 batch 250 batch_loss: 0.22242149710655212\n",
      "training: 80 batch 251 batch_loss: 0.21763992309570312\n",
      "training: 80 batch 252 batch_loss: 0.2213650941848755\n",
      "training: 80 batch 253 batch_loss: 0.22503355145454407\n",
      "training: 80 batch 254 batch_loss: 0.2225663959980011\n",
      "training: 80 batch 255 batch_loss: 0.22195512056350708\n",
      "training: 80 batch 256 batch_loss: 0.21902716159820557\n",
      "training: 80 batch 257 batch_loss: 0.2206106185913086\n",
      "training: 80 batch 258 batch_loss: 0.22137689590454102\n",
      "training: 80 batch 259 batch_loss: 0.2174798846244812\n",
      "training: 80 batch 260 batch_loss: 0.22098442912101746\n",
      "training: 80 batch 261 batch_loss: 0.22284653782844543\n",
      "training: 80 batch 262 batch_loss: 0.2217598557472229\n",
      "training: 80 batch 263 batch_loss: 0.21970093250274658\n",
      "training: 80 batch 264 batch_loss: 0.21980759501457214\n",
      "training: 80 batch 265 batch_loss: 0.22127962112426758\n",
      "training: 80 batch 266 batch_loss: 0.21755540370941162\n",
      "training: 80 batch 267 batch_loss: 0.22437933087348938\n",
      "training: 80 batch 268 batch_loss: 0.21632200479507446\n",
      "training: 80 batch 269 batch_loss: 0.2170161008834839\n",
      "training: 80 batch 270 batch_loss: 0.22219714522361755\n",
      "training: 80 batch 271 batch_loss: 0.22016549110412598\n",
      "training: 80 batch 272 batch_loss: 0.21919915080070496\n",
      "training: 80 batch 273 batch_loss: 0.2169896960258484\n",
      "training: 80 batch 274 batch_loss: 0.22070413827896118\n",
      "training: 80 batch 275 batch_loss: 0.2235371172428131\n",
      "training: 80 batch 276 batch_loss: 0.21723642945289612\n",
      "training: 80 batch 277 batch_loss: 0.22154831886291504\n",
      "training: 80 batch 278 batch_loss: 0.21650540828704834\n",
      "training: 80 batch 279 batch_loss: 0.2208128571510315\n",
      "training: 80 batch 280 batch_loss: 0.22054049372673035\n",
      "training: 80 batch 281 batch_loss: 0.22232910990715027\n",
      "training: 80 batch 282 batch_loss: 0.21820038557052612\n",
      "training: 80 batch 283 batch_loss: 0.2185402810573578\n",
      "training: 80 batch 284 batch_loss: 0.22172126173973083\n",
      "training: 80 batch 285 batch_loss: 0.21804580092430115\n",
      "training: 80 batch 286 batch_loss: 0.2207651138305664\n",
      "training: 80 batch 287 batch_loss: 0.2221086621284485\n",
      "training: 80 batch 288 batch_loss: 0.21816420555114746\n",
      "training: 80 batch 289 batch_loss: 0.22250288724899292\n",
      "training: 80 batch 290 batch_loss: 0.21735894680023193\n",
      "training: 80 batch 291 batch_loss: 0.2182418704032898\n",
      "training: 80 batch 292 batch_loss: 0.21730446815490723\n",
      "training: 80 batch 293 batch_loss: 0.22465813159942627\n",
      "training: 80 batch 294 batch_loss: 0.2230505347251892\n",
      "training: 80 batch 295 batch_loss: 0.2252994179725647\n",
      "training: 80 batch 296 batch_loss: 0.22069191932678223\n",
      "training: 80 batch 297 batch_loss: 0.22085046768188477\n",
      "training: 80 batch 298 batch_loss: 0.2213936746120453\n",
      "training: 80 batch 299 batch_loss: 0.2229389250278473\n",
      "training: 80 batch 300 batch_loss: 0.21680223941802979\n",
      "training: 80 batch 301 batch_loss: 0.21748113632202148\n",
      "training: 80 batch 302 batch_loss: 0.22084516286849976\n",
      "training: 80 batch 303 batch_loss: 0.22303611040115356\n",
      "training: 80 batch 304 batch_loss: 0.22431567311286926\n",
      "training: 80 batch 305 batch_loss: 0.22098466753959656\n",
      "training: 80 batch 306 batch_loss: 0.2205999791622162\n",
      "training: 80 batch 307 batch_loss: 0.22039520740509033\n",
      "training: 80 batch 308 batch_loss: 0.22175508737564087\n",
      "training: 80 batch 309 batch_loss: 0.22199144959449768\n",
      "training: 80 batch 310 batch_loss: 0.22192975878715515\n",
      "training: 80 batch 311 batch_loss: 0.22156202793121338\n",
      "training: 80 batch 312 batch_loss: 0.22272124886512756\n",
      "training: 80 batch 313 batch_loss: 0.22600457072257996\n",
      "training: 80 batch 314 batch_loss: 0.22232332825660706\n",
      "training: 80 batch 315 batch_loss: 0.22038504481315613\n",
      "training: 80 batch 316 batch_loss: 0.21771258115768433\n",
      "training: 80 batch 317 batch_loss: 0.2237032949924469\n",
      "training: 80 batch 318 batch_loss: 0.223284512758255\n",
      "training: 80 batch 319 batch_loss: 0.21902936697006226\n",
      "training: 80 batch 320 batch_loss: 0.22136425971984863\n",
      "training: 80 batch 321 batch_loss: 0.22390279173851013\n",
      "training: 80 batch 322 batch_loss: 0.2212696373462677\n",
      "training: 80 batch 323 batch_loss: 0.22240418195724487\n",
      "training: 80 batch 324 batch_loss: 0.22236034274101257\n",
      "training: 80 batch 325 batch_loss: 0.2188681960105896\n",
      "training: 80 batch 326 batch_loss: 0.21838045120239258\n",
      "training: 80 batch 327 batch_loss: 0.22312426567077637\n",
      "training: 80 batch 328 batch_loss: 0.2216678261756897\n",
      "training: 80 batch 329 batch_loss: 0.2232661247253418\n",
      "training: 80 batch 330 batch_loss: 0.22486594319343567\n",
      "training: 80 batch 331 batch_loss: 0.21879148483276367\n",
      "training: 80 batch 332 batch_loss: 0.22254234552383423\n",
      "training: 80 batch 333 batch_loss: 0.22332137823104858\n",
      "training: 80 batch 334 batch_loss: 0.2183629870414734\n",
      "training: 80 batch 335 batch_loss: 0.22212854027748108\n",
      "training: 80 batch 336 batch_loss: 0.21959933638572693\n",
      "training: 80 batch 337 batch_loss: 0.22159424424171448\n",
      "training: 80 batch 338 batch_loss: 0.22405007481575012\n",
      "training: 80 batch 339 batch_loss: 0.2220231294631958\n",
      "training: 80 batch 340 batch_loss: 0.22255465388298035\n",
      "training: 80 batch 341 batch_loss: 0.2164689004421234\n",
      "training: 80 batch 342 batch_loss: 0.220004141330719\n",
      "training: 80 batch 343 batch_loss: 0.223635733127594\n",
      "training: 80 batch 344 batch_loss: 0.2194507122039795\n",
      "training: 80 batch 345 batch_loss: 0.21837610006332397\n",
      "training: 80 batch 346 batch_loss: 0.22428780794143677\n",
      "training: 80 batch 347 batch_loss: 0.2195071578025818\n",
      "training: 80 batch 348 batch_loss: 0.2191755175590515\n",
      "training: 80 batch 349 batch_loss: 0.21810293197631836\n",
      "training: 80 batch 350 batch_loss: 0.22200119495391846\n",
      "training: 80 batch 351 batch_loss: 0.2222272753715515\n",
      "training: 80 batch 352 batch_loss: 0.22167283296585083\n",
      "training: 80 batch 353 batch_loss: 0.2220466434955597\n",
      "training: 80 batch 354 batch_loss: 0.2186739444732666\n",
      "training: 80 batch 355 batch_loss: 0.2160334587097168\n",
      "training: 80 batch 356 batch_loss: 0.2233980894088745\n",
      "training: 80 batch 357 batch_loss: 0.22174358367919922\n",
      "training: 80 batch 358 batch_loss: 0.22495928406715393\n",
      "training: 80 batch 359 batch_loss: 0.2158302664756775\n",
      "training: 80 batch 360 batch_loss: 0.21849623322486877\n",
      "training: 80 batch 361 batch_loss: 0.22274985909461975\n",
      "training: 80 batch 362 batch_loss: 0.22254905104637146\n",
      "training: 80 batch 363 batch_loss: 0.21774452924728394\n",
      "training: 80 batch 364 batch_loss: 0.22480005025863647\n",
      "training: 80 batch 365 batch_loss: 0.21813350915908813\n",
      "training: 80 batch 366 batch_loss: 0.2233538031578064\n",
      "training: 80 batch 367 batch_loss: 0.21761548519134521\n",
      "training: 80 batch 368 batch_loss: 0.22689494490623474\n",
      "training: 80 batch 369 batch_loss: 0.22278088331222534\n",
      "training: 80 batch 370 batch_loss: 0.21784758567810059\n",
      "training: 80 batch 371 batch_loss: 0.22225773334503174\n",
      "training: 80 batch 372 batch_loss: 0.22328591346740723\n",
      "training: 80 batch 373 batch_loss: 0.22021573781967163\n",
      "training: 80 batch 374 batch_loss: 0.22182613611221313\n",
      "training: 80 batch 375 batch_loss: 0.2261987328529358\n",
      "training: 80 batch 376 batch_loss: 0.21774756908416748\n",
      "training: 80 batch 377 batch_loss: 0.2244386076927185\n",
      "training: 80 batch 378 batch_loss: 0.21890929341316223\n",
      "training: 80 batch 379 batch_loss: 0.22080984711647034\n",
      "training: 80 batch 380 batch_loss: 0.22610312700271606\n",
      "training: 80 batch 381 batch_loss: 0.22370871901512146\n",
      "training: 80 batch 382 batch_loss: 0.21713531017303467\n",
      "training: 80 batch 383 batch_loss: 0.21996855735778809\n",
      "training: 80 batch 384 batch_loss: 0.22025829553604126\n",
      "training: 80 batch 385 batch_loss: 0.2192782461643219\n",
      "training: 80 batch 386 batch_loss: 0.2201727032661438\n",
      "training: 80 batch 387 batch_loss: 0.22287243604660034\n",
      "training: 80 batch 388 batch_loss: 0.22179603576660156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 80 batch 389 batch_loss: 0.22033146023750305\n",
      "training: 80 batch 390 batch_loss: 0.22201859951019287\n",
      "training: 80 batch 391 batch_loss: 0.22435542941093445\n",
      "training: 80 batch 392 batch_loss: 0.22103983163833618\n",
      "training: 80 batch 393 batch_loss: 0.2261415421962738\n",
      "training: 80 batch 394 batch_loss: 0.21949675679206848\n",
      "training: 80 batch 395 batch_loss: 0.22234386205673218\n",
      "training: 80 batch 396 batch_loss: 0.22263318300247192\n",
      "training: 80 batch 397 batch_loss: 0.21943995356559753\n",
      "training: 80 batch 398 batch_loss: 0.2232450246810913\n",
      "training: 80 batch 399 batch_loss: 0.22172945737838745\n",
      "training: 80 batch 400 batch_loss: 0.2197953164577484\n",
      "training: 80 batch 401 batch_loss: 0.21775123476982117\n",
      "training: 80 batch 402 batch_loss: 0.22291362285614014\n",
      "training: 80 batch 403 batch_loss: 0.2216082513332367\n",
      "training: 80 batch 404 batch_loss: 0.2253940999507904\n",
      "training: 80 batch 405 batch_loss: 0.22061046957969666\n",
      "training: 80 batch 406 batch_loss: 0.21792861819267273\n",
      "training: 80 batch 407 batch_loss: 0.22398394346237183\n",
      "training: 80 batch 408 batch_loss: 0.22093060612678528\n",
      "training: 80 batch 409 batch_loss: 0.22179144620895386\n",
      "training: 80 batch 410 batch_loss: 0.22183358669281006\n",
      "training: 80 batch 411 batch_loss: 0.21980339288711548\n",
      "training: 80 batch 412 batch_loss: 0.2218707799911499\n",
      "training: 80 batch 413 batch_loss: 0.21862122416496277\n",
      "training: 80 batch 414 batch_loss: 0.2159796953201294\n",
      "training: 80 batch 415 batch_loss: 0.22024759650230408\n",
      "training: 80 batch 416 batch_loss: 0.21712064743041992\n",
      "training: 80 batch 417 batch_loss: 0.22002959251403809\n",
      "training: 80 batch 418 batch_loss: 0.2182822823524475\n",
      "training: 80 batch 419 batch_loss: 0.22018033266067505\n",
      "training: 80 batch 420 batch_loss: 0.2228206992149353\n",
      "training: 80 batch 421 batch_loss: 0.22090288996696472\n",
      "training: 80 batch 422 batch_loss: 0.2265819013118744\n",
      "training: 80 batch 423 batch_loss: 0.22047147154808044\n",
      "training: 80 batch 424 batch_loss: 0.22342932224273682\n",
      "training: 80 batch 425 batch_loss: 0.2222214937210083\n",
      "training: 80 batch 426 batch_loss: 0.2236729860305786\n",
      "training: 80 batch 427 batch_loss: 0.21793398261070251\n",
      "training: 80 batch 428 batch_loss: 0.2229163944721222\n",
      "training: 80 batch 429 batch_loss: 0.22179144620895386\n",
      "training: 80 batch 430 batch_loss: 0.2217540442943573\n",
      "training: 80 batch 431 batch_loss: 0.2194541096687317\n",
      "training: 80 batch 432 batch_loss: 0.2182612419128418\n",
      "training: 80 batch 433 batch_loss: 0.22194978594779968\n",
      "training: 80 batch 434 batch_loss: 0.21823367476463318\n",
      "training: 80 batch 435 batch_loss: 0.22151827812194824\n",
      "training: 80 batch 436 batch_loss: 0.22714388370513916\n",
      "training: 80 batch 437 batch_loss: 0.2187981903553009\n",
      "training: 80 batch 438 batch_loss: 0.21996799111366272\n",
      "training: 80 batch 439 batch_loss: 0.22119590640068054\n",
      "training: 80 batch 440 batch_loss: 0.22114819288253784\n",
      "training: 80 batch 441 batch_loss: 0.219673752784729\n",
      "training: 80 batch 442 batch_loss: 0.22480633854866028\n",
      "training: 80 batch 443 batch_loss: 0.22199910879135132\n",
      "training: 80 batch 444 batch_loss: 0.2217240333557129\n",
      "training: 80 batch 445 batch_loss: 0.2203247845172882\n",
      "training: 80 batch 446 batch_loss: 0.22384759783744812\n",
      "training: 80 batch 447 batch_loss: 0.22201266884803772\n",
      "training: 80 batch 448 batch_loss: 0.21488654613494873\n",
      "training: 80 batch 449 batch_loss: 0.2238711416721344\n",
      "training: 80 batch 450 batch_loss: 0.21991190314292908\n",
      "training: 80 batch 451 batch_loss: 0.22117236256599426\n",
      "training: 80 batch 452 batch_loss: 0.22241002321243286\n",
      "training: 80 batch 453 batch_loss: 0.21959856152534485\n",
      "training: 80 batch 454 batch_loss: 0.2261972725391388\n",
      "training: 80 batch 455 batch_loss: 0.22255364060401917\n",
      "training: 80 batch 456 batch_loss: 0.2164820432662964\n",
      "training: 80 batch 457 batch_loss: 0.222795307636261\n",
      "training: 80 batch 458 batch_loss: 0.2211175262928009\n",
      "training: 80 batch 459 batch_loss: 0.22148537635803223\n",
      "training: 80 batch 460 batch_loss: 0.22042202949523926\n",
      "training: 80 batch 461 batch_loss: 0.21840780973434448\n",
      "training: 80 batch 462 batch_loss: 0.22038814425468445\n",
      "training: 80 batch 463 batch_loss: 0.22216886281967163\n",
      "training: 80 batch 464 batch_loss: 0.2237621247768402\n",
      "training: 80 batch 465 batch_loss: 0.22089558839797974\n",
      "training: 80 batch 466 batch_loss: 0.2195790410041809\n",
      "training: 80 batch 467 batch_loss: 0.21970289945602417\n",
      "training: 80 batch 468 batch_loss: 0.22110381722450256\n",
      "training: 80 batch 469 batch_loss: 0.22296109795570374\n",
      "training: 80 batch 470 batch_loss: 0.21896785497665405\n",
      "training: 80 batch 471 batch_loss: 0.21975573897361755\n",
      "training: 80 batch 472 batch_loss: 0.22364002466201782\n",
      "training: 80 batch 473 batch_loss: 0.22166362404823303\n",
      "training: 80 batch 474 batch_loss: 0.2183147370815277\n",
      "training: 80 batch 475 batch_loss: 0.22333934903144836\n",
      "training: 80 batch 476 batch_loss: 0.224348783493042\n",
      "training: 80 batch 477 batch_loss: 0.22121882438659668\n",
      "training: 80 batch 478 batch_loss: 0.22327613830566406\n",
      "training: 80 batch 479 batch_loss: 0.22274494171142578\n",
      "training: 80 batch 480 batch_loss: 0.22042423486709595\n",
      "training: 80 batch 481 batch_loss: 0.21940326690673828\n",
      "training: 80 batch 482 batch_loss: 0.22431865334510803\n",
      "training: 80 batch 483 batch_loss: 0.2239137589931488\n",
      "training: 80 batch 484 batch_loss: 0.2212829291820526\n",
      "training: 80 batch 485 batch_loss: 0.22268828749656677\n",
      "training: 80 batch 486 batch_loss: 0.2236274778842926\n",
      "training: 80 batch 487 batch_loss: 0.2233278453350067\n",
      "training: 80 batch 488 batch_loss: 0.22088897228240967\n",
      "training: 80 batch 489 batch_loss: 0.22378340363502502\n",
      "training: 80 batch 490 batch_loss: 0.22481584548950195\n",
      "training: 80 batch 491 batch_loss: 0.22258278727531433\n",
      "training: 80 batch 492 batch_loss: 0.2219429910182953\n",
      "training: 80 batch 493 batch_loss: 0.22067207098007202\n",
      "training: 80 batch 494 batch_loss: 0.22627407312393188\n",
      "training: 80 batch 495 batch_loss: 0.22219401597976685\n",
      "training: 80 batch 496 batch_loss: 0.22070282697677612\n",
      "training: 80 batch 497 batch_loss: 0.21920767426490784\n",
      "training: 80 batch 498 batch_loss: 0.21966376900672913\n",
      "training: 80 batch 499 batch_loss: 0.22337102890014648\n",
      "training: 80 batch 500 batch_loss: 0.22202885150909424\n",
      "training: 80 batch 501 batch_loss: 0.2199808657169342\n",
      "training: 80 batch 502 batch_loss: 0.21809381246566772\n",
      "training: 80 batch 503 batch_loss: 0.22054702043533325\n",
      "training: 80 batch 504 batch_loss: 0.22005894780158997\n",
      "training: 80 batch 505 batch_loss: 0.22415202856063843\n",
      "training: 80 batch 506 batch_loss: 0.22139281034469604\n",
      "training: 80 batch 507 batch_loss: 0.2225915789604187\n",
      "training: 80 batch 508 batch_loss: 0.22121736407279968\n",
      "training: 80 batch 509 batch_loss: 0.22205469012260437\n",
      "training: 80 batch 510 batch_loss: 0.2234380841255188\n",
      "training: 80 batch 511 batch_loss: 0.21868592500686646\n",
      "training: 80 batch 512 batch_loss: 0.21800997853279114\n",
      "training: 80 batch 513 batch_loss: 0.21998083591461182\n",
      "training: 80 batch 514 batch_loss: 0.22151553630828857\n",
      "training: 80 batch 515 batch_loss: 0.21934780478477478\n",
      "training: 80 batch 516 batch_loss: 0.22063589096069336\n",
      "training: 80 batch 517 batch_loss: 0.22127071022987366\n",
      "training: 80 batch 518 batch_loss: 0.22259390354156494\n",
      "training: 80 batch 519 batch_loss: 0.2199552357196808\n",
      "training: 80 batch 520 batch_loss: 0.2239879071712494\n",
      "training: 80 batch 521 batch_loss: 0.21888041496276855\n",
      "training: 80 batch 522 batch_loss: 0.22041690349578857\n",
      "training: 80 batch 523 batch_loss: 0.2207130789756775\n",
      "training: 80 batch 524 batch_loss: 0.22206732630729675\n",
      "training: 80 batch 525 batch_loss: 0.2181895673274994\n",
      "training: 80 batch 526 batch_loss: 0.2183344066143036\n",
      "training: 80 batch 527 batch_loss: 0.22586652636528015\n",
      "training: 80 batch 528 batch_loss: 0.219146728515625\n",
      "training: 80 batch 529 batch_loss: 0.22008538246154785\n",
      "training: 80 batch 530 batch_loss: 0.22321093082427979\n",
      "training: 80 batch 531 batch_loss: 0.22105488181114197\n",
      "training: 80 batch 532 batch_loss: 0.2225131392478943\n",
      "training: 80 batch 533 batch_loss: 0.22411119937896729\n",
      "training: 80 batch 534 batch_loss: 0.2239198088645935\n",
      "training: 80 batch 535 batch_loss: 0.22274956107139587\n",
      "training: 80 batch 536 batch_loss: 0.22234168648719788\n",
      "training: 80 batch 537 batch_loss: 0.21836793422698975\n",
      "training: 80 batch 538 batch_loss: 0.21525424718856812\n",
      "training: 80 batch 539 batch_loss: 0.22205105423927307\n",
      "training: 80 batch 540 batch_loss: 0.22474440932273865\n",
      "training: 80 batch 541 batch_loss: 0.22043156623840332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 80 batch 542 batch_loss: 0.2239323854446411\n",
      "training: 80 batch 543 batch_loss: 0.22664323449134827\n",
      "training: 80 batch 544 batch_loss: 0.21952268481254578\n",
      "training: 80 batch 545 batch_loss: 0.22327986359596252\n",
      "training: 80 batch 546 batch_loss: 0.2200707197189331\n",
      "training: 80 batch 547 batch_loss: 0.21988022327423096\n",
      "training: 80 batch 548 batch_loss: 0.22350022196769714\n",
      "training: 80 batch 549 batch_loss: 0.2215825617313385\n",
      "training: 80 batch 550 batch_loss: 0.22356504201889038\n",
      "training: 80 batch 551 batch_loss: 0.2220444679260254\n",
      "training: 80 batch 552 batch_loss: 0.22360852360725403\n",
      "training: 80 batch 553 batch_loss: 0.22026121616363525\n",
      "training: 80 batch 554 batch_loss: 0.22098064422607422\n",
      "training: 80 batch 555 batch_loss: 0.22466877102851868\n",
      "training: 80 batch 556 batch_loss: 0.22242876887321472\n",
      "training: 80 batch 557 batch_loss: 0.22464007139205933\n",
      "training: 80 batch 558 batch_loss: 0.22476771473884583\n",
      "training: 80 batch 559 batch_loss: 0.22497162222862244\n",
      "training: 80 batch 560 batch_loss: 0.22818171977996826\n",
      "training: 80 batch 561 batch_loss: 0.2236621379852295\n",
      "training: 80 batch 562 batch_loss: 0.2232072949409485\n",
      "training: 80 batch 563 batch_loss: 0.22327542304992676\n",
      "training: 80 batch 564 batch_loss: 0.22166964411735535\n",
      "training: 80 batch 565 batch_loss: 0.22408631443977356\n",
      "training: 80 batch 566 batch_loss: 0.2229493260383606\n",
      "training: 80 batch 567 batch_loss: 0.21802306175231934\n",
      "training: 80 batch 568 batch_loss: 0.2219875454902649\n",
      "training: 80 batch 569 batch_loss: 0.2224106788635254\n",
      "training: 80 batch 570 batch_loss: 0.2229306995868683\n",
      "training: 80 batch 571 batch_loss: 0.22777745127677917\n",
      "training: 80 batch 572 batch_loss: 0.217765212059021\n",
      "training: 80 batch 573 batch_loss: 0.22117912769317627\n",
      "training: 80 batch 574 batch_loss: 0.2236385941505432\n",
      "training: 80 batch 575 batch_loss: 0.21722033619880676\n",
      "training: 80 batch 576 batch_loss: 0.22225776314735413\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 80, Hit Ratio:0.03359505384869067 | Precision:0.04956748255185294 | Recall:0.06522380630574594 | NDCG:0.06456882854467651\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 81 batch 0 batch_loss: 0.21386778354644775\n",
      "training: 81 batch 1 batch_loss: 0.22503802180290222\n",
      "training: 81 batch 2 batch_loss: 0.22349390387535095\n",
      "training: 81 batch 3 batch_loss: 0.21855491399765015\n",
      "training: 81 batch 4 batch_loss: 0.22000572085380554\n",
      "training: 81 batch 5 batch_loss: 0.21843427419662476\n",
      "training: 81 batch 6 batch_loss: 0.21616461873054504\n",
      "training: 81 batch 7 batch_loss: 0.21798670291900635\n",
      "training: 81 batch 8 batch_loss: 0.22267699241638184\n",
      "training: 81 batch 9 batch_loss: 0.21949627995491028\n",
      "training: 81 batch 10 batch_loss: 0.22116824984550476\n",
      "training: 81 batch 11 batch_loss: 0.21521064639091492\n",
      "training: 81 batch 12 batch_loss: 0.22347179055213928\n",
      "training: 81 batch 13 batch_loss: 0.22165265679359436\n",
      "training: 81 batch 14 batch_loss: 0.2201998233795166\n",
      "training: 81 batch 15 batch_loss: 0.21914729475975037\n",
      "training: 81 batch 16 batch_loss: 0.22368237376213074\n",
      "training: 81 batch 17 batch_loss: 0.22213459014892578\n",
      "training: 81 batch 18 batch_loss: 0.22484156489372253\n",
      "training: 81 batch 19 batch_loss: 0.21898072957992554\n",
      "training: 81 batch 20 batch_loss: 0.21802347898483276\n",
      "training: 81 batch 21 batch_loss: 0.222359299659729\n",
      "training: 81 batch 22 batch_loss: 0.2203751504421234\n",
      "training: 81 batch 23 batch_loss: 0.22202068567276\n",
      "training: 81 batch 24 batch_loss: 0.22107207775115967\n",
      "training: 81 batch 25 batch_loss: 0.22209376096725464\n",
      "training: 81 batch 26 batch_loss: 0.2220299243927002\n",
      "training: 81 batch 27 batch_loss: 0.22197794914245605\n",
      "training: 81 batch 28 batch_loss: 0.21665138006210327\n",
      "training: 81 batch 29 batch_loss: 0.22077542543411255\n",
      "training: 81 batch 30 batch_loss: 0.2220030128955841\n",
      "training: 81 batch 31 batch_loss: 0.2191794216632843\n",
      "training: 81 batch 32 batch_loss: 0.21795961260795593\n",
      "training: 81 batch 33 batch_loss: 0.224533349275589\n",
      "training: 81 batch 34 batch_loss: 0.2229875922203064\n",
      "training: 81 batch 35 batch_loss: 0.21990463137626648\n",
      "training: 81 batch 36 batch_loss: 0.21913310885429382\n",
      "training: 81 batch 37 batch_loss: 0.22101342678070068\n",
      "training: 81 batch 38 batch_loss: 0.21898633241653442\n",
      "training: 81 batch 39 batch_loss: 0.21718117594718933\n",
      "training: 81 batch 40 batch_loss: 0.22325897216796875\n",
      "training: 81 batch 41 batch_loss: 0.2241595983505249\n",
      "training: 81 batch 42 batch_loss: 0.21966317296028137\n",
      "training: 81 batch 43 batch_loss: 0.22497105598449707\n",
      "training: 81 batch 44 batch_loss: 0.21723860502243042\n",
      "training: 81 batch 45 batch_loss: 0.2204626202583313\n",
      "training: 81 batch 46 batch_loss: 0.21542266011238098\n",
      "training: 81 batch 47 batch_loss: 0.22116392850875854\n",
      "training: 81 batch 48 batch_loss: 0.21698006987571716\n",
      "training: 81 batch 49 batch_loss: 0.21844711899757385\n",
      "training: 81 batch 50 batch_loss: 0.21836423873901367\n",
      "training: 81 batch 51 batch_loss: 0.21995526552200317\n",
      "training: 81 batch 52 batch_loss: 0.22278833389282227\n",
      "training: 81 batch 53 batch_loss: 0.2239222228527069\n",
      "training: 81 batch 54 batch_loss: 0.21924960613250732\n",
      "training: 81 batch 55 batch_loss: 0.22090056538581848\n",
      "training: 81 batch 56 batch_loss: 0.21927997469902039\n",
      "training: 81 batch 57 batch_loss: 0.22400888800621033\n",
      "training: 81 batch 58 batch_loss: 0.21994438767433167\n",
      "training: 81 batch 59 batch_loss: 0.22240886092185974\n",
      "training: 81 batch 60 batch_loss: 0.2195647954940796\n",
      "training: 81 batch 61 batch_loss: 0.2202572524547577\n",
      "training: 81 batch 62 batch_loss: 0.22090959548950195\n",
      "training: 81 batch 63 batch_loss: 0.2170729637145996\n",
      "training: 81 batch 64 batch_loss: 0.2195165455341339\n",
      "training: 81 batch 65 batch_loss: 0.22167575359344482\n",
      "training: 81 batch 66 batch_loss: 0.22210288047790527\n",
      "training: 81 batch 67 batch_loss: 0.21808388829231262\n",
      "training: 81 batch 68 batch_loss: 0.2235303521156311\n",
      "training: 81 batch 69 batch_loss: 0.22331535816192627\n",
      "training: 81 batch 70 batch_loss: 0.22102054953575134\n",
      "training: 81 batch 71 batch_loss: 0.2213519811630249\n",
      "training: 81 batch 72 batch_loss: 0.21986150741577148\n",
      "training: 81 batch 73 batch_loss: 0.22316336631774902\n",
      "training: 81 batch 74 batch_loss: 0.22364258766174316\n",
      "training: 81 batch 75 batch_loss: 0.2260846495628357\n",
      "training: 81 batch 76 batch_loss: 0.21990758180618286\n",
      "training: 81 batch 77 batch_loss: 0.22116932272911072\n",
      "training: 81 batch 78 batch_loss: 0.21919992566108704\n",
      "training: 81 batch 79 batch_loss: 0.22037163376808167\n",
      "training: 81 batch 80 batch_loss: 0.22188323736190796\n",
      "training: 81 batch 81 batch_loss: 0.21900081634521484\n",
      "training: 81 batch 82 batch_loss: 0.21896609663963318\n",
      "training: 81 batch 83 batch_loss: 0.21693113446235657\n",
      "training: 81 batch 84 batch_loss: 0.22475844621658325\n",
      "training: 81 batch 85 batch_loss: 0.22267350554466248\n",
      "training: 81 batch 86 batch_loss: 0.22347596287727356\n",
      "training: 81 batch 87 batch_loss: 0.22324851155281067\n",
      "training: 81 batch 88 batch_loss: 0.21909496188163757\n",
      "training: 81 batch 89 batch_loss: 0.22215157747268677\n",
      "training: 81 batch 90 batch_loss: 0.22277671098709106\n",
      "training: 81 batch 91 batch_loss: 0.21943411231040955\n",
      "training: 81 batch 92 batch_loss: 0.21982493996620178\n",
      "training: 81 batch 93 batch_loss: 0.22152578830718994\n",
      "training: 81 batch 94 batch_loss: 0.22172993421554565\n",
      "training: 81 batch 95 batch_loss: 0.22201842069625854\n",
      "training: 81 batch 96 batch_loss: 0.22024914622306824\n",
      "training: 81 batch 97 batch_loss: 0.21847718954086304\n",
      "training: 81 batch 98 batch_loss: 0.22238010168075562\n",
      "training: 81 batch 99 batch_loss: 0.22274810075759888\n",
      "training: 81 batch 100 batch_loss: 0.21973127126693726\n",
      "training: 81 batch 101 batch_loss: 0.2204601764678955\n",
      "training: 81 batch 102 batch_loss: 0.22011449933052063\n",
      "training: 81 batch 103 batch_loss: 0.22100582718849182\n",
      "training: 81 batch 104 batch_loss: 0.2224053144454956\n",
      "training: 81 batch 105 batch_loss: 0.22212934494018555\n",
      "training: 81 batch 106 batch_loss: 0.22226649522781372\n",
      "training: 81 batch 107 batch_loss: 0.22279521822929382\n",
      "training: 81 batch 108 batch_loss: 0.22123682498931885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 81 batch 109 batch_loss: 0.2177739143371582\n",
      "training: 81 batch 110 batch_loss: 0.22183609008789062\n",
      "training: 81 batch 111 batch_loss: 0.22022908926010132\n",
      "training: 81 batch 112 batch_loss: 0.21883583068847656\n",
      "training: 81 batch 113 batch_loss: 0.22290363907814026\n",
      "training: 81 batch 114 batch_loss: 0.22174766659736633\n",
      "training: 81 batch 115 batch_loss: 0.22047275304794312\n",
      "training: 81 batch 116 batch_loss: 0.21865105628967285\n",
      "training: 81 batch 117 batch_loss: 0.2200241982936859\n",
      "training: 81 batch 118 batch_loss: 0.21719691157341003\n",
      "training: 81 batch 119 batch_loss: 0.21803945302963257\n",
      "training: 81 batch 120 batch_loss: 0.21898898482322693\n",
      "training: 81 batch 121 batch_loss: 0.22481179237365723\n",
      "training: 81 batch 122 batch_loss: 0.2211109697818756\n",
      "training: 81 batch 123 batch_loss: 0.2227662205696106\n",
      "training: 81 batch 124 batch_loss: 0.21730834245681763\n",
      "training: 81 batch 125 batch_loss: 0.22027701139450073\n",
      "training: 81 batch 126 batch_loss: 0.219434916973114\n",
      "training: 81 batch 127 batch_loss: 0.2183988392353058\n",
      "training: 81 batch 128 batch_loss: 0.22055992484092712\n",
      "training: 81 batch 129 batch_loss: 0.21947389841079712\n",
      "training: 81 batch 130 batch_loss: 0.22133377194404602\n",
      "training: 81 batch 131 batch_loss: 0.2230294942855835\n",
      "training: 81 batch 132 batch_loss: 0.2250520884990692\n",
      "training: 81 batch 133 batch_loss: 0.2207164764404297\n",
      "training: 81 batch 134 batch_loss: 0.22119560837745667\n",
      "training: 81 batch 135 batch_loss: 0.22221660614013672\n",
      "training: 81 batch 136 batch_loss: 0.2240886688232422\n",
      "training: 81 batch 137 batch_loss: 0.22385981678962708\n",
      "training: 81 batch 138 batch_loss: 0.22337716817855835\n",
      "training: 81 batch 139 batch_loss: 0.22407877445220947\n",
      "training: 81 batch 140 batch_loss: 0.22049415111541748\n",
      "training: 81 batch 141 batch_loss: 0.22490093111991882\n",
      "training: 81 batch 142 batch_loss: 0.22397398948669434\n",
      "training: 81 batch 143 batch_loss: 0.22340703010559082\n",
      "training: 81 batch 144 batch_loss: 0.22523540258407593\n",
      "training: 81 batch 145 batch_loss: 0.21656900644302368\n",
      "training: 81 batch 146 batch_loss: 0.22114244103431702\n",
      "training: 81 batch 147 batch_loss: 0.2214268147945404\n",
      "training: 81 batch 148 batch_loss: 0.2224528193473816\n",
      "training: 81 batch 149 batch_loss: 0.21866893768310547\n",
      "training: 81 batch 150 batch_loss: 0.21922501921653748\n",
      "training: 81 batch 151 batch_loss: 0.22026753425598145\n",
      "training: 81 batch 152 batch_loss: 0.21884149312973022\n",
      "training: 81 batch 153 batch_loss: 0.21934837102890015\n",
      "training: 81 batch 154 batch_loss: 0.22277063131332397\n",
      "training: 81 batch 155 batch_loss: 0.22326692938804626\n",
      "training: 81 batch 156 batch_loss: 0.21904978156089783\n",
      "training: 81 batch 157 batch_loss: 0.21850094199180603\n",
      "training: 81 batch 158 batch_loss: 0.22139549255371094\n",
      "training: 81 batch 159 batch_loss: 0.22326874732971191\n",
      "training: 81 batch 160 batch_loss: 0.22039082646369934\n",
      "training: 81 batch 161 batch_loss: 0.22553953528404236\n",
      "training: 81 batch 162 batch_loss: 0.22259211540222168\n",
      "training: 81 batch 163 batch_loss: 0.22395062446594238\n",
      "training: 81 batch 164 batch_loss: 0.2185097336769104\n",
      "training: 81 batch 165 batch_loss: 0.22266846895217896\n",
      "training: 81 batch 166 batch_loss: 0.22653228044509888\n",
      "training: 81 batch 167 batch_loss: 0.21935373544692993\n",
      "training: 81 batch 168 batch_loss: 0.21921229362487793\n",
      "training: 81 batch 169 batch_loss: 0.22124412655830383\n",
      "training: 81 batch 170 batch_loss: 0.22033962607383728\n",
      "training: 81 batch 171 batch_loss: 0.21876242756843567\n",
      "training: 81 batch 172 batch_loss: 0.22643637657165527\n",
      "training: 81 batch 173 batch_loss: 0.22181221842765808\n",
      "training: 81 batch 174 batch_loss: 0.22238442301750183\n",
      "training: 81 batch 175 batch_loss: 0.22251400351524353\n",
      "training: 81 batch 176 batch_loss: 0.22050410509109497\n",
      "training: 81 batch 177 batch_loss: 0.21760740876197815\n",
      "training: 81 batch 178 batch_loss: 0.22387754917144775\n",
      "training: 81 batch 179 batch_loss: 0.22360539436340332\n",
      "training: 81 batch 180 batch_loss: 0.22384986281394958\n",
      "training: 81 batch 181 batch_loss: 0.22365543246269226\n",
      "training: 81 batch 182 batch_loss: 0.22149619460105896\n",
      "training: 81 batch 183 batch_loss: 0.221843421459198\n",
      "training: 81 batch 184 batch_loss: 0.22099891304969788\n",
      "training: 81 batch 185 batch_loss: 0.22132480144500732\n",
      "training: 81 batch 186 batch_loss: 0.22135627269744873\n",
      "training: 81 batch 187 batch_loss: 0.22729706764221191\n",
      "training: 81 batch 188 batch_loss: 0.22246450185775757\n",
      "training: 81 batch 189 batch_loss: 0.22201764583587646\n",
      "training: 81 batch 190 batch_loss: 0.21617275476455688\n",
      "training: 81 batch 191 batch_loss: 0.2222939431667328\n",
      "training: 81 batch 192 batch_loss: 0.21551164984703064\n",
      "training: 81 batch 193 batch_loss: 0.22075402736663818\n",
      "training: 81 batch 194 batch_loss: 0.2224406599998474\n",
      "training: 81 batch 195 batch_loss: 0.22035101056098938\n",
      "training: 81 batch 196 batch_loss: 0.21939778327941895\n",
      "training: 81 batch 197 batch_loss: 0.2237519919872284\n",
      "training: 81 batch 198 batch_loss: 0.22245100140571594\n",
      "training: 81 batch 199 batch_loss: 0.22234609723091125\n",
      "training: 81 batch 200 batch_loss: 0.21793383359909058\n",
      "training: 81 batch 201 batch_loss: 0.2248765528202057\n",
      "training: 81 batch 202 batch_loss: 0.2211734652519226\n",
      "training: 81 batch 203 batch_loss: 0.21973878145217896\n",
      "training: 81 batch 204 batch_loss: 0.22174641489982605\n",
      "training: 81 batch 205 batch_loss: 0.224394291639328\n",
      "training: 81 batch 206 batch_loss: 0.2203371524810791\n",
      "training: 81 batch 207 batch_loss: 0.2218785583972931\n",
      "training: 81 batch 208 batch_loss: 0.22393879294395447\n",
      "training: 81 batch 209 batch_loss: 0.2192523181438446\n",
      "training: 81 batch 210 batch_loss: 0.2234800159931183\n",
      "training: 81 batch 211 batch_loss: 0.21545535326004028\n",
      "training: 81 batch 212 batch_loss: 0.22137099504470825\n",
      "training: 81 batch 213 batch_loss: 0.22113940119743347\n",
      "training: 81 batch 214 batch_loss: 0.21970736980438232\n",
      "training: 81 batch 215 batch_loss: 0.22253119945526123\n",
      "training: 81 batch 216 batch_loss: 0.22096329927444458\n",
      "training: 81 batch 217 batch_loss: 0.21952366828918457\n",
      "training: 81 batch 218 batch_loss: 0.22099345922470093\n",
      "training: 81 batch 219 batch_loss: 0.22061127424240112\n",
      "training: 81 batch 220 batch_loss: 0.21988385915756226\n",
      "training: 81 batch 221 batch_loss: 0.22103309631347656\n",
      "training: 81 batch 222 batch_loss: 0.21876099705696106\n",
      "training: 81 batch 223 batch_loss: 0.2247987687587738\n",
      "training: 81 batch 224 batch_loss: 0.21737277507781982\n",
      "training: 81 batch 225 batch_loss: 0.22073984146118164\n",
      "training: 81 batch 226 batch_loss: 0.21938872337341309\n",
      "training: 81 batch 227 batch_loss: 0.22234269976615906\n",
      "training: 81 batch 228 batch_loss: 0.22017070651054382\n",
      "training: 81 batch 229 batch_loss: 0.22502100467681885\n",
      "training: 81 batch 230 batch_loss: 0.22267425060272217\n",
      "training: 81 batch 231 batch_loss: 0.22094562649726868\n",
      "training: 81 batch 232 batch_loss: 0.21806585788726807\n",
      "training: 81 batch 233 batch_loss: 0.22330820560455322\n",
      "training: 81 batch 234 batch_loss: 0.22306093573570251\n",
      "training: 81 batch 235 batch_loss: 0.22137019038200378\n",
      "training: 81 batch 236 batch_loss: 0.22118598222732544\n",
      "training: 81 batch 237 batch_loss: 0.22188201546669006\n",
      "training: 81 batch 238 batch_loss: 0.2246147096157074\n",
      "training: 81 batch 239 batch_loss: 0.21999740600585938\n",
      "training: 81 batch 240 batch_loss: 0.22439917922019958\n",
      "training: 81 batch 241 batch_loss: 0.21573978662490845\n",
      "training: 81 batch 242 batch_loss: 0.22084438800811768\n",
      "training: 81 batch 243 batch_loss: 0.22145751118659973\n",
      "training: 81 batch 244 batch_loss: 0.22006160020828247\n",
      "training: 81 batch 245 batch_loss: 0.22076326608657837\n",
      "training: 81 batch 246 batch_loss: 0.2235282063484192\n",
      "training: 81 batch 247 batch_loss: 0.22213906049728394\n",
      "training: 81 batch 248 batch_loss: 0.22238504886627197\n",
      "training: 81 batch 249 batch_loss: 0.22592276334762573\n",
      "training: 81 batch 250 batch_loss: 0.22279709577560425\n",
      "training: 81 batch 251 batch_loss: 0.22517234086990356\n",
      "training: 81 batch 252 batch_loss: 0.2230667769908905\n",
      "training: 81 batch 253 batch_loss: 0.2159120738506317\n",
      "training: 81 batch 254 batch_loss: 0.22056186199188232\n",
      "training: 81 batch 255 batch_loss: 0.21745669841766357\n",
      "training: 81 batch 256 batch_loss: 0.21804293990135193\n",
      "training: 81 batch 257 batch_loss: 0.22052481770515442\n",
      "training: 81 batch 258 batch_loss: 0.2211490273475647\n",
      "training: 81 batch 259 batch_loss: 0.22441011667251587\n",
      "training: 81 batch 260 batch_loss: 0.21804538369178772\n",
      "training: 81 batch 261 batch_loss: 0.2204839587211609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 81 batch 262 batch_loss: 0.22441917657852173\n",
      "training: 81 batch 263 batch_loss: 0.22349315881729126\n",
      "training: 81 batch 264 batch_loss: 0.22426432371139526\n",
      "training: 81 batch 265 batch_loss: 0.22047200798988342\n",
      "training: 81 batch 266 batch_loss: 0.2203141450881958\n",
      "training: 81 batch 267 batch_loss: 0.22221574187278748\n",
      "training: 81 batch 268 batch_loss: 0.21835395693778992\n",
      "training: 81 batch 269 batch_loss: 0.22213229537010193\n",
      "training: 81 batch 270 batch_loss: 0.21670454740524292\n",
      "training: 81 batch 271 batch_loss: 0.22337225079536438\n",
      "training: 81 batch 272 batch_loss: 0.2188572883605957\n",
      "training: 81 batch 273 batch_loss: 0.22286850214004517\n",
      "training: 81 batch 274 batch_loss: 0.22530323266983032\n",
      "training: 81 batch 275 batch_loss: 0.22181963920593262\n",
      "training: 81 batch 276 batch_loss: 0.21845698356628418\n",
      "training: 81 batch 277 batch_loss: 0.2178175151348114\n",
      "training: 81 batch 278 batch_loss: 0.21795356273651123\n",
      "training: 81 batch 279 batch_loss: 0.2239028811454773\n",
      "training: 81 batch 280 batch_loss: 0.22258764505386353\n",
      "training: 81 batch 281 batch_loss: 0.21879446506500244\n",
      "training: 81 batch 282 batch_loss: 0.22060298919677734\n",
      "training: 81 batch 283 batch_loss: 0.22107809782028198\n",
      "training: 81 batch 284 batch_loss: 0.22269955277442932\n",
      "training: 81 batch 285 batch_loss: 0.22273731231689453\n",
      "training: 81 batch 286 batch_loss: 0.22276711463928223\n",
      "training: 81 batch 287 batch_loss: 0.21958786249160767\n",
      "training: 81 batch 288 batch_loss: 0.223637193441391\n",
      "training: 81 batch 289 batch_loss: 0.22122353315353394\n",
      "training: 81 batch 290 batch_loss: 0.22483345866203308\n",
      "training: 81 batch 291 batch_loss: 0.22437778115272522\n",
      "training: 81 batch 292 batch_loss: 0.2201818823814392\n",
      "training: 81 batch 293 batch_loss: 0.22177523374557495\n",
      "training: 81 batch 294 batch_loss: 0.22029918432235718\n",
      "training: 81 batch 295 batch_loss: 0.21930688619613647\n",
      "training: 81 batch 296 batch_loss: 0.22614794969558716\n",
      "training: 81 batch 297 batch_loss: 0.2194679081439972\n",
      "training: 81 batch 298 batch_loss: 0.2231966257095337\n",
      "training: 81 batch 299 batch_loss: 0.21959105134010315\n",
      "training: 81 batch 300 batch_loss: 0.225981205701828\n",
      "training: 81 batch 301 batch_loss: 0.2280978560447693\n",
      "training: 81 batch 302 batch_loss: 0.22192254662513733\n",
      "training: 81 batch 303 batch_loss: 0.2228364646434784\n",
      "training: 81 batch 304 batch_loss: 0.22284552454948425\n",
      "training: 81 batch 305 batch_loss: 0.2236846387386322\n",
      "training: 81 batch 306 batch_loss: 0.22575336694717407\n",
      "training: 81 batch 307 batch_loss: 0.21947914361953735\n",
      "training: 81 batch 308 batch_loss: 0.22351476550102234\n",
      "training: 81 batch 309 batch_loss: 0.22408968210220337\n",
      "training: 81 batch 310 batch_loss: 0.21914330124855042\n",
      "training: 81 batch 311 batch_loss: 0.21735569834709167\n",
      "training: 81 batch 312 batch_loss: 0.22161585092544556\n",
      "training: 81 batch 313 batch_loss: 0.2247941792011261\n",
      "training: 81 batch 314 batch_loss: 0.22289472818374634\n",
      "training: 81 batch 315 batch_loss: 0.2217666208744049\n",
      "training: 81 batch 316 batch_loss: 0.22411629557609558\n",
      "training: 81 batch 317 batch_loss: 0.22160714864730835\n",
      "training: 81 batch 318 batch_loss: 0.22367319464683533\n",
      "training: 81 batch 319 batch_loss: 0.22169777750968933\n",
      "training: 81 batch 320 batch_loss: 0.22108173370361328\n",
      "training: 81 batch 321 batch_loss: 0.2206953763961792\n",
      "training: 81 batch 322 batch_loss: 0.2216385006904602\n",
      "training: 81 batch 323 batch_loss: 0.22029229998588562\n",
      "training: 81 batch 324 batch_loss: 0.2238069772720337\n",
      "training: 81 batch 325 batch_loss: 0.21847224235534668\n",
      "training: 81 batch 326 batch_loss: 0.2218361496925354\n",
      "training: 81 batch 327 batch_loss: 0.22068080306053162\n",
      "training: 81 batch 328 batch_loss: 0.22575214505195618\n",
      "training: 81 batch 329 batch_loss: 0.22819364070892334\n",
      "training: 81 batch 330 batch_loss: 0.22417396306991577\n",
      "training: 81 batch 331 batch_loss: 0.22474461793899536\n",
      "training: 81 batch 332 batch_loss: 0.22083646059036255\n",
      "training: 81 batch 333 batch_loss: 0.22274371981620789\n",
      "training: 81 batch 334 batch_loss: 0.21807429194450378\n",
      "training: 81 batch 335 batch_loss: 0.2230551838874817\n",
      "training: 81 batch 336 batch_loss: 0.2179262936115265\n",
      "training: 81 batch 337 batch_loss: 0.22058957815170288\n",
      "training: 81 batch 338 batch_loss: 0.22377389669418335\n",
      "training: 81 batch 339 batch_loss: 0.2183327078819275\n",
      "training: 81 batch 340 batch_loss: 0.22158056497573853\n",
      "training: 81 batch 341 batch_loss: 0.21918153762817383\n",
      "training: 81 batch 342 batch_loss: 0.2231704592704773\n",
      "training: 81 batch 343 batch_loss: 0.22390764951705933\n",
      "training: 81 batch 344 batch_loss: 0.22081300616264343\n",
      "training: 81 batch 345 batch_loss: 0.2265806496143341\n",
      "training: 81 batch 346 batch_loss: 0.2208808958530426\n",
      "training: 81 batch 347 batch_loss: 0.22029608488082886\n",
      "training: 81 batch 348 batch_loss: 0.21901869773864746\n",
      "training: 81 batch 349 batch_loss: 0.21881189942359924\n",
      "training: 81 batch 350 batch_loss: 0.21767672896385193\n",
      "training: 81 batch 351 batch_loss: 0.21971085667610168\n",
      "training: 81 batch 352 batch_loss: 0.22057121992111206\n",
      "training: 81 batch 353 batch_loss: 0.21865805983543396\n",
      "training: 81 batch 354 batch_loss: 0.22414249181747437\n",
      "training: 81 batch 355 batch_loss: 0.22315505146980286\n",
      "training: 81 batch 356 batch_loss: 0.22367766499519348\n",
      "training: 81 batch 357 batch_loss: 0.22084110975265503\n",
      "training: 81 batch 358 batch_loss: 0.22348317503929138\n",
      "training: 81 batch 359 batch_loss: 0.22017639875411987\n",
      "training: 81 batch 360 batch_loss: 0.2168770432472229\n",
      "training: 81 batch 361 batch_loss: 0.22265636920928955\n",
      "training: 81 batch 362 batch_loss: 0.22250431776046753\n",
      "training: 81 batch 363 batch_loss: 0.21651852130889893\n",
      "training: 81 batch 364 batch_loss: 0.22358524799346924\n",
      "training: 81 batch 365 batch_loss: 0.2214461863040924\n",
      "training: 81 batch 366 batch_loss: 0.22235393524169922\n",
      "training: 81 batch 367 batch_loss: 0.22404035925865173\n",
      "training: 81 batch 368 batch_loss: 0.22301411628723145\n",
      "training: 81 batch 369 batch_loss: 0.2179209589958191\n",
      "training: 81 batch 370 batch_loss: 0.2217680811882019\n",
      "training: 81 batch 371 batch_loss: 0.221754789352417\n",
      "training: 81 batch 372 batch_loss: 0.22534537315368652\n",
      "training: 81 batch 373 batch_loss: 0.22741976380348206\n",
      "training: 81 batch 374 batch_loss: 0.22054323554039001\n",
      "training: 81 batch 375 batch_loss: 0.22505691647529602\n",
      "training: 81 batch 376 batch_loss: 0.2204592525959015\n",
      "training: 81 batch 377 batch_loss: 0.2225244641304016\n",
      "training: 81 batch 378 batch_loss: 0.21881315112113953\n",
      "training: 81 batch 379 batch_loss: 0.22506293654441833\n",
      "training: 81 batch 380 batch_loss: 0.22113120555877686\n",
      "training: 81 batch 381 batch_loss: 0.22446811199188232\n",
      "training: 81 batch 382 batch_loss: 0.22316506505012512\n",
      "training: 81 batch 383 batch_loss: 0.2217179536819458\n",
      "training: 81 batch 384 batch_loss: 0.22185221314430237\n",
      "training: 81 batch 385 batch_loss: 0.2218388020992279\n",
      "training: 81 batch 386 batch_loss: 0.22049063444137573\n",
      "training: 81 batch 387 batch_loss: 0.22357523441314697\n",
      "training: 81 batch 388 batch_loss: 0.21903851628303528\n",
      "training: 81 batch 389 batch_loss: 0.22208437323570251\n",
      "training: 81 batch 390 batch_loss: 0.22052538394927979\n",
      "training: 81 batch 391 batch_loss: 0.2261645793914795\n",
      "training: 81 batch 392 batch_loss: 0.2222028374671936\n",
      "training: 81 batch 393 batch_loss: 0.22014302015304565\n",
      "training: 81 batch 394 batch_loss: 0.2229982316493988\n",
      "training: 81 batch 395 batch_loss: 0.22173398733139038\n",
      "training: 81 batch 396 batch_loss: 0.22074764966964722\n",
      "training: 81 batch 397 batch_loss: 0.22298160195350647\n",
      "training: 81 batch 398 batch_loss: 0.22199690341949463\n",
      "training: 81 batch 399 batch_loss: 0.21756213903427124\n",
      "training: 81 batch 400 batch_loss: 0.2203281819820404\n",
      "training: 81 batch 401 batch_loss: 0.2186616063117981\n",
      "training: 81 batch 402 batch_loss: 0.21925589442253113\n",
      "training: 81 batch 403 batch_loss: 0.21966025233268738\n",
      "training: 81 batch 404 batch_loss: 0.22437256574630737\n",
      "training: 81 batch 405 batch_loss: 0.21887683868408203\n",
      "training: 81 batch 406 batch_loss: 0.2186659276485443\n",
      "training: 81 batch 407 batch_loss: 0.21990284323692322\n",
      "training: 81 batch 408 batch_loss: 0.21961680054664612\n",
      "training: 81 batch 409 batch_loss: 0.22593280673027039\n",
      "training: 81 batch 410 batch_loss: 0.22370171546936035\n",
      "training: 81 batch 411 batch_loss: 0.217966228723526\n",
      "training: 81 batch 412 batch_loss: 0.22457355260849\n",
      "training: 81 batch 413 batch_loss: 0.22502440214157104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 81 batch 414 batch_loss: 0.2219482660293579\n",
      "training: 81 batch 415 batch_loss: 0.21964585781097412\n",
      "training: 81 batch 416 batch_loss: 0.2205302119255066\n",
      "training: 81 batch 417 batch_loss: 0.2222701907157898\n",
      "training: 81 batch 418 batch_loss: 0.2189910113811493\n",
      "training: 81 batch 419 batch_loss: 0.2209838628768921\n",
      "training: 81 batch 420 batch_loss: 0.221962571144104\n",
      "training: 81 batch 421 batch_loss: 0.22336766123771667\n",
      "training: 81 batch 422 batch_loss: 0.22649690508842468\n",
      "training: 81 batch 423 batch_loss: 0.22250846028327942\n",
      "training: 81 batch 424 batch_loss: 0.22456064820289612\n",
      "training: 81 batch 425 batch_loss: 0.22287970781326294\n",
      "training: 81 batch 426 batch_loss: 0.22028952836990356\n",
      "training: 81 batch 427 batch_loss: 0.21942085027694702\n",
      "training: 81 batch 428 batch_loss: 0.2241899073123932\n",
      "training: 81 batch 429 batch_loss: 0.22010818123817444\n",
      "training: 81 batch 430 batch_loss: 0.2191915214061737\n",
      "training: 81 batch 431 batch_loss: 0.22153547406196594\n",
      "training: 81 batch 432 batch_loss: 0.22344672679901123\n",
      "training: 81 batch 433 batch_loss: 0.22319072484970093\n",
      "training: 81 batch 434 batch_loss: 0.22420305013656616\n",
      "training: 81 batch 435 batch_loss: 0.21849173307418823\n",
      "training: 81 batch 436 batch_loss: 0.21927613019943237\n",
      "training: 81 batch 437 batch_loss: 0.22392809391021729\n",
      "training: 81 batch 438 batch_loss: 0.22043582797050476\n",
      "training: 81 batch 439 batch_loss: 0.22118860483169556\n",
      "training: 81 batch 440 batch_loss: 0.22174599766731262\n",
      "training: 81 batch 441 batch_loss: 0.22579824924468994\n",
      "training: 81 batch 442 batch_loss: 0.220094233751297\n",
      "training: 81 batch 443 batch_loss: 0.22057753801345825\n",
      "training: 81 batch 444 batch_loss: 0.22382909059524536\n",
      "training: 81 batch 445 batch_loss: 0.2214210033416748\n",
      "training: 81 batch 446 batch_loss: 0.2185000479221344\n",
      "training: 81 batch 447 batch_loss: 0.22398918867111206\n",
      "training: 81 batch 448 batch_loss: 0.21489819884300232\n",
      "training: 81 batch 449 batch_loss: 0.21809792518615723\n",
      "training: 81 batch 450 batch_loss: 0.22556054592132568\n",
      "training: 81 batch 451 batch_loss: 0.22044464945793152\n",
      "training: 81 batch 452 batch_loss: 0.22477614879608154\n",
      "training: 81 batch 453 batch_loss: 0.22011882066726685\n",
      "training: 81 batch 454 batch_loss: 0.22157758474349976\n",
      "training: 81 batch 455 batch_loss: 0.2162957787513733\n",
      "training: 81 batch 456 batch_loss: 0.22318962216377258\n",
      "training: 81 batch 457 batch_loss: 0.22036737203598022\n",
      "training: 81 batch 458 batch_loss: 0.22151142358779907\n",
      "training: 81 batch 459 batch_loss: 0.22019147872924805\n",
      "training: 81 batch 460 batch_loss: 0.21864432096481323\n",
      "training: 81 batch 461 batch_loss: 0.22082814574241638\n",
      "training: 81 batch 462 batch_loss: 0.2199992537498474\n",
      "training: 81 batch 463 batch_loss: 0.2234756350517273\n",
      "training: 81 batch 464 batch_loss: 0.22064638137817383\n",
      "training: 81 batch 465 batch_loss: 0.22133341431617737\n",
      "training: 81 batch 466 batch_loss: 0.22866642475128174\n",
      "training: 81 batch 467 batch_loss: 0.22327002882957458\n",
      "training: 81 batch 468 batch_loss: 0.22199296951293945\n",
      "training: 81 batch 469 batch_loss: 0.2218114733695984\n",
      "training: 81 batch 470 batch_loss: 0.21678820252418518\n",
      "training: 81 batch 471 batch_loss: 0.22503972053527832\n",
      "training: 81 batch 472 batch_loss: 0.2225797176361084\n",
      "training: 81 batch 473 batch_loss: 0.2191350758075714\n",
      "training: 81 batch 474 batch_loss: 0.21960103511810303\n",
      "training: 81 batch 475 batch_loss: 0.21647372841835022\n",
      "training: 81 batch 476 batch_loss: 0.2200241982936859\n",
      "training: 81 batch 477 batch_loss: 0.2211219072341919\n",
      "training: 81 batch 478 batch_loss: 0.22373446822166443\n",
      "training: 81 batch 479 batch_loss: 0.22571733593940735\n",
      "training: 81 batch 480 batch_loss: 0.2199280858039856\n",
      "training: 81 batch 481 batch_loss: 0.21950671076774597\n",
      "training: 81 batch 482 batch_loss: 0.2249576449394226\n",
      "training: 81 batch 483 batch_loss: 0.22199422121047974\n",
      "training: 81 batch 484 batch_loss: 0.21838870644569397\n",
      "training: 81 batch 485 batch_loss: 0.22441217303276062\n",
      "training: 81 batch 486 batch_loss: 0.22113019227981567\n",
      "training: 81 batch 487 batch_loss: 0.22591859102249146\n",
      "training: 81 batch 488 batch_loss: 0.22169721126556396\n",
      "training: 81 batch 489 batch_loss: 0.2241920530796051\n",
      "training: 81 batch 490 batch_loss: 0.2198491394519806\n",
      "training: 81 batch 491 batch_loss: 0.22058412432670593\n",
      "training: 81 batch 492 batch_loss: 0.22070196270942688\n",
      "training: 81 batch 493 batch_loss: 0.22324848175048828\n",
      "training: 81 batch 494 batch_loss: 0.22118276357650757\n",
      "training: 81 batch 495 batch_loss: 0.2212349772453308\n",
      "training: 81 batch 496 batch_loss: 0.22300490736961365\n",
      "training: 81 batch 497 batch_loss: 0.22127705812454224\n",
      "training: 81 batch 498 batch_loss: 0.22028741240501404\n",
      "training: 81 batch 499 batch_loss: 0.22201061248779297\n",
      "training: 81 batch 500 batch_loss: 0.22393575310707092\n",
      "training: 81 batch 501 batch_loss: 0.21573302149772644\n",
      "training: 81 batch 502 batch_loss: 0.22322872281074524\n",
      "training: 81 batch 503 batch_loss: 0.22194141149520874\n",
      "training: 81 batch 504 batch_loss: 0.22605013847351074\n",
      "training: 81 batch 505 batch_loss: 0.22307413816452026\n",
      "training: 81 batch 506 batch_loss: 0.22033515572547913\n",
      "training: 81 batch 507 batch_loss: 0.224642813205719\n",
      "training: 81 batch 508 batch_loss: 0.22418683767318726\n",
      "training: 81 batch 509 batch_loss: 0.2246774435043335\n",
      "training: 81 batch 510 batch_loss: 0.22553381323814392\n",
      "training: 81 batch 511 batch_loss: 0.22040045261383057\n",
      "training: 81 batch 512 batch_loss: 0.22009381651878357\n",
      "training: 81 batch 513 batch_loss: 0.2222534418106079\n",
      "training: 81 batch 514 batch_loss: 0.21760645508766174\n",
      "training: 81 batch 515 batch_loss: 0.22462153434753418\n",
      "training: 81 batch 516 batch_loss: 0.2252843677997589\n",
      "training: 81 batch 517 batch_loss: 0.22361350059509277\n",
      "training: 81 batch 518 batch_loss: 0.22166109085083008\n",
      "training: 81 batch 519 batch_loss: 0.22276067733764648\n",
      "training: 81 batch 520 batch_loss: 0.2217210829257965\n",
      "training: 81 batch 521 batch_loss: 0.22290655970573425\n",
      "training: 81 batch 522 batch_loss: 0.2233409881591797\n",
      "training: 81 batch 523 batch_loss: 0.2157863974571228\n",
      "training: 81 batch 524 batch_loss: 0.21983036398887634\n",
      "training: 81 batch 525 batch_loss: 0.21971014142036438\n",
      "training: 81 batch 526 batch_loss: 0.22540634870529175\n",
      "training: 81 batch 527 batch_loss: 0.22571143507957458\n",
      "training: 81 batch 528 batch_loss: 0.22247269749641418\n",
      "training: 81 batch 529 batch_loss: 0.21882376074790955\n",
      "training: 81 batch 530 batch_loss: 0.2240055501461029\n",
      "training: 81 batch 531 batch_loss: 0.21899423003196716\n",
      "training: 81 batch 532 batch_loss: 0.2247658371925354\n",
      "training: 81 batch 533 batch_loss: 0.22037845849990845\n",
      "training: 81 batch 534 batch_loss: 0.22263887524604797\n",
      "training: 81 batch 535 batch_loss: 0.22159746289253235\n",
      "training: 81 batch 536 batch_loss: 0.22172051668167114\n",
      "training: 81 batch 537 batch_loss: 0.22598856687545776\n",
      "training: 81 batch 538 batch_loss: 0.22250241041183472\n",
      "training: 81 batch 539 batch_loss: 0.21857061982154846\n",
      "training: 81 batch 540 batch_loss: 0.22270607948303223\n",
      "training: 81 batch 541 batch_loss: 0.22044986486434937\n",
      "training: 81 batch 542 batch_loss: 0.22431457042694092\n",
      "training: 81 batch 543 batch_loss: 0.22272002696990967\n",
      "training: 81 batch 544 batch_loss: 0.21928077936172485\n",
      "training: 81 batch 545 batch_loss: 0.21932390332221985\n",
      "training: 81 batch 546 batch_loss: 0.21729040145874023\n",
      "training: 81 batch 547 batch_loss: 0.22267931699752808\n",
      "training: 81 batch 548 batch_loss: 0.22157052159309387\n",
      "training: 81 batch 549 batch_loss: 0.22208207845687866\n",
      "training: 81 batch 550 batch_loss: 0.22174882888793945\n",
      "training: 81 batch 551 batch_loss: 0.22040536999702454\n",
      "training: 81 batch 552 batch_loss: 0.2279071807861328\n",
      "training: 81 batch 553 batch_loss: 0.22251033782958984\n",
      "training: 81 batch 554 batch_loss: 0.2210431694984436\n",
      "training: 81 batch 555 batch_loss: 0.22326183319091797\n",
      "training: 81 batch 556 batch_loss: 0.22262287139892578\n",
      "training: 81 batch 557 batch_loss: 0.22165513038635254\n",
      "training: 81 batch 558 batch_loss: 0.2203303575515747\n",
      "training: 81 batch 559 batch_loss: 0.22453445196151733\n",
      "training: 81 batch 560 batch_loss: 0.22375190258026123\n",
      "training: 81 batch 561 batch_loss: 0.22159889340400696\n",
      "training: 81 batch 562 batch_loss: 0.22752970457077026\n",
      "training: 81 batch 563 batch_loss: 0.21776950359344482\n",
      "training: 81 batch 564 batch_loss: 0.22808504104614258\n",
      "training: 81 batch 565 batch_loss: 0.21978342533111572\n",
      "training: 81 batch 566 batch_loss: 0.22073432803153992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 81 batch 567 batch_loss: 0.22122222185134888\n",
      "training: 81 batch 568 batch_loss: 0.22012653946876526\n",
      "training: 81 batch 569 batch_loss: 0.22386500239372253\n",
      "training: 81 batch 570 batch_loss: 0.2251490354537964\n",
      "training: 81 batch 571 batch_loss: 0.22547277808189392\n",
      "training: 81 batch 572 batch_loss: 0.22872895002365112\n",
      "training: 81 batch 573 batch_loss: 0.22411704063415527\n",
      "training: 81 batch 574 batch_loss: 0.22107484936714172\n",
      "training: 81 batch 575 batch_loss: 0.22126758098602295\n",
      "training: 81 batch 576 batch_loss: 0.22157827019691467\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 81, Hit Ratio:0.03392484168518253 | Precision:0.05005406468101838 | Recall:0.0658100006779999 | NDCG:0.0652008939742899\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 82 batch 0 batch_loss: 0.2192009687423706\n",
      "training: 82 batch 1 batch_loss: 0.22200697660446167\n",
      "training: 82 batch 2 batch_loss: 0.22137346863746643\n",
      "training: 82 batch 3 batch_loss: 0.22146734595298767\n",
      "training: 82 batch 4 batch_loss: 0.2202194333076477\n",
      "training: 82 batch 5 batch_loss: 0.2217368483543396\n",
      "training: 82 batch 6 batch_loss: 0.21749085187911987\n",
      "training: 82 batch 7 batch_loss: 0.22377127408981323\n",
      "training: 82 batch 8 batch_loss: 0.2239140272140503\n",
      "training: 82 batch 9 batch_loss: 0.2198566198348999\n",
      "training: 82 batch 10 batch_loss: 0.2183154821395874\n",
      "training: 82 batch 11 batch_loss: 0.2161676287651062\n",
      "training: 82 batch 12 batch_loss: 0.21721899509429932\n",
      "training: 82 batch 13 batch_loss: 0.22304534912109375\n",
      "training: 82 batch 14 batch_loss: 0.21924427151679993\n",
      "training: 82 batch 15 batch_loss: 0.2217411994934082\n",
      "training: 82 batch 16 batch_loss: 0.21844226121902466\n",
      "training: 82 batch 17 batch_loss: 0.22302943468093872\n",
      "training: 82 batch 18 batch_loss: 0.218949556350708\n",
      "training: 82 batch 19 batch_loss: 0.2209216058254242\n",
      "training: 82 batch 20 batch_loss: 0.21603286266326904\n",
      "training: 82 batch 21 batch_loss: 0.2168833315372467\n",
      "training: 82 batch 22 batch_loss: 0.22146299481391907\n",
      "training: 82 batch 23 batch_loss: 0.22165781259536743\n",
      "training: 82 batch 24 batch_loss: 0.2184200882911682\n",
      "training: 82 batch 25 batch_loss: 0.22111886739730835\n",
      "training: 82 batch 26 batch_loss: 0.21904125809669495\n",
      "training: 82 batch 27 batch_loss: 0.22958528995513916\n",
      "training: 82 batch 28 batch_loss: 0.21686267852783203\n",
      "training: 82 batch 29 batch_loss: 0.22534877061843872\n",
      "training: 82 batch 30 batch_loss: 0.22114691138267517\n",
      "training: 82 batch 31 batch_loss: 0.21846416592597961\n",
      "training: 82 batch 32 batch_loss: 0.22284233570098877\n",
      "training: 82 batch 33 batch_loss: 0.21875977516174316\n",
      "training: 82 batch 34 batch_loss: 0.21940100193023682\n",
      "training: 82 batch 35 batch_loss: 0.2230866551399231\n",
      "training: 82 batch 36 batch_loss: 0.2261466383934021\n",
      "training: 82 batch 37 batch_loss: 0.22081714868545532\n",
      "training: 82 batch 38 batch_loss: 0.22177433967590332\n",
      "training: 82 batch 39 batch_loss: 0.2200760543346405\n",
      "training: 82 batch 40 batch_loss: 0.21951648592948914\n",
      "training: 82 batch 41 batch_loss: 0.22224628925323486\n",
      "training: 82 batch 42 batch_loss: 0.21877631545066833\n",
      "training: 82 batch 43 batch_loss: 0.21846139430999756\n",
      "training: 82 batch 44 batch_loss: 0.221600741147995\n",
      "training: 82 batch 45 batch_loss: 0.21759331226348877\n",
      "training: 82 batch 46 batch_loss: 0.2227991223335266\n",
      "training: 82 batch 47 batch_loss: 0.21927279233932495\n",
      "training: 82 batch 48 batch_loss: 0.22350677847862244\n",
      "training: 82 batch 49 batch_loss: 0.2182270884513855\n",
      "training: 82 batch 50 batch_loss: 0.2228999137878418\n",
      "training: 82 batch 51 batch_loss: 0.22226345539093018\n",
      "training: 82 batch 52 batch_loss: 0.22102782130241394\n",
      "training: 82 batch 53 batch_loss: 0.21966445446014404\n",
      "training: 82 batch 54 batch_loss: 0.22037985920906067\n",
      "training: 82 batch 55 batch_loss: 0.21967408061027527\n",
      "training: 82 batch 56 batch_loss: 0.22131898999214172\n",
      "training: 82 batch 57 batch_loss: 0.2227543592453003\n",
      "training: 82 batch 58 batch_loss: 0.22552785277366638\n",
      "training: 82 batch 59 batch_loss: 0.21749135851860046\n",
      "training: 82 batch 60 batch_loss: 0.22224947810173035\n",
      "training: 82 batch 61 batch_loss: 0.21599924564361572\n",
      "training: 82 batch 62 batch_loss: 0.23008659482002258\n",
      "training: 82 batch 63 batch_loss: 0.21936887502670288\n",
      "training: 82 batch 64 batch_loss: 0.21959900856018066\n",
      "training: 82 batch 65 batch_loss: 0.22249770164489746\n",
      "training: 82 batch 66 batch_loss: 0.22409498691558838\n",
      "training: 82 batch 67 batch_loss: 0.21767497062683105\n",
      "training: 82 batch 68 batch_loss: 0.2182454764842987\n",
      "training: 82 batch 69 batch_loss: 0.22348874807357788\n",
      "training: 82 batch 70 batch_loss: 0.22075700759887695\n",
      "training: 82 batch 71 batch_loss: 0.22374215722084045\n",
      "training: 82 batch 72 batch_loss: 0.2235274612903595\n",
      "training: 82 batch 73 batch_loss: 0.22164523601531982\n",
      "training: 82 batch 74 batch_loss: 0.22264257073402405\n",
      "training: 82 batch 75 batch_loss: 0.22258847951889038\n",
      "training: 82 batch 76 batch_loss: 0.2161668837070465\n",
      "training: 82 batch 77 batch_loss: 0.22271868586540222\n",
      "training: 82 batch 78 batch_loss: 0.22011947631835938\n",
      "training: 82 batch 79 batch_loss: 0.2219516634941101\n",
      "training: 82 batch 80 batch_loss: 0.22193506360054016\n",
      "training: 82 batch 81 batch_loss: 0.2199491560459137\n",
      "training: 82 batch 82 batch_loss: 0.21920150518417358\n",
      "training: 82 batch 83 batch_loss: 0.22240141034126282\n",
      "training: 82 batch 84 batch_loss: 0.22221684455871582\n",
      "training: 82 batch 85 batch_loss: 0.22195765376091003\n",
      "training: 82 batch 86 batch_loss: 0.21994483470916748\n",
      "training: 82 batch 87 batch_loss: 0.22054404020309448\n",
      "training: 82 batch 88 batch_loss: 0.2178635597229004\n",
      "training: 82 batch 89 batch_loss: 0.22139710187911987\n",
      "training: 82 batch 90 batch_loss: 0.22240006923675537\n",
      "training: 82 batch 91 batch_loss: 0.21992087364196777\n",
      "training: 82 batch 92 batch_loss: 0.22380489110946655\n",
      "training: 82 batch 93 batch_loss: 0.22328588366508484\n",
      "training: 82 batch 94 batch_loss: 0.21955761313438416\n",
      "training: 82 batch 95 batch_loss: 0.22191941738128662\n",
      "training: 82 batch 96 batch_loss: 0.21789777278900146\n",
      "training: 82 batch 97 batch_loss: 0.2170504629611969\n",
      "training: 82 batch 98 batch_loss: 0.22623670101165771\n",
      "training: 82 batch 99 batch_loss: 0.2238239347934723\n",
      "training: 82 batch 100 batch_loss: 0.22502508759498596\n",
      "training: 82 batch 101 batch_loss: 0.22355949878692627\n",
      "training: 82 batch 102 batch_loss: 0.2196158468723297\n",
      "training: 82 batch 103 batch_loss: 0.2204519510269165\n",
      "training: 82 batch 104 batch_loss: 0.22112450003623962\n",
      "training: 82 batch 105 batch_loss: 0.2238553762435913\n",
      "training: 82 batch 106 batch_loss: 0.2228328287601471\n",
      "training: 82 batch 107 batch_loss: 0.2202116847038269\n",
      "training: 82 batch 108 batch_loss: 0.22262519598007202\n",
      "training: 82 batch 109 batch_loss: 0.22537502646446228\n",
      "training: 82 batch 110 batch_loss: 0.22370001673698425\n",
      "training: 82 batch 111 batch_loss: 0.22143834829330444\n",
      "training: 82 batch 112 batch_loss: 0.22333469986915588\n",
      "training: 82 batch 113 batch_loss: 0.22085687518119812\n",
      "training: 82 batch 114 batch_loss: 0.22015756368637085\n",
      "training: 82 batch 115 batch_loss: 0.22049012780189514\n",
      "training: 82 batch 116 batch_loss: 0.22317767143249512\n",
      "training: 82 batch 117 batch_loss: 0.221568763256073\n",
      "training: 82 batch 118 batch_loss: 0.22048702836036682\n",
      "training: 82 batch 119 batch_loss: 0.21384093165397644\n",
      "training: 82 batch 120 batch_loss: 0.21954017877578735\n",
      "training: 82 batch 121 batch_loss: 0.2251695692539215\n",
      "training: 82 batch 122 batch_loss: 0.22198358178138733\n",
      "training: 82 batch 123 batch_loss: 0.2214510142803192\n",
      "training: 82 batch 124 batch_loss: 0.21992754936218262\n",
      "training: 82 batch 125 batch_loss: 0.2204967737197876\n",
      "training: 82 batch 126 batch_loss: 0.21605518460273743\n",
      "training: 82 batch 127 batch_loss: 0.22255882620811462\n",
      "training: 82 batch 128 batch_loss: 0.21861499547958374\n",
      "training: 82 batch 129 batch_loss: 0.21923315525054932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 82 batch 130 batch_loss: 0.2261684536933899\n",
      "training: 82 batch 131 batch_loss: 0.22335046529769897\n",
      "training: 82 batch 132 batch_loss: 0.2202751636505127\n",
      "training: 82 batch 133 batch_loss: 0.22087353467941284\n",
      "training: 82 batch 134 batch_loss: 0.22432458400726318\n",
      "training: 82 batch 135 batch_loss: 0.22614780068397522\n",
      "training: 82 batch 136 batch_loss: 0.2246389389038086\n",
      "training: 82 batch 137 batch_loss: 0.21974188089370728\n",
      "training: 82 batch 138 batch_loss: 0.22222113609313965\n",
      "training: 82 batch 139 batch_loss: 0.21747902035713196\n",
      "training: 82 batch 140 batch_loss: 0.22272849082946777\n",
      "training: 82 batch 141 batch_loss: 0.22280049324035645\n",
      "training: 82 batch 142 batch_loss: 0.22334522008895874\n",
      "training: 82 batch 143 batch_loss: 0.22441250085830688\n",
      "training: 82 batch 144 batch_loss: 0.22200465202331543\n",
      "training: 82 batch 145 batch_loss: 0.22129088640213013\n",
      "training: 82 batch 146 batch_loss: 0.22111377120018005\n",
      "training: 82 batch 147 batch_loss: 0.2178635597229004\n",
      "training: 82 batch 148 batch_loss: 0.222212016582489\n",
      "training: 82 batch 149 batch_loss: 0.22210660576820374\n",
      "training: 82 batch 150 batch_loss: 0.22092610597610474\n",
      "training: 82 batch 151 batch_loss: 0.220470130443573\n",
      "training: 82 batch 152 batch_loss: 0.22402423620224\n",
      "training: 82 batch 153 batch_loss: 0.22696274518966675\n",
      "training: 82 batch 154 batch_loss: 0.2225256860256195\n",
      "training: 82 batch 155 batch_loss: 0.22246837615966797\n",
      "training: 82 batch 156 batch_loss: 0.22261863946914673\n",
      "training: 82 batch 157 batch_loss: 0.22270411252975464\n",
      "training: 82 batch 158 batch_loss: 0.22175437211990356\n",
      "training: 82 batch 159 batch_loss: 0.22070997953414917\n",
      "training: 82 batch 160 batch_loss: 0.22051942348480225\n",
      "training: 82 batch 161 batch_loss: 0.21946892142295837\n",
      "training: 82 batch 162 batch_loss: 0.21664026379585266\n",
      "training: 82 batch 163 batch_loss: 0.2214001715183258\n",
      "training: 82 batch 164 batch_loss: 0.22328299283981323\n",
      "training: 82 batch 165 batch_loss: 0.22120028734207153\n",
      "training: 82 batch 166 batch_loss: 0.22017908096313477\n",
      "training: 82 batch 167 batch_loss: 0.2221854329109192\n",
      "training: 82 batch 168 batch_loss: 0.22494018077850342\n",
      "training: 82 batch 169 batch_loss: 0.22565734386444092\n",
      "training: 82 batch 170 batch_loss: 0.22485992312431335\n",
      "training: 82 batch 171 batch_loss: 0.22163772583007812\n",
      "training: 82 batch 172 batch_loss: 0.2241954207420349\n",
      "training: 82 batch 173 batch_loss: 0.224634051322937\n",
      "training: 82 batch 174 batch_loss: 0.22249239683151245\n",
      "training: 82 batch 175 batch_loss: 0.22694489359855652\n",
      "training: 82 batch 176 batch_loss: 0.22392386198043823\n",
      "training: 82 batch 177 batch_loss: 0.22152525186538696\n",
      "training: 82 batch 178 batch_loss: 0.22440668940544128\n",
      "training: 82 batch 179 batch_loss: 0.22446191310882568\n",
      "training: 82 batch 180 batch_loss: 0.22510838508605957\n",
      "training: 82 batch 181 batch_loss: 0.22055211663246155\n",
      "training: 82 batch 182 batch_loss: 0.2249709963798523\n",
      "training: 82 batch 183 batch_loss: 0.2213890254497528\n",
      "training: 82 batch 184 batch_loss: 0.22345438599586487\n",
      "training: 82 batch 185 batch_loss: 0.2214166820049286\n",
      "training: 82 batch 186 batch_loss: 0.21947824954986572\n",
      "training: 82 batch 187 batch_loss: 0.21709126234054565\n",
      "training: 82 batch 188 batch_loss: 0.22291666269302368\n",
      "training: 82 batch 189 batch_loss: 0.22597992420196533\n",
      "training: 82 batch 190 batch_loss: 0.2237553596496582\n",
      "training: 82 batch 191 batch_loss: 0.22445636987686157\n",
      "training: 82 batch 192 batch_loss: 0.22061720490455627\n",
      "training: 82 batch 193 batch_loss: 0.2227345108985901\n",
      "training: 82 batch 194 batch_loss: 0.21641328930854797\n",
      "training: 82 batch 195 batch_loss: 0.22205820679664612\n",
      "training: 82 batch 196 batch_loss: 0.22304630279541016\n",
      "training: 82 batch 197 batch_loss: 0.22386434674263\n",
      "training: 82 batch 198 batch_loss: 0.22329065203666687\n",
      "training: 82 batch 199 batch_loss: 0.2193639576435089\n",
      "training: 82 batch 200 batch_loss: 0.22013673186302185\n",
      "training: 82 batch 201 batch_loss: 0.22625789046287537\n",
      "training: 82 batch 202 batch_loss: 0.22644665837287903\n",
      "training: 82 batch 203 batch_loss: 0.22009947896003723\n",
      "training: 82 batch 204 batch_loss: 0.2197985053062439\n",
      "training: 82 batch 205 batch_loss: 0.22096657752990723\n",
      "training: 82 batch 206 batch_loss: 0.2280564308166504\n",
      "training: 82 batch 207 batch_loss: 0.2253783643245697\n",
      "training: 82 batch 208 batch_loss: 0.21838295459747314\n",
      "training: 82 batch 209 batch_loss: 0.22299593687057495\n",
      "training: 82 batch 210 batch_loss: 0.22231537103652954\n",
      "training: 82 batch 211 batch_loss: 0.22546863555908203\n",
      "training: 82 batch 212 batch_loss: 0.2291308343410492\n",
      "training: 82 batch 213 batch_loss: 0.22008246183395386\n",
      "training: 82 batch 214 batch_loss: 0.22449123859405518\n",
      "training: 82 batch 215 batch_loss: 0.21682995557785034\n",
      "training: 82 batch 216 batch_loss: 0.22507423162460327\n",
      "training: 82 batch 217 batch_loss: 0.21777760982513428\n",
      "training: 82 batch 218 batch_loss: 0.2262071967124939\n",
      "training: 82 batch 219 batch_loss: 0.21841800212860107\n",
      "training: 82 batch 220 batch_loss: 0.22584888339042664\n",
      "training: 82 batch 221 batch_loss: 0.218526691198349\n",
      "training: 82 batch 222 batch_loss: 0.21718567609786987\n",
      "training: 82 batch 223 batch_loss: 0.21866849064826965\n",
      "training: 82 batch 224 batch_loss: 0.2195427119731903\n",
      "training: 82 batch 225 batch_loss: 0.22455543279647827\n",
      "training: 82 batch 226 batch_loss: 0.22516486048698425\n",
      "training: 82 batch 227 batch_loss: 0.22349107265472412\n",
      "training: 82 batch 228 batch_loss: 0.22010672092437744\n",
      "training: 82 batch 229 batch_loss: 0.22304409742355347\n",
      "training: 82 batch 230 batch_loss: 0.2224103808403015\n",
      "training: 82 batch 231 batch_loss: 0.217879056930542\n",
      "training: 82 batch 232 batch_loss: 0.2214738130569458\n",
      "training: 82 batch 233 batch_loss: 0.22021621465682983\n",
      "training: 82 batch 234 batch_loss: 0.21867519617080688\n",
      "training: 82 batch 235 batch_loss: 0.21932697296142578\n",
      "training: 82 batch 236 batch_loss: 0.22361993789672852\n",
      "training: 82 batch 237 batch_loss: 0.2222699522972107\n",
      "training: 82 batch 238 batch_loss: 0.22276201844215393\n",
      "training: 82 batch 239 batch_loss: 0.22387439012527466\n",
      "training: 82 batch 240 batch_loss: 0.22380155324935913\n",
      "training: 82 batch 241 batch_loss: 0.22003793716430664\n",
      "training: 82 batch 242 batch_loss: 0.22390204668045044\n",
      "training: 82 batch 243 batch_loss: 0.2194730043411255\n",
      "training: 82 batch 244 batch_loss: 0.22241276502609253\n",
      "training: 82 batch 245 batch_loss: 0.22164201736450195\n",
      "training: 82 batch 246 batch_loss: 0.21894344687461853\n",
      "training: 82 batch 247 batch_loss: 0.22072026133537292\n",
      "training: 82 batch 248 batch_loss: 0.21924597024917603\n",
      "training: 82 batch 249 batch_loss: 0.22363460063934326\n",
      "training: 82 batch 250 batch_loss: 0.22734588384628296\n",
      "training: 82 batch 251 batch_loss: 0.22298258543014526\n",
      "training: 82 batch 252 batch_loss: 0.22672909498214722\n",
      "training: 82 batch 253 batch_loss: 0.22652193903923035\n",
      "training: 82 batch 254 batch_loss: 0.2232685387134552\n",
      "training: 82 batch 255 batch_loss: 0.21756690740585327\n",
      "training: 82 batch 256 batch_loss: 0.22474682331085205\n",
      "training: 82 batch 257 batch_loss: 0.22195571660995483\n",
      "training: 82 batch 258 batch_loss: 0.21785292029380798\n",
      "training: 82 batch 259 batch_loss: 0.21927550435066223\n",
      "training: 82 batch 260 batch_loss: 0.22763380408287048\n",
      "training: 82 batch 261 batch_loss: 0.22681307792663574\n",
      "training: 82 batch 262 batch_loss: 0.2222922146320343\n",
      "training: 82 batch 263 batch_loss: 0.22045573592185974\n",
      "training: 82 batch 264 batch_loss: 0.2231701910495758\n",
      "training: 82 batch 265 batch_loss: 0.2208164930343628\n",
      "training: 82 batch 266 batch_loss: 0.22255584597587585\n",
      "training: 82 batch 267 batch_loss: 0.22208082675933838\n",
      "training: 82 batch 268 batch_loss: 0.22562938928604126\n",
      "training: 82 batch 269 batch_loss: 0.21783757209777832\n",
      "training: 82 batch 270 batch_loss: 0.2221257984638214\n",
      "training: 82 batch 271 batch_loss: 0.22213223576545715\n",
      "training: 82 batch 272 batch_loss: 0.2148410975933075\n",
      "training: 82 batch 273 batch_loss: 0.2217881679534912\n",
      "training: 82 batch 274 batch_loss: 0.22194907069206238\n",
      "training: 82 batch 275 batch_loss: 0.2215748131275177\n",
      "training: 82 batch 276 batch_loss: 0.22108641266822815\n",
      "training: 82 batch 277 batch_loss: 0.22158321738243103\n",
      "training: 82 batch 278 batch_loss: 0.22201195359230042\n",
      "training: 82 batch 279 batch_loss: 0.2201940417289734\n",
      "training: 82 batch 280 batch_loss: 0.22096768021583557\n",
      "training: 82 batch 281 batch_loss: 0.220575213432312\n",
      "training: 82 batch 282 batch_loss: 0.2227436900138855\n",
      "training: 82 batch 283 batch_loss: 0.22394639253616333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 82 batch 284 batch_loss: 0.22231370210647583\n",
      "training: 82 batch 285 batch_loss: 0.22159218788146973\n",
      "training: 82 batch 286 batch_loss: 0.2223604917526245\n",
      "training: 82 batch 287 batch_loss: 0.22768938541412354\n",
      "training: 82 batch 288 batch_loss: 0.21902668476104736\n",
      "training: 82 batch 289 batch_loss: 0.22188860177993774\n",
      "training: 82 batch 290 batch_loss: 0.21754944324493408\n",
      "training: 82 batch 291 batch_loss: 0.22918230295181274\n",
      "training: 82 batch 292 batch_loss: 0.22545939683914185\n",
      "training: 82 batch 293 batch_loss: 0.22457945346832275\n",
      "training: 82 batch 294 batch_loss: 0.22538334131240845\n",
      "training: 82 batch 295 batch_loss: 0.2219657301902771\n",
      "training: 82 batch 296 batch_loss: 0.2179708182811737\n",
      "training: 82 batch 297 batch_loss: 0.22333920001983643\n",
      "training: 82 batch 298 batch_loss: 0.2236419916152954\n",
      "training: 82 batch 299 batch_loss: 0.22333192825317383\n",
      "training: 82 batch 300 batch_loss: 0.2229057252407074\n",
      "training: 82 batch 301 batch_loss: 0.2200329601764679\n",
      "training: 82 batch 302 batch_loss: 0.2241971790790558\n",
      "training: 82 batch 303 batch_loss: 0.21752867102622986\n",
      "training: 82 batch 304 batch_loss: 0.2254866361618042\n",
      "training: 82 batch 305 batch_loss: 0.22024577856063843\n",
      "training: 82 batch 306 batch_loss: 0.22279521822929382\n",
      "training: 82 batch 307 batch_loss: 0.22219887375831604\n",
      "training: 82 batch 308 batch_loss: 0.2255207896232605\n",
      "training: 82 batch 309 batch_loss: 0.21819645166397095\n",
      "training: 82 batch 310 batch_loss: 0.22107216715812683\n",
      "training: 82 batch 311 batch_loss: 0.22470083832740784\n",
      "training: 82 batch 312 batch_loss: 0.21966469287872314\n",
      "training: 82 batch 313 batch_loss: 0.22258242964744568\n",
      "training: 82 batch 314 batch_loss: 0.22175759077072144\n",
      "training: 82 batch 315 batch_loss: 0.220921128988266\n",
      "training: 82 batch 316 batch_loss: 0.22104474902153015\n",
      "training: 82 batch 317 batch_loss: 0.2174021601676941\n",
      "training: 82 batch 318 batch_loss: 0.22484037280082703\n",
      "training: 82 batch 319 batch_loss: 0.22033289074897766\n",
      "training: 82 batch 320 batch_loss: 0.21875444054603577\n",
      "training: 82 batch 321 batch_loss: 0.22163820266723633\n",
      "training: 82 batch 322 batch_loss: 0.22118309140205383\n",
      "training: 82 batch 323 batch_loss: 0.22510936856269836\n",
      "training: 82 batch 324 batch_loss: 0.22194981575012207\n",
      "training: 82 batch 325 batch_loss: 0.22044524550437927\n",
      "training: 82 batch 326 batch_loss: 0.22107478976249695\n",
      "training: 82 batch 327 batch_loss: 0.2220308482646942\n",
      "training: 82 batch 328 batch_loss: 0.22676703333854675\n",
      "training: 82 batch 329 batch_loss: 0.22023427486419678\n",
      "training: 82 batch 330 batch_loss: 0.22449004650115967\n",
      "training: 82 batch 331 batch_loss: 0.22049501538276672\n",
      "training: 82 batch 332 batch_loss: 0.21962526440620422\n",
      "training: 82 batch 333 batch_loss: 0.22498691082000732\n",
      "training: 82 batch 334 batch_loss: 0.22150477766990662\n",
      "training: 82 batch 335 batch_loss: 0.22679030895233154\n",
      "training: 82 batch 336 batch_loss: 0.2244890332221985\n",
      "training: 82 batch 337 batch_loss: 0.22280192375183105\n",
      "training: 82 batch 338 batch_loss: 0.22635477781295776\n",
      "training: 82 batch 339 batch_loss: 0.2220320999622345\n",
      "training: 82 batch 340 batch_loss: 0.22113797068595886\n",
      "training: 82 batch 341 batch_loss: 0.22063347697257996\n",
      "training: 82 batch 342 batch_loss: 0.22129622101783752\n",
      "training: 82 batch 343 batch_loss: 0.22000324726104736\n",
      "training: 82 batch 344 batch_loss: 0.22096306085586548\n",
      "training: 82 batch 345 batch_loss: 0.22184768319129944\n",
      "training: 82 batch 346 batch_loss: 0.2205105423927307\n",
      "training: 82 batch 347 batch_loss: 0.22378703951835632\n",
      "training: 82 batch 348 batch_loss: 0.22403523325920105\n",
      "training: 82 batch 349 batch_loss: 0.2219429612159729\n",
      "training: 82 batch 350 batch_loss: 0.22576463222503662\n",
      "training: 82 batch 351 batch_loss: 0.2243899703025818\n",
      "training: 82 batch 352 batch_loss: 0.2246965765953064\n",
      "training: 82 batch 353 batch_loss: 0.2209634780883789\n",
      "training: 82 batch 354 batch_loss: 0.22334519028663635\n",
      "training: 82 batch 355 batch_loss: 0.22255674004554749\n",
      "training: 82 batch 356 batch_loss: 0.2233310341835022\n",
      "training: 82 batch 357 batch_loss: 0.2232796847820282\n",
      "training: 82 batch 358 batch_loss: 0.2254679799079895\n",
      "training: 82 batch 359 batch_loss: 0.22335931658744812\n",
      "training: 82 batch 360 batch_loss: 0.22716858983039856\n",
      "training: 82 batch 361 batch_loss: 0.22157317399978638\n",
      "training: 82 batch 362 batch_loss: 0.22515326738357544\n",
      "training: 82 batch 363 batch_loss: 0.2248813807964325\n",
      "training: 82 batch 364 batch_loss: 0.2228868305683136\n",
      "training: 82 batch 365 batch_loss: 0.2200503945350647\n",
      "training: 82 batch 366 batch_loss: 0.21946847438812256\n",
      "training: 82 batch 367 batch_loss: 0.22280976176261902\n",
      "training: 82 batch 368 batch_loss: 0.22214847803115845\n",
      "training: 82 batch 369 batch_loss: 0.22127237915992737\n",
      "training: 82 batch 370 batch_loss: 0.2216600477695465\n",
      "training: 82 batch 371 batch_loss: 0.21962380409240723\n",
      "training: 82 batch 372 batch_loss: 0.2229541838169098\n",
      "training: 82 batch 373 batch_loss: 0.22595849633216858\n",
      "training: 82 batch 374 batch_loss: 0.22431206703186035\n",
      "training: 82 batch 375 batch_loss: 0.22466632723808289\n",
      "training: 82 batch 376 batch_loss: 0.22333142161369324\n",
      "training: 82 batch 377 batch_loss: 0.22386029362678528\n",
      "training: 82 batch 378 batch_loss: 0.22192320227622986\n",
      "training: 82 batch 379 batch_loss: 0.22388488054275513\n",
      "training: 82 batch 380 batch_loss: 0.22702747583389282\n",
      "training: 82 batch 381 batch_loss: 0.22616592049598694\n",
      "training: 82 batch 382 batch_loss: 0.22185179591178894\n",
      "training: 82 batch 383 batch_loss: 0.21999916434288025\n",
      "training: 82 batch 384 batch_loss: 0.22009342908859253\n",
      "training: 82 batch 385 batch_loss: 0.22560861706733704\n",
      "training: 82 batch 386 batch_loss: 0.22681906819343567\n",
      "training: 82 batch 387 batch_loss: 0.2207714021205902\n",
      "training: 82 batch 388 batch_loss: 0.22681209444999695\n",
      "training: 82 batch 389 batch_loss: 0.22215783596038818\n",
      "training: 82 batch 390 batch_loss: 0.22086885571479797\n",
      "training: 82 batch 391 batch_loss: 0.22324341535568237\n",
      "training: 82 batch 392 batch_loss: 0.22125861048698425\n",
      "training: 82 batch 393 batch_loss: 0.2205110788345337\n",
      "training: 82 batch 394 batch_loss: 0.2243840992450714\n",
      "training: 82 batch 395 batch_loss: 0.21848538517951965\n",
      "training: 82 batch 396 batch_loss: 0.22819039225578308\n",
      "training: 82 batch 397 batch_loss: 0.21724951267242432\n",
      "training: 82 batch 398 batch_loss: 0.2220715582370758\n",
      "training: 82 batch 399 batch_loss: 0.22480231523513794\n",
      "training: 82 batch 400 batch_loss: 0.22015655040740967\n",
      "training: 82 batch 401 batch_loss: 0.21863368153572083\n",
      "training: 82 batch 402 batch_loss: 0.22145342826843262\n",
      "training: 82 batch 403 batch_loss: 0.2238484025001526\n",
      "training: 82 batch 404 batch_loss: 0.22179603576660156\n",
      "training: 82 batch 405 batch_loss: 0.2203846275806427\n",
      "training: 82 batch 406 batch_loss: 0.2186526656150818\n",
      "training: 82 batch 407 batch_loss: 0.2221396565437317\n",
      "training: 82 batch 408 batch_loss: 0.2205769121646881\n",
      "training: 82 batch 409 batch_loss: 0.22002506256103516\n",
      "training: 82 batch 410 batch_loss: 0.22674143314361572\n",
      "training: 82 batch 411 batch_loss: 0.22337201237678528\n",
      "training: 82 batch 412 batch_loss: 0.22333940863609314\n",
      "training: 82 batch 413 batch_loss: 0.21760672330856323\n",
      "training: 82 batch 414 batch_loss: 0.22673606872558594\n",
      "training: 82 batch 415 batch_loss: 0.22367602586746216\n",
      "training: 82 batch 416 batch_loss: 0.22760623693466187\n",
      "training: 82 batch 417 batch_loss: 0.22569701075553894\n",
      "training: 82 batch 418 batch_loss: 0.21951305866241455\n",
      "training: 82 batch 419 batch_loss: 0.22195541858673096\n",
      "training: 82 batch 420 batch_loss: 0.22107988595962524\n",
      "training: 82 batch 421 batch_loss: 0.22515931725502014\n",
      "training: 82 batch 422 batch_loss: 0.2217884659767151\n",
      "training: 82 batch 423 batch_loss: 0.22215646505355835\n",
      "training: 82 batch 424 batch_loss: 0.2245866060256958\n",
      "training: 82 batch 425 batch_loss: 0.2187536060810089\n",
      "training: 82 batch 426 batch_loss: 0.22407028079032898\n",
      "training: 82 batch 427 batch_loss: 0.2215099036693573\n",
      "training: 82 batch 428 batch_loss: 0.22359830141067505\n",
      "training: 82 batch 429 batch_loss: 0.22095268964767456\n",
      "training: 82 batch 430 batch_loss: 0.2174803912639618\n",
      "training: 82 batch 431 batch_loss: 0.22018343210220337\n",
      "training: 82 batch 432 batch_loss: 0.22384467720985413\n",
      "training: 82 batch 433 batch_loss: 0.2248133420944214\n",
      "training: 82 batch 434 batch_loss: 0.2234516143798828\n",
      "training: 82 batch 435 batch_loss: 0.21804970502853394\n",
      "training: 82 batch 436 batch_loss: 0.22050100564956665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 82 batch 437 batch_loss: 0.22136127948760986\n",
      "training: 82 batch 438 batch_loss: 0.22376009821891785\n",
      "training: 82 batch 439 batch_loss: 0.22415125370025635\n",
      "training: 82 batch 440 batch_loss: 0.2249385416507721\n",
      "training: 82 batch 441 batch_loss: 0.22266390919685364\n",
      "training: 82 batch 442 batch_loss: 0.22058367729187012\n",
      "training: 82 batch 443 batch_loss: 0.22020739316940308\n",
      "training: 82 batch 444 batch_loss: 0.22314584255218506\n",
      "training: 82 batch 445 batch_loss: 0.21930232644081116\n",
      "training: 82 batch 446 batch_loss: 0.22515618801116943\n",
      "training: 82 batch 447 batch_loss: 0.22044792771339417\n",
      "training: 82 batch 448 batch_loss: 0.22236400842666626\n",
      "training: 82 batch 449 batch_loss: 0.22104579210281372\n",
      "training: 82 batch 450 batch_loss: 0.22581955790519714\n",
      "training: 82 batch 451 batch_loss: 0.2225182056427002\n",
      "training: 82 batch 452 batch_loss: 0.2206338346004486\n",
      "training: 82 batch 453 batch_loss: 0.22274497151374817\n",
      "training: 82 batch 454 batch_loss: 0.22218531370162964\n",
      "training: 82 batch 455 batch_loss: 0.22236600518226624\n",
      "training: 82 batch 456 batch_loss: 0.22396963834762573\n",
      "training: 82 batch 457 batch_loss: 0.22536152601242065\n",
      "training: 82 batch 458 batch_loss: 0.22648337483406067\n",
      "training: 82 batch 459 batch_loss: 0.22266507148742676\n",
      "training: 82 batch 460 batch_loss: 0.2283441722393036\n",
      "training: 82 batch 461 batch_loss: 0.22046571969985962\n",
      "training: 82 batch 462 batch_loss: 0.22875651717185974\n",
      "training: 82 batch 463 batch_loss: 0.2242700159549713\n",
      "training: 82 batch 464 batch_loss: 0.22324669361114502\n",
      "training: 82 batch 465 batch_loss: 0.22677016258239746\n",
      "training: 82 batch 466 batch_loss: 0.2243008017539978\n",
      "training: 82 batch 467 batch_loss: 0.22180256247520447\n",
      "training: 82 batch 468 batch_loss: 0.2220952808856964\n",
      "training: 82 batch 469 batch_loss: 0.22408932447433472\n",
      "training: 82 batch 470 batch_loss: 0.22804981470108032\n",
      "training: 82 batch 471 batch_loss: 0.22165384888648987\n",
      "training: 82 batch 472 batch_loss: 0.22398924827575684\n",
      "training: 82 batch 473 batch_loss: 0.22799310088157654\n",
      "training: 82 batch 474 batch_loss: 0.2244497537612915\n",
      "training: 82 batch 475 batch_loss: 0.22331735491752625\n",
      "training: 82 batch 476 batch_loss: 0.2218746542930603\n",
      "training: 82 batch 477 batch_loss: 0.22038444876670837\n",
      "training: 82 batch 478 batch_loss: 0.2254755198955536\n",
      "training: 82 batch 479 batch_loss: 0.22801679372787476\n",
      "training: 82 batch 480 batch_loss: 0.21768403053283691\n",
      "training: 82 batch 481 batch_loss: 0.22183895111083984\n",
      "training: 82 batch 482 batch_loss: 0.22129502892494202\n",
      "training: 82 batch 483 batch_loss: 0.22084248065948486\n",
      "training: 82 batch 484 batch_loss: 0.22263577580451965\n",
      "training: 82 batch 485 batch_loss: 0.22166433930397034\n",
      "training: 82 batch 486 batch_loss: 0.22188353538513184\n",
      "training: 82 batch 487 batch_loss: 0.2216159999370575\n",
      "training: 82 batch 488 batch_loss: 0.22477036714553833\n",
      "training: 82 batch 489 batch_loss: 0.22650524973869324\n",
      "training: 82 batch 490 batch_loss: 0.22100847959518433\n",
      "training: 82 batch 491 batch_loss: 0.22559389472007751\n",
      "training: 82 batch 492 batch_loss: 0.22140875458717346\n",
      "training: 82 batch 493 batch_loss: 0.22258487343788147\n",
      "training: 82 batch 494 batch_loss: 0.22362971305847168\n",
      "training: 82 batch 495 batch_loss: 0.22214871644973755\n",
      "training: 82 batch 496 batch_loss: 0.22258734703063965\n",
      "training: 82 batch 497 batch_loss: 0.22085899114608765\n",
      "training: 82 batch 498 batch_loss: 0.22459354996681213\n",
      "training: 82 batch 499 batch_loss: 0.22015193104743958\n",
      "training: 82 batch 500 batch_loss: 0.2249400019645691\n",
      "training: 82 batch 501 batch_loss: 0.2211146354675293\n",
      "training: 82 batch 502 batch_loss: 0.22118884325027466\n",
      "training: 82 batch 503 batch_loss: 0.22422754764556885\n",
      "training: 82 batch 504 batch_loss: 0.21658951044082642\n",
      "training: 82 batch 505 batch_loss: 0.2238522171974182\n",
      "training: 82 batch 506 batch_loss: 0.2232973575592041\n",
      "training: 82 batch 507 batch_loss: 0.22123345732688904\n",
      "training: 82 batch 508 batch_loss: 0.22522860765457153\n",
      "training: 82 batch 509 batch_loss: 0.2212291955947876\n",
      "training: 82 batch 510 batch_loss: 0.22654256224632263\n",
      "training: 82 batch 511 batch_loss: 0.2235015332698822\n",
      "training: 82 batch 512 batch_loss: 0.22261780500411987\n",
      "training: 82 batch 513 batch_loss: 0.22333142161369324\n",
      "training: 82 batch 514 batch_loss: 0.22360363602638245\n",
      "training: 82 batch 515 batch_loss: 0.22472193837165833\n",
      "training: 82 batch 516 batch_loss: 0.22283855080604553\n",
      "training: 82 batch 517 batch_loss: 0.22090473771095276\n",
      "training: 82 batch 518 batch_loss: 0.22383078932762146\n",
      "training: 82 batch 519 batch_loss: 0.22147750854492188\n",
      "training: 82 batch 520 batch_loss: 0.21674200892448425\n",
      "training: 82 batch 521 batch_loss: 0.22221213579177856\n",
      "training: 82 batch 522 batch_loss: 0.21755394339561462\n",
      "training: 82 batch 523 batch_loss: 0.22099876403808594\n",
      "training: 82 batch 524 batch_loss: 0.22012478113174438\n",
      "training: 82 batch 525 batch_loss: 0.2244364619255066\n",
      "training: 82 batch 526 batch_loss: 0.22346532344818115\n",
      "training: 82 batch 527 batch_loss: 0.22056487202644348\n",
      "training: 82 batch 528 batch_loss: 0.2265382707118988\n",
      "training: 82 batch 529 batch_loss: 0.22368913888931274\n",
      "training: 82 batch 530 batch_loss: 0.22180959582328796\n",
      "training: 82 batch 531 batch_loss: 0.2244437038898468\n",
      "training: 82 batch 532 batch_loss: 0.22214171290397644\n",
      "training: 82 batch 533 batch_loss: 0.2222895324230194\n",
      "training: 82 batch 534 batch_loss: 0.22345757484436035\n",
      "training: 82 batch 535 batch_loss: 0.22630435228347778\n",
      "training: 82 batch 536 batch_loss: 0.21957546472549438\n",
      "training: 82 batch 537 batch_loss: 0.22264251112937927\n",
      "training: 82 batch 538 batch_loss: 0.22380009293556213\n",
      "training: 82 batch 539 batch_loss: 0.22328001260757446\n",
      "training: 82 batch 540 batch_loss: 0.2244185209274292\n",
      "training: 82 batch 541 batch_loss: 0.22144117951393127\n",
      "training: 82 batch 542 batch_loss: 0.22463786602020264\n",
      "training: 82 batch 543 batch_loss: 0.2227972447872162\n",
      "training: 82 batch 544 batch_loss: 0.22477751970291138\n",
      "training: 82 batch 545 batch_loss: 0.22326117753982544\n",
      "training: 82 batch 546 batch_loss: 0.22473546862602234\n",
      "training: 82 batch 547 batch_loss: 0.22461587190628052\n",
      "training: 82 batch 548 batch_loss: 0.22644376754760742\n",
      "training: 82 batch 549 batch_loss: 0.22481361031532288\n",
      "training: 82 batch 550 batch_loss: 0.22346919775009155\n",
      "training: 82 batch 551 batch_loss: 0.22286048531532288\n",
      "training: 82 batch 552 batch_loss: 0.22678601741790771\n",
      "training: 82 batch 553 batch_loss: 0.22160792350769043\n",
      "training: 82 batch 554 batch_loss: 0.2236088514328003\n",
      "training: 82 batch 555 batch_loss: 0.22101587057113647\n",
      "training: 82 batch 556 batch_loss: 0.22474610805511475\n",
      "training: 82 batch 557 batch_loss: 0.22256681323051453\n",
      "training: 82 batch 558 batch_loss: 0.2266853153705597\n",
      "training: 82 batch 559 batch_loss: 0.22704148292541504\n",
      "training: 82 batch 560 batch_loss: 0.225802481174469\n",
      "training: 82 batch 561 batch_loss: 0.22482621669769287\n",
      "training: 82 batch 562 batch_loss: 0.22320222854614258\n",
      "training: 82 batch 563 batch_loss: 0.22490406036376953\n",
      "training: 82 batch 564 batch_loss: 0.21860980987548828\n",
      "training: 82 batch 565 batch_loss: 0.22533512115478516\n",
      "training: 82 batch 566 batch_loss: 0.22004768252372742\n",
      "training: 82 batch 567 batch_loss: 0.2243298888206482\n",
      "training: 82 batch 568 batch_loss: 0.22134745121002197\n",
      "training: 82 batch 569 batch_loss: 0.22621136903762817\n",
      "training: 82 batch 570 batch_loss: 0.22129720449447632\n",
      "training: 82 batch 571 batch_loss: 0.22325599193572998\n",
      "training: 82 batch 572 batch_loss: 0.2244882583618164\n",
      "training: 82 batch 573 batch_loss: 0.22131183743476868\n",
      "training: 82 batch 574 batch_loss: 0.22458261251449585\n",
      "training: 82 batch 575 batch_loss: 0.2230183482170105\n",
      "training: 82 batch 576 batch_loss: 0.21971318125724792\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 82, Hit Ratio:0.03363502813190181 | Precision:0.049626462203873 | Recall:0.06568345159470594 | NDCG:0.0646979336554984\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 83 batch 0 batch_loss: 0.22102108597755432\n",
      "training: 83 batch 1 batch_loss: 0.2156098186969757\n",
      "training: 83 batch 2 batch_loss: 0.21938788890838623\n",
      "training: 83 batch 3 batch_loss: 0.21944615244865417\n",
      "training: 83 batch 4 batch_loss: 0.22255867719650269\n",
      "training: 83 batch 5 batch_loss: 0.2196735441684723\n",
      "training: 83 batch 6 batch_loss: 0.21969074010849\n",
      "training: 83 batch 7 batch_loss: 0.21946924924850464\n",
      "training: 83 batch 8 batch_loss: 0.2246323823928833\n",
      "training: 83 batch 9 batch_loss: 0.22077614068984985\n",
      "training: 83 batch 10 batch_loss: 0.2230910360813141\n",
      "training: 83 batch 11 batch_loss: 0.21869200468063354\n",
      "training: 83 batch 12 batch_loss: 0.22535991668701172\n",
      "training: 83 batch 13 batch_loss: 0.22388145327568054\n",
      "training: 83 batch 14 batch_loss: 0.22042104601860046\n",
      "training: 83 batch 15 batch_loss: 0.22381776571273804\n",
      "training: 83 batch 16 batch_loss: 0.2200576663017273\n",
      "training: 83 batch 17 batch_loss: 0.22780734300613403\n",
      "training: 83 batch 18 batch_loss: 0.2224973440170288\n",
      "training: 83 batch 19 batch_loss: 0.22222959995269775\n",
      "training: 83 batch 20 batch_loss: 0.22258898615837097\n",
      "training: 83 batch 21 batch_loss: 0.21891111135482788\n",
      "training: 83 batch 22 batch_loss: 0.22167319059371948\n",
      "training: 83 batch 23 batch_loss: 0.22450917959213257\n",
      "training: 83 batch 24 batch_loss: 0.22345814108848572\n",
      "training: 83 batch 25 batch_loss: 0.21725013852119446\n",
      "training: 83 batch 26 batch_loss: 0.22347071766853333\n",
      "training: 83 batch 27 batch_loss: 0.22500202059745789\n",
      "training: 83 batch 28 batch_loss: 0.21540644764900208\n",
      "training: 83 batch 29 batch_loss: 0.21747201681137085\n",
      "training: 83 batch 30 batch_loss: 0.2231784164905548\n",
      "training: 83 batch 31 batch_loss: 0.22165751457214355\n",
      "training: 83 batch 32 batch_loss: 0.2192399799823761\n",
      "training: 83 batch 33 batch_loss: 0.2204928696155548\n",
      "training: 83 batch 34 batch_loss: 0.22199976444244385\n",
      "training: 83 batch 35 batch_loss: 0.224978506565094\n",
      "training: 83 batch 36 batch_loss: 0.22408533096313477\n",
      "training: 83 batch 37 batch_loss: 0.2231796681880951\n",
      "training: 83 batch 38 batch_loss: 0.22426962852478027\n",
      "training: 83 batch 39 batch_loss: 0.21711444854736328\n",
      "training: 83 batch 40 batch_loss: 0.22387734055519104\n",
      "training: 83 batch 41 batch_loss: 0.22206777334213257\n",
      "training: 83 batch 42 batch_loss: 0.22173774242401123\n",
      "training: 83 batch 43 batch_loss: 0.22169828414916992\n",
      "training: 83 batch 44 batch_loss: 0.22354474663734436\n",
      "training: 83 batch 45 batch_loss: 0.22342374920845032\n",
      "training: 83 batch 46 batch_loss: 0.22103172540664673\n",
      "training: 83 batch 47 batch_loss: 0.21098127961158752\n",
      "training: 83 batch 48 batch_loss: 0.22647976875305176\n",
      "training: 83 batch 49 batch_loss: 0.2215627133846283\n",
      "training: 83 batch 50 batch_loss: 0.21631455421447754\n",
      "training: 83 batch 51 batch_loss: 0.22191384434700012\n",
      "training: 83 batch 52 batch_loss: 0.2206130027770996\n",
      "training: 83 batch 53 batch_loss: 0.22320032119750977\n",
      "training: 83 batch 54 batch_loss: 0.22066986560821533\n",
      "training: 83 batch 55 batch_loss: 0.22088861465454102\n",
      "training: 83 batch 56 batch_loss: 0.2257019579410553\n",
      "training: 83 batch 57 batch_loss: 0.2177850604057312\n",
      "training: 83 batch 58 batch_loss: 0.22124773263931274\n",
      "training: 83 batch 59 batch_loss: 0.2239970862865448\n",
      "training: 83 batch 60 batch_loss: 0.21773293614387512\n",
      "training: 83 batch 61 batch_loss: 0.21869802474975586\n",
      "training: 83 batch 62 batch_loss: 0.22094270586967468\n",
      "training: 83 batch 63 batch_loss: 0.22426480054855347\n",
      "training: 83 batch 64 batch_loss: 0.22111856937408447\n",
      "training: 83 batch 65 batch_loss: 0.22728371620178223\n",
      "training: 83 batch 66 batch_loss: 0.2226390540599823\n",
      "training: 83 batch 67 batch_loss: 0.22161900997161865\n",
      "training: 83 batch 68 batch_loss: 0.22380325198173523\n",
      "training: 83 batch 69 batch_loss: 0.22387263178825378\n",
      "training: 83 batch 70 batch_loss: 0.2256653904914856\n",
      "training: 83 batch 71 batch_loss: 0.21813428401947021\n",
      "training: 83 batch 72 batch_loss: 0.21895289421081543\n",
      "training: 83 batch 73 batch_loss: 0.22330796718597412\n",
      "training: 83 batch 74 batch_loss: 0.22423699498176575\n",
      "training: 83 batch 75 batch_loss: 0.21963658928871155\n",
      "training: 83 batch 76 batch_loss: 0.22784516215324402\n",
      "training: 83 batch 77 batch_loss: 0.22344917058944702\n",
      "training: 83 batch 78 batch_loss: 0.21756139397621155\n",
      "training: 83 batch 79 batch_loss: 0.2227107286453247\n",
      "training: 83 batch 80 batch_loss: 0.22637096047401428\n",
      "training: 83 batch 81 batch_loss: 0.2211378812789917\n",
      "training: 83 batch 82 batch_loss: 0.22395071387290955\n",
      "training: 83 batch 83 batch_loss: 0.2240014672279358\n",
      "training: 83 batch 84 batch_loss: 0.22479918599128723\n",
      "training: 83 batch 85 batch_loss: 0.22161611914634705\n",
      "training: 83 batch 86 batch_loss: 0.22209081053733826\n",
      "training: 83 batch 87 batch_loss: 0.22124511003494263\n",
      "training: 83 batch 88 batch_loss: 0.22382521629333496\n",
      "training: 83 batch 89 batch_loss: 0.22162753343582153\n",
      "training: 83 batch 90 batch_loss: 0.22137460112571716\n",
      "training: 83 batch 91 batch_loss: 0.22509536147117615\n",
      "training: 83 batch 92 batch_loss: 0.22249484062194824\n",
      "training: 83 batch 93 batch_loss: 0.22389918565750122\n",
      "training: 83 batch 94 batch_loss: 0.21771034598350525\n",
      "training: 83 batch 95 batch_loss: 0.22479256987571716\n",
      "training: 83 batch 96 batch_loss: 0.22540265321731567\n",
      "training: 83 batch 97 batch_loss: 0.22555837035179138\n",
      "training: 83 batch 98 batch_loss: 0.2215273082256317\n",
      "training: 83 batch 99 batch_loss: 0.222591370344162\n",
      "training: 83 batch 100 batch_loss: 0.22226783633232117\n",
      "training: 83 batch 101 batch_loss: 0.22228890657424927\n",
      "training: 83 batch 102 batch_loss: 0.22535738348960876\n",
      "training: 83 batch 103 batch_loss: 0.22217175364494324\n",
      "training: 83 batch 104 batch_loss: 0.22336611151695251\n",
      "training: 83 batch 105 batch_loss: 0.22518262267112732\n",
      "training: 83 batch 106 batch_loss: 0.223724365234375\n",
      "training: 83 batch 107 batch_loss: 0.22105690836906433\n",
      "training: 83 batch 108 batch_loss: 0.22899866104125977\n",
      "training: 83 batch 109 batch_loss: 0.21881544589996338\n",
      "training: 83 batch 110 batch_loss: 0.21820634603500366\n",
      "training: 83 batch 111 batch_loss: 0.2226576805114746\n",
      "training: 83 batch 112 batch_loss: 0.2200741171836853\n",
      "training: 83 batch 113 batch_loss: 0.21984615921974182\n",
      "training: 83 batch 114 batch_loss: 0.22095578908920288\n",
      "training: 83 batch 115 batch_loss: 0.221463143825531\n",
      "training: 83 batch 116 batch_loss: 0.22560709714889526\n",
      "training: 83 batch 117 batch_loss: 0.22505024075508118\n",
      "training: 83 batch 118 batch_loss: 0.21834588050842285\n",
      "training: 83 batch 119 batch_loss: 0.21746933460235596\n",
      "training: 83 batch 120 batch_loss: 0.22027802467346191\n",
      "training: 83 batch 121 batch_loss: 0.21707278490066528\n",
      "training: 83 batch 122 batch_loss: 0.22413510084152222\n",
      "training: 83 batch 123 batch_loss: 0.22406774759292603\n",
      "training: 83 batch 124 batch_loss: 0.22173520922660828\n",
      "training: 83 batch 125 batch_loss: 0.21845215559005737\n",
      "training: 83 batch 126 batch_loss: 0.22294384241104126\n",
      "training: 83 batch 127 batch_loss: 0.22261017560958862\n",
      "training: 83 batch 128 batch_loss: 0.22172725200653076\n",
      "training: 83 batch 129 batch_loss: 0.22154691815376282\n",
      "training: 83 batch 130 batch_loss: 0.222327321767807\n",
      "training: 83 batch 131 batch_loss: 0.2226845920085907\n",
      "training: 83 batch 132 batch_loss: 0.2187773585319519\n",
      "training: 83 batch 133 batch_loss: 0.22003859281539917\n",
      "training: 83 batch 134 batch_loss: 0.22420114278793335\n",
      "training: 83 batch 135 batch_loss: 0.22137033939361572\n",
      "training: 83 batch 136 batch_loss: 0.2250138521194458\n",
      "training: 83 batch 137 batch_loss: 0.22114843130111694\n",
      "training: 83 batch 138 batch_loss: 0.21596592664718628\n",
      "training: 83 batch 139 batch_loss: 0.2244347631931305\n",
      "training: 83 batch 140 batch_loss: 0.22423425316810608\n",
      "training: 83 batch 141 batch_loss: 0.2232607901096344\n",
      "training: 83 batch 142 batch_loss: 0.22432899475097656\n",
      "training: 83 batch 143 batch_loss: 0.22106465697288513\n",
      "training: 83 batch 144 batch_loss: 0.22507557272911072\n",
      "training: 83 batch 145 batch_loss: 0.22094297409057617\n",
      "training: 83 batch 146 batch_loss: 0.22326728701591492\n",
      "training: 83 batch 147 batch_loss: 0.2249552309513092\n",
      "training: 83 batch 148 batch_loss: 0.2232871651649475\n",
      "training: 83 batch 149 batch_loss: 0.21897801756858826\n",
      "training: 83 batch 150 batch_loss: 0.21933668851852417\n",
      "training: 83 batch 151 batch_loss: 0.22350412607192993\n",
      "training: 83 batch 152 batch_loss: 0.22307059168815613\n",
      "training: 83 batch 153 batch_loss: 0.22238177061080933\n",
      "training: 83 batch 154 batch_loss: 0.22465044260025024\n",
      "training: 83 batch 155 batch_loss: 0.2230176031589508\n",
      "training: 83 batch 156 batch_loss: 0.22021234035491943\n",
      "training: 83 batch 157 batch_loss: 0.22612333297729492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 83 batch 158 batch_loss: 0.22043132781982422\n",
      "training: 83 batch 159 batch_loss: 0.22170120477676392\n",
      "training: 83 batch 160 batch_loss: 0.22284501791000366\n",
      "training: 83 batch 161 batch_loss: 0.2199283242225647\n",
      "training: 83 batch 162 batch_loss: 0.2258690893650055\n",
      "training: 83 batch 163 batch_loss: 0.2241331934928894\n",
      "training: 83 batch 164 batch_loss: 0.22103983163833618\n",
      "training: 83 batch 165 batch_loss: 0.2232338786125183\n",
      "training: 83 batch 166 batch_loss: 0.22201275825500488\n",
      "training: 83 batch 167 batch_loss: 0.22370347380638123\n",
      "training: 83 batch 168 batch_loss: 0.22652390599250793\n",
      "training: 83 batch 169 batch_loss: 0.22665247321128845\n",
      "training: 83 batch 170 batch_loss: 0.2207411527633667\n",
      "training: 83 batch 171 batch_loss: 0.2272537350654602\n",
      "training: 83 batch 172 batch_loss: 0.2205122709274292\n",
      "training: 83 batch 173 batch_loss: 0.2216944396495819\n",
      "training: 83 batch 174 batch_loss: 0.22120440006256104\n",
      "training: 83 batch 175 batch_loss: 0.22338339686393738\n",
      "training: 83 batch 176 batch_loss: 0.22195744514465332\n",
      "training: 83 batch 177 batch_loss: 0.22147461771965027\n",
      "training: 83 batch 178 batch_loss: 0.21893739700317383\n",
      "training: 83 batch 179 batch_loss: 0.21850267052650452\n",
      "training: 83 batch 180 batch_loss: 0.22044619917869568\n",
      "training: 83 batch 181 batch_loss: 0.2244877815246582\n",
      "training: 83 batch 182 batch_loss: 0.22362059354782104\n",
      "training: 83 batch 183 batch_loss: 0.21897053718566895\n",
      "training: 83 batch 184 batch_loss: 0.21861165761947632\n",
      "training: 83 batch 185 batch_loss: 0.22571918368339539\n",
      "training: 83 batch 186 batch_loss: 0.21913403272628784\n",
      "training: 83 batch 187 batch_loss: 0.22119194269180298\n",
      "training: 83 batch 188 batch_loss: 0.21661531925201416\n",
      "training: 83 batch 189 batch_loss: 0.22468164563179016\n",
      "training: 83 batch 190 batch_loss: 0.22167372703552246\n",
      "training: 83 batch 191 batch_loss: 0.22263899445533752\n",
      "training: 83 batch 192 batch_loss: 0.2270638644695282\n",
      "training: 83 batch 193 batch_loss: 0.22483724355697632\n",
      "training: 83 batch 194 batch_loss: 0.22513732314109802\n",
      "training: 83 batch 195 batch_loss: 0.2234545648097992\n",
      "training: 83 batch 196 batch_loss: 0.22495582699775696\n",
      "training: 83 batch 197 batch_loss: 0.22179022431373596\n",
      "training: 83 batch 198 batch_loss: 0.22425121068954468\n",
      "training: 83 batch 199 batch_loss: 0.22718459367752075\n",
      "training: 83 batch 200 batch_loss: 0.2227058708667755\n",
      "training: 83 batch 201 batch_loss: 0.21935206651687622\n",
      "training: 83 batch 202 batch_loss: 0.2255791425704956\n",
      "training: 83 batch 203 batch_loss: 0.22203749418258667\n",
      "training: 83 batch 204 batch_loss: 0.2200215458869934\n",
      "training: 83 batch 205 batch_loss: 0.21926912665367126\n",
      "training: 83 batch 206 batch_loss: 0.2210368812084198\n",
      "training: 83 batch 207 batch_loss: 0.22424232959747314\n",
      "training: 83 batch 208 batch_loss: 0.21977102756500244\n",
      "training: 83 batch 209 batch_loss: 0.2227795124053955\n",
      "training: 83 batch 210 batch_loss: 0.22283199429512024\n",
      "training: 83 batch 211 batch_loss: 0.2242071032524109\n",
      "training: 83 batch 212 batch_loss: 0.2200356125831604\n",
      "training: 83 batch 213 batch_loss: 0.21728664636611938\n",
      "training: 83 batch 214 batch_loss: 0.2235787808895111\n",
      "training: 83 batch 215 batch_loss: 0.21823644638061523\n",
      "training: 83 batch 216 batch_loss: 0.22140786051750183\n",
      "training: 83 batch 217 batch_loss: 0.22272425889968872\n",
      "training: 83 batch 218 batch_loss: 0.22293692827224731\n",
      "training: 83 batch 219 batch_loss: 0.22291308641433716\n",
      "training: 83 batch 220 batch_loss: 0.22165170311927795\n",
      "training: 83 batch 221 batch_loss: 0.2228497564792633\n",
      "training: 83 batch 222 batch_loss: 0.22265207767486572\n",
      "training: 83 batch 223 batch_loss: 0.22602024674415588\n",
      "training: 83 batch 224 batch_loss: 0.22385722398757935\n",
      "training: 83 batch 225 batch_loss: 0.22387376427650452\n",
      "training: 83 batch 226 batch_loss: 0.2189292311668396\n",
      "training: 83 batch 227 batch_loss: 0.21779027581214905\n",
      "training: 83 batch 228 batch_loss: 0.22432735562324524\n",
      "training: 83 batch 229 batch_loss: 0.22291690111160278\n",
      "training: 83 batch 230 batch_loss: 0.21912869811058044\n",
      "training: 83 batch 231 batch_loss: 0.22134539484977722\n",
      "training: 83 batch 232 batch_loss: 0.2174619436264038\n",
      "training: 83 batch 233 batch_loss: 0.22320082783699036\n",
      "training: 83 batch 234 batch_loss: 0.22269737720489502\n",
      "training: 83 batch 235 batch_loss: 0.22379937767982483\n",
      "training: 83 batch 236 batch_loss: 0.22403499484062195\n",
      "training: 83 batch 237 batch_loss: 0.2199871838092804\n",
      "training: 83 batch 238 batch_loss: 0.22532209753990173\n",
      "training: 83 batch 239 batch_loss: 0.21803846955299377\n",
      "training: 83 batch 240 batch_loss: 0.22118765115737915\n",
      "training: 83 batch 241 batch_loss: 0.22381988167762756\n",
      "training: 83 batch 242 batch_loss: 0.22106516361236572\n",
      "training: 83 batch 243 batch_loss: 0.22212082147598267\n",
      "training: 83 batch 244 batch_loss: 0.22333145141601562\n",
      "training: 83 batch 245 batch_loss: 0.22267019748687744\n",
      "training: 83 batch 246 batch_loss: 0.2201634645462036\n",
      "training: 83 batch 247 batch_loss: 0.221693754196167\n",
      "training: 83 batch 248 batch_loss: 0.2215578258037567\n",
      "training: 83 batch 249 batch_loss: 0.22298187017440796\n",
      "training: 83 batch 250 batch_loss: 0.22078442573547363\n",
      "training: 83 batch 251 batch_loss: 0.2196483612060547\n",
      "training: 83 batch 252 batch_loss: 0.22487834095954895\n",
      "training: 83 batch 253 batch_loss: 0.22309839725494385\n",
      "training: 83 batch 254 batch_loss: 0.22727099061012268\n",
      "training: 83 batch 255 batch_loss: 0.2194756269454956\n",
      "training: 83 batch 256 batch_loss: 0.2272205352783203\n",
      "training: 83 batch 257 batch_loss: 0.22322601079940796\n",
      "training: 83 batch 258 batch_loss: 0.22025206685066223\n",
      "training: 83 batch 259 batch_loss: 0.22204947471618652\n",
      "training: 83 batch 260 batch_loss: 0.22549304366111755\n",
      "training: 83 batch 261 batch_loss: 0.22855907678604126\n",
      "training: 83 batch 262 batch_loss: 0.22337713837623596\n",
      "training: 83 batch 263 batch_loss: 0.2217206358909607\n",
      "training: 83 batch 264 batch_loss: 0.21987947821617126\n",
      "training: 83 batch 265 batch_loss: 0.22054877877235413\n",
      "training: 83 batch 266 batch_loss: 0.22176170349121094\n",
      "training: 83 batch 267 batch_loss: 0.222476989030838\n",
      "training: 83 batch 268 batch_loss: 0.22378501296043396\n",
      "training: 83 batch 269 batch_loss: 0.22078222036361694\n",
      "training: 83 batch 270 batch_loss: 0.22262448072433472\n",
      "training: 83 batch 271 batch_loss: 0.21904706954956055\n",
      "training: 83 batch 272 batch_loss: 0.2235516905784607\n",
      "training: 83 batch 273 batch_loss: 0.21994373202323914\n",
      "training: 83 batch 274 batch_loss: 0.21943268179893494\n",
      "training: 83 batch 275 batch_loss: 0.21680545806884766\n",
      "training: 83 batch 276 batch_loss: 0.22035738825798035\n",
      "training: 83 batch 277 batch_loss: 0.22715938091278076\n",
      "training: 83 batch 278 batch_loss: 0.21705219149589539\n",
      "training: 83 batch 279 batch_loss: 0.22060799598693848\n",
      "training: 83 batch 280 batch_loss: 0.22429189085960388\n",
      "training: 83 batch 281 batch_loss: 0.22454708814620972\n",
      "training: 83 batch 282 batch_loss: 0.22299420833587646\n",
      "training: 83 batch 283 batch_loss: 0.22427651286125183\n",
      "training: 83 batch 284 batch_loss: 0.2209782898426056\n",
      "training: 83 batch 285 batch_loss: 0.21764686703681946\n",
      "training: 83 batch 286 batch_loss: 0.22016280889511108\n",
      "training: 83 batch 287 batch_loss: 0.22079351544380188\n",
      "training: 83 batch 288 batch_loss: 0.22434121370315552\n",
      "training: 83 batch 289 batch_loss: 0.22153079509735107\n",
      "training: 83 batch 290 batch_loss: 0.2243902087211609\n",
      "training: 83 batch 291 batch_loss: 0.22087708115577698\n",
      "training: 83 batch 292 batch_loss: 0.22286251187324524\n",
      "training: 83 batch 293 batch_loss: 0.22337785363197327\n",
      "training: 83 batch 294 batch_loss: 0.22259947657585144\n",
      "training: 83 batch 295 batch_loss: 0.2236155867576599\n",
      "training: 83 batch 296 batch_loss: 0.22565212845802307\n",
      "training: 83 batch 297 batch_loss: 0.2273937463760376\n",
      "training: 83 batch 298 batch_loss: 0.21834760904312134\n",
      "training: 83 batch 299 batch_loss: 0.2248380482196808\n",
      "training: 83 batch 300 batch_loss: 0.22344905138015747\n",
      "training: 83 batch 301 batch_loss: 0.2174018919467926\n",
      "training: 83 batch 302 batch_loss: 0.22316628694534302\n",
      "training: 83 batch 303 batch_loss: 0.22349289059638977\n",
      "training: 83 batch 304 batch_loss: 0.22089195251464844\n",
      "training: 83 batch 305 batch_loss: 0.22391805052757263\n",
      "training: 83 batch 306 batch_loss: 0.22130802273750305\n",
      "training: 83 batch 307 batch_loss: 0.22266972064971924\n",
      "training: 83 batch 308 batch_loss: 0.225087970495224\n",
      "training: 83 batch 309 batch_loss: 0.2217746376991272\n",
      "training: 83 batch 310 batch_loss: 0.22453999519348145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 83 batch 311 batch_loss: 0.22547435760498047\n",
      "training: 83 batch 312 batch_loss: 0.22190600633621216\n",
      "training: 83 batch 313 batch_loss: 0.21759158372879028\n",
      "training: 83 batch 314 batch_loss: 0.22242090106010437\n",
      "training: 83 batch 315 batch_loss: 0.22398298978805542\n",
      "training: 83 batch 316 batch_loss: 0.22296148538589478\n",
      "training: 83 batch 317 batch_loss: 0.2237725555896759\n",
      "training: 83 batch 318 batch_loss: 0.22031381726264954\n",
      "training: 83 batch 319 batch_loss: 0.22142302989959717\n",
      "training: 83 batch 320 batch_loss: 0.2230478823184967\n",
      "training: 83 batch 321 batch_loss: 0.22102069854736328\n",
      "training: 83 batch 322 batch_loss: 0.22483819723129272\n",
      "training: 83 batch 323 batch_loss: 0.22573250532150269\n",
      "training: 83 batch 324 batch_loss: 0.22348827123641968\n",
      "training: 83 batch 325 batch_loss: 0.22439754009246826\n",
      "training: 83 batch 326 batch_loss: 0.22565993666648865\n",
      "training: 83 batch 327 batch_loss: 0.2264428734779358\n",
      "training: 83 batch 328 batch_loss: 0.2216498851776123\n",
      "training: 83 batch 329 batch_loss: 0.2262050211429596\n",
      "training: 83 batch 330 batch_loss: 0.22266173362731934\n",
      "training: 83 batch 331 batch_loss: 0.22244057059288025\n",
      "training: 83 batch 332 batch_loss: 0.2226424515247345\n",
      "training: 83 batch 333 batch_loss: 0.22173583507537842\n",
      "training: 83 batch 334 batch_loss: 0.2186470925807953\n",
      "training: 83 batch 335 batch_loss: 0.22286802530288696\n",
      "training: 83 batch 336 batch_loss: 0.2274075150489807\n",
      "training: 83 batch 337 batch_loss: 0.22524142265319824\n",
      "training: 83 batch 338 batch_loss: 0.22104495763778687\n",
      "training: 83 batch 339 batch_loss: 0.22567105293273926\n",
      "training: 83 batch 340 batch_loss: 0.22614184021949768\n",
      "training: 83 batch 341 batch_loss: 0.22357532382011414\n",
      "training: 83 batch 342 batch_loss: 0.22162675857543945\n",
      "training: 83 batch 343 batch_loss: 0.21891441941261292\n",
      "training: 83 batch 344 batch_loss: 0.22739970684051514\n",
      "training: 83 batch 345 batch_loss: 0.22043603658676147\n",
      "training: 83 batch 346 batch_loss: 0.22492775321006775\n",
      "training: 83 batch 347 batch_loss: 0.21866267919540405\n",
      "training: 83 batch 348 batch_loss: 0.2178015112876892\n",
      "training: 83 batch 349 batch_loss: 0.22400116920471191\n",
      "training: 83 batch 350 batch_loss: 0.22523003816604614\n",
      "training: 83 batch 351 batch_loss: 0.22460368275642395\n",
      "training: 83 batch 352 batch_loss: 0.2236626148223877\n",
      "training: 83 batch 353 batch_loss: 0.22099894285202026\n",
      "training: 83 batch 354 batch_loss: 0.22479617595672607\n",
      "training: 83 batch 355 batch_loss: 0.22347992658615112\n",
      "training: 83 batch 356 batch_loss: 0.22681236267089844\n",
      "training: 83 batch 357 batch_loss: 0.22402995824813843\n",
      "training: 83 batch 358 batch_loss: 0.22372150421142578\n",
      "training: 83 batch 359 batch_loss: 0.22556161880493164\n",
      "training: 83 batch 360 batch_loss: 0.22187799215316772\n",
      "training: 83 batch 361 batch_loss: 0.22098976373672485\n",
      "training: 83 batch 362 batch_loss: 0.22102412581443787\n",
      "training: 83 batch 363 batch_loss: 0.22714608907699585\n",
      "training: 83 batch 364 batch_loss: 0.22908654808998108\n",
      "training: 83 batch 365 batch_loss: 0.22889721393585205\n",
      "training: 83 batch 366 batch_loss: 0.22462588548660278\n",
      "training: 83 batch 367 batch_loss: 0.22368276119232178\n",
      "training: 83 batch 368 batch_loss: 0.21984240412712097\n",
      "training: 83 batch 369 batch_loss: 0.22133758664131165\n",
      "training: 83 batch 370 batch_loss: 0.22272008657455444\n",
      "training: 83 batch 371 batch_loss: 0.22353458404541016\n",
      "training: 83 batch 372 batch_loss: 0.22483110427856445\n",
      "training: 83 batch 373 batch_loss: 0.22740420699119568\n",
      "training: 83 batch 374 batch_loss: 0.22159183025360107\n",
      "training: 83 batch 375 batch_loss: 0.22388216853141785\n",
      "training: 83 batch 376 batch_loss: 0.22530585527420044\n",
      "training: 83 batch 377 batch_loss: 0.22500550746917725\n",
      "training: 83 batch 378 batch_loss: 0.2237604260444641\n",
      "training: 83 batch 379 batch_loss: 0.2266574501991272\n",
      "training: 83 batch 380 batch_loss: 0.22476696968078613\n",
      "training: 83 batch 381 batch_loss: 0.2223232388496399\n",
      "training: 83 batch 382 batch_loss: 0.21975639462471008\n",
      "training: 83 batch 383 batch_loss: 0.22251677513122559\n",
      "training: 83 batch 384 batch_loss: 0.22644466161727905\n",
      "training: 83 batch 385 batch_loss: 0.2237074077129364\n",
      "training: 83 batch 386 batch_loss: 0.22799280285835266\n",
      "training: 83 batch 387 batch_loss: 0.22598659992218018\n",
      "training: 83 batch 388 batch_loss: 0.22026658058166504\n",
      "training: 83 batch 389 batch_loss: 0.21941018104553223\n",
      "training: 83 batch 390 batch_loss: 0.22240254282951355\n",
      "training: 83 batch 391 batch_loss: 0.22063517570495605\n",
      "training: 83 batch 392 batch_loss: 0.22051656246185303\n",
      "training: 83 batch 393 batch_loss: 0.2236720323562622\n",
      "training: 83 batch 394 batch_loss: 0.2246282696723938\n",
      "training: 83 batch 395 batch_loss: 0.22579464316368103\n",
      "training: 83 batch 396 batch_loss: 0.22226843237876892\n",
      "training: 83 batch 397 batch_loss: 0.22409376502037048\n",
      "training: 83 batch 398 batch_loss: 0.22204303741455078\n",
      "training: 83 batch 399 batch_loss: 0.22171255946159363\n",
      "training: 83 batch 400 batch_loss: 0.22269892692565918\n",
      "training: 83 batch 401 batch_loss: 0.22391283512115479\n",
      "training: 83 batch 402 batch_loss: 0.22385352849960327\n",
      "training: 83 batch 403 batch_loss: 0.22668641805648804\n",
      "training: 83 batch 404 batch_loss: 0.21966683864593506\n",
      "training: 83 batch 405 batch_loss: 0.2182813286781311\n",
      "training: 83 batch 406 batch_loss: 0.2304646372795105\n",
      "training: 83 batch 407 batch_loss: 0.22477605938911438\n",
      "training: 83 batch 408 batch_loss: 0.22476071119308472\n",
      "training: 83 batch 409 batch_loss: 0.22591561079025269\n",
      "training: 83 batch 410 batch_loss: 0.22581088542938232\n",
      "training: 83 batch 411 batch_loss: 0.2220195233821869\n",
      "training: 83 batch 412 batch_loss: 0.22586801648139954\n",
      "training: 83 batch 413 batch_loss: 0.22003713250160217\n",
      "training: 83 batch 414 batch_loss: 0.2257767915725708\n",
      "training: 83 batch 415 batch_loss: 0.22586771845817566\n",
      "training: 83 batch 416 batch_loss: 0.22356188297271729\n",
      "training: 83 batch 417 batch_loss: 0.22387045621871948\n",
      "training: 83 batch 418 batch_loss: 0.2249959111213684\n",
      "training: 83 batch 419 batch_loss: 0.2232200801372528\n",
      "training: 83 batch 420 batch_loss: 0.22491124272346497\n",
      "training: 83 batch 421 batch_loss: 0.22261205315589905\n",
      "training: 83 batch 422 batch_loss: 0.224134624004364\n",
      "training: 83 batch 423 batch_loss: 0.22024694085121155\n",
      "training: 83 batch 424 batch_loss: 0.22462967038154602\n",
      "training: 83 batch 425 batch_loss: 0.2198677659034729\n",
      "training: 83 batch 426 batch_loss: 0.22157010436058044\n",
      "training: 83 batch 427 batch_loss: 0.22505724430084229\n",
      "training: 83 batch 428 batch_loss: 0.22000792622566223\n",
      "training: 83 batch 429 batch_loss: 0.22050046920776367\n",
      "training: 83 batch 430 batch_loss: 0.22339355945587158\n",
      "training: 83 batch 431 batch_loss: 0.2230742871761322\n",
      "training: 83 batch 432 batch_loss: 0.2214166522026062\n",
      "training: 83 batch 433 batch_loss: 0.224694162607193\n",
      "training: 83 batch 434 batch_loss: 0.21835121512413025\n",
      "training: 83 batch 435 batch_loss: 0.22606664896011353\n",
      "training: 83 batch 436 batch_loss: 0.22054758667945862\n",
      "training: 83 batch 437 batch_loss: 0.22862032055854797\n",
      "training: 83 batch 438 batch_loss: 0.2231554388999939\n",
      "training: 83 batch 439 batch_loss: 0.22207945585250854\n",
      "training: 83 batch 440 batch_loss: 0.22324419021606445\n",
      "training: 83 batch 441 batch_loss: 0.22401320934295654\n",
      "training: 83 batch 442 batch_loss: 0.22006195783615112\n",
      "training: 83 batch 443 batch_loss: 0.22522932291030884\n",
      "training: 83 batch 444 batch_loss: 0.22434678673744202\n",
      "training: 83 batch 445 batch_loss: 0.2215745747089386\n",
      "training: 83 batch 446 batch_loss: 0.22343307733535767\n",
      "training: 83 batch 447 batch_loss: 0.22397655248641968\n",
      "training: 83 batch 448 batch_loss: 0.22531801462173462\n",
      "training: 83 batch 449 batch_loss: 0.2269287407398224\n",
      "training: 83 batch 450 batch_loss: 0.22340300679206848\n",
      "training: 83 batch 451 batch_loss: 0.22602307796478271\n",
      "training: 83 batch 452 batch_loss: 0.22563591599464417\n",
      "training: 83 batch 453 batch_loss: 0.22291046380996704\n",
      "training: 83 batch 454 batch_loss: 0.22589656710624695\n",
      "training: 83 batch 455 batch_loss: 0.2203184962272644\n",
      "training: 83 batch 456 batch_loss: 0.2232900857925415\n",
      "training: 83 batch 457 batch_loss: 0.21983659267425537\n",
      "training: 83 batch 458 batch_loss: 0.21906384825706482\n",
      "training: 83 batch 459 batch_loss: 0.22303038835525513\n",
      "training: 83 batch 460 batch_loss: 0.22657275199890137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 83 batch 461 batch_loss: 0.2238941192626953\n",
      "training: 83 batch 462 batch_loss: 0.22257369756698608\n",
      "training: 83 batch 463 batch_loss: 0.22083720564842224\n",
      "training: 83 batch 464 batch_loss: 0.22388631105422974\n",
      "training: 83 batch 465 batch_loss: 0.220941960811615\n",
      "training: 83 batch 466 batch_loss: 0.22720393538475037\n",
      "training: 83 batch 467 batch_loss: 0.22167858481407166\n",
      "training: 83 batch 468 batch_loss: 0.22723546624183655\n",
      "training: 83 batch 469 batch_loss: 0.22479742765426636\n",
      "training: 83 batch 470 batch_loss: 0.22532641887664795\n",
      "training: 83 batch 471 batch_loss: 0.22235864400863647\n",
      "training: 83 batch 472 batch_loss: 0.22231745719909668\n",
      "training: 83 batch 473 batch_loss: 0.22727978229522705\n",
      "training: 83 batch 474 batch_loss: 0.21988606452941895\n",
      "training: 83 batch 475 batch_loss: 0.22289496660232544\n",
      "training: 83 batch 476 batch_loss: 0.22733891010284424\n",
      "training: 83 batch 477 batch_loss: 0.22375863790512085\n",
      "training: 83 batch 478 batch_loss: 0.22276923060417175\n",
      "training: 83 batch 479 batch_loss: 0.22392794489860535\n",
      "training: 83 batch 480 batch_loss: 0.22401395440101624\n",
      "training: 83 batch 481 batch_loss: 0.22680988907814026\n",
      "training: 83 batch 482 batch_loss: 0.22391259670257568\n",
      "training: 83 batch 483 batch_loss: 0.21958130598068237\n",
      "training: 83 batch 484 batch_loss: 0.22185847163200378\n",
      "training: 83 batch 485 batch_loss: 0.2262687385082245\n",
      "training: 83 batch 486 batch_loss: 0.2184412181377411\n",
      "training: 83 batch 487 batch_loss: 0.22506564855575562\n",
      "training: 83 batch 488 batch_loss: 0.22173669934272766\n",
      "training: 83 batch 489 batch_loss: 0.2228916585445404\n",
      "training: 83 batch 490 batch_loss: 0.22655943036079407\n",
      "training: 83 batch 491 batch_loss: 0.22051095962524414\n",
      "training: 83 batch 492 batch_loss: 0.22249054908752441\n",
      "training: 83 batch 493 batch_loss: 0.22775667905807495\n",
      "training: 83 batch 494 batch_loss: 0.22111546993255615\n",
      "training: 83 batch 495 batch_loss: 0.228326678276062\n",
      "training: 83 batch 496 batch_loss: 0.22613784670829773\n",
      "training: 83 batch 497 batch_loss: 0.22198721766471863\n",
      "training: 83 batch 498 batch_loss: 0.22295191884040833\n",
      "training: 83 batch 499 batch_loss: 0.2212216556072235\n",
      "training: 83 batch 500 batch_loss: 0.21958908438682556\n",
      "training: 83 batch 501 batch_loss: 0.22311773896217346\n",
      "training: 83 batch 502 batch_loss: 0.22415876388549805\n",
      "training: 83 batch 503 batch_loss: 0.2296721339225769\n",
      "training: 83 batch 504 batch_loss: 0.22538310289382935\n",
      "training: 83 batch 505 batch_loss: 0.22469210624694824\n",
      "training: 83 batch 506 batch_loss: 0.22403255105018616\n",
      "training: 83 batch 507 batch_loss: 0.22740373015403748\n",
      "training: 83 batch 508 batch_loss: 0.2199384570121765\n",
      "training: 83 batch 509 batch_loss: 0.22018027305603027\n",
      "training: 83 batch 510 batch_loss: 0.22215288877487183\n",
      "training: 83 batch 511 batch_loss: 0.22585153579711914\n",
      "training: 83 batch 512 batch_loss: 0.2191735804080963\n",
      "training: 83 batch 513 batch_loss: 0.22231030464172363\n",
      "training: 83 batch 514 batch_loss: 0.2271011471748352\n",
      "training: 83 batch 515 batch_loss: 0.22869253158569336\n",
      "training: 83 batch 516 batch_loss: 0.22879406809806824\n",
      "training: 83 batch 517 batch_loss: 0.222307950258255\n",
      "training: 83 batch 518 batch_loss: 0.23158496618270874\n",
      "training: 83 batch 519 batch_loss: 0.22316887974739075\n",
      "training: 83 batch 520 batch_loss: 0.22385767102241516\n",
      "training: 83 batch 521 batch_loss: 0.22271090745925903\n",
      "training: 83 batch 522 batch_loss: 0.22350198030471802\n",
      "training: 83 batch 523 batch_loss: 0.22821170091629028\n",
      "training: 83 batch 524 batch_loss: 0.22044360637664795\n",
      "training: 83 batch 525 batch_loss: 0.2245998978614807\n",
      "training: 83 batch 526 batch_loss: 0.22324874997138977\n",
      "training: 83 batch 527 batch_loss: 0.22715511918067932\n",
      "training: 83 batch 528 batch_loss: 0.22795629501342773\n",
      "training: 83 batch 529 batch_loss: 0.22643545269966125\n",
      "training: 83 batch 530 batch_loss: 0.22415369749069214\n",
      "training: 83 batch 531 batch_loss: 0.22350633144378662\n",
      "training: 83 batch 532 batch_loss: 0.22246146202087402\n",
      "training: 83 batch 533 batch_loss: 0.2216719388961792\n",
      "training: 83 batch 534 batch_loss: 0.2238972783088684\n",
      "training: 83 batch 535 batch_loss: 0.22566348314285278\n",
      "training: 83 batch 536 batch_loss: 0.22413823008537292\n",
      "training: 83 batch 537 batch_loss: 0.22513622045516968\n",
      "training: 83 batch 538 batch_loss: 0.22348058223724365\n",
      "training: 83 batch 539 batch_loss: 0.22282510995864868\n",
      "training: 83 batch 540 batch_loss: 0.22120872139930725\n",
      "training: 83 batch 541 batch_loss: 0.22883358597755432\n",
      "training: 83 batch 542 batch_loss: 0.2248418927192688\n",
      "training: 83 batch 543 batch_loss: 0.21993419528007507\n",
      "training: 83 batch 544 batch_loss: 0.22519451379776\n",
      "training: 83 batch 545 batch_loss: 0.22529062628746033\n",
      "training: 83 batch 546 batch_loss: 0.22616791725158691\n",
      "training: 83 batch 547 batch_loss: 0.22304123640060425\n",
      "training: 83 batch 548 batch_loss: 0.22545269131660461\n",
      "training: 83 batch 549 batch_loss: 0.22579830884933472\n",
      "training: 83 batch 550 batch_loss: 0.22471073269844055\n",
      "training: 83 batch 551 batch_loss: 0.22231706976890564\n",
      "training: 83 batch 552 batch_loss: 0.22729098796844482\n",
      "training: 83 batch 553 batch_loss: 0.22135087847709656\n",
      "training: 83 batch 554 batch_loss: 0.2277626395225525\n",
      "training: 83 batch 555 batch_loss: 0.22449344396591187\n",
      "training: 83 batch 556 batch_loss: 0.22481927275657654\n",
      "training: 83 batch 557 batch_loss: 0.22447329759597778\n",
      "training: 83 batch 558 batch_loss: 0.22532647848129272\n",
      "training: 83 batch 559 batch_loss: 0.2239912450313568\n",
      "training: 83 batch 560 batch_loss: 0.2236385941505432\n",
      "training: 83 batch 561 batch_loss: 0.22883206605911255\n",
      "training: 83 batch 562 batch_loss: 0.2226381003856659\n",
      "training: 83 batch 563 batch_loss: 0.2207927703857422\n",
      "training: 83 batch 564 batch_loss: 0.22184374928474426\n",
      "training: 83 batch 565 batch_loss: 0.2262071967124939\n",
      "training: 83 batch 566 batch_loss: 0.22418373823165894\n",
      "training: 83 batch 567 batch_loss: 0.2232225239276886\n",
      "training: 83 batch 568 batch_loss: 0.22225061058998108\n",
      "training: 83 batch 569 batch_loss: 0.22420796751976013\n",
      "training: 83 batch 570 batch_loss: 0.2245350182056427\n",
      "training: 83 batch 571 batch_loss: 0.22481456398963928\n",
      "training: 83 batch 572 batch_loss: 0.2250308394432068\n",
      "training: 83 batch 573 batch_loss: 0.22856661677360535\n",
      "training: 83 batch 574 batch_loss: 0.2244335114955902\n",
      "training: 83 batch 575 batch_loss: 0.22658884525299072\n",
      "training: 83 batch 576 batch_loss: 0.22594553232192993\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 83, Hit Ratio:0.03379825645501394 | Precision:0.04986729578295488 | Recall:0.06597390717846902 | NDCG:0.06507282181947334\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 84 batch 0 batch_loss: 0.22383901476860046\n",
      "training: 84 batch 1 batch_loss: 0.2228299379348755\n",
      "training: 84 batch 2 batch_loss: 0.22716546058654785\n",
      "training: 84 batch 3 batch_loss: 0.2220858335494995\n",
      "training: 84 batch 4 batch_loss: 0.22469615936279297\n",
      "training: 84 batch 5 batch_loss: 0.22382932901382446\n",
      "training: 84 batch 6 batch_loss: 0.22399258613586426\n",
      "training: 84 batch 7 batch_loss: 0.21848946809768677\n",
      "training: 84 batch 8 batch_loss: 0.22090372443199158\n",
      "training: 84 batch 9 batch_loss: 0.2233031988143921\n",
      "training: 84 batch 10 batch_loss: 0.22071346640586853\n",
      "training: 84 batch 11 batch_loss: 0.2231859266757965\n",
      "training: 84 batch 12 batch_loss: 0.21739453077316284\n",
      "training: 84 batch 13 batch_loss: 0.22396868467330933\n",
      "training: 84 batch 14 batch_loss: 0.2226029932498932\n",
      "training: 84 batch 15 batch_loss: 0.21765810251235962\n",
      "training: 84 batch 16 batch_loss: 0.21868354082107544\n",
      "training: 84 batch 17 batch_loss: 0.21897929906845093\n",
      "training: 84 batch 18 batch_loss: 0.22063875198364258\n",
      "training: 84 batch 19 batch_loss: 0.22771811485290527\n",
      "training: 84 batch 20 batch_loss: 0.22315996885299683\n",
      "training: 84 batch 21 batch_loss: 0.21715664863586426\n",
      "training: 84 batch 22 batch_loss: 0.22045767307281494\n",
      "training: 84 batch 23 batch_loss: 0.22404181957244873\n",
      "training: 84 batch 24 batch_loss: 0.22006183862686157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 84 batch 25 batch_loss: 0.22595706582069397\n",
      "training: 84 batch 26 batch_loss: 0.22508850693702698\n",
      "training: 84 batch 27 batch_loss: 0.22427958250045776\n",
      "training: 84 batch 28 batch_loss: 0.22674229741096497\n",
      "training: 84 batch 29 batch_loss: 0.22105178236961365\n",
      "training: 84 batch 30 batch_loss: 0.21639513969421387\n",
      "training: 84 batch 31 batch_loss: 0.22228407859802246\n",
      "training: 84 batch 32 batch_loss: 0.22418978810310364\n",
      "training: 84 batch 33 batch_loss: 0.2204640805721283\n",
      "training: 84 batch 34 batch_loss: 0.21670809388160706\n",
      "training: 84 batch 35 batch_loss: 0.22344836592674255\n",
      "training: 84 batch 36 batch_loss: 0.22363656759262085\n",
      "training: 84 batch 37 batch_loss: 0.22509682178497314\n",
      "training: 84 batch 38 batch_loss: 0.2238519787788391\n",
      "training: 84 batch 39 batch_loss: 0.22365710139274597\n",
      "training: 84 batch 40 batch_loss: 0.22508513927459717\n",
      "training: 84 batch 41 batch_loss: 0.2199101746082306\n",
      "training: 84 batch 42 batch_loss: 0.22228989005088806\n",
      "training: 84 batch 43 batch_loss: 0.22114434838294983\n",
      "training: 84 batch 44 batch_loss: 0.22127264738082886\n",
      "training: 84 batch 45 batch_loss: 0.2242927849292755\n",
      "training: 84 batch 46 batch_loss: 0.22775337100028992\n",
      "training: 84 batch 47 batch_loss: 0.22325357794761658\n",
      "training: 84 batch 48 batch_loss: 0.2259233295917511\n",
      "training: 84 batch 49 batch_loss: 0.2267792820930481\n",
      "training: 84 batch 50 batch_loss: 0.2228785753250122\n",
      "training: 84 batch 51 batch_loss: 0.2233380675315857\n",
      "training: 84 batch 52 batch_loss: 0.2246187925338745\n",
      "training: 84 batch 53 batch_loss: 0.2185133993625641\n",
      "training: 84 batch 54 batch_loss: 0.2265225350856781\n",
      "training: 84 batch 55 batch_loss: 0.22200021147727966\n",
      "training: 84 batch 56 batch_loss: 0.218327134847641\n",
      "training: 84 batch 57 batch_loss: 0.2215847671031952\n",
      "training: 84 batch 58 batch_loss: 0.22114360332489014\n",
      "training: 84 batch 59 batch_loss: 0.22014227509498596\n",
      "training: 84 batch 60 batch_loss: 0.22469428181648254\n",
      "training: 84 batch 61 batch_loss: 0.22328686714172363\n",
      "training: 84 batch 62 batch_loss: 0.2267548143863678\n",
      "training: 84 batch 63 batch_loss: 0.21914130449295044\n",
      "training: 84 batch 64 batch_loss: 0.21648097038269043\n",
      "training: 84 batch 65 batch_loss: 0.22268754243850708\n",
      "training: 84 batch 66 batch_loss: 0.2232927680015564\n",
      "training: 84 batch 67 batch_loss: 0.22025859355926514\n",
      "training: 84 batch 68 batch_loss: 0.22134768962860107\n",
      "training: 84 batch 69 batch_loss: 0.22274625301361084\n",
      "training: 84 batch 70 batch_loss: 0.2242853343486786\n",
      "training: 84 batch 71 batch_loss: 0.2203179895877838\n",
      "training: 84 batch 72 batch_loss: 0.22624871134757996\n",
      "training: 84 batch 73 batch_loss: 0.22225064039230347\n",
      "training: 84 batch 74 batch_loss: 0.22131401300430298\n",
      "training: 84 batch 75 batch_loss: 0.21923914551734924\n",
      "training: 84 batch 76 batch_loss: 0.22043448686599731\n",
      "training: 84 batch 77 batch_loss: 0.22791340947151184\n",
      "training: 84 batch 78 batch_loss: 0.22099366784095764\n",
      "training: 84 batch 79 batch_loss: 0.22618460655212402\n",
      "training: 84 batch 80 batch_loss: 0.2221618890762329\n",
      "training: 84 batch 81 batch_loss: 0.22542589902877808\n",
      "training: 84 batch 82 batch_loss: 0.2205427885055542\n",
      "training: 84 batch 83 batch_loss: 0.22210702300071716\n",
      "training: 84 batch 84 batch_loss: 0.2233477532863617\n",
      "training: 84 batch 85 batch_loss: 0.224461168050766\n",
      "training: 84 batch 86 batch_loss: 0.22607913613319397\n",
      "training: 84 batch 87 batch_loss: 0.22643601894378662\n",
      "training: 84 batch 88 batch_loss: 0.2262270152568817\n",
      "training: 84 batch 89 batch_loss: 0.21861618757247925\n",
      "training: 84 batch 90 batch_loss: 0.22459760308265686\n",
      "training: 84 batch 91 batch_loss: 0.21951216459274292\n",
      "training: 84 batch 92 batch_loss: 0.22198286652565002\n",
      "training: 84 batch 93 batch_loss: 0.22258031368255615\n",
      "training: 84 batch 94 batch_loss: 0.22214734554290771\n",
      "training: 84 batch 95 batch_loss: 0.2249571681022644\n",
      "training: 84 batch 96 batch_loss: 0.21857520937919617\n",
      "training: 84 batch 97 batch_loss: 0.22573280334472656\n",
      "training: 84 batch 98 batch_loss: 0.22386693954467773\n",
      "training: 84 batch 99 batch_loss: 0.2261527180671692\n",
      "training: 84 batch 100 batch_loss: 0.22177347540855408\n",
      "training: 84 batch 101 batch_loss: 0.2210194170475006\n",
      "training: 84 batch 102 batch_loss: 0.2235759198665619\n",
      "training: 84 batch 103 batch_loss: 0.21942120790481567\n",
      "training: 84 batch 104 batch_loss: 0.22109484672546387\n",
      "training: 84 batch 105 batch_loss: 0.22175222635269165\n",
      "training: 84 batch 106 batch_loss: 0.2227214276790619\n",
      "training: 84 batch 107 batch_loss: 0.21958479285240173\n",
      "training: 84 batch 108 batch_loss: 0.22180554270744324\n",
      "training: 84 batch 109 batch_loss: 0.22035032510757446\n",
      "training: 84 batch 110 batch_loss: 0.2247782051563263\n",
      "training: 84 batch 111 batch_loss: 0.22662749886512756\n",
      "training: 84 batch 112 batch_loss: 0.22227603197097778\n",
      "training: 84 batch 113 batch_loss: 0.22424262762069702\n",
      "training: 84 batch 114 batch_loss: 0.22249501943588257\n",
      "training: 84 batch 115 batch_loss: 0.22344541549682617\n",
      "training: 84 batch 116 batch_loss: 0.21894505620002747\n",
      "training: 84 batch 117 batch_loss: 0.2229231595993042\n",
      "training: 84 batch 118 batch_loss: 0.22653698921203613\n",
      "training: 84 batch 119 batch_loss: 0.22291094064712524\n",
      "training: 84 batch 120 batch_loss: 0.22211524844169617\n",
      "training: 84 batch 121 batch_loss: 0.22440841794013977\n",
      "training: 84 batch 122 batch_loss: 0.21876177191734314\n",
      "training: 84 batch 123 batch_loss: 0.22058409452438354\n",
      "training: 84 batch 124 batch_loss: 0.22249934077262878\n",
      "training: 84 batch 125 batch_loss: 0.22218242287635803\n",
      "training: 84 batch 126 batch_loss: 0.2248915135860443\n",
      "training: 84 batch 127 batch_loss: 0.22056341171264648\n",
      "training: 84 batch 128 batch_loss: 0.2217944860458374\n",
      "training: 84 batch 129 batch_loss: 0.2217649221420288\n",
      "training: 84 batch 130 batch_loss: 0.2255537509918213\n",
      "training: 84 batch 131 batch_loss: 0.22741129994392395\n",
      "training: 84 batch 132 batch_loss: 0.22236528992652893\n",
      "training: 84 batch 133 batch_loss: 0.22297078371047974\n",
      "training: 84 batch 134 batch_loss: 0.22370421886444092\n",
      "training: 84 batch 135 batch_loss: 0.22238096594810486\n",
      "training: 84 batch 136 batch_loss: 0.22523030638694763\n",
      "training: 84 batch 137 batch_loss: 0.22715714573860168\n",
      "training: 84 batch 138 batch_loss: 0.22429955005645752\n",
      "training: 84 batch 139 batch_loss: 0.22367939352989197\n",
      "training: 84 batch 140 batch_loss: 0.22308838367462158\n",
      "training: 84 batch 141 batch_loss: 0.22395524382591248\n",
      "training: 84 batch 142 batch_loss: 0.22529590129852295\n",
      "training: 84 batch 143 batch_loss: 0.22041422128677368\n",
      "training: 84 batch 144 batch_loss: 0.22084391117095947\n",
      "training: 84 batch 145 batch_loss: 0.22452127933502197\n",
      "training: 84 batch 146 batch_loss: 0.2210409939289093\n",
      "training: 84 batch 147 batch_loss: 0.2230626344680786\n",
      "training: 84 batch 148 batch_loss: 0.21853312849998474\n",
      "training: 84 batch 149 batch_loss: 0.22093281149864197\n",
      "training: 84 batch 150 batch_loss: 0.22107216715812683\n",
      "training: 84 batch 151 batch_loss: 0.2228218913078308\n",
      "training: 84 batch 152 batch_loss: 0.22776132822036743\n",
      "training: 84 batch 153 batch_loss: 0.22479039430618286\n",
      "training: 84 batch 154 batch_loss: 0.22474557161331177\n",
      "training: 84 batch 155 batch_loss: 0.22575345635414124\n",
      "training: 84 batch 156 batch_loss: 0.2190168797969818\n",
      "training: 84 batch 157 batch_loss: 0.22877192497253418\n",
      "training: 84 batch 158 batch_loss: 0.22298145294189453\n",
      "training: 84 batch 159 batch_loss: 0.21802625060081482\n",
      "training: 84 batch 160 batch_loss: 0.22164499759674072\n",
      "training: 84 batch 161 batch_loss: 0.22078406810760498\n",
      "training: 84 batch 162 batch_loss: 0.22292721271514893\n",
      "training: 84 batch 163 batch_loss: 0.22774159908294678\n",
      "training: 84 batch 164 batch_loss: 0.2276291847229004\n",
      "training: 84 batch 165 batch_loss: 0.22355535626411438\n",
      "training: 84 batch 166 batch_loss: 0.2215234935283661\n",
      "training: 84 batch 167 batch_loss: 0.22486862540245056\n",
      "training: 84 batch 168 batch_loss: 0.22163867950439453\n",
      "training: 84 batch 169 batch_loss: 0.22618836164474487\n",
      "training: 84 batch 170 batch_loss: 0.22418099641799927\n",
      "training: 84 batch 171 batch_loss: 0.2251376509666443\n",
      "training: 84 batch 172 batch_loss: 0.22939592599868774\n",
      "training: 84 batch 173 batch_loss: 0.21795260906219482\n",
      "training: 84 batch 174 batch_loss: 0.2216309905052185\n",
      "training: 84 batch 175 batch_loss: 0.22046440839767456\n",
      "training: 84 batch 176 batch_loss: 0.2265077829360962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 84 batch 177 batch_loss: 0.2261582612991333\n",
      "training: 84 batch 178 batch_loss: 0.22078335285186768\n",
      "training: 84 batch 179 batch_loss: 0.21777373552322388\n",
      "training: 84 batch 180 batch_loss: 0.21725046634674072\n",
      "training: 84 batch 181 batch_loss: 0.22435426712036133\n",
      "training: 84 batch 182 batch_loss: 0.22534039616584778\n",
      "training: 84 batch 183 batch_loss: 0.22561347484588623\n",
      "training: 84 batch 184 batch_loss: 0.22063788771629333\n",
      "training: 84 batch 185 batch_loss: 0.22109168767929077\n",
      "training: 84 batch 186 batch_loss: 0.22238612174987793\n",
      "training: 84 batch 187 batch_loss: 0.22557085752487183\n",
      "training: 84 batch 188 batch_loss: 0.22174933552742004\n",
      "training: 84 batch 189 batch_loss: 0.22564920783042908\n",
      "training: 84 batch 190 batch_loss: 0.22382968664169312\n",
      "training: 84 batch 191 batch_loss: 0.22244256734848022\n",
      "training: 84 batch 192 batch_loss: 0.223785400390625\n",
      "training: 84 batch 193 batch_loss: 0.2224075198173523\n",
      "training: 84 batch 194 batch_loss: 0.22430533170700073\n",
      "training: 84 batch 195 batch_loss: 0.22345194220542908\n",
      "training: 84 batch 196 batch_loss: 0.22343769669532776\n",
      "training: 84 batch 197 batch_loss: 0.22251534461975098\n",
      "training: 84 batch 198 batch_loss: 0.22266808152198792\n",
      "training: 84 batch 199 batch_loss: 0.2264399528503418\n",
      "training: 84 batch 200 batch_loss: 0.2271529734134674\n",
      "training: 84 batch 201 batch_loss: 0.22139036655426025\n",
      "training: 84 batch 202 batch_loss: 0.22452718019485474\n",
      "training: 84 batch 203 batch_loss: 0.21831876039505005\n",
      "training: 84 batch 204 batch_loss: 0.21976473927497864\n",
      "training: 84 batch 205 batch_loss: 0.22565388679504395\n",
      "training: 84 batch 206 batch_loss: 0.22304373979568481\n",
      "training: 84 batch 207 batch_loss: 0.22410094738006592\n",
      "training: 84 batch 208 batch_loss: 0.22345873713493347\n",
      "training: 84 batch 209 batch_loss: 0.22357505559921265\n",
      "training: 84 batch 210 batch_loss: 0.22170159220695496\n",
      "training: 84 batch 211 batch_loss: 0.22841081023216248\n",
      "training: 84 batch 212 batch_loss: 0.22153034806251526\n",
      "training: 84 batch 213 batch_loss: 0.22209841012954712\n",
      "training: 84 batch 214 batch_loss: 0.2239508032798767\n",
      "training: 84 batch 215 batch_loss: 0.2216869592666626\n",
      "training: 84 batch 216 batch_loss: 0.21903222799301147\n",
      "training: 84 batch 217 batch_loss: 0.22625285387039185\n",
      "training: 84 batch 218 batch_loss: 0.22318708896636963\n",
      "training: 84 batch 219 batch_loss: 0.22097843885421753\n",
      "training: 84 batch 220 batch_loss: 0.21934247016906738\n",
      "training: 84 batch 221 batch_loss: 0.22367176413536072\n",
      "training: 84 batch 222 batch_loss: 0.22042691707611084\n",
      "training: 84 batch 223 batch_loss: 0.22021859884262085\n",
      "training: 84 batch 224 batch_loss: 0.22360774874687195\n",
      "training: 84 batch 225 batch_loss: 0.22651129961013794\n",
      "training: 84 batch 226 batch_loss: 0.22254467010498047\n",
      "training: 84 batch 227 batch_loss: 0.22504061460494995\n",
      "training: 84 batch 228 batch_loss: 0.2205926775932312\n",
      "training: 84 batch 229 batch_loss: 0.2207103967666626\n",
      "training: 84 batch 230 batch_loss: 0.223204106092453\n",
      "training: 84 batch 231 batch_loss: 0.22990167140960693\n",
      "training: 84 batch 232 batch_loss: 0.22411203384399414\n",
      "training: 84 batch 233 batch_loss: 0.2217767834663391\n",
      "training: 84 batch 234 batch_loss: 0.22638624906539917\n",
      "training: 84 batch 235 batch_loss: 0.22254619002342224\n",
      "training: 84 batch 236 batch_loss: 0.22059863805770874\n",
      "training: 84 batch 237 batch_loss: 0.22109505534172058\n",
      "training: 84 batch 238 batch_loss: 0.2166014313697815\n",
      "training: 84 batch 239 batch_loss: 0.22066962718963623\n",
      "training: 84 batch 240 batch_loss: 0.22276228666305542\n",
      "training: 84 batch 241 batch_loss: 0.22407737374305725\n",
      "training: 84 batch 242 batch_loss: 0.2283463180065155\n",
      "training: 84 batch 243 batch_loss: 0.22331595420837402\n",
      "training: 84 batch 244 batch_loss: 0.22301054000854492\n",
      "training: 84 batch 245 batch_loss: 0.22670474648475647\n",
      "training: 84 batch 246 batch_loss: 0.2188795506954193\n",
      "training: 84 batch 247 batch_loss: 0.2253289818763733\n",
      "training: 84 batch 248 batch_loss: 0.22072678804397583\n",
      "training: 84 batch 249 batch_loss: 0.22491386532783508\n",
      "training: 84 batch 250 batch_loss: 0.22733217477798462\n",
      "training: 84 batch 251 batch_loss: 0.2240806519985199\n",
      "training: 84 batch 252 batch_loss: 0.22823089361190796\n",
      "training: 84 batch 253 batch_loss: 0.22418087720870972\n",
      "training: 84 batch 254 batch_loss: 0.227126806974411\n",
      "training: 84 batch 255 batch_loss: 0.22052624821662903\n",
      "training: 84 batch 256 batch_loss: 0.22591200470924377\n",
      "training: 84 batch 257 batch_loss: 0.2247527539730072\n",
      "training: 84 batch 258 batch_loss: 0.2223454713821411\n",
      "training: 84 batch 259 batch_loss: 0.22218266129493713\n",
      "training: 84 batch 260 batch_loss: 0.22655239701271057\n",
      "training: 84 batch 261 batch_loss: 0.22127020359039307\n",
      "training: 84 batch 262 batch_loss: 0.2211420238018036\n",
      "training: 84 batch 263 batch_loss: 0.22524428367614746\n",
      "training: 84 batch 264 batch_loss: 0.22672080993652344\n",
      "training: 84 batch 265 batch_loss: 0.22131559252738953\n",
      "training: 84 batch 266 batch_loss: 0.224796324968338\n",
      "training: 84 batch 267 batch_loss: 0.22246810793876648\n",
      "training: 84 batch 268 batch_loss: 0.22700798511505127\n",
      "training: 84 batch 269 batch_loss: 0.22484242916107178\n",
      "training: 84 batch 270 batch_loss: 0.22513020038604736\n",
      "training: 84 batch 271 batch_loss: 0.22563037276268005\n",
      "training: 84 batch 272 batch_loss: 0.22406718134880066\n",
      "training: 84 batch 273 batch_loss: 0.2231290638446808\n",
      "training: 84 batch 274 batch_loss: 0.2251007854938507\n",
      "training: 84 batch 275 batch_loss: 0.22522109746932983\n",
      "training: 84 batch 276 batch_loss: 0.22019904851913452\n",
      "training: 84 batch 277 batch_loss: 0.22176194190979004\n",
      "training: 84 batch 278 batch_loss: 0.22125163674354553\n",
      "training: 84 batch 279 batch_loss: 0.22185927629470825\n",
      "training: 84 batch 280 batch_loss: 0.22249963879585266\n",
      "training: 84 batch 281 batch_loss: 0.22860610485076904\n",
      "training: 84 batch 282 batch_loss: 0.2260672152042389\n",
      "training: 84 batch 283 batch_loss: 0.22147414088249207\n",
      "training: 84 batch 284 batch_loss: 0.22448411583900452\n",
      "training: 84 batch 285 batch_loss: 0.22848576307296753\n",
      "training: 84 batch 286 batch_loss: 0.22202876210212708\n",
      "training: 84 batch 287 batch_loss: 0.22168511152267456\n",
      "training: 84 batch 288 batch_loss: 0.22361674904823303\n",
      "training: 84 batch 289 batch_loss: 0.2237635850906372\n",
      "training: 84 batch 290 batch_loss: 0.22223567962646484\n",
      "training: 84 batch 291 batch_loss: 0.2256968915462494\n",
      "training: 84 batch 292 batch_loss: 0.2222813069820404\n",
      "training: 84 batch 293 batch_loss: 0.2206605076789856\n",
      "training: 84 batch 294 batch_loss: 0.2256593108177185\n",
      "training: 84 batch 295 batch_loss: 0.22716784477233887\n",
      "training: 84 batch 296 batch_loss: 0.22745481133460999\n",
      "training: 84 batch 297 batch_loss: 0.22393172979354858\n",
      "training: 84 batch 298 batch_loss: 0.2188374400138855\n",
      "training: 84 batch 299 batch_loss: 0.22511205077171326\n",
      "training: 84 batch 300 batch_loss: 0.22368445992469788\n",
      "training: 84 batch 301 batch_loss: 0.22335362434387207\n",
      "training: 84 batch 302 batch_loss: 0.22767525911331177\n",
      "training: 84 batch 303 batch_loss: 0.22992074489593506\n",
      "training: 84 batch 304 batch_loss: 0.22256523370742798\n",
      "training: 84 batch 305 batch_loss: 0.221811443567276\n",
      "training: 84 batch 306 batch_loss: 0.22374343872070312\n",
      "training: 84 batch 307 batch_loss: 0.22411483526229858\n",
      "training: 84 batch 308 batch_loss: 0.22465580701828003\n",
      "training: 84 batch 309 batch_loss: 0.22036835551261902\n",
      "training: 84 batch 310 batch_loss: 0.22148817777633667\n",
      "training: 84 batch 311 batch_loss: 0.22703570127487183\n",
      "training: 84 batch 312 batch_loss: 0.22932004928588867\n",
      "training: 84 batch 313 batch_loss: 0.22490540146827698\n",
      "training: 84 batch 314 batch_loss: 0.22081947326660156\n",
      "training: 84 batch 315 batch_loss: 0.2286786437034607\n",
      "training: 84 batch 316 batch_loss: 0.2213086485862732\n",
      "training: 84 batch 317 batch_loss: 0.22624075412750244\n",
      "training: 84 batch 318 batch_loss: 0.2284572720527649\n",
      "training: 84 batch 319 batch_loss: 0.2193429172039032\n",
      "training: 84 batch 320 batch_loss: 0.22541147470474243\n",
      "training: 84 batch 321 batch_loss: 0.22513526678085327\n",
      "training: 84 batch 322 batch_loss: 0.22326946258544922\n",
      "training: 84 batch 323 batch_loss: 0.22432133555412292\n",
      "training: 84 batch 324 batch_loss: 0.22558945417404175\n",
      "training: 84 batch 325 batch_loss: 0.22162646055221558\n",
      "training: 84 batch 326 batch_loss: 0.2235543131828308\n",
      "training: 84 batch 327 batch_loss: 0.2200840711593628\n",
      "training: 84 batch 328 batch_loss: 0.2238776683807373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 84 batch 329 batch_loss: 0.2270021140575409\n",
      "training: 84 batch 330 batch_loss: 0.22138524055480957\n",
      "training: 84 batch 331 batch_loss: 0.2223668098449707\n",
      "training: 84 batch 332 batch_loss: 0.2222311794757843\n",
      "training: 84 batch 333 batch_loss: 0.22858643531799316\n",
      "training: 84 batch 334 batch_loss: 0.2243158519268036\n",
      "training: 84 batch 335 batch_loss: 0.22154784202575684\n",
      "training: 84 batch 336 batch_loss: 0.22204187512397766\n",
      "training: 84 batch 337 batch_loss: 0.2233259677886963\n",
      "training: 84 batch 338 batch_loss: 0.21967670321464539\n",
      "training: 84 batch 339 batch_loss: 0.22533610463142395\n",
      "training: 84 batch 340 batch_loss: 0.2255290150642395\n",
      "training: 84 batch 341 batch_loss: 0.2192729413509369\n",
      "training: 84 batch 342 batch_loss: 0.22902360558509827\n",
      "training: 84 batch 343 batch_loss: 0.22144585847854614\n",
      "training: 84 batch 344 batch_loss: 0.22589528560638428\n",
      "training: 84 batch 345 batch_loss: 0.22361800074577332\n",
      "training: 84 batch 346 batch_loss: 0.22412407398223877\n",
      "training: 84 batch 347 batch_loss: 0.22359007596969604\n",
      "training: 84 batch 348 batch_loss: 0.2202993631362915\n",
      "training: 84 batch 349 batch_loss: 0.22101303935050964\n",
      "training: 84 batch 350 batch_loss: 0.2267988622188568\n",
      "training: 84 batch 351 batch_loss: 0.22440069913864136\n",
      "training: 84 batch 352 batch_loss: 0.22771161794662476\n",
      "training: 84 batch 353 batch_loss: 0.2220655083656311\n",
      "training: 84 batch 354 batch_loss: 0.22218191623687744\n",
      "training: 84 batch 355 batch_loss: 0.22675257921218872\n",
      "training: 84 batch 356 batch_loss: 0.2260325849056244\n",
      "training: 84 batch 357 batch_loss: 0.22209888696670532\n",
      "training: 84 batch 358 batch_loss: 0.22095966339111328\n",
      "training: 84 batch 359 batch_loss: 0.22315305471420288\n",
      "training: 84 batch 360 batch_loss: 0.2242852747440338\n",
      "training: 84 batch 361 batch_loss: 0.22139126062393188\n",
      "training: 84 batch 362 batch_loss: 0.22742843627929688\n",
      "training: 84 batch 363 batch_loss: 0.22462788224220276\n",
      "training: 84 batch 364 batch_loss: 0.21618586778640747\n",
      "training: 84 batch 365 batch_loss: 0.22252529859542847\n",
      "training: 84 batch 366 batch_loss: 0.22170430421829224\n",
      "training: 84 batch 367 batch_loss: 0.2197873890399933\n",
      "training: 84 batch 368 batch_loss: 0.22166916728019714\n",
      "training: 84 batch 369 batch_loss: 0.22390910983085632\n",
      "training: 84 batch 370 batch_loss: 0.22383463382720947\n",
      "training: 84 batch 371 batch_loss: 0.22738298773765564\n",
      "training: 84 batch 372 batch_loss: 0.22492510080337524\n",
      "training: 84 batch 373 batch_loss: 0.2233923375606537\n",
      "training: 84 batch 374 batch_loss: 0.22104957699775696\n",
      "training: 84 batch 375 batch_loss: 0.2251124083995819\n",
      "training: 84 batch 376 batch_loss: 0.22447603940963745\n",
      "training: 84 batch 377 batch_loss: 0.2264752984046936\n",
      "training: 84 batch 378 batch_loss: 0.2278023362159729\n",
      "training: 84 batch 379 batch_loss: 0.22184151411056519\n",
      "training: 84 batch 380 batch_loss: 0.22518280148506165\n",
      "training: 84 batch 381 batch_loss: 0.22080817818641663\n",
      "training: 84 batch 382 batch_loss: 0.2205851674079895\n",
      "training: 84 batch 383 batch_loss: 0.22154277563095093\n",
      "training: 84 batch 384 batch_loss: 0.22511082887649536\n",
      "training: 84 batch 385 batch_loss: 0.22182652354240417\n",
      "training: 84 batch 386 batch_loss: 0.22201618552207947\n",
      "training: 84 batch 387 batch_loss: 0.22232356667518616\n",
      "training: 84 batch 388 batch_loss: 0.2226594090461731\n",
      "training: 84 batch 389 batch_loss: 0.22582870721817017\n",
      "training: 84 batch 390 batch_loss: 0.22724953293800354\n",
      "training: 84 batch 391 batch_loss: 0.22270861268043518\n",
      "training: 84 batch 392 batch_loss: 0.2283008098602295\n",
      "training: 84 batch 393 batch_loss: 0.22401398420333862\n",
      "training: 84 batch 394 batch_loss: 0.22782737016677856\n",
      "training: 84 batch 395 batch_loss: 0.22905001044273376\n",
      "training: 84 batch 396 batch_loss: 0.2218605875968933\n",
      "training: 84 batch 397 batch_loss: 0.2262856364250183\n",
      "training: 84 batch 398 batch_loss: 0.22306370735168457\n",
      "training: 84 batch 399 batch_loss: 0.2264150083065033\n",
      "training: 84 batch 400 batch_loss: 0.2245776653289795\n",
      "training: 84 batch 401 batch_loss: 0.2232520878314972\n",
      "training: 84 batch 402 batch_loss: 0.22133022546768188\n",
      "training: 84 batch 403 batch_loss: 0.22449880838394165\n",
      "training: 84 batch 404 batch_loss: 0.22737833857536316\n",
      "training: 84 batch 405 batch_loss: 0.2252458930015564\n",
      "training: 84 batch 406 batch_loss: 0.22461947798728943\n",
      "training: 84 batch 407 batch_loss: 0.22277602553367615\n",
      "training: 84 batch 408 batch_loss: 0.22483587265014648\n",
      "training: 84 batch 409 batch_loss: 0.22077885270118713\n",
      "training: 84 batch 410 batch_loss: 0.22503691911697388\n",
      "training: 84 batch 411 batch_loss: 0.22894102334976196\n",
      "training: 84 batch 412 batch_loss: 0.22503042221069336\n",
      "training: 84 batch 413 batch_loss: 0.2238755226135254\n",
      "training: 84 batch 414 batch_loss: 0.22763442993164062\n",
      "training: 84 batch 415 batch_loss: 0.22254714369773865\n",
      "training: 84 batch 416 batch_loss: 0.22379332780838013\n",
      "training: 84 batch 417 batch_loss: 0.2253522276878357\n",
      "training: 84 batch 418 batch_loss: 0.22659501433372498\n",
      "training: 84 batch 419 batch_loss: 0.22236108779907227\n",
      "training: 84 batch 420 batch_loss: 0.22431039810180664\n",
      "training: 84 batch 421 batch_loss: 0.22629660367965698\n",
      "training: 84 batch 422 batch_loss: 0.22266125679016113\n",
      "training: 84 batch 423 batch_loss: 0.21992450952529907\n",
      "training: 84 batch 424 batch_loss: 0.22064849734306335\n",
      "training: 84 batch 425 batch_loss: 0.22151455283164978\n",
      "training: 84 batch 426 batch_loss: 0.22405576705932617\n",
      "training: 84 batch 427 batch_loss: 0.2252591848373413\n",
      "training: 84 batch 428 batch_loss: 0.2253614366054535\n",
      "training: 84 batch 429 batch_loss: 0.22345107793807983\n",
      "training: 84 batch 430 batch_loss: 0.2241903841495514\n",
      "training: 84 batch 431 batch_loss: 0.22918277978897095\n",
      "training: 84 batch 432 batch_loss: 0.22156986594200134\n",
      "training: 84 batch 433 batch_loss: 0.22216060757637024\n",
      "training: 84 batch 434 batch_loss: 0.22078847885131836\n",
      "training: 84 batch 435 batch_loss: 0.22594785690307617\n",
      "training: 84 batch 436 batch_loss: 0.22177433967590332\n",
      "training: 84 batch 437 batch_loss: 0.2295970618724823\n",
      "training: 84 batch 438 batch_loss: 0.22489309310913086\n",
      "training: 84 batch 439 batch_loss: 0.2244798243045807\n",
      "training: 84 batch 440 batch_loss: 0.22187143564224243\n",
      "training: 84 batch 441 batch_loss: 0.2238042652606964\n",
      "training: 84 batch 442 batch_loss: 0.22344496846199036\n",
      "training: 84 batch 443 batch_loss: 0.22474250197410583\n",
      "training: 84 batch 444 batch_loss: 0.22836318612098694\n",
      "training: 84 batch 445 batch_loss: 0.22531437873840332\n",
      "training: 84 batch 446 batch_loss: 0.22700700163841248\n",
      "training: 84 batch 447 batch_loss: 0.22733554244041443\n",
      "training: 84 batch 448 batch_loss: 0.22431021928787231\n",
      "training: 84 batch 449 batch_loss: 0.22727477550506592\n",
      "training: 84 batch 450 batch_loss: 0.2256370186805725\n",
      "training: 84 batch 451 batch_loss: 0.22222712635993958\n",
      "training: 84 batch 452 batch_loss: 0.22582438588142395\n",
      "training: 84 batch 453 batch_loss: 0.22520822286605835\n",
      "training: 84 batch 454 batch_loss: 0.22483277320861816\n",
      "training: 84 batch 455 batch_loss: 0.22128897905349731\n",
      "training: 84 batch 456 batch_loss: 0.22602427005767822\n",
      "training: 84 batch 457 batch_loss: 0.22066104412078857\n",
      "training: 84 batch 458 batch_loss: 0.22468456625938416\n",
      "training: 84 batch 459 batch_loss: 0.22397887706756592\n",
      "training: 84 batch 460 batch_loss: 0.22003424167633057\n",
      "training: 84 batch 461 batch_loss: 0.223501056432724\n",
      "training: 84 batch 462 batch_loss: 0.22599413990974426\n",
      "training: 84 batch 463 batch_loss: 0.22687333822250366\n",
      "training: 84 batch 464 batch_loss: 0.22342965006828308\n",
      "training: 84 batch 465 batch_loss: 0.22485125064849854\n",
      "training: 84 batch 466 batch_loss: 0.22618624567985535\n",
      "training: 84 batch 467 batch_loss: 0.22442889213562012\n",
      "training: 84 batch 468 batch_loss: 0.2314867079257965\n",
      "training: 84 batch 469 batch_loss: 0.223811537027359\n",
      "training: 84 batch 470 batch_loss: 0.2235836684703827\n",
      "training: 84 batch 471 batch_loss: 0.22425734996795654\n",
      "training: 84 batch 472 batch_loss: 0.22254407405853271\n",
      "training: 84 batch 473 batch_loss: 0.22347792983055115\n",
      "training: 84 batch 474 batch_loss: 0.22000658512115479\n",
      "training: 84 batch 475 batch_loss: 0.22163963317871094\n",
      "training: 84 batch 476 batch_loss: 0.22738981246948242\n",
      "training: 84 batch 477 batch_loss: 0.2261943817138672\n",
      "training: 84 batch 478 batch_loss: 0.2290320098400116\n",
      "training: 84 batch 479 batch_loss: 0.22922533750534058\n",
      "training: 84 batch 480 batch_loss: 0.22684761881828308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 84 batch 481 batch_loss: 0.2200297713279724\n",
      "training: 84 batch 482 batch_loss: 0.2225843369960785\n",
      "training: 84 batch 483 batch_loss: 0.22559934854507446\n",
      "training: 84 batch 484 batch_loss: 0.2269251048564911\n",
      "training: 84 batch 485 batch_loss: 0.22725331783294678\n",
      "training: 84 batch 486 batch_loss: 0.22516342997550964\n",
      "training: 84 batch 487 batch_loss: 0.2230009138584137\n",
      "training: 84 batch 488 batch_loss: 0.23045378923416138\n",
      "training: 84 batch 489 batch_loss: 0.2282446026802063\n",
      "training: 84 batch 490 batch_loss: 0.21936184167861938\n",
      "training: 84 batch 491 batch_loss: 0.2245187759399414\n",
      "training: 84 batch 492 batch_loss: 0.2255471646785736\n",
      "training: 84 batch 493 batch_loss: 0.22397029399871826\n",
      "training: 84 batch 494 batch_loss: 0.22266221046447754\n",
      "training: 84 batch 495 batch_loss: 0.22393983602523804\n",
      "training: 84 batch 496 batch_loss: 0.21977940201759338\n",
      "training: 84 batch 497 batch_loss: 0.22568148374557495\n",
      "training: 84 batch 498 batch_loss: 0.22464853525161743\n",
      "training: 84 batch 499 batch_loss: 0.22477471828460693\n",
      "training: 84 batch 500 batch_loss: 0.22598060965538025\n",
      "training: 84 batch 501 batch_loss: 0.2262069582939148\n",
      "training: 84 batch 502 batch_loss: 0.22540169954299927\n",
      "training: 84 batch 503 batch_loss: 0.2238595187664032\n",
      "training: 84 batch 504 batch_loss: 0.22608914971351624\n",
      "training: 84 batch 505 batch_loss: 0.22816351056098938\n",
      "training: 84 batch 506 batch_loss: 0.22743719816207886\n",
      "training: 84 batch 507 batch_loss: 0.22236943244934082\n",
      "training: 84 batch 508 batch_loss: 0.21932438015937805\n",
      "training: 84 batch 509 batch_loss: 0.22491076588630676\n",
      "training: 84 batch 510 batch_loss: 0.22107917070388794\n",
      "training: 84 batch 511 batch_loss: 0.22745874524116516\n",
      "training: 84 batch 512 batch_loss: 0.22454285621643066\n",
      "training: 84 batch 513 batch_loss: 0.22348126769065857\n",
      "training: 84 batch 514 batch_loss: 0.22522681951522827\n",
      "training: 84 batch 515 batch_loss: 0.2208479344844818\n",
      "training: 84 batch 516 batch_loss: 0.2263316512107849\n",
      "training: 84 batch 517 batch_loss: 0.2216322422027588\n",
      "training: 84 batch 518 batch_loss: 0.22590431571006775\n",
      "training: 84 batch 519 batch_loss: 0.2226426601409912\n",
      "training: 84 batch 520 batch_loss: 0.2218216359615326\n",
      "training: 84 batch 521 batch_loss: 0.22587931156158447\n",
      "training: 84 batch 522 batch_loss: 0.22505858540534973\n",
      "training: 84 batch 523 batch_loss: 0.22792920470237732\n",
      "training: 84 batch 524 batch_loss: 0.2236168086528778\n",
      "training: 84 batch 525 batch_loss: 0.23084816336631775\n",
      "training: 84 batch 526 batch_loss: 0.22655731439590454\n",
      "training: 84 batch 527 batch_loss: 0.2267666459083557\n",
      "training: 84 batch 528 batch_loss: 0.2243935763835907\n",
      "training: 84 batch 529 batch_loss: 0.22623002529144287\n",
      "training: 84 batch 530 batch_loss: 0.2235165536403656\n",
      "training: 84 batch 531 batch_loss: 0.22098711133003235\n",
      "training: 84 batch 532 batch_loss: 0.2266445755958557\n",
      "training: 84 batch 533 batch_loss: 0.22067391872406006\n",
      "training: 84 batch 534 batch_loss: 0.22125500440597534\n",
      "training: 84 batch 535 batch_loss: 0.22613653540611267\n",
      "training: 84 batch 536 batch_loss: 0.2267688512802124\n",
      "training: 84 batch 537 batch_loss: 0.224864661693573\n",
      "training: 84 batch 538 batch_loss: 0.2253417670726776\n",
      "training: 84 batch 539 batch_loss: 0.22726210951805115\n",
      "training: 84 batch 540 batch_loss: 0.22445940971374512\n",
      "training: 84 batch 541 batch_loss: 0.22742930054664612\n",
      "training: 84 batch 542 batch_loss: 0.22654950618743896\n",
      "training: 84 batch 543 batch_loss: 0.22475922107696533\n",
      "training: 84 batch 544 batch_loss: 0.22515219449996948\n",
      "training: 84 batch 545 batch_loss: 0.2217901051044464\n",
      "training: 84 batch 546 batch_loss: 0.2224838137626648\n",
      "training: 84 batch 547 batch_loss: 0.2217915952205658\n",
      "training: 84 batch 548 batch_loss: 0.22380658984184265\n",
      "training: 84 batch 549 batch_loss: 0.22307360172271729\n",
      "training: 84 batch 550 batch_loss: 0.22647523880004883\n",
      "training: 84 batch 551 batch_loss: 0.22389060258865356\n",
      "training: 84 batch 552 batch_loss: 0.2227422297000885\n",
      "training: 84 batch 553 batch_loss: 0.22223705053329468\n",
      "training: 84 batch 554 batch_loss: 0.22347649931907654\n",
      "training: 84 batch 555 batch_loss: 0.22560304403305054\n",
      "training: 84 batch 556 batch_loss: 0.22264811396598816\n",
      "training: 84 batch 557 batch_loss: 0.22728538513183594\n",
      "training: 84 batch 558 batch_loss: 0.21906229853630066\n",
      "training: 84 batch 559 batch_loss: 0.2245771586894989\n",
      "training: 84 batch 560 batch_loss: 0.2278611660003662\n",
      "training: 84 batch 561 batch_loss: 0.21962743997573853\n",
      "training: 84 batch 562 batch_loss: 0.22569730877876282\n",
      "training: 84 batch 563 batch_loss: 0.22724270820617676\n",
      "training: 84 batch 564 batch_loss: 0.2217230498790741\n",
      "training: 84 batch 565 batch_loss: 0.22669023275375366\n",
      "training: 84 batch 566 batch_loss: 0.22508075833320618\n",
      "training: 84 batch 567 batch_loss: 0.22327154874801636\n",
      "training: 84 batch 568 batch_loss: 0.22261196374893188\n",
      "training: 84 batch 569 batch_loss: 0.21984413266181946\n",
      "training: 84 batch 570 batch_loss: 0.2244141697883606\n",
      "training: 84 batch 571 batch_loss: 0.2238955795764923\n",
      "training: 84 batch 572 batch_loss: 0.22674089670181274\n",
      "training: 84 batch 573 batch_loss: 0.2236708700656891\n",
      "training: 84 batch 574 batch_loss: 0.22005638480186462\n",
      "training: 84 batch 575 batch_loss: 0.22182321548461914\n",
      "training: 84 batch 576 batch_loss: 0.2341059148311615\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 84, Hit Ratio:0.03372497026912686 | Precision:0.04975916642091811 | Recall:0.06572280290203765 | NDCG:0.06485364476430899\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 85 batch 0 batch_loss: 0.2265472710132599\n",
      "training: 85 batch 1 batch_loss: 0.22143062949180603\n",
      "training: 85 batch 2 batch_loss: 0.22176775336265564\n",
      "training: 85 batch 3 batch_loss: 0.2196969985961914\n",
      "training: 85 batch 4 batch_loss: 0.22373396158218384\n",
      "training: 85 batch 5 batch_loss: 0.22153475880622864\n",
      "training: 85 batch 6 batch_loss: 0.21944648027420044\n",
      "training: 85 batch 7 batch_loss: 0.22159641981124878\n",
      "training: 85 batch 8 batch_loss: 0.22022360563278198\n",
      "training: 85 batch 9 batch_loss: 0.21920964121818542\n",
      "training: 85 batch 10 batch_loss: 0.22928497195243835\n",
      "training: 85 batch 11 batch_loss: 0.22382807731628418\n",
      "training: 85 batch 12 batch_loss: 0.2202254831790924\n",
      "training: 85 batch 13 batch_loss: 0.22685471177101135\n",
      "training: 85 batch 14 batch_loss: 0.2269611954689026\n",
      "training: 85 batch 15 batch_loss: 0.22655847668647766\n",
      "training: 85 batch 16 batch_loss: 0.22223079204559326\n",
      "training: 85 batch 17 batch_loss: 0.21429786086082458\n",
      "training: 85 batch 18 batch_loss: 0.22128844261169434\n",
      "training: 85 batch 19 batch_loss: 0.22649270296096802\n",
      "training: 85 batch 20 batch_loss: 0.22399204969406128\n",
      "training: 85 batch 21 batch_loss: 0.22493773698806763\n",
      "training: 85 batch 22 batch_loss: 0.21745431423187256\n",
      "training: 85 batch 23 batch_loss: 0.22437474131584167\n",
      "training: 85 batch 24 batch_loss: 0.22133123874664307\n",
      "training: 85 batch 25 batch_loss: 0.22255995869636536\n",
      "training: 85 batch 26 batch_loss: 0.22744932770729065\n",
      "training: 85 batch 27 batch_loss: 0.22467395663261414\n",
      "training: 85 batch 28 batch_loss: 0.22251954674720764\n",
      "training: 85 batch 29 batch_loss: 0.2238512933254242\n",
      "training: 85 batch 30 batch_loss: 0.2225252389907837\n",
      "training: 85 batch 31 batch_loss: 0.2245120108127594\n",
      "training: 85 batch 32 batch_loss: 0.22588080167770386\n",
      "training: 85 batch 33 batch_loss: 0.22400984168052673\n",
      "training: 85 batch 34 batch_loss: 0.22076702117919922\n",
      "training: 85 batch 35 batch_loss: 0.2262858748435974\n",
      "training: 85 batch 36 batch_loss: 0.22256669402122498\n",
      "training: 85 batch 37 batch_loss: 0.22631174325942993\n",
      "training: 85 batch 38 batch_loss: 0.22171252965927124\n",
      "training: 85 batch 39 batch_loss: 0.21997186541557312\n",
      "training: 85 batch 40 batch_loss: 0.2164939045906067\n",
      "training: 85 batch 41 batch_loss: 0.23020526766777039\n",
      "training: 85 batch 42 batch_loss: 0.22841250896453857\n",
      "training: 85 batch 43 batch_loss: 0.22127598524093628\n",
      "training: 85 batch 44 batch_loss: 0.22340232133865356\n",
      "training: 85 batch 45 batch_loss: 0.2190110683441162\n",
      "training: 85 batch 46 batch_loss: 0.21991819143295288\n",
      "training: 85 batch 47 batch_loss: 0.21866199374198914\n",
      "training: 85 batch 48 batch_loss: 0.2276705801486969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 85 batch 49 batch_loss: 0.221943199634552\n",
      "training: 85 batch 50 batch_loss: 0.21696707606315613\n",
      "training: 85 batch 51 batch_loss: 0.22062334418296814\n",
      "training: 85 batch 52 batch_loss: 0.22301572561264038\n",
      "training: 85 batch 53 batch_loss: 0.22232860326766968\n",
      "training: 85 batch 54 batch_loss: 0.2226710021495819\n",
      "training: 85 batch 55 batch_loss: 0.22443729639053345\n",
      "training: 85 batch 56 batch_loss: 0.22832298278808594\n",
      "training: 85 batch 57 batch_loss: 0.2236936092376709\n",
      "training: 85 batch 58 batch_loss: 0.2238958477973938\n",
      "training: 85 batch 59 batch_loss: 0.2218194603919983\n",
      "training: 85 batch 60 batch_loss: 0.22344210743904114\n",
      "training: 85 batch 61 batch_loss: 0.22220271825790405\n",
      "training: 85 batch 62 batch_loss: 0.22637030482292175\n",
      "training: 85 batch 63 batch_loss: 0.22651806473731995\n",
      "training: 85 batch 64 batch_loss: 0.22101280093193054\n",
      "training: 85 batch 65 batch_loss: 0.22168180346488953\n",
      "training: 85 batch 66 batch_loss: 0.220074862241745\n",
      "training: 85 batch 67 batch_loss: 0.22292569279670715\n",
      "training: 85 batch 68 batch_loss: 0.22537896037101746\n",
      "training: 85 batch 69 batch_loss: 0.220772385597229\n",
      "training: 85 batch 70 batch_loss: 0.22380861639976501\n",
      "training: 85 batch 71 batch_loss: 0.22160837054252625\n",
      "training: 85 batch 72 batch_loss: 0.2244994044303894\n",
      "training: 85 batch 73 batch_loss: 0.2289978265762329\n",
      "training: 85 batch 74 batch_loss: 0.22393381595611572\n",
      "training: 85 batch 75 batch_loss: 0.22438868880271912\n",
      "training: 85 batch 76 batch_loss: 0.22075942158699036\n",
      "training: 85 batch 77 batch_loss: 0.22474926710128784\n",
      "training: 85 batch 78 batch_loss: 0.22253775596618652\n",
      "training: 85 batch 79 batch_loss: 0.22328108549118042\n",
      "training: 85 batch 80 batch_loss: 0.22082701325416565\n",
      "training: 85 batch 81 batch_loss: 0.22377514839172363\n",
      "training: 85 batch 82 batch_loss: 0.22373640537261963\n",
      "training: 85 batch 83 batch_loss: 0.2238239049911499\n",
      "training: 85 batch 84 batch_loss: 0.2282688319683075\n",
      "training: 85 batch 85 batch_loss: 0.22312337160110474\n",
      "training: 85 batch 86 batch_loss: 0.22165775299072266\n",
      "training: 85 batch 87 batch_loss: 0.22403603792190552\n",
      "training: 85 batch 88 batch_loss: 0.22494447231292725\n",
      "training: 85 batch 89 batch_loss: 0.22376811504364014\n",
      "training: 85 batch 90 batch_loss: 0.22540020942687988\n",
      "training: 85 batch 91 batch_loss: 0.22185349464416504\n",
      "training: 85 batch 92 batch_loss: 0.22345182299613953\n",
      "training: 85 batch 93 batch_loss: 0.22447162866592407\n",
      "training: 85 batch 94 batch_loss: 0.22212785482406616\n",
      "training: 85 batch 95 batch_loss: 0.22543329000473022\n",
      "training: 85 batch 96 batch_loss: 0.2220780849456787\n",
      "training: 85 batch 97 batch_loss: 0.22225019335746765\n",
      "training: 85 batch 98 batch_loss: 0.22540006041526794\n",
      "training: 85 batch 99 batch_loss: 0.22535789012908936\n",
      "training: 85 batch 100 batch_loss: 0.2228294014930725\n",
      "training: 85 batch 101 batch_loss: 0.22448289394378662\n",
      "training: 85 batch 102 batch_loss: 0.2268657684326172\n",
      "training: 85 batch 103 batch_loss: 0.22224459052085876\n",
      "training: 85 batch 104 batch_loss: 0.22596114873886108\n",
      "training: 85 batch 105 batch_loss: 0.22385376691818237\n",
      "training: 85 batch 106 batch_loss: 0.21904662251472473\n",
      "training: 85 batch 107 batch_loss: 0.22726833820343018\n",
      "training: 85 batch 108 batch_loss: 0.2237091064453125\n",
      "training: 85 batch 109 batch_loss: 0.22846847772598267\n",
      "training: 85 batch 110 batch_loss: 0.22204864025115967\n",
      "training: 85 batch 111 batch_loss: 0.22528913617134094\n",
      "training: 85 batch 112 batch_loss: 0.22396132349967957\n",
      "training: 85 batch 113 batch_loss: 0.22112220525741577\n",
      "training: 85 batch 114 batch_loss: 0.2228471338748932\n",
      "training: 85 batch 115 batch_loss: 0.22174885869026184\n",
      "training: 85 batch 116 batch_loss: 0.22262293100357056\n",
      "training: 85 batch 117 batch_loss: 0.22682806849479675\n",
      "training: 85 batch 118 batch_loss: 0.22753459215164185\n",
      "training: 85 batch 119 batch_loss: 0.22363322973251343\n",
      "training: 85 batch 120 batch_loss: 0.22270438075065613\n",
      "training: 85 batch 121 batch_loss: 0.2223595380783081\n",
      "training: 85 batch 122 batch_loss: 0.22573688626289368\n",
      "training: 85 batch 123 batch_loss: 0.22521597146987915\n",
      "training: 85 batch 124 batch_loss: 0.22255605459213257\n",
      "training: 85 batch 125 batch_loss: 0.22044125199317932\n",
      "training: 85 batch 126 batch_loss: 0.22507131099700928\n",
      "training: 85 batch 127 batch_loss: 0.2212931513786316\n",
      "training: 85 batch 128 batch_loss: 0.22443288564682007\n",
      "training: 85 batch 129 batch_loss: 0.2267945408821106\n",
      "training: 85 batch 130 batch_loss: 0.22028613090515137\n",
      "training: 85 batch 131 batch_loss: 0.2194073498249054\n",
      "training: 85 batch 132 batch_loss: 0.2249758541584015\n",
      "training: 85 batch 133 batch_loss: 0.22163456678390503\n",
      "training: 85 batch 134 batch_loss: 0.22231432795524597\n",
      "training: 85 batch 135 batch_loss: 0.22078809142112732\n",
      "training: 85 batch 136 batch_loss: 0.2235734462738037\n",
      "training: 85 batch 137 batch_loss: 0.2241584062576294\n",
      "training: 85 batch 138 batch_loss: 0.22879159450531006\n",
      "training: 85 batch 139 batch_loss: 0.22317183017730713\n",
      "training: 85 batch 140 batch_loss: 0.2221699059009552\n",
      "training: 85 batch 141 batch_loss: 0.2241632640361786\n",
      "training: 85 batch 142 batch_loss: 0.2265176773071289\n",
      "training: 85 batch 143 batch_loss: 0.22323065996170044\n",
      "training: 85 batch 144 batch_loss: 0.22454804182052612\n",
      "training: 85 batch 145 batch_loss: 0.2225569188594818\n",
      "training: 85 batch 146 batch_loss: 0.22652262449264526\n",
      "training: 85 batch 147 batch_loss: 0.22376027703285217\n",
      "training: 85 batch 148 batch_loss: 0.2259226143360138\n",
      "training: 85 batch 149 batch_loss: 0.22614818811416626\n",
      "training: 85 batch 150 batch_loss: 0.2227318286895752\n",
      "training: 85 batch 151 batch_loss: 0.22101658582687378\n",
      "training: 85 batch 152 batch_loss: 0.22363224625587463\n",
      "training: 85 batch 153 batch_loss: 0.22089499235153198\n",
      "training: 85 batch 154 batch_loss: 0.22225093841552734\n",
      "training: 85 batch 155 batch_loss: 0.2205629050731659\n",
      "training: 85 batch 156 batch_loss: 0.22707021236419678\n",
      "training: 85 batch 157 batch_loss: 0.22951379418373108\n",
      "training: 85 batch 158 batch_loss: 0.22788241505622864\n",
      "training: 85 batch 159 batch_loss: 0.2241700291633606\n",
      "training: 85 batch 160 batch_loss: 0.22479498386383057\n",
      "training: 85 batch 161 batch_loss: 0.22615814208984375\n",
      "training: 85 batch 162 batch_loss: 0.22048646211624146\n",
      "training: 85 batch 163 batch_loss: 0.2258966565132141\n",
      "training: 85 batch 164 batch_loss: 0.22475102543830872\n",
      "training: 85 batch 165 batch_loss: 0.22375866770744324\n",
      "training: 85 batch 166 batch_loss: 0.22719475626945496\n",
      "training: 85 batch 167 batch_loss: 0.22522786259651184\n",
      "training: 85 batch 168 batch_loss: 0.2252160906791687\n",
      "training: 85 batch 169 batch_loss: 0.23018735647201538\n",
      "training: 85 batch 170 batch_loss: 0.2242334485054016\n",
      "training: 85 batch 171 batch_loss: 0.22367674112319946\n",
      "training: 85 batch 172 batch_loss: 0.22362440824508667\n",
      "training: 85 batch 173 batch_loss: 0.22112798690795898\n",
      "training: 85 batch 174 batch_loss: 0.22440105676651\n",
      "training: 85 batch 175 batch_loss: 0.2231001853942871\n",
      "training: 85 batch 176 batch_loss: 0.22504517436027527\n",
      "training: 85 batch 177 batch_loss: 0.21835798025131226\n",
      "training: 85 batch 178 batch_loss: 0.2228282392024994\n",
      "training: 85 batch 179 batch_loss: 0.22296744585037231\n",
      "training: 85 batch 180 batch_loss: 0.22173219919204712\n",
      "training: 85 batch 181 batch_loss: 0.22679397463798523\n",
      "training: 85 batch 182 batch_loss: 0.22520020604133606\n",
      "training: 85 batch 183 batch_loss: 0.22187113761901855\n",
      "training: 85 batch 184 batch_loss: 0.22141611576080322\n",
      "training: 85 batch 185 batch_loss: 0.22190088033676147\n",
      "training: 85 batch 186 batch_loss: 0.22584086656570435\n",
      "training: 85 batch 187 batch_loss: 0.22793275117874146\n",
      "training: 85 batch 188 batch_loss: 0.22446507215499878\n",
      "training: 85 batch 189 batch_loss: 0.22548505663871765\n",
      "training: 85 batch 190 batch_loss: 0.2238442599773407\n",
      "training: 85 batch 191 batch_loss: 0.22525382041931152\n",
      "training: 85 batch 192 batch_loss: 0.2232533097267151\n",
      "training: 85 batch 193 batch_loss: 0.2320612370967865\n",
      "training: 85 batch 194 batch_loss: 0.2254379689693451\n",
      "training: 85 batch 195 batch_loss: 0.22120049595832825\n",
      "training: 85 batch 196 batch_loss: 0.2216050624847412\n",
      "training: 85 batch 197 batch_loss: 0.22355732321739197\n",
      "training: 85 batch 198 batch_loss: 0.22802096605300903\n",
      "training: 85 batch 199 batch_loss: 0.22199147939682007\n",
      "training: 85 batch 200 batch_loss: 0.22324854135513306\n",
      "training: 85 batch 201 batch_loss: 0.2227400541305542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 85 batch 202 batch_loss: 0.2242024540901184\n",
      "training: 85 batch 203 batch_loss: 0.2262544333934784\n",
      "training: 85 batch 204 batch_loss: 0.22353199124336243\n",
      "training: 85 batch 205 batch_loss: 0.22394612431526184\n",
      "training: 85 batch 206 batch_loss: 0.22673359513282776\n",
      "training: 85 batch 207 batch_loss: 0.22174406051635742\n",
      "training: 85 batch 208 batch_loss: 0.2240738570690155\n",
      "training: 85 batch 209 batch_loss: 0.2224711775779724\n",
      "training: 85 batch 210 batch_loss: 0.21968400478363037\n",
      "training: 85 batch 211 batch_loss: 0.22371846437454224\n",
      "training: 85 batch 212 batch_loss: 0.21850058436393738\n",
      "training: 85 batch 213 batch_loss: 0.22269544005393982\n",
      "training: 85 batch 214 batch_loss: 0.22126099467277527\n",
      "training: 85 batch 215 batch_loss: 0.2229243814945221\n",
      "training: 85 batch 216 batch_loss: 0.220342218875885\n",
      "training: 85 batch 217 batch_loss: 0.2278132438659668\n",
      "training: 85 batch 218 batch_loss: 0.21954938769340515\n",
      "training: 85 batch 219 batch_loss: 0.22395044565200806\n",
      "training: 85 batch 220 batch_loss: 0.22390589118003845\n",
      "training: 85 batch 221 batch_loss: 0.2246984839439392\n",
      "training: 85 batch 222 batch_loss: 0.2217380702495575\n",
      "training: 85 batch 223 batch_loss: 0.2225644290447235\n",
      "training: 85 batch 224 batch_loss: 0.22469156980514526\n",
      "training: 85 batch 225 batch_loss: 0.22451773285865784\n",
      "training: 85 batch 226 batch_loss: 0.2272847592830658\n",
      "training: 85 batch 227 batch_loss: 0.22545817494392395\n",
      "training: 85 batch 228 batch_loss: 0.22165989875793457\n",
      "training: 85 batch 229 batch_loss: 0.22525757551193237\n",
      "training: 85 batch 230 batch_loss: 0.21914678812026978\n",
      "training: 85 batch 231 batch_loss: 0.22413399815559387\n",
      "training: 85 batch 232 batch_loss: 0.22517067193984985\n",
      "training: 85 batch 233 batch_loss: 0.22498613595962524\n",
      "training: 85 batch 234 batch_loss: 0.22159552574157715\n",
      "training: 85 batch 235 batch_loss: 0.22481709718704224\n",
      "training: 85 batch 236 batch_loss: 0.22301191091537476\n",
      "training: 85 batch 237 batch_loss: 0.22387152910232544\n",
      "training: 85 batch 238 batch_loss: 0.2274724543094635\n",
      "training: 85 batch 239 batch_loss: 0.2251582145690918\n",
      "training: 85 batch 240 batch_loss: 0.22549977898597717\n",
      "training: 85 batch 241 batch_loss: 0.23154610395431519\n",
      "training: 85 batch 242 batch_loss: 0.22854307293891907\n",
      "training: 85 batch 243 batch_loss: 0.2264765501022339\n",
      "training: 85 batch 244 batch_loss: 0.22895127534866333\n",
      "training: 85 batch 245 batch_loss: 0.2252907156944275\n",
      "training: 85 batch 246 batch_loss: 0.22092914581298828\n",
      "training: 85 batch 247 batch_loss: 0.2204018235206604\n",
      "training: 85 batch 248 batch_loss: 0.21953004598617554\n",
      "training: 85 batch 249 batch_loss: 0.22959178686141968\n",
      "training: 85 batch 250 batch_loss: 0.22431707382202148\n",
      "training: 85 batch 251 batch_loss: 0.2209322452545166\n",
      "training: 85 batch 252 batch_loss: 0.2209908664226532\n",
      "training: 85 batch 253 batch_loss: 0.22444814443588257\n",
      "training: 85 batch 254 batch_loss: 0.2232266366481781\n",
      "training: 85 batch 255 batch_loss: 0.2220391035079956\n",
      "training: 85 batch 256 batch_loss: 0.2251589596271515\n",
      "training: 85 batch 257 batch_loss: 0.22630080580711365\n",
      "training: 85 batch 258 batch_loss: 0.22464781999588013\n",
      "training: 85 batch 259 batch_loss: 0.22544193267822266\n",
      "training: 85 batch 260 batch_loss: 0.22472047805786133\n",
      "training: 85 batch 261 batch_loss: 0.22328782081604004\n",
      "training: 85 batch 262 batch_loss: 0.22523677349090576\n",
      "training: 85 batch 263 batch_loss: 0.22487196326255798\n",
      "training: 85 batch 264 batch_loss: 0.2225446105003357\n",
      "training: 85 batch 265 batch_loss: 0.22502318024635315\n",
      "training: 85 batch 266 batch_loss: 0.2269880473613739\n",
      "training: 85 batch 267 batch_loss: 0.22314220666885376\n",
      "training: 85 batch 268 batch_loss: 0.22314593195915222\n",
      "training: 85 batch 269 batch_loss: 0.2251591682434082\n",
      "training: 85 batch 270 batch_loss: 0.22106051445007324\n",
      "training: 85 batch 271 batch_loss: 0.2252485752105713\n",
      "training: 85 batch 272 batch_loss: 0.22372877597808838\n",
      "training: 85 batch 273 batch_loss: 0.22112411260604858\n",
      "training: 85 batch 274 batch_loss: 0.22308972477912903\n",
      "training: 85 batch 275 batch_loss: 0.2217634916305542\n",
      "training: 85 batch 276 batch_loss: 0.22517529129981995\n",
      "training: 85 batch 277 batch_loss: 0.22441643476486206\n",
      "training: 85 batch 278 batch_loss: 0.21959656476974487\n",
      "training: 85 batch 279 batch_loss: 0.2264167070388794\n",
      "training: 85 batch 280 batch_loss: 0.22381064295768738\n",
      "training: 85 batch 281 batch_loss: 0.22581246495246887\n",
      "training: 85 batch 282 batch_loss: 0.22471854090690613\n",
      "training: 85 batch 283 batch_loss: 0.2214227020740509\n",
      "training: 85 batch 284 batch_loss: 0.223300039768219\n",
      "training: 85 batch 285 batch_loss: 0.22570359706878662\n",
      "training: 85 batch 286 batch_loss: 0.2166513204574585\n",
      "training: 85 batch 287 batch_loss: 0.2203701138496399\n",
      "training: 85 batch 288 batch_loss: 0.22365927696228027\n",
      "training: 85 batch 289 batch_loss: 0.22380387783050537\n",
      "training: 85 batch 290 batch_loss: 0.22634363174438477\n",
      "training: 85 batch 291 batch_loss: 0.22156187891960144\n",
      "training: 85 batch 292 batch_loss: 0.2252991795539856\n",
      "training: 85 batch 293 batch_loss: 0.22679996490478516\n",
      "training: 85 batch 294 batch_loss: 0.22532624006271362\n",
      "training: 85 batch 295 batch_loss: 0.22132712602615356\n",
      "training: 85 batch 296 batch_loss: 0.22805556654930115\n",
      "training: 85 batch 297 batch_loss: 0.22863101959228516\n",
      "training: 85 batch 298 batch_loss: 0.224907785654068\n",
      "training: 85 batch 299 batch_loss: 0.22616931796073914\n",
      "training: 85 batch 300 batch_loss: 0.2235463559627533\n",
      "training: 85 batch 301 batch_loss: 0.22662800550460815\n",
      "training: 85 batch 302 batch_loss: 0.22241443395614624\n",
      "training: 85 batch 303 batch_loss: 0.2227678894996643\n",
      "training: 85 batch 304 batch_loss: 0.22383353114128113\n",
      "training: 85 batch 305 batch_loss: 0.22615393996238708\n",
      "training: 85 batch 306 batch_loss: 0.22165483236312866\n",
      "training: 85 batch 307 batch_loss: 0.2207161784172058\n",
      "training: 85 batch 308 batch_loss: 0.22784915566444397\n",
      "training: 85 batch 309 batch_loss: 0.22211936116218567\n",
      "training: 85 batch 310 batch_loss: 0.22587907314300537\n",
      "training: 85 batch 311 batch_loss: 0.22416171431541443\n",
      "training: 85 batch 312 batch_loss: 0.22632285952568054\n",
      "training: 85 batch 313 batch_loss: 0.21841087937355042\n",
      "training: 85 batch 314 batch_loss: 0.22564968466758728\n",
      "training: 85 batch 315 batch_loss: 0.226106196641922\n",
      "training: 85 batch 316 batch_loss: 0.2294045090675354\n",
      "training: 85 batch 317 batch_loss: 0.2262134552001953\n",
      "training: 85 batch 318 batch_loss: 0.21887490153312683\n",
      "training: 85 batch 319 batch_loss: 0.22168153524398804\n",
      "training: 85 batch 320 batch_loss: 0.22510197758674622\n",
      "training: 85 batch 321 batch_loss: 0.22449997067451477\n",
      "training: 85 batch 322 batch_loss: 0.22481009364128113\n",
      "training: 85 batch 323 batch_loss: 0.22613093256950378\n",
      "training: 85 batch 324 batch_loss: 0.22311896085739136\n",
      "training: 85 batch 325 batch_loss: 0.2229287326335907\n",
      "training: 85 batch 326 batch_loss: 0.22606587409973145\n",
      "training: 85 batch 327 batch_loss: 0.22870147228240967\n",
      "training: 85 batch 328 batch_loss: 0.22865742444992065\n",
      "training: 85 batch 329 batch_loss: 0.22514429688453674\n",
      "training: 85 batch 330 batch_loss: 0.22384846210479736\n",
      "training: 85 batch 331 batch_loss: 0.2243971824645996\n",
      "training: 85 batch 332 batch_loss: 0.2265978753566742\n",
      "training: 85 batch 333 batch_loss: 0.22591632604599\n",
      "training: 85 batch 334 batch_loss: 0.22393015027046204\n",
      "training: 85 batch 335 batch_loss: 0.2268587350845337\n",
      "training: 85 batch 336 batch_loss: 0.22393479943275452\n",
      "training: 85 batch 337 batch_loss: 0.2244146466255188\n",
      "training: 85 batch 338 batch_loss: 0.22656479477882385\n",
      "training: 85 batch 339 batch_loss: 0.22758084535598755\n",
      "training: 85 batch 340 batch_loss: 0.22400909662246704\n",
      "training: 85 batch 341 batch_loss: 0.22631803154945374\n",
      "training: 85 batch 342 batch_loss: 0.22538471221923828\n",
      "training: 85 batch 343 batch_loss: 0.22676220536231995\n",
      "training: 85 batch 344 batch_loss: 0.22688204050064087\n",
      "training: 85 batch 345 batch_loss: 0.2248806655406952\n",
      "training: 85 batch 346 batch_loss: 0.22293591499328613\n",
      "training: 85 batch 347 batch_loss: 0.22193068265914917\n",
      "training: 85 batch 348 batch_loss: 0.2276642620563507\n",
      "training: 85 batch 349 batch_loss: 0.22015967965126038\n",
      "training: 85 batch 350 batch_loss: 0.22470849752426147\n",
      "training: 85 batch 351 batch_loss: 0.22393548488616943\n",
      "training: 85 batch 352 batch_loss: 0.22592037916183472\n",
      "training: 85 batch 353 batch_loss: 0.21786615252494812\n",
      "training: 85 batch 354 batch_loss: 0.22726908326148987\n",
      "training: 85 batch 355 batch_loss: 0.2290685772895813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 85 batch 356 batch_loss: 0.22267326712608337\n",
      "training: 85 batch 357 batch_loss: 0.22118309140205383\n",
      "training: 85 batch 358 batch_loss: 0.22282138466835022\n",
      "training: 85 batch 359 batch_loss: 0.22250962257385254\n",
      "training: 85 batch 360 batch_loss: 0.22279870510101318\n",
      "training: 85 batch 361 batch_loss: 0.22505396604537964\n",
      "training: 85 batch 362 batch_loss: 0.22018703818321228\n",
      "training: 85 batch 363 batch_loss: 0.2242201566696167\n",
      "training: 85 batch 364 batch_loss: 0.2271067202091217\n",
      "training: 85 batch 365 batch_loss: 0.22534534335136414\n",
      "training: 85 batch 366 batch_loss: 0.2251497507095337\n",
      "training: 85 batch 367 batch_loss: 0.22411897778511047\n",
      "training: 85 batch 368 batch_loss: 0.2298157811164856\n",
      "training: 85 batch 369 batch_loss: 0.2242322564125061\n",
      "training: 85 batch 370 batch_loss: 0.2215929627418518\n",
      "training: 85 batch 371 batch_loss: 0.22014614939689636\n",
      "training: 85 batch 372 batch_loss: 0.22586023807525635\n",
      "training: 85 batch 373 batch_loss: 0.22695940732955933\n",
      "training: 85 batch 374 batch_loss: 0.22771289944648743\n",
      "training: 85 batch 375 batch_loss: 0.21975213289260864\n",
      "training: 85 batch 376 batch_loss: 0.22371983528137207\n",
      "training: 85 batch 377 batch_loss: 0.22321376204490662\n",
      "training: 85 batch 378 batch_loss: 0.22260987758636475\n",
      "training: 85 batch 379 batch_loss: 0.22141286730766296\n",
      "training: 85 batch 380 batch_loss: 0.2244867980480194\n",
      "training: 85 batch 381 batch_loss: 0.2236638367176056\n",
      "training: 85 batch 382 batch_loss: 0.22538012266159058\n",
      "training: 85 batch 383 batch_loss: 0.2247907817363739\n",
      "training: 85 batch 384 batch_loss: 0.22131052613258362\n",
      "training: 85 batch 385 batch_loss: 0.2281804084777832\n",
      "training: 85 batch 386 batch_loss: 0.22648853063583374\n",
      "training: 85 batch 387 batch_loss: 0.22387337684631348\n",
      "training: 85 batch 388 batch_loss: 0.22764307260513306\n",
      "training: 85 batch 389 batch_loss: 0.22374525666236877\n",
      "training: 85 batch 390 batch_loss: 0.22319939732551575\n",
      "training: 85 batch 391 batch_loss: 0.2245885133743286\n",
      "training: 85 batch 392 batch_loss: 0.22170749306678772\n",
      "training: 85 batch 393 batch_loss: 0.22172492742538452\n",
      "training: 85 batch 394 batch_loss: 0.22282695770263672\n",
      "training: 85 batch 395 batch_loss: 0.22788840532302856\n",
      "training: 85 batch 396 batch_loss: 0.22550857067108154\n",
      "training: 85 batch 397 batch_loss: 0.2229672074317932\n",
      "training: 85 batch 398 batch_loss: 0.23000752925872803\n",
      "training: 85 batch 399 batch_loss: 0.22280672192573547\n",
      "training: 85 batch 400 batch_loss: 0.22497886419296265\n",
      "training: 85 batch 401 batch_loss: 0.22404181957244873\n",
      "training: 85 batch 402 batch_loss: 0.22518736124038696\n",
      "training: 85 batch 403 batch_loss: 0.22955560684204102\n",
      "training: 85 batch 404 batch_loss: 0.2267591655254364\n",
      "training: 85 batch 405 batch_loss: 0.2211846113204956\n",
      "training: 85 batch 406 batch_loss: 0.23157930374145508\n",
      "training: 85 batch 407 batch_loss: 0.23062554001808167\n",
      "training: 85 batch 408 batch_loss: 0.2228672206401825\n",
      "training: 85 batch 409 batch_loss: 0.2218261957168579\n",
      "training: 85 batch 410 batch_loss: 0.2234935760498047\n",
      "training: 85 batch 411 batch_loss: 0.22469371557235718\n",
      "training: 85 batch 412 batch_loss: 0.22787752747535706\n",
      "training: 85 batch 413 batch_loss: 0.22242975234985352\n",
      "training: 85 batch 414 batch_loss: 0.22524607181549072\n",
      "training: 85 batch 415 batch_loss: 0.22160103917121887\n",
      "training: 85 batch 416 batch_loss: 0.22863155603408813\n",
      "training: 85 batch 417 batch_loss: 0.22680604457855225\n",
      "training: 85 batch 418 batch_loss: 0.22356531023979187\n",
      "training: 85 batch 419 batch_loss: 0.2195192277431488\n",
      "training: 85 batch 420 batch_loss: 0.22594046592712402\n",
      "training: 85 batch 421 batch_loss: 0.22564011812210083\n",
      "training: 85 batch 422 batch_loss: 0.22356638312339783\n",
      "training: 85 batch 423 batch_loss: 0.22643360495567322\n",
      "training: 85 batch 424 batch_loss: 0.22391414642333984\n",
      "training: 85 batch 425 batch_loss: 0.22801417112350464\n",
      "training: 85 batch 426 batch_loss: 0.2270694375038147\n",
      "training: 85 batch 427 batch_loss: 0.22591978311538696\n",
      "training: 85 batch 428 batch_loss: 0.22595694661140442\n",
      "training: 85 batch 429 batch_loss: 0.22341573238372803\n",
      "training: 85 batch 430 batch_loss: 0.2262347936630249\n",
      "training: 85 batch 431 batch_loss: 0.22427499294281006\n",
      "training: 85 batch 432 batch_loss: 0.22260934114456177\n",
      "training: 85 batch 433 batch_loss: 0.22586870193481445\n",
      "training: 85 batch 434 batch_loss: 0.22761550545692444\n",
      "training: 85 batch 435 batch_loss: 0.22480368614196777\n",
      "training: 85 batch 436 batch_loss: 0.22572213411331177\n",
      "training: 85 batch 437 batch_loss: 0.22649043798446655\n",
      "training: 85 batch 438 batch_loss: 0.2280193269252777\n",
      "training: 85 batch 439 batch_loss: 0.22813040018081665\n",
      "training: 85 batch 440 batch_loss: 0.2240849733352661\n",
      "training: 85 batch 441 batch_loss: 0.22492548823356628\n",
      "training: 85 batch 442 batch_loss: 0.2257634997367859\n",
      "training: 85 batch 443 batch_loss: 0.22614312171936035\n",
      "training: 85 batch 444 batch_loss: 0.224125474691391\n",
      "training: 85 batch 445 batch_loss: 0.22400498390197754\n",
      "training: 85 batch 446 batch_loss: 0.22431600093841553\n",
      "training: 85 batch 447 batch_loss: 0.22711971402168274\n",
      "training: 85 batch 448 batch_loss: 0.22424650192260742\n",
      "training: 85 batch 449 batch_loss: 0.22473037242889404\n",
      "training: 85 batch 450 batch_loss: 0.2251897156238556\n",
      "training: 85 batch 451 batch_loss: 0.22609838843345642\n",
      "training: 85 batch 452 batch_loss: 0.22518610954284668\n",
      "training: 85 batch 453 batch_loss: 0.22613131999969482\n",
      "training: 85 batch 454 batch_loss: 0.22573959827423096\n",
      "training: 85 batch 455 batch_loss: 0.22680428624153137\n",
      "training: 85 batch 456 batch_loss: 0.22303315997123718\n",
      "training: 85 batch 457 batch_loss: 0.22768664360046387\n",
      "training: 85 batch 458 batch_loss: 0.2273775339126587\n",
      "training: 85 batch 459 batch_loss: 0.22301992774009705\n",
      "training: 85 batch 460 batch_loss: 0.22034570574760437\n",
      "training: 85 batch 461 batch_loss: 0.224276602268219\n",
      "training: 85 batch 462 batch_loss: 0.22797226905822754\n",
      "training: 85 batch 463 batch_loss: 0.21858474612236023\n",
      "training: 85 batch 464 batch_loss: 0.2247585952281952\n",
      "training: 85 batch 465 batch_loss: 0.2291145920753479\n",
      "training: 85 batch 466 batch_loss: 0.22430258989334106\n",
      "training: 85 batch 467 batch_loss: 0.22509163618087769\n",
      "training: 85 batch 468 batch_loss: 0.224087655544281\n",
      "training: 85 batch 469 batch_loss: 0.22450295090675354\n",
      "training: 85 batch 470 batch_loss: 0.228990375995636\n",
      "training: 85 batch 471 batch_loss: 0.22524648904800415\n",
      "training: 85 batch 472 batch_loss: 0.2231806516647339\n",
      "training: 85 batch 473 batch_loss: 0.2254604697227478\n",
      "training: 85 batch 474 batch_loss: 0.22670120000839233\n",
      "training: 85 batch 475 batch_loss: 0.22635143995285034\n",
      "training: 85 batch 476 batch_loss: 0.22462496161460876\n",
      "training: 85 batch 477 batch_loss: 0.22603535652160645\n",
      "training: 85 batch 478 batch_loss: 0.22255125641822815\n",
      "training: 85 batch 479 batch_loss: 0.22770264744758606\n",
      "training: 85 batch 480 batch_loss: 0.22614413499832153\n",
      "training: 85 batch 481 batch_loss: 0.22777345776557922\n",
      "training: 85 batch 482 batch_loss: 0.22387048602104187\n",
      "training: 85 batch 483 batch_loss: 0.2272413671016693\n",
      "training: 85 batch 484 batch_loss: 0.2232254445552826\n",
      "training: 85 batch 485 batch_loss: 0.22833514213562012\n",
      "training: 85 batch 486 batch_loss: 0.2259829044342041\n",
      "training: 85 batch 487 batch_loss: 0.22696805000305176\n",
      "training: 85 batch 488 batch_loss: 0.2184189260005951\n",
      "training: 85 batch 489 batch_loss: 0.22115948796272278\n",
      "training: 85 batch 490 batch_loss: 0.2261110544204712\n",
      "training: 85 batch 491 batch_loss: 0.22894486784934998\n",
      "training: 85 batch 492 batch_loss: 0.22117650508880615\n",
      "training: 85 batch 493 batch_loss: 0.2285010814666748\n",
      "training: 85 batch 494 batch_loss: 0.22296062111854553\n",
      "training: 85 batch 495 batch_loss: 0.2255801558494568\n",
      "training: 85 batch 496 batch_loss: 0.22405871748924255\n",
      "training: 85 batch 497 batch_loss: 0.2267950475215912\n",
      "training: 85 batch 498 batch_loss: 0.22227978706359863\n",
      "training: 85 batch 499 batch_loss: 0.22575539350509644\n",
      "training: 85 batch 500 batch_loss: 0.22433719038963318\n",
      "training: 85 batch 501 batch_loss: 0.22703677415847778\n",
      "training: 85 batch 502 batch_loss: 0.22262325882911682\n",
      "training: 85 batch 503 batch_loss: 0.22846943140029907\n",
      "training: 85 batch 504 batch_loss: 0.22317403554916382\n",
      "training: 85 batch 505 batch_loss: 0.22414356470108032\n",
      "training: 85 batch 506 batch_loss: 0.2229401171207428\n",
      "training: 85 batch 507 batch_loss: 0.22437706589698792\n",
      "training: 85 batch 508 batch_loss: 0.22628557682037354\n",
      "training: 85 batch 509 batch_loss: 0.22790607810020447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 85 batch 510 batch_loss: 0.22328513860702515\n",
      "training: 85 batch 511 batch_loss: 0.22340717911720276\n",
      "training: 85 batch 512 batch_loss: 0.2230338156223297\n",
      "training: 85 batch 513 batch_loss: 0.22665220499038696\n",
      "training: 85 batch 514 batch_loss: 0.23117655515670776\n",
      "training: 85 batch 515 batch_loss: 0.2210288941860199\n",
      "training: 85 batch 516 batch_loss: 0.225420743227005\n",
      "training: 85 batch 517 batch_loss: 0.2258480191230774\n",
      "training: 85 batch 518 batch_loss: 0.2268829345703125\n",
      "training: 85 batch 519 batch_loss: 0.22305512428283691\n",
      "training: 85 batch 520 batch_loss: 0.2225477397441864\n",
      "training: 85 batch 521 batch_loss: 0.22431576251983643\n",
      "training: 85 batch 522 batch_loss: 0.22639712691307068\n",
      "training: 85 batch 523 batch_loss: 0.22110438346862793\n",
      "training: 85 batch 524 batch_loss: 0.22609087824821472\n",
      "training: 85 batch 525 batch_loss: 0.22826188802719116\n",
      "training: 85 batch 526 batch_loss: 0.228726327419281\n",
      "training: 85 batch 527 batch_loss: 0.22701486945152283\n",
      "training: 85 batch 528 batch_loss: 0.2214823067188263\n",
      "training: 85 batch 529 batch_loss: 0.22983130812644958\n",
      "training: 85 batch 530 batch_loss: 0.22666102647781372\n",
      "training: 85 batch 531 batch_loss: 0.22318428754806519\n",
      "training: 85 batch 532 batch_loss: 0.22487449645996094\n",
      "training: 85 batch 533 batch_loss: 0.22366076707839966\n",
      "training: 85 batch 534 batch_loss: 0.22392916679382324\n",
      "training: 85 batch 535 batch_loss: 0.220316082239151\n",
      "training: 85 batch 536 batch_loss: 0.22809603810310364\n",
      "training: 85 batch 537 batch_loss: 0.22461628913879395\n",
      "training: 85 batch 538 batch_loss: 0.228296160697937\n",
      "training: 85 batch 539 batch_loss: 0.22730684280395508\n",
      "training: 85 batch 540 batch_loss: 0.22743433713912964\n",
      "training: 85 batch 541 batch_loss: 0.22614556550979614\n",
      "training: 85 batch 542 batch_loss: 0.2206379771232605\n",
      "training: 85 batch 543 batch_loss: 0.22629278898239136\n",
      "training: 85 batch 544 batch_loss: 0.22175315022468567\n",
      "training: 85 batch 545 batch_loss: 0.2229146957397461\n",
      "training: 85 batch 546 batch_loss: 0.2241230309009552\n",
      "training: 85 batch 547 batch_loss: 0.22611844539642334\n",
      "training: 85 batch 548 batch_loss: 0.21893727779388428\n",
      "training: 85 batch 549 batch_loss: 0.22416892647743225\n",
      "training: 85 batch 550 batch_loss: 0.2259339690208435\n",
      "training: 85 batch 551 batch_loss: 0.22561520338058472\n",
      "training: 85 batch 552 batch_loss: 0.22675460577011108\n",
      "training: 85 batch 553 batch_loss: 0.22657740116119385\n",
      "training: 85 batch 554 batch_loss: 0.22138217091560364\n",
      "training: 85 batch 555 batch_loss: 0.22438585758209229\n",
      "training: 85 batch 556 batch_loss: 0.22507953643798828\n",
      "training: 85 batch 557 batch_loss: 0.22448697686195374\n",
      "training: 85 batch 558 batch_loss: 0.22543713450431824\n",
      "training: 85 batch 559 batch_loss: 0.22757551074028015\n",
      "training: 85 batch 560 batch_loss: 0.22227391600608826\n",
      "training: 85 batch 561 batch_loss: 0.228380024433136\n",
      "training: 85 batch 562 batch_loss: 0.2257244884967804\n",
      "training: 85 batch 563 batch_loss: 0.22707831859588623\n",
      "training: 85 batch 564 batch_loss: 0.22233957052230835\n",
      "training: 85 batch 565 batch_loss: 0.22790566086769104\n",
      "training: 85 batch 566 batch_loss: 0.22595995664596558\n",
      "training: 85 batch 567 batch_loss: 0.22864219546318054\n",
      "training: 85 batch 568 batch_loss: 0.2264096438884735\n",
      "training: 85 batch 569 batch_loss: 0.22513404488563538\n",
      "training: 85 batch 570 batch_loss: 0.22615498304367065\n",
      "training: 85 batch 571 batch_loss: 0.22058194875717163\n",
      "training: 85 batch 572 batch_loss: 0.2217717170715332\n",
      "training: 85 batch 573 batch_loss: 0.22212114930152893\n",
      "training: 85 batch 574 batch_loss: 0.22658860683441162\n",
      "training: 85 batch 575 batch_loss: 0.2206612229347229\n",
      "training: 85 batch 576 batch_loss: 0.221085786819458\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 85, Hit Ratio:0.03398480310999923 | Precision:0.050142534159048464 | Recall:0.06611618644270788 | NDCG:0.06544339273465592\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 86 batch 0 batch_loss: 0.22788089513778687\n",
      "training: 86 batch 1 batch_loss: 0.21487551927566528\n",
      "training: 86 batch 2 batch_loss: 0.22146165370941162\n",
      "training: 86 batch 3 batch_loss: 0.22731277346611023\n",
      "training: 86 batch 4 batch_loss: 0.2258782982826233\n",
      "training: 86 batch 5 batch_loss: 0.2225688397884369\n",
      "training: 86 batch 6 batch_loss: 0.2266758680343628\n",
      "training: 86 batch 7 batch_loss: 0.22023829817771912\n",
      "training: 86 batch 8 batch_loss: 0.22477754950523376\n",
      "training: 86 batch 9 batch_loss: 0.2206336259841919\n",
      "training: 86 batch 10 batch_loss: 0.22672384977340698\n",
      "training: 86 batch 11 batch_loss: 0.22720348834991455\n",
      "training: 86 batch 12 batch_loss: 0.2202850580215454\n",
      "training: 86 batch 13 batch_loss: 0.218575119972229\n",
      "training: 86 batch 14 batch_loss: 0.21895110607147217\n",
      "training: 86 batch 15 batch_loss: 0.2234463095664978\n",
      "training: 86 batch 16 batch_loss: 0.22176963090896606\n",
      "training: 86 batch 17 batch_loss: 0.22429567575454712\n",
      "training: 86 batch 18 batch_loss: 0.22408002614974976\n",
      "training: 86 batch 19 batch_loss: 0.22011184692382812\n",
      "training: 86 batch 20 batch_loss: 0.22313982248306274\n",
      "training: 86 batch 21 batch_loss: 0.2278628945350647\n",
      "training: 86 batch 22 batch_loss: 0.22392362356185913\n",
      "training: 86 batch 23 batch_loss: 0.22211480140686035\n",
      "training: 86 batch 24 batch_loss: 0.22307923436164856\n",
      "training: 86 batch 25 batch_loss: 0.22290581464767456\n",
      "training: 86 batch 26 batch_loss: 0.2238166630268097\n",
      "training: 86 batch 27 batch_loss: 0.22086051106452942\n",
      "training: 86 batch 28 batch_loss: 0.22454383969306946\n",
      "training: 86 batch 29 batch_loss: 0.22431910037994385\n",
      "training: 86 batch 30 batch_loss: 0.22647401690483093\n",
      "training: 86 batch 31 batch_loss: 0.220550537109375\n",
      "training: 86 batch 32 batch_loss: 0.22588196396827698\n",
      "training: 86 batch 33 batch_loss: 0.22227314114570618\n",
      "training: 86 batch 34 batch_loss: 0.22822263836860657\n",
      "training: 86 batch 35 batch_loss: 0.22683769464492798\n",
      "training: 86 batch 36 batch_loss: 0.22178909182548523\n",
      "training: 86 batch 37 batch_loss: 0.22211426496505737\n",
      "training: 86 batch 38 batch_loss: 0.22551029920578003\n",
      "training: 86 batch 39 batch_loss: 0.22167357802391052\n",
      "training: 86 batch 40 batch_loss: 0.22445589303970337\n",
      "training: 86 batch 41 batch_loss: 0.2200186848640442\n",
      "training: 86 batch 42 batch_loss: 0.2200685739517212\n",
      "training: 86 batch 43 batch_loss: 0.2248106598854065\n",
      "training: 86 batch 44 batch_loss: 0.22103357315063477\n",
      "training: 86 batch 45 batch_loss: 0.2241378128528595\n",
      "training: 86 batch 46 batch_loss: 0.22132691740989685\n",
      "training: 86 batch 47 batch_loss: 0.2188485562801361\n",
      "training: 86 batch 48 batch_loss: 0.2256445586681366\n",
      "training: 86 batch 49 batch_loss: 0.2270028293132782\n",
      "training: 86 batch 50 batch_loss: 0.22178319096565247\n",
      "training: 86 batch 51 batch_loss: 0.224137544631958\n",
      "training: 86 batch 52 batch_loss: 0.22463497519493103\n",
      "training: 86 batch 53 batch_loss: 0.22222307324409485\n",
      "training: 86 batch 54 batch_loss: 0.22244331240653992\n",
      "training: 86 batch 55 batch_loss: 0.22205400466918945\n",
      "training: 86 batch 56 batch_loss: 0.22618705034255981\n",
      "training: 86 batch 57 batch_loss: 0.22304457426071167\n",
      "training: 86 batch 58 batch_loss: 0.2225211262702942\n",
      "training: 86 batch 59 batch_loss: 0.22711586952209473\n",
      "training: 86 batch 60 batch_loss: 0.2249024510383606\n",
      "training: 86 batch 61 batch_loss: 0.22628703713417053\n",
      "training: 86 batch 62 batch_loss: 0.22929197549819946\n",
      "training: 86 batch 63 batch_loss: 0.22168192267417908\n",
      "training: 86 batch 64 batch_loss: 0.22675323486328125\n",
      "training: 86 batch 65 batch_loss: 0.22272279858589172\n",
      "training: 86 batch 66 batch_loss: 0.22490078210830688\n",
      "training: 86 batch 67 batch_loss: 0.22318771481513977\n",
      "training: 86 batch 68 batch_loss: 0.23035651445388794\n",
      "training: 86 batch 69 batch_loss: 0.22338879108428955\n",
      "training: 86 batch 70 batch_loss: 0.22256439924240112\n",
      "training: 86 batch 71 batch_loss: 0.22374284267425537\n",
      "training: 86 batch 72 batch_loss: 0.22378581762313843\n",
      "training: 86 batch 73 batch_loss: 0.22136518359184265\n",
      "training: 86 batch 74 batch_loss: 0.22392606735229492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 86 batch 75 batch_loss: 0.22738704085350037\n",
      "training: 86 batch 76 batch_loss: 0.22253242135047913\n",
      "training: 86 batch 77 batch_loss: 0.22169488668441772\n",
      "training: 86 batch 78 batch_loss: 0.22037148475646973\n",
      "training: 86 batch 79 batch_loss: 0.22487100958824158\n",
      "training: 86 batch 80 batch_loss: 0.22160962224006653\n",
      "training: 86 batch 81 batch_loss: 0.22165805101394653\n",
      "training: 86 batch 82 batch_loss: 0.22624027729034424\n",
      "training: 86 batch 83 batch_loss: 0.22281506657600403\n",
      "training: 86 batch 84 batch_loss: 0.22371631860733032\n",
      "training: 86 batch 85 batch_loss: 0.22118937969207764\n",
      "training: 86 batch 86 batch_loss: 0.22527694702148438\n",
      "training: 86 batch 87 batch_loss: 0.22157073020935059\n",
      "training: 86 batch 88 batch_loss: 0.2272731363773346\n",
      "training: 86 batch 89 batch_loss: 0.2226492464542389\n",
      "training: 86 batch 90 batch_loss: 0.22536087036132812\n",
      "training: 86 batch 91 batch_loss: 0.22745293378829956\n",
      "training: 86 batch 92 batch_loss: 0.22177088260650635\n",
      "training: 86 batch 93 batch_loss: 0.22630327939987183\n",
      "training: 86 batch 94 batch_loss: 0.21970880031585693\n",
      "training: 86 batch 95 batch_loss: 0.2259691059589386\n",
      "training: 86 batch 96 batch_loss: 0.2251492738723755\n",
      "training: 86 batch 97 batch_loss: 0.2245490849018097\n",
      "training: 86 batch 98 batch_loss: 0.22399726510047913\n",
      "training: 86 batch 99 batch_loss: 0.2234182357788086\n",
      "training: 86 batch 100 batch_loss: 0.22508293390274048\n",
      "training: 86 batch 101 batch_loss: 0.2242017388343811\n",
      "training: 86 batch 102 batch_loss: 0.2212037742137909\n",
      "training: 86 batch 103 batch_loss: 0.22529321908950806\n",
      "training: 86 batch 104 batch_loss: 0.2217954397201538\n",
      "training: 86 batch 105 batch_loss: 0.2259410321712494\n",
      "training: 86 batch 106 batch_loss: 0.225472092628479\n",
      "training: 86 batch 107 batch_loss: 0.22234377264976501\n",
      "training: 86 batch 108 batch_loss: 0.2210557758808136\n",
      "training: 86 batch 109 batch_loss: 0.22550669312477112\n",
      "training: 86 batch 110 batch_loss: 0.22610712051391602\n",
      "training: 86 batch 111 batch_loss: 0.22360345721244812\n",
      "training: 86 batch 112 batch_loss: 0.22412165999412537\n",
      "training: 86 batch 113 batch_loss: 0.224706768989563\n",
      "training: 86 batch 114 batch_loss: 0.22778701782226562\n",
      "training: 86 batch 115 batch_loss: 0.22473958134651184\n",
      "training: 86 batch 116 batch_loss: 0.22916680574417114\n",
      "training: 86 batch 117 batch_loss: 0.22173550724983215\n",
      "training: 86 batch 118 batch_loss: 0.231048583984375\n",
      "training: 86 batch 119 batch_loss: 0.22379934787750244\n",
      "training: 86 batch 120 batch_loss: 0.221979558467865\n",
      "training: 86 batch 121 batch_loss: 0.21819347143173218\n",
      "training: 86 batch 122 batch_loss: 0.22790801525115967\n",
      "training: 86 batch 123 batch_loss: 0.22675427794456482\n",
      "training: 86 batch 124 batch_loss: 0.22190356254577637\n",
      "training: 86 batch 125 batch_loss: 0.22500908374786377\n",
      "training: 86 batch 126 batch_loss: 0.2213135063648224\n",
      "training: 86 batch 127 batch_loss: 0.22216346859931946\n",
      "training: 86 batch 128 batch_loss: 0.22246715426445007\n",
      "training: 86 batch 129 batch_loss: 0.22803175449371338\n",
      "training: 86 batch 130 batch_loss: 0.22203156352043152\n",
      "training: 86 batch 131 batch_loss: 0.22338560223579407\n",
      "training: 86 batch 132 batch_loss: 0.22273114323616028\n",
      "training: 86 batch 133 batch_loss: 0.22079581022262573\n",
      "training: 86 batch 134 batch_loss: 0.22541174292564392\n",
      "training: 86 batch 135 batch_loss: 0.22812390327453613\n",
      "training: 86 batch 136 batch_loss: 0.22474247217178345\n",
      "training: 86 batch 137 batch_loss: 0.2236354649066925\n",
      "training: 86 batch 138 batch_loss: 0.2240084707736969\n",
      "training: 86 batch 139 batch_loss: 0.22602766752243042\n",
      "training: 86 batch 140 batch_loss: 0.2208288311958313\n",
      "training: 86 batch 141 batch_loss: 0.22273877263069153\n",
      "training: 86 batch 142 batch_loss: 0.22271576523780823\n",
      "training: 86 batch 143 batch_loss: 0.2267652451992035\n",
      "training: 86 batch 144 batch_loss: 0.22339686751365662\n",
      "training: 86 batch 145 batch_loss: 0.22260895371437073\n",
      "training: 86 batch 146 batch_loss: 0.21917769312858582\n",
      "training: 86 batch 147 batch_loss: 0.2264513075351715\n",
      "training: 86 batch 148 batch_loss: 0.222883939743042\n",
      "training: 86 batch 149 batch_loss: 0.22490665316581726\n",
      "training: 86 batch 150 batch_loss: 0.22344806790351868\n",
      "training: 86 batch 151 batch_loss: 0.22543829679489136\n",
      "training: 86 batch 152 batch_loss: 0.22697022557258606\n",
      "training: 86 batch 153 batch_loss: 0.22091132402420044\n",
      "training: 86 batch 154 batch_loss: 0.22615686058998108\n",
      "training: 86 batch 155 batch_loss: 0.22692444920539856\n",
      "training: 86 batch 156 batch_loss: 0.22513043880462646\n",
      "training: 86 batch 157 batch_loss: 0.22619789838790894\n",
      "training: 86 batch 158 batch_loss: 0.22386765480041504\n",
      "training: 86 batch 159 batch_loss: 0.2242366373538971\n",
      "training: 86 batch 160 batch_loss: 0.2233428657054901\n",
      "training: 86 batch 161 batch_loss: 0.2236345410346985\n",
      "training: 86 batch 162 batch_loss: 0.22905102372169495\n",
      "training: 86 batch 163 batch_loss: 0.22689953446388245\n",
      "training: 86 batch 164 batch_loss: 0.22642552852630615\n",
      "training: 86 batch 165 batch_loss: 0.2213141918182373\n",
      "training: 86 batch 166 batch_loss: 0.22055840492248535\n",
      "training: 86 batch 167 batch_loss: 0.22702255845069885\n",
      "training: 86 batch 168 batch_loss: 0.22259488701820374\n",
      "training: 86 batch 169 batch_loss: 0.2237299680709839\n",
      "training: 86 batch 170 batch_loss: 0.2263754904270172\n",
      "training: 86 batch 171 batch_loss: 0.2216128706932068\n",
      "training: 86 batch 172 batch_loss: 0.2157231867313385\n",
      "training: 86 batch 173 batch_loss: 0.22689926624298096\n",
      "training: 86 batch 174 batch_loss: 0.2265760600566864\n",
      "training: 86 batch 175 batch_loss: 0.22005808353424072\n",
      "training: 86 batch 176 batch_loss: 0.22013628482818604\n",
      "training: 86 batch 177 batch_loss: 0.2237301468849182\n",
      "training: 86 batch 178 batch_loss: 0.22704258561134338\n",
      "training: 86 batch 179 batch_loss: 0.2252066731452942\n",
      "training: 86 batch 180 batch_loss: 0.22322195768356323\n",
      "training: 86 batch 181 batch_loss: 0.22393906116485596\n",
      "training: 86 batch 182 batch_loss: 0.2235751450061798\n",
      "training: 86 batch 183 batch_loss: 0.21911007165908813\n",
      "training: 86 batch 184 batch_loss: 0.2252587080001831\n",
      "training: 86 batch 185 batch_loss: 0.22335824370384216\n",
      "training: 86 batch 186 batch_loss: 0.22516053915023804\n",
      "training: 86 batch 187 batch_loss: 0.2214198112487793\n",
      "training: 86 batch 188 batch_loss: 0.2259892225265503\n",
      "training: 86 batch 189 batch_loss: 0.22268113493919373\n",
      "training: 86 batch 190 batch_loss: 0.2248033583164215\n",
      "training: 86 batch 191 batch_loss: 0.2295311987400055\n",
      "training: 86 batch 192 batch_loss: 0.22253134846687317\n",
      "training: 86 batch 193 batch_loss: 0.22790595889091492\n",
      "training: 86 batch 194 batch_loss: 0.2246764898300171\n",
      "training: 86 batch 195 batch_loss: 0.22395777702331543\n",
      "training: 86 batch 196 batch_loss: 0.22798824310302734\n",
      "training: 86 batch 197 batch_loss: 0.22562503814697266\n",
      "training: 86 batch 198 batch_loss: 0.2263522446155548\n",
      "training: 86 batch 199 batch_loss: 0.22369489073753357\n",
      "training: 86 batch 200 batch_loss: 0.2220916450023651\n",
      "training: 86 batch 201 batch_loss: 0.2250739336013794\n",
      "training: 86 batch 202 batch_loss: 0.22706088423728943\n",
      "training: 86 batch 203 batch_loss: 0.22095489501953125\n",
      "training: 86 batch 204 batch_loss: 0.22299349308013916\n",
      "training: 86 batch 205 batch_loss: 0.22776660323143005\n",
      "training: 86 batch 206 batch_loss: 0.22633492946624756\n",
      "training: 86 batch 207 batch_loss: 0.22648295760154724\n",
      "training: 86 batch 208 batch_loss: 0.2248726487159729\n",
      "training: 86 batch 209 batch_loss: 0.22864878177642822\n",
      "training: 86 batch 210 batch_loss: 0.22725293040275574\n",
      "training: 86 batch 211 batch_loss: 0.2226065993309021\n",
      "training: 86 batch 212 batch_loss: 0.2221149504184723\n",
      "training: 86 batch 213 batch_loss: 0.22357523441314697\n",
      "training: 86 batch 214 batch_loss: 0.2215285301208496\n",
      "training: 86 batch 215 batch_loss: 0.2276264727115631\n",
      "training: 86 batch 216 batch_loss: 0.22886574268341064\n",
      "training: 86 batch 217 batch_loss: 0.2258588969707489\n",
      "training: 86 batch 218 batch_loss: 0.22590768337249756\n",
      "training: 86 batch 219 batch_loss: 0.22861698269844055\n",
      "training: 86 batch 220 batch_loss: 0.22542157769203186\n",
      "training: 86 batch 221 batch_loss: 0.22582703828811646\n",
      "training: 86 batch 222 batch_loss: 0.2209843099117279\n",
      "training: 86 batch 223 batch_loss: 0.23065859079360962\n",
      "training: 86 batch 224 batch_loss: 0.22820216417312622\n",
      "training: 86 batch 225 batch_loss: 0.2267056703567505\n",
      "training: 86 batch 226 batch_loss: 0.23106873035430908\n",
      "training: 86 batch 227 batch_loss: 0.221074640750885\n",
      "training: 86 batch 228 batch_loss: 0.22670498490333557\n",
      "training: 86 batch 229 batch_loss: 0.22680321335792542\n",
      "training: 86 batch 230 batch_loss: 0.22583773732185364\n",
      "training: 86 batch 231 batch_loss: 0.22218981385231018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 86 batch 232 batch_loss: 0.2275412678718567\n",
      "training: 86 batch 233 batch_loss: 0.2252410650253296\n",
      "training: 86 batch 234 batch_loss: 0.22257483005523682\n",
      "training: 86 batch 235 batch_loss: 0.22052866220474243\n",
      "training: 86 batch 236 batch_loss: 0.22157716751098633\n",
      "training: 86 batch 237 batch_loss: 0.22814378142356873\n",
      "training: 86 batch 238 batch_loss: 0.226673424243927\n",
      "training: 86 batch 239 batch_loss: 0.22299906611442566\n",
      "training: 86 batch 240 batch_loss: 0.2311364710330963\n",
      "training: 86 batch 241 batch_loss: 0.22915372252464294\n",
      "training: 86 batch 242 batch_loss: 0.22473087906837463\n",
      "training: 86 batch 243 batch_loss: 0.22417235374450684\n",
      "training: 86 batch 244 batch_loss: 0.2270694375038147\n",
      "training: 86 batch 245 batch_loss: 0.22885540127754211\n",
      "training: 86 batch 246 batch_loss: 0.22883519530296326\n",
      "training: 86 batch 247 batch_loss: 0.22952914237976074\n",
      "training: 86 batch 248 batch_loss: 0.2241572141647339\n",
      "training: 86 batch 249 batch_loss: 0.22509515285491943\n",
      "training: 86 batch 250 batch_loss: 0.2240636646747589\n",
      "training: 86 batch 251 batch_loss: 0.22143888473510742\n",
      "training: 86 batch 252 batch_loss: 0.22405394911766052\n",
      "training: 86 batch 253 batch_loss: 0.2259300947189331\n",
      "training: 86 batch 254 batch_loss: 0.2262853980064392\n",
      "training: 86 batch 255 batch_loss: 0.2233932912349701\n",
      "training: 86 batch 256 batch_loss: 0.22603949904441833\n",
      "training: 86 batch 257 batch_loss: 0.22385361790657043\n",
      "training: 86 batch 258 batch_loss: 0.22996604442596436\n",
      "training: 86 batch 259 batch_loss: 0.22071650624275208\n",
      "training: 86 batch 260 batch_loss: 0.22560998797416687\n",
      "training: 86 batch 261 batch_loss: 0.22362977266311646\n",
      "training: 86 batch 262 batch_loss: 0.22315475344657898\n",
      "training: 86 batch 263 batch_loss: 0.22252118587493896\n",
      "training: 86 batch 264 batch_loss: 0.21901103854179382\n",
      "training: 86 batch 265 batch_loss: 0.22187387943267822\n",
      "training: 86 batch 266 batch_loss: 0.2261524498462677\n",
      "training: 86 batch 267 batch_loss: 0.22921136021614075\n",
      "training: 86 batch 268 batch_loss: 0.2246524691581726\n",
      "training: 86 batch 269 batch_loss: 0.2252730131149292\n",
      "training: 86 batch 270 batch_loss: 0.2278858721256256\n",
      "training: 86 batch 271 batch_loss: 0.22625204920768738\n",
      "training: 86 batch 272 batch_loss: 0.22877910733222961\n",
      "training: 86 batch 273 batch_loss: 0.2245238721370697\n",
      "training: 86 batch 274 batch_loss: 0.22679713368415833\n",
      "training: 86 batch 275 batch_loss: 0.22439411282539368\n",
      "training: 86 batch 276 batch_loss: 0.2223144769668579\n",
      "training: 86 batch 277 batch_loss: 0.22807395458221436\n",
      "training: 86 batch 278 batch_loss: 0.22444486618041992\n",
      "training: 86 batch 279 batch_loss: 0.22924813628196716\n",
      "training: 86 batch 280 batch_loss: 0.22785326838493347\n",
      "training: 86 batch 281 batch_loss: 0.22557425498962402\n",
      "training: 86 batch 282 batch_loss: 0.23089927434921265\n",
      "training: 86 batch 283 batch_loss: 0.2244722545146942\n",
      "training: 86 batch 284 batch_loss: 0.22135084867477417\n",
      "training: 86 batch 285 batch_loss: 0.22989702224731445\n",
      "training: 86 batch 286 batch_loss: 0.22638705372810364\n",
      "training: 86 batch 287 batch_loss: 0.2249770164489746\n",
      "training: 86 batch 288 batch_loss: 0.22177723050117493\n",
      "training: 86 batch 289 batch_loss: 0.2247639298439026\n",
      "training: 86 batch 290 batch_loss: 0.2286570966243744\n",
      "training: 86 batch 291 batch_loss: 0.2269122302532196\n",
      "training: 86 batch 292 batch_loss: 0.2302931249141693\n",
      "training: 86 batch 293 batch_loss: 0.2273326814174652\n",
      "training: 86 batch 294 batch_loss: 0.2247827649116516\n",
      "training: 86 batch 295 batch_loss: 0.22183582186698914\n",
      "training: 86 batch 296 batch_loss: 0.22028690576553345\n",
      "training: 86 batch 297 batch_loss: 0.223039448261261\n",
      "training: 86 batch 298 batch_loss: 0.22298887372016907\n",
      "training: 86 batch 299 batch_loss: 0.22305580973625183\n",
      "training: 86 batch 300 batch_loss: 0.221920907497406\n",
      "training: 86 batch 301 batch_loss: 0.22658878564834595\n",
      "training: 86 batch 302 batch_loss: 0.2243306040763855\n",
      "training: 86 batch 303 batch_loss: 0.22676503658294678\n",
      "training: 86 batch 304 batch_loss: 0.22230762243270874\n",
      "training: 86 batch 305 batch_loss: 0.22239527106285095\n",
      "training: 86 batch 306 batch_loss: 0.22301027178764343\n",
      "training: 86 batch 307 batch_loss: 0.22634410858154297\n",
      "training: 86 batch 308 batch_loss: 0.22014015913009644\n",
      "training: 86 batch 309 batch_loss: 0.22078955173492432\n",
      "training: 86 batch 310 batch_loss: 0.22536471486091614\n",
      "training: 86 batch 311 batch_loss: 0.22486215829849243\n",
      "training: 86 batch 312 batch_loss: 0.2264930009841919\n",
      "training: 86 batch 313 batch_loss: 0.22355300188064575\n",
      "training: 86 batch 314 batch_loss: 0.22279736399650574\n",
      "training: 86 batch 315 batch_loss: 0.2232917845249176\n",
      "training: 86 batch 316 batch_loss: 0.22889119386672974\n",
      "training: 86 batch 317 batch_loss: 0.22430947422981262\n",
      "training: 86 batch 318 batch_loss: 0.224916011095047\n",
      "training: 86 batch 319 batch_loss: 0.22630992531776428\n",
      "training: 86 batch 320 batch_loss: 0.22974252700805664\n",
      "training: 86 batch 321 batch_loss: 0.22417467832565308\n",
      "training: 86 batch 322 batch_loss: 0.2272227704524994\n",
      "training: 86 batch 323 batch_loss: 0.22751018404960632\n",
      "training: 86 batch 324 batch_loss: 0.2272389829158783\n",
      "training: 86 batch 325 batch_loss: 0.22386261820793152\n",
      "training: 86 batch 326 batch_loss: 0.22397980093955994\n",
      "training: 86 batch 327 batch_loss: 0.22628381848335266\n",
      "training: 86 batch 328 batch_loss: 0.22846278548240662\n",
      "training: 86 batch 329 batch_loss: 0.2238534688949585\n",
      "training: 86 batch 330 batch_loss: 0.2223300337791443\n",
      "training: 86 batch 331 batch_loss: 0.22336214780807495\n",
      "training: 86 batch 332 batch_loss: 0.22362884879112244\n",
      "training: 86 batch 333 batch_loss: 0.2257426679134369\n",
      "training: 86 batch 334 batch_loss: 0.22534340620040894\n",
      "training: 86 batch 335 batch_loss: 0.23251637816429138\n",
      "training: 86 batch 336 batch_loss: 0.22576430439949036\n",
      "training: 86 batch 337 batch_loss: 0.22406351566314697\n",
      "training: 86 batch 338 batch_loss: 0.22297686338424683\n",
      "training: 86 batch 339 batch_loss: 0.2237820029258728\n",
      "training: 86 batch 340 batch_loss: 0.22487813234329224\n",
      "training: 86 batch 341 batch_loss: 0.2267419993877411\n",
      "training: 86 batch 342 batch_loss: 0.22827529907226562\n",
      "training: 86 batch 343 batch_loss: 0.22460013628005981\n",
      "training: 86 batch 344 batch_loss: 0.2307720184326172\n",
      "training: 86 batch 345 batch_loss: 0.22294676303863525\n",
      "training: 86 batch 346 batch_loss: 0.22503253817558289\n",
      "training: 86 batch 347 batch_loss: 0.22389304637908936\n",
      "training: 86 batch 348 batch_loss: 0.2224147617816925\n",
      "training: 86 batch 349 batch_loss: 0.22703737020492554\n",
      "training: 86 batch 350 batch_loss: 0.22562900185585022\n",
      "training: 86 batch 351 batch_loss: 0.22601473331451416\n",
      "training: 86 batch 352 batch_loss: 0.22959190607070923\n",
      "training: 86 batch 353 batch_loss: 0.22938966751098633\n",
      "training: 86 batch 354 batch_loss: 0.22291606664657593\n",
      "training: 86 batch 355 batch_loss: 0.22594627737998962\n",
      "training: 86 batch 356 batch_loss: 0.2270020842552185\n",
      "training: 86 batch 357 batch_loss: 0.22486484050750732\n",
      "training: 86 batch 358 batch_loss: 0.22485774755477905\n",
      "training: 86 batch 359 batch_loss: 0.2275933027267456\n",
      "training: 86 batch 360 batch_loss: 0.2265905737876892\n",
      "training: 86 batch 361 batch_loss: 0.2270066738128662\n",
      "training: 86 batch 362 batch_loss: 0.2242528200149536\n",
      "training: 86 batch 363 batch_loss: 0.2259383201599121\n",
      "training: 86 batch 364 batch_loss: 0.22747933864593506\n",
      "training: 86 batch 365 batch_loss: 0.22571849822998047\n",
      "training: 86 batch 366 batch_loss: 0.2275778353214264\n",
      "training: 86 batch 367 batch_loss: 0.2242548167705536\n",
      "training: 86 batch 368 batch_loss: 0.22352904081344604\n",
      "training: 86 batch 369 batch_loss: 0.22814825177192688\n",
      "training: 86 batch 370 batch_loss: 0.22324493527412415\n",
      "training: 86 batch 371 batch_loss: 0.22516247630119324\n",
      "training: 86 batch 372 batch_loss: 0.22669756412506104\n",
      "training: 86 batch 373 batch_loss: 0.2238304615020752\n",
      "training: 86 batch 374 batch_loss: 0.22283774614334106\n",
      "training: 86 batch 375 batch_loss: 0.2232290506362915\n",
      "training: 86 batch 376 batch_loss: 0.228695809841156\n",
      "training: 86 batch 377 batch_loss: 0.2263946831226349\n",
      "training: 86 batch 378 batch_loss: 0.22118481993675232\n",
      "training: 86 batch 379 batch_loss: 0.22437721490859985\n",
      "training: 86 batch 380 batch_loss: 0.22605103254318237\n",
      "training: 86 batch 381 batch_loss: 0.22489464282989502\n",
      "training: 86 batch 382 batch_loss: 0.22769752144813538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 86 batch 383 batch_loss: 0.22565877437591553\n",
      "training: 86 batch 384 batch_loss: 0.22691011428833008\n",
      "training: 86 batch 385 batch_loss: 0.22558432817459106\n",
      "training: 86 batch 386 batch_loss: 0.22658461332321167\n",
      "training: 86 batch 387 batch_loss: 0.223791241645813\n",
      "training: 86 batch 388 batch_loss: 0.2297959327697754\n",
      "training: 86 batch 389 batch_loss: 0.2258239984512329\n",
      "training: 86 batch 390 batch_loss: 0.2286284863948822\n",
      "training: 86 batch 391 batch_loss: 0.2220304310321808\n",
      "training: 86 batch 392 batch_loss: 0.22011521458625793\n",
      "training: 86 batch 393 batch_loss: 0.22636359930038452\n",
      "training: 86 batch 394 batch_loss: 0.22670212388038635\n",
      "training: 86 batch 395 batch_loss: 0.22598880529403687\n",
      "training: 86 batch 396 batch_loss: 0.22874706983566284\n",
      "training: 86 batch 397 batch_loss: 0.22973352670669556\n",
      "training: 86 batch 398 batch_loss: 0.22297203540802002\n",
      "training: 86 batch 399 batch_loss: 0.22439244389533997\n",
      "training: 86 batch 400 batch_loss: 0.22587373852729797\n",
      "training: 86 batch 401 batch_loss: 0.22331339120864868\n",
      "training: 86 batch 402 batch_loss: 0.2209913730621338\n",
      "training: 86 batch 403 batch_loss: 0.22842103242874146\n",
      "training: 86 batch 404 batch_loss: 0.22647905349731445\n",
      "training: 86 batch 405 batch_loss: 0.22618958353996277\n",
      "training: 86 batch 406 batch_loss: 0.22098633646965027\n",
      "training: 86 batch 407 batch_loss: 0.22597739100456238\n",
      "training: 86 batch 408 batch_loss: 0.22467881441116333\n",
      "training: 86 batch 409 batch_loss: 0.2250150740146637\n",
      "training: 86 batch 410 batch_loss: 0.22304394841194153\n",
      "training: 86 batch 411 batch_loss: 0.2248247265815735\n",
      "training: 86 batch 412 batch_loss: 0.23056617379188538\n",
      "training: 86 batch 413 batch_loss: 0.22506463527679443\n",
      "training: 86 batch 414 batch_loss: 0.2242809534072876\n",
      "training: 86 batch 415 batch_loss: 0.22809168696403503\n",
      "training: 86 batch 416 batch_loss: 0.22299739718437195\n",
      "training: 86 batch 417 batch_loss: 0.22657006978988647\n",
      "training: 86 batch 418 batch_loss: 0.2233806848526001\n",
      "training: 86 batch 419 batch_loss: 0.22511425614356995\n",
      "training: 86 batch 420 batch_loss: 0.22237694263458252\n",
      "training: 86 batch 421 batch_loss: 0.223226398229599\n",
      "training: 86 batch 422 batch_loss: 0.22802501916885376\n",
      "training: 86 batch 423 batch_loss: 0.2259247899055481\n",
      "training: 86 batch 424 batch_loss: 0.228099524974823\n",
      "training: 86 batch 425 batch_loss: 0.22311371564865112\n",
      "training: 86 batch 426 batch_loss: 0.2256864607334137\n",
      "training: 86 batch 427 batch_loss: 0.2283068299293518\n",
      "training: 86 batch 428 batch_loss: 0.22637364268302917\n",
      "training: 86 batch 429 batch_loss: 0.2223086953163147\n",
      "training: 86 batch 430 batch_loss: 0.22505944967269897\n",
      "training: 86 batch 431 batch_loss: 0.2243976593017578\n",
      "training: 86 batch 432 batch_loss: 0.22498711943626404\n",
      "training: 86 batch 433 batch_loss: 0.22581139206886292\n",
      "training: 86 batch 434 batch_loss: 0.2249181568622589\n",
      "training: 86 batch 435 batch_loss: 0.224037766456604\n",
      "training: 86 batch 436 batch_loss: 0.22180646657943726\n",
      "training: 86 batch 437 batch_loss: 0.22442302107810974\n",
      "training: 86 batch 438 batch_loss: 0.22291111946105957\n",
      "training: 86 batch 439 batch_loss: 0.22255492210388184\n",
      "training: 86 batch 440 batch_loss: 0.224480539560318\n",
      "training: 86 batch 441 batch_loss: 0.2309546172618866\n",
      "training: 86 batch 442 batch_loss: 0.2298603653907776\n",
      "training: 86 batch 443 batch_loss: 0.22466647624969482\n",
      "training: 86 batch 444 batch_loss: 0.2244962453842163\n",
      "training: 86 batch 445 batch_loss: 0.22469931840896606\n",
      "training: 86 batch 446 batch_loss: 0.22654682397842407\n",
      "training: 86 batch 447 batch_loss: 0.22496014833450317\n",
      "training: 86 batch 448 batch_loss: 0.22651222348213196\n",
      "training: 86 batch 449 batch_loss: 0.22404271364212036\n",
      "training: 86 batch 450 batch_loss: 0.22319555282592773\n",
      "training: 86 batch 451 batch_loss: 0.22739502787590027\n",
      "training: 86 batch 452 batch_loss: 0.22655948996543884\n",
      "training: 86 batch 453 batch_loss: 0.22496208548545837\n",
      "training: 86 batch 454 batch_loss: 0.22109586000442505\n",
      "training: 86 batch 455 batch_loss: 0.22225511074066162\n",
      "training: 86 batch 456 batch_loss: 0.22777041792869568\n",
      "training: 86 batch 457 batch_loss: 0.22516736388206482\n",
      "training: 86 batch 458 batch_loss: 0.2280493676662445\n",
      "training: 86 batch 459 batch_loss: 0.22259357571601868\n",
      "training: 86 batch 460 batch_loss: 0.2260470688343048\n",
      "training: 86 batch 461 batch_loss: 0.22379648685455322\n",
      "training: 86 batch 462 batch_loss: 0.22243955731391907\n",
      "training: 86 batch 463 batch_loss: 0.22573351860046387\n",
      "training: 86 batch 464 batch_loss: 0.228105366230011\n",
      "training: 86 batch 465 batch_loss: 0.22540372610092163\n",
      "training: 86 batch 466 batch_loss: 0.22713446617126465\n",
      "training: 86 batch 467 batch_loss: 0.22665587067604065\n",
      "training: 86 batch 468 batch_loss: 0.22528070211410522\n",
      "training: 86 batch 469 batch_loss: 0.2230094075202942\n",
      "training: 86 batch 470 batch_loss: 0.22427308559417725\n",
      "training: 86 batch 471 batch_loss: 0.2280803918838501\n",
      "training: 86 batch 472 batch_loss: 0.22823655605316162\n",
      "training: 86 batch 473 batch_loss: 0.22387051582336426\n",
      "training: 86 batch 474 batch_loss: 0.22884437441825867\n",
      "training: 86 batch 475 batch_loss: 0.22636470198631287\n",
      "training: 86 batch 476 batch_loss: 0.2232041358947754\n",
      "training: 86 batch 477 batch_loss: 0.22312331199645996\n",
      "training: 86 batch 478 batch_loss: 0.2245340347290039\n",
      "training: 86 batch 479 batch_loss: 0.22843009233474731\n",
      "training: 86 batch 480 batch_loss: 0.22725144028663635\n",
      "training: 86 batch 481 batch_loss: 0.2264949083328247\n",
      "training: 86 batch 482 batch_loss: 0.2241046130657196\n",
      "training: 86 batch 483 batch_loss: 0.223669171333313\n",
      "training: 86 batch 484 batch_loss: 0.21954283118247986\n",
      "training: 86 batch 485 batch_loss: 0.22881442308425903\n",
      "training: 86 batch 486 batch_loss: 0.22390031814575195\n",
      "training: 86 batch 487 batch_loss: 0.2234179973602295\n",
      "training: 86 batch 488 batch_loss: 0.22892630100250244\n",
      "training: 86 batch 489 batch_loss: 0.22628521919250488\n",
      "training: 86 batch 490 batch_loss: 0.22762736678123474\n",
      "training: 86 batch 491 batch_loss: 0.22786813974380493\n",
      "training: 86 batch 492 batch_loss: 0.22860127687454224\n",
      "training: 86 batch 493 batch_loss: 0.22785761952400208\n",
      "training: 86 batch 494 batch_loss: 0.2292870581150055\n",
      "training: 86 batch 495 batch_loss: 0.2215481996536255\n",
      "training: 86 batch 496 batch_loss: 0.23047173023223877\n",
      "training: 86 batch 497 batch_loss: 0.22799402475357056\n",
      "training: 86 batch 498 batch_loss: 0.2275896966457367\n",
      "training: 86 batch 499 batch_loss: 0.22665303945541382\n",
      "training: 86 batch 500 batch_loss: 0.2271326780319214\n",
      "training: 86 batch 501 batch_loss: 0.22626209259033203\n",
      "training: 86 batch 502 batch_loss: 0.22750085592269897\n",
      "training: 86 batch 503 batch_loss: 0.2259790003299713\n",
      "training: 86 batch 504 batch_loss: 0.23057842254638672\n",
      "training: 86 batch 505 batch_loss: 0.22269690036773682\n",
      "training: 86 batch 506 batch_loss: 0.22983339428901672\n",
      "training: 86 batch 507 batch_loss: 0.22794005274772644\n",
      "training: 86 batch 508 batch_loss: 0.22388336062431335\n",
      "training: 86 batch 509 batch_loss: 0.2240261733531952\n",
      "training: 86 batch 510 batch_loss: 0.22281628847122192\n",
      "training: 86 batch 511 batch_loss: 0.22588303685188293\n",
      "training: 86 batch 512 batch_loss: 0.22617316246032715\n",
      "training: 86 batch 513 batch_loss: 0.22604098916053772\n",
      "training: 86 batch 514 batch_loss: 0.22560051083564758\n",
      "training: 86 batch 515 batch_loss: 0.2232825756072998\n",
      "training: 86 batch 516 batch_loss: 0.2248992919921875\n",
      "training: 86 batch 517 batch_loss: 0.22550392150878906\n",
      "training: 86 batch 518 batch_loss: 0.22380581498146057\n",
      "training: 86 batch 519 batch_loss: 0.224942147731781\n",
      "training: 86 batch 520 batch_loss: 0.2218153476715088\n",
      "training: 86 batch 521 batch_loss: 0.22532165050506592\n",
      "training: 86 batch 522 batch_loss: 0.22194892168045044\n",
      "training: 86 batch 523 batch_loss: 0.22786617279052734\n",
      "training: 86 batch 524 batch_loss: 0.23008030652999878\n",
      "training: 86 batch 525 batch_loss: 0.22522875666618347\n",
      "training: 86 batch 526 batch_loss: 0.22219213843345642\n",
      "training: 86 batch 527 batch_loss: 0.2247920036315918\n",
      "training: 86 batch 528 batch_loss: 0.22772592306137085\n",
      "training: 86 batch 529 batch_loss: 0.22187986969947815\n",
      "training: 86 batch 530 batch_loss: 0.23234602808952332\n",
      "training: 86 batch 531 batch_loss: 0.22711870074272156\n",
      "training: 86 batch 532 batch_loss: 0.22380629181861877\n",
      "training: 86 batch 533 batch_loss: 0.22000181674957275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 86 batch 534 batch_loss: 0.2266189157962799\n",
      "training: 86 batch 535 batch_loss: 0.22147837281227112\n",
      "training: 86 batch 536 batch_loss: 0.22904175519943237\n",
      "training: 86 batch 537 batch_loss: 0.2296631634235382\n",
      "training: 86 batch 538 batch_loss: 0.22611761093139648\n",
      "training: 86 batch 539 batch_loss: 0.23023244738578796\n",
      "training: 86 batch 540 batch_loss: 0.2308598756790161\n",
      "training: 86 batch 541 batch_loss: 0.2256595492362976\n",
      "training: 86 batch 542 batch_loss: 0.22330912947654724\n",
      "training: 86 batch 543 batch_loss: 0.22401121258735657\n",
      "training: 86 batch 544 batch_loss: 0.22593647241592407\n",
      "training: 86 batch 545 batch_loss: 0.22866961359977722\n",
      "training: 86 batch 546 batch_loss: 0.22608715295791626\n",
      "training: 86 batch 547 batch_loss: 0.22977715730667114\n",
      "training: 86 batch 548 batch_loss: 0.2258293330669403\n",
      "training: 86 batch 549 batch_loss: 0.2242245376110077\n",
      "training: 86 batch 550 batch_loss: 0.22440534830093384\n",
      "training: 86 batch 551 batch_loss: 0.22379738092422485\n",
      "training: 86 batch 552 batch_loss: 0.2282513678073883\n",
      "training: 86 batch 553 batch_loss: 0.22824910283088684\n",
      "training: 86 batch 554 batch_loss: 0.2258504033088684\n",
      "training: 86 batch 555 batch_loss: 0.23048686981201172\n",
      "training: 86 batch 556 batch_loss: 0.22585442662239075\n",
      "training: 86 batch 557 batch_loss: 0.22826820611953735\n",
      "training: 86 batch 558 batch_loss: 0.22529804706573486\n",
      "training: 86 batch 559 batch_loss: 0.2283138632774353\n",
      "training: 86 batch 560 batch_loss: 0.22596436738967896\n",
      "training: 86 batch 561 batch_loss: 0.2261676788330078\n",
      "training: 86 batch 562 batch_loss: 0.228110671043396\n",
      "training: 86 batch 563 batch_loss: 0.2235814332962036\n",
      "training: 86 batch 564 batch_loss: 0.22557881474494934\n",
      "training: 86 batch 565 batch_loss: 0.22558951377868652\n",
      "training: 86 batch 566 batch_loss: 0.22911953926086426\n",
      "training: 86 batch 567 batch_loss: 0.2257661521434784\n",
      "training: 86 batch 568 batch_loss: 0.22595316171646118\n",
      "training: 86 batch 569 batch_loss: 0.22450801730155945\n",
      "training: 86 batch 570 batch_loss: 0.22338247299194336\n",
      "training: 86 batch 571 batch_loss: 0.22800090909004211\n",
      "training: 86 batch 572 batch_loss: 0.22275486588478088\n",
      "training: 86 batch 573 batch_loss: 0.2291853427886963\n",
      "training: 86 batch 574 batch_loss: 0.22332507371902466\n",
      "training: 86 batch 575 batch_loss: 0.22784799337387085\n",
      "training: 86 batch 576 batch_loss: 0.2292054295539856\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 86, Hit Ratio:0.03359505384869067 | Precision:0.04956748255185294 | Recall:0.0658217869806027 | NDCG:0.06466970584930463\n",
      "*Best Performance* \n",
      "Epoch: 72, Hit Ratio:0.03404476453481593 | Precision:0.05023100363707854 | Recall:0.06609018873710529 | MDCG:0.06528136987177953\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 87 batch 0 batch_loss: 0.2226032018661499\n",
      "training: 87 batch 1 batch_loss: 0.2216128706932068\n",
      "training: 87 batch 2 batch_loss: 0.22531506419181824\n",
      "training: 87 batch 3 batch_loss: 0.2249225378036499\n",
      "training: 87 batch 4 batch_loss: 0.22603923082351685\n",
      "training: 87 batch 5 batch_loss: 0.22639045119285583\n",
      "training: 87 batch 6 batch_loss: 0.22561714053153992\n",
      "training: 87 batch 7 batch_loss: 0.22546178102493286\n",
      "training: 87 batch 8 batch_loss: 0.2236807644367218\n",
      "training: 87 batch 9 batch_loss: 0.23134967684745789\n",
      "training: 87 batch 10 batch_loss: 0.22562676668167114\n",
      "training: 87 batch 11 batch_loss: 0.2233659029006958\n",
      "training: 87 batch 12 batch_loss: 0.22033369541168213\n",
      "training: 87 batch 13 batch_loss: 0.2294161021709442\n",
      "training: 87 batch 14 batch_loss: 0.22612380981445312\n",
      "training: 87 batch 15 batch_loss: 0.22124215960502625\n",
      "training: 87 batch 16 batch_loss: 0.22464478015899658\n",
      "training: 87 batch 17 batch_loss: 0.22375673055648804\n",
      "training: 87 batch 18 batch_loss: 0.2223464846611023\n",
      "training: 87 batch 19 batch_loss: 0.22484871745109558\n",
      "training: 87 batch 20 batch_loss: 0.22590306401252747\n",
      "training: 87 batch 21 batch_loss: 0.22358009219169617\n",
      "training: 87 batch 22 batch_loss: 0.2238374650478363\n",
      "training: 87 batch 23 batch_loss: 0.22707918286323547\n",
      "training: 87 batch 24 batch_loss: 0.22907930612564087\n",
      "training: 87 batch 25 batch_loss: 0.22874274849891663\n",
      "training: 87 batch 26 batch_loss: 0.22865590453147888\n",
      "training: 87 batch 27 batch_loss: 0.22377115488052368\n",
      "training: 87 batch 28 batch_loss: 0.22419682145118713\n",
      "training: 87 batch 29 batch_loss: 0.22561171650886536\n",
      "training: 87 batch 30 batch_loss: 0.22522950172424316\n",
      "training: 87 batch 31 batch_loss: 0.22663015127182007\n",
      "training: 87 batch 32 batch_loss: 0.2303127944469452\n",
      "training: 87 batch 33 batch_loss: 0.21934548020362854\n",
      "training: 87 batch 34 batch_loss: 0.22236332297325134\n",
      "training: 87 batch 35 batch_loss: 0.22486993670463562\n",
      "training: 87 batch 36 batch_loss: 0.2248254418373108\n",
      "training: 87 batch 37 batch_loss: 0.22346842288970947\n",
      "training: 87 batch 38 batch_loss: 0.224574476480484\n",
      "training: 87 batch 39 batch_loss: 0.2234596610069275\n",
      "training: 87 batch 40 batch_loss: 0.22680434584617615\n",
      "training: 87 batch 41 batch_loss: 0.22853779792785645\n",
      "training: 87 batch 42 batch_loss: 0.22116747498512268\n",
      "training: 87 batch 43 batch_loss: 0.2270016074180603\n",
      "training: 87 batch 44 batch_loss: 0.22434940934181213\n",
      "training: 87 batch 45 batch_loss: 0.22490468621253967\n",
      "training: 87 batch 46 batch_loss: 0.22189652919769287\n",
      "training: 87 batch 47 batch_loss: 0.22416174411773682\n",
      "training: 87 batch 48 batch_loss: 0.22207170724868774\n",
      "training: 87 batch 49 batch_loss: 0.22257137298583984\n",
      "training: 87 batch 50 batch_loss: 0.2241637110710144\n",
      "training: 87 batch 51 batch_loss: 0.2246335744857788\n",
      "training: 87 batch 52 batch_loss: 0.2247805893421173\n",
      "training: 87 batch 53 batch_loss: 0.22520223259925842\n",
      "training: 87 batch 54 batch_loss: 0.22566521167755127\n",
      "training: 87 batch 55 batch_loss: 0.22806626558303833\n",
      "training: 87 batch 56 batch_loss: 0.22519472241401672\n",
      "training: 87 batch 57 batch_loss: 0.22421902418136597\n",
      "training: 87 batch 58 batch_loss: 0.22325003147125244\n",
      "training: 87 batch 59 batch_loss: 0.2193099856376648\n",
      "training: 87 batch 60 batch_loss: 0.22345417737960815\n",
      "training: 87 batch 61 batch_loss: 0.2225017547607422\n",
      "training: 87 batch 62 batch_loss: 0.21982210874557495\n",
      "training: 87 batch 63 batch_loss: 0.22811812162399292\n",
      "training: 87 batch 64 batch_loss: 0.22628864645957947\n",
      "training: 87 batch 65 batch_loss: 0.2244054079055786\n",
      "training: 87 batch 66 batch_loss: 0.2297266721725464\n",
      "training: 87 batch 67 batch_loss: 0.22321945428848267\n",
      "training: 87 batch 68 batch_loss: 0.22882425785064697\n",
      "training: 87 batch 69 batch_loss: 0.22329044342041016\n",
      "training: 87 batch 70 batch_loss: 0.2254016101360321\n",
      "training: 87 batch 71 batch_loss: 0.22283947467803955\n",
      "training: 87 batch 72 batch_loss: 0.21914038062095642\n",
      "training: 87 batch 73 batch_loss: 0.22424861788749695\n",
      "training: 87 batch 74 batch_loss: 0.2223607897758484\n",
      "training: 87 batch 75 batch_loss: 0.2225179374217987\n",
      "training: 87 batch 76 batch_loss: 0.22427931427955627\n",
      "training: 87 batch 77 batch_loss: 0.2244437336921692\n",
      "training: 87 batch 78 batch_loss: 0.2214183211326599\n",
      "training: 87 batch 79 batch_loss: 0.2239791750907898\n",
      "training: 87 batch 80 batch_loss: 0.22460389137268066\n",
      "training: 87 batch 81 batch_loss: 0.2185184359550476\n",
      "training: 87 batch 82 batch_loss: 0.22524210810661316\n",
      "training: 87 batch 83 batch_loss: 0.2231540083885193\n",
      "training: 87 batch 84 batch_loss: 0.22404533624649048\n",
      "training: 87 batch 85 batch_loss: 0.22674545645713806\n",
      "training: 87 batch 86 batch_loss: 0.22852417826652527\n",
      "training: 87 batch 87 batch_loss: 0.22655880451202393\n",
      "training: 87 batch 88 batch_loss: 0.22655972838401794\n",
      "training: 87 batch 89 batch_loss: 0.22753822803497314\n",
      "training: 87 batch 90 batch_loss: 0.22449356317520142\n",
      "training: 87 batch 91 batch_loss: 0.22483909130096436\n",
      "training: 87 batch 92 batch_loss: 0.22487422823905945\n",
      "training: 87 batch 93 batch_loss: 0.2204107940196991\n",
      "training: 87 batch 94 batch_loss: 0.2223305106163025\n",
      "training: 87 batch 95 batch_loss: 0.22523975372314453\n",
      "training: 87 batch 96 batch_loss: 0.22448480129241943\n",
      "training: 87 batch 97 batch_loss: 0.22589564323425293\n",
      "training: 87 batch 98 batch_loss: 0.22388023138046265\n",
      "training: 87 batch 99 batch_loss: 0.23217764496803284\n",
      "training: 87 batch 100 batch_loss: 0.22342899441719055\n",
      "training: 87 batch 101 batch_loss: 0.2214885652065277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 87 batch 102 batch_loss: 0.2198379635810852\n",
      "training: 87 batch 103 batch_loss: 0.226018488407135\n",
      "training: 87 batch 104 batch_loss: 0.22373390197753906\n",
      "training: 87 batch 105 batch_loss: 0.2270575761795044\n",
      "training: 87 batch 106 batch_loss: 0.22716233134269714\n",
      "training: 87 batch 107 batch_loss: 0.22670122981071472\n",
      "training: 87 batch 108 batch_loss: 0.22505870461463928\n",
      "training: 87 batch 109 batch_loss: 0.22656530141830444\n",
      "training: 87 batch 110 batch_loss: 0.226362407207489\n",
      "training: 87 batch 111 batch_loss: 0.2235589623451233\n",
      "training: 87 batch 112 batch_loss: 0.22557368874549866\n",
      "training: 87 batch 113 batch_loss: 0.22419196367263794\n",
      "training: 87 batch 114 batch_loss: 0.2264840304851532\n",
      "training: 87 batch 115 batch_loss: 0.22316712141036987\n",
      "training: 87 batch 116 batch_loss: 0.22347566485404968\n",
      "training: 87 batch 117 batch_loss: 0.22743064165115356\n",
      "training: 87 batch 118 batch_loss: 0.221674382686615\n",
      "training: 87 batch 119 batch_loss: 0.22958466410636902\n",
      "training: 87 batch 120 batch_loss: 0.22280579805374146\n",
      "training: 87 batch 121 batch_loss: 0.2256367802619934\n",
      "training: 87 batch 122 batch_loss: 0.22692716121673584\n",
      "training: 87 batch 123 batch_loss: 0.22454148530960083\n",
      "training: 87 batch 124 batch_loss: 0.2231842279434204\n",
      "training: 87 batch 125 batch_loss: 0.22358250617980957\n",
      "training: 87 batch 126 batch_loss: 0.22247028350830078\n",
      "training: 87 batch 127 batch_loss: 0.22667032480239868\n",
      "training: 87 batch 128 batch_loss: 0.2212797999382019\n",
      "training: 87 batch 129 batch_loss: 0.2276250123977661\n",
      "training: 87 batch 130 batch_loss: 0.22396725416183472\n",
      "training: 87 batch 131 batch_loss: 0.22647768259048462\n",
      "training: 87 batch 132 batch_loss: 0.2255520224571228\n",
      "training: 87 batch 133 batch_loss: 0.22158768773078918\n",
      "training: 87 batch 134 batch_loss: 0.22039291262626648\n",
      "training: 87 batch 135 batch_loss: 0.22394707798957825\n",
      "training: 87 batch 136 batch_loss: 0.22778090834617615\n",
      "training: 87 batch 137 batch_loss: 0.22235837578773499\n",
      "training: 87 batch 138 batch_loss: 0.22248810529708862\n",
      "training: 87 batch 139 batch_loss: 0.22810408473014832\n",
      "training: 87 batch 140 batch_loss: 0.22471129894256592\n",
      "training: 87 batch 141 batch_loss: 0.22537356615066528\n",
      "training: 87 batch 142 batch_loss: 0.22455820441246033\n",
      "training: 87 batch 143 batch_loss: 0.22227880358695984\n",
      "training: 87 batch 144 batch_loss: 0.2280627191066742\n",
      "training: 87 batch 145 batch_loss: 0.22145617008209229\n",
      "training: 87 batch 146 batch_loss: 0.22846746444702148\n",
      "training: 87 batch 147 batch_loss: 0.2230198085308075\n",
      "training: 87 batch 148 batch_loss: 0.22605472803115845\n",
      "training: 87 batch 149 batch_loss: 0.22440090775489807\n",
      "training: 87 batch 150 batch_loss: 0.2245272397994995\n",
      "training: 87 batch 151 batch_loss: 0.2242957055568695\n",
      "training: 87 batch 152 batch_loss: 0.2327573299407959\n",
      "training: 87 batch 153 batch_loss: 0.21831485629081726\n",
      "training: 87 batch 154 batch_loss: 0.22628813982009888\n",
      "training: 87 batch 155 batch_loss: 0.226332426071167\n",
      "training: 87 batch 156 batch_loss: 0.2260437309741974\n",
      "training: 87 batch 157 batch_loss: 0.2257886528968811\n",
      "training: 87 batch 158 batch_loss: 0.22383886575698853\n",
      "training: 87 batch 159 batch_loss: 0.22451460361480713\n",
      "training: 87 batch 160 batch_loss: 0.22533392906188965\n",
      "training: 87 batch 161 batch_loss: 0.22370252013206482\n",
      "training: 87 batch 162 batch_loss: 0.2214968502521515\n",
      "training: 87 batch 163 batch_loss: 0.22346162796020508\n",
      "training: 87 batch 164 batch_loss: 0.22589915990829468\n",
      "training: 87 batch 165 batch_loss: 0.22257724404335022\n",
      "training: 87 batch 166 batch_loss: 0.22333621978759766\n",
      "training: 87 batch 167 batch_loss: 0.22496095299720764\n",
      "training: 87 batch 168 batch_loss: 0.22867023944854736\n",
      "training: 87 batch 169 batch_loss: 0.2273087203502655\n",
      "training: 87 batch 170 batch_loss: 0.22493919730186462\n",
      "training: 87 batch 171 batch_loss: 0.22416609525680542\n",
      "training: 87 batch 172 batch_loss: 0.22748050093650818\n",
      "training: 87 batch 173 batch_loss: 0.22553062438964844\n",
      "training: 87 batch 174 batch_loss: 0.22560334205627441\n",
      "training: 87 batch 175 batch_loss: 0.22405269742012024\n",
      "training: 87 batch 176 batch_loss: 0.22666668891906738\n",
      "training: 87 batch 177 batch_loss: 0.22367531061172485\n",
      "training: 87 batch 178 batch_loss: 0.2229359745979309\n",
      "training: 87 batch 179 batch_loss: 0.2242003083229065\n",
      "training: 87 batch 180 batch_loss: 0.22347313165664673\n",
      "training: 87 batch 181 batch_loss: 0.2267235517501831\n",
      "training: 87 batch 182 batch_loss: 0.22092750668525696\n",
      "training: 87 batch 183 batch_loss: 0.22677040100097656\n",
      "training: 87 batch 184 batch_loss: 0.22666719555854797\n",
      "training: 87 batch 185 batch_loss: 0.2260090410709381\n",
      "training: 87 batch 186 batch_loss: 0.22077953815460205\n",
      "training: 87 batch 187 batch_loss: 0.22223418951034546\n",
      "training: 87 batch 188 batch_loss: 0.22478753328323364\n",
      "training: 87 batch 189 batch_loss: 0.2231215238571167\n",
      "training: 87 batch 190 batch_loss: 0.22655874490737915\n",
      "training: 87 batch 191 batch_loss: 0.22659185528755188\n",
      "training: 87 batch 192 batch_loss: 0.2264001965522766\n",
      "training: 87 batch 193 batch_loss: 0.2250305712223053\n",
      "training: 87 batch 194 batch_loss: 0.22642725706100464\n",
      "training: 87 batch 195 batch_loss: 0.2220139503479004\n",
      "training: 87 batch 196 batch_loss: 0.23609289526939392\n",
      "training: 87 batch 197 batch_loss: 0.22539052367210388\n",
      "training: 87 batch 198 batch_loss: 0.22561854124069214\n",
      "training: 87 batch 199 batch_loss: 0.2205268144607544\n",
      "training: 87 batch 200 batch_loss: 0.222455233335495\n",
      "training: 87 batch 201 batch_loss: 0.22672972083091736\n",
      "training: 87 batch 202 batch_loss: 0.22544437646865845\n",
      "training: 87 batch 203 batch_loss: 0.22421735525131226\n",
      "training: 87 batch 204 batch_loss: 0.22928965091705322\n",
      "training: 87 batch 205 batch_loss: 0.2274046242237091\n",
      "training: 87 batch 206 batch_loss: 0.22549638152122498\n",
      "training: 87 batch 207 batch_loss: 0.2228144109249115\n",
      "training: 87 batch 208 batch_loss: 0.2283739447593689\n",
      "training: 87 batch 209 batch_loss: 0.22437474131584167\n",
      "training: 87 batch 210 batch_loss: 0.22526460886001587\n",
      "training: 87 batch 211 batch_loss: 0.22046151757240295\n",
      "training: 87 batch 212 batch_loss: 0.22400259971618652\n",
      "training: 87 batch 213 batch_loss: 0.22395595908164978\n",
      "training: 87 batch 214 batch_loss: 0.22498595714569092\n",
      "training: 87 batch 215 batch_loss: 0.22755640745162964\n",
      "training: 87 batch 216 batch_loss: 0.22919970750808716\n",
      "training: 87 batch 217 batch_loss: 0.22462236881256104\n",
      "training: 87 batch 218 batch_loss: 0.22558405995368958\n",
      "training: 87 batch 219 batch_loss: 0.22631055116653442\n",
      "training: 87 batch 220 batch_loss: 0.2280605435371399\n",
      "training: 87 batch 221 batch_loss: 0.22512328624725342\n",
      "training: 87 batch 222 batch_loss: 0.22431135177612305\n",
      "training: 87 batch 223 batch_loss: 0.22583580017089844\n",
      "training: 87 batch 224 batch_loss: 0.22061607241630554\n",
      "training: 87 batch 225 batch_loss: 0.22521033883094788\n",
      "training: 87 batch 226 batch_loss: 0.22260180115699768\n",
      "training: 87 batch 227 batch_loss: 0.2306157946586609\n",
      "training: 87 batch 228 batch_loss: 0.22384688258171082\n",
      "training: 87 batch 229 batch_loss: 0.22243833541870117\n",
      "training: 87 batch 230 batch_loss: 0.22831562161445618\n",
      "training: 87 batch 231 batch_loss: 0.22427979111671448\n",
      "training: 87 batch 232 batch_loss: 0.22399333119392395\n",
      "training: 87 batch 233 batch_loss: 0.22556132078170776\n",
      "training: 87 batch 234 batch_loss: 0.22438663244247437\n",
      "training: 87 batch 235 batch_loss: 0.2236495018005371\n",
      "training: 87 batch 236 batch_loss: 0.22345513105392456\n",
      "training: 87 batch 237 batch_loss: 0.22440874576568604\n",
      "training: 87 batch 238 batch_loss: 0.22402894496917725\n",
      "training: 87 batch 239 batch_loss: 0.2264481782913208\n",
      "training: 87 batch 240 batch_loss: 0.22413408756256104\n",
      "training: 87 batch 241 batch_loss: 0.22892802953720093\n",
      "training: 87 batch 242 batch_loss: 0.22728556394577026\n",
      "training: 87 batch 243 batch_loss: 0.22304454445838928\n",
      "training: 87 batch 244 batch_loss: 0.22554922103881836\n",
      "training: 87 batch 245 batch_loss: 0.22365468740463257\n",
      "training: 87 batch 246 batch_loss: 0.2279076874256134\n",
      "training: 87 batch 247 batch_loss: 0.22566121816635132\n",
      "training: 87 batch 248 batch_loss: 0.22435766458511353\n",
      "training: 87 batch 249 batch_loss: 0.2287028431892395\n",
      "training: 87 batch 250 batch_loss: 0.22240519523620605\n",
      "training: 87 batch 251 batch_loss: 0.2262498140335083\n",
      "training: 87 batch 252 batch_loss: 0.22324877977371216\n",
      "training: 87 batch 253 batch_loss: 0.2217862606048584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 87 batch 254 batch_loss: 0.23104998469352722\n",
      "training: 87 batch 255 batch_loss: 0.22750720381736755\n",
      "training: 87 batch 256 batch_loss: 0.2271772027015686\n",
      "training: 87 batch 257 batch_loss: 0.22618907690048218\n",
      "training: 87 batch 258 batch_loss: 0.22791293263435364\n",
      "training: 87 batch 259 batch_loss: 0.2217303216457367\n",
      "training: 87 batch 260 batch_loss: 0.22183707356452942\n",
      "training: 87 batch 261 batch_loss: 0.22268736362457275\n",
      "training: 87 batch 262 batch_loss: 0.22779640555381775\n",
      "training: 87 batch 263 batch_loss: 0.22550389170646667\n",
      "training: 87 batch 264 batch_loss: 0.231026291847229\n",
      "training: 87 batch 265 batch_loss: 0.22270852327346802\n",
      "training: 87 batch 266 batch_loss: 0.22538092732429504\n",
      "training: 87 batch 267 batch_loss: 0.23076429963111877\n",
      "training: 87 batch 268 batch_loss: 0.2256878912448883\n",
      "training: 87 batch 269 batch_loss: 0.2254422903060913\n",
      "training: 87 batch 270 batch_loss: 0.2250416874885559\n",
      "training: 87 batch 271 batch_loss: 0.22817465662956238\n",
      "training: 87 batch 272 batch_loss: 0.22176074981689453\n",
      "training: 87 batch 273 batch_loss: 0.22601035237312317\n",
      "training: 87 batch 274 batch_loss: 0.22806069254875183\n",
      "training: 87 batch 275 batch_loss: 0.22889399528503418\n",
      "training: 87 batch 276 batch_loss: 0.2251012921333313\n",
      "training: 87 batch 277 batch_loss: 0.2284349799156189\n",
      "training: 87 batch 278 batch_loss: 0.2241310477256775\n",
      "training: 87 batch 279 batch_loss: 0.22618338465690613\n",
      "training: 87 batch 280 batch_loss: 0.2289116084575653\n",
      "training: 87 batch 281 batch_loss: 0.22282874584197998\n",
      "training: 87 batch 282 batch_loss: 0.22924742102622986\n",
      "training: 87 batch 283 batch_loss: 0.22900962829589844\n",
      "training: 87 batch 284 batch_loss: 0.2230955958366394\n",
      "training: 87 batch 285 batch_loss: 0.22479596734046936\n",
      "training: 87 batch 286 batch_loss: 0.22387707233428955\n",
      "training: 87 batch 287 batch_loss: 0.22035425901412964\n",
      "training: 87 batch 288 batch_loss: 0.2261904776096344\n",
      "training: 87 batch 289 batch_loss: 0.2226376235485077\n",
      "training: 87 batch 290 batch_loss: 0.22878068685531616\n",
      "training: 87 batch 291 batch_loss: 0.22626805305480957\n",
      "training: 87 batch 292 batch_loss: 0.21978643536567688\n",
      "training: 87 batch 293 batch_loss: 0.22632700204849243\n",
      "training: 87 batch 294 batch_loss: 0.2265675663948059\n",
      "training: 87 batch 295 batch_loss: 0.2246246635913849\n",
      "training: 87 batch 296 batch_loss: 0.2219773530960083\n",
      "training: 87 batch 297 batch_loss: 0.2263968288898468\n",
      "training: 87 batch 298 batch_loss: 0.22493529319763184\n",
      "training: 87 batch 299 batch_loss: 0.2254321575164795\n",
      "training: 87 batch 300 batch_loss: 0.2219436764717102\n",
      "training: 87 batch 301 batch_loss: 0.2242925465106964\n",
      "training: 87 batch 302 batch_loss: 0.22650787234306335\n",
      "training: 87 batch 303 batch_loss: 0.22846302390098572\n",
      "training: 87 batch 304 batch_loss: 0.2208157479763031\n",
      "training: 87 batch 305 batch_loss: 0.22539961338043213\n",
      "training: 87 batch 306 batch_loss: 0.22747725248336792\n",
      "training: 87 batch 307 batch_loss: 0.2267519235610962\n",
      "training: 87 batch 308 batch_loss: 0.22752559185028076\n",
      "training: 87 batch 309 batch_loss: 0.22344696521759033\n",
      "training: 87 batch 310 batch_loss: 0.22480052709579468\n",
      "training: 87 batch 311 batch_loss: 0.22337180376052856\n",
      "training: 87 batch 312 batch_loss: 0.2255251705646515\n",
      "training: 87 batch 313 batch_loss: 0.2295520007610321\n",
      "training: 87 batch 314 batch_loss: 0.22677621245384216\n",
      "training: 87 batch 315 batch_loss: 0.2267240583896637\n",
      "training: 87 batch 316 batch_loss: 0.22298038005828857\n",
      "training: 87 batch 317 batch_loss: 0.22346407175064087\n",
      "training: 87 batch 318 batch_loss: 0.22903013229370117\n",
      "training: 87 batch 319 batch_loss: 0.22979971766471863\n",
      "training: 87 batch 320 batch_loss: 0.22443446516990662\n",
      "training: 87 batch 321 batch_loss: 0.22528734803199768\n",
      "training: 87 batch 322 batch_loss: 0.23048031330108643\n",
      "training: 87 batch 323 batch_loss: 0.2247287929058075\n",
      "training: 87 batch 324 batch_loss: 0.2286357879638672\n",
      "training: 87 batch 325 batch_loss: 0.22655636072158813\n",
      "training: 87 batch 326 batch_loss: 0.22521215677261353\n",
      "training: 87 batch 327 batch_loss: 0.22795730829238892\n",
      "training: 87 batch 328 batch_loss: 0.22639873623847961\n",
      "training: 87 batch 329 batch_loss: 0.22525551915168762\n",
      "training: 87 batch 330 batch_loss: 0.23090025782585144\n",
      "training: 87 batch 331 batch_loss: 0.23009124398231506\n",
      "training: 87 batch 332 batch_loss: 0.22590145468711853\n",
      "training: 87 batch 333 batch_loss: 0.21918317675590515\n",
      "training: 87 batch 334 batch_loss: 0.22479695081710815\n",
      "training: 87 batch 335 batch_loss: 0.2256338894367218\n",
      "training: 87 batch 336 batch_loss: 0.22991257905960083\n",
      "training: 87 batch 337 batch_loss: 0.22449567914009094\n",
      "training: 87 batch 338 batch_loss: 0.22240757942199707\n",
      "training: 87 batch 339 batch_loss: 0.22676289081573486\n",
      "training: 87 batch 340 batch_loss: 0.22374531626701355\n",
      "training: 87 batch 341 batch_loss: 0.2294921875\n",
      "training: 87 batch 342 batch_loss: 0.2269512414932251\n",
      "training: 87 batch 343 batch_loss: 0.22625869512557983\n",
      "training: 87 batch 344 batch_loss: 0.22696581482887268\n",
      "training: 87 batch 345 batch_loss: 0.22448468208312988\n",
      "training: 87 batch 346 batch_loss: 0.22933223843574524\n",
      "training: 87 batch 347 batch_loss: 0.2275393009185791\n",
      "training: 87 batch 348 batch_loss: 0.22687536478042603\n",
      "training: 87 batch 349 batch_loss: 0.23021739721298218\n",
      "training: 87 batch 350 batch_loss: 0.2263890504837036\n",
      "training: 87 batch 351 batch_loss: 0.22927933931350708\n",
      "training: 87 batch 352 batch_loss: 0.22833457589149475\n",
      "training: 87 batch 353 batch_loss: 0.22741851210594177\n",
      "training: 87 batch 354 batch_loss: 0.22691136598587036\n",
      "training: 87 batch 355 batch_loss: 0.2278262972831726\n",
      "training: 87 batch 356 batch_loss: 0.225501149892807\n",
      "training: 87 batch 357 batch_loss: 0.2261359691619873\n",
      "training: 87 batch 358 batch_loss: 0.2241477370262146\n",
      "training: 87 batch 359 batch_loss: 0.22744396328926086\n",
      "training: 87 batch 360 batch_loss: 0.2265128195285797\n",
      "training: 87 batch 361 batch_loss: 0.22350138425827026\n",
      "training: 87 batch 362 batch_loss: 0.22550246119499207\n",
      "training: 87 batch 363 batch_loss: 0.22643834352493286\n",
      "training: 87 batch 364 batch_loss: 0.2219231128692627\n",
      "training: 87 batch 365 batch_loss: 0.22688555717468262\n",
      "training: 87 batch 366 batch_loss: 0.2241503894329071\n",
      "training: 87 batch 367 batch_loss: 0.22695699334144592\n",
      "training: 87 batch 368 batch_loss: 0.22717663645744324\n",
      "training: 87 batch 369 batch_loss: 0.22561347484588623\n",
      "training: 87 batch 370 batch_loss: 0.22202467918395996\n",
      "training: 87 batch 371 batch_loss: 0.22819960117340088\n",
      "training: 87 batch 372 batch_loss: 0.22831851243972778\n",
      "training: 87 batch 373 batch_loss: 0.22329851984977722\n",
      "training: 87 batch 374 batch_loss: 0.22306165099143982\n",
      "training: 87 batch 375 batch_loss: 0.22271385788917542\n",
      "training: 87 batch 376 batch_loss: 0.23398864269256592\n",
      "training: 87 batch 377 batch_loss: 0.23045435547828674\n",
      "training: 87 batch 378 batch_loss: 0.2317478358745575\n",
      "training: 87 batch 379 batch_loss: 0.223722904920578\n",
      "training: 87 batch 380 batch_loss: 0.22649815678596497\n",
      "training: 87 batch 381 batch_loss: 0.22369712591171265\n",
      "training: 87 batch 382 batch_loss: 0.22648540139198303\n",
      "training: 87 batch 383 batch_loss: 0.2290543019771576\n",
      "training: 87 batch 384 batch_loss: 0.22816592454910278\n",
      "training: 87 batch 385 batch_loss: 0.22475594282150269\n",
      "training: 87 batch 386 batch_loss: 0.22699066996574402\n",
      "training: 87 batch 387 batch_loss: 0.22715675830841064\n",
      "training: 87 batch 388 batch_loss: 0.227286696434021\n",
      "training: 87 batch 389 batch_loss: 0.21942666172981262\n",
      "training: 87 batch 390 batch_loss: 0.2275063693523407\n",
      "training: 87 batch 391 batch_loss: 0.2266402244567871\n",
      "training: 87 batch 392 batch_loss: 0.22052493691444397\n",
      "training: 87 batch 393 batch_loss: 0.22920995950698853\n",
      "training: 87 batch 394 batch_loss: 0.22782200574874878\n",
      "training: 87 batch 395 batch_loss: 0.22836029529571533\n",
      "training: 87 batch 396 batch_loss: 0.22662484645843506\n",
      "training: 87 batch 397 batch_loss: 0.22665423154830933\n",
      "training: 87 batch 398 batch_loss: 0.22498995065689087\n",
      "training: 87 batch 399 batch_loss: 0.22780963778495789\n",
      "training: 87 batch 400 batch_loss: 0.22695285081863403\n",
      "training: 87 batch 401 batch_loss: 0.22449186444282532\n",
      "training: 87 batch 402 batch_loss: 0.2279774248600006\n",
      "training: 87 batch 403 batch_loss: 0.22432845830917358\n",
      "training: 87 batch 404 batch_loss: 0.227645605802536\n",
      "training: 87 batch 405 batch_loss: 0.22612687945365906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 87 batch 406 batch_loss: 0.23025667667388916\n",
      "training: 87 batch 407 batch_loss: 0.22770151495933533\n",
      "training: 87 batch 408 batch_loss: 0.22308534383773804\n",
      "training: 87 batch 409 batch_loss: 0.22620049118995667\n",
      "training: 87 batch 410 batch_loss: 0.2275458574295044\n",
      "training: 87 batch 411 batch_loss: 0.22530582547187805\n",
      "training: 87 batch 412 batch_loss: 0.22154590487480164\n",
      "training: 87 batch 413 batch_loss: 0.2257882058620453\n",
      "training: 87 batch 414 batch_loss: 0.2283381223678589\n",
      "training: 87 batch 415 batch_loss: 0.22593072056770325\n",
      "training: 87 batch 416 batch_loss: 0.22278812527656555\n",
      "training: 87 batch 417 batch_loss: 0.22684118151664734\n",
      "training: 87 batch 418 batch_loss: 0.2240253984928131\n",
      "training: 87 batch 419 batch_loss: 0.22531551122665405\n",
      "training: 87 batch 420 batch_loss: 0.22812604904174805\n",
      "training: 87 batch 421 batch_loss: 0.2248067557811737\n",
      "training: 87 batch 422 batch_loss: 0.22844138741493225\n",
      "training: 87 batch 423 batch_loss: 0.2245888113975525\n",
      "training: 87 batch 424 batch_loss: 0.22110074758529663\n",
      "training: 87 batch 425 batch_loss: 0.2257462739944458\n",
      "training: 87 batch 426 batch_loss: 0.22803807258605957\n",
      "training: 87 batch 427 batch_loss: 0.22457703948020935\n",
      "training: 87 batch 428 batch_loss: 0.22941499948501587\n",
      "training: 87 batch 429 batch_loss: 0.2298591136932373\n",
      "training: 87 batch 430 batch_loss: 0.22816351056098938\n",
      "training: 87 batch 431 batch_loss: 0.23093891143798828\n",
      "training: 87 batch 432 batch_loss: 0.2262152135372162\n",
      "training: 87 batch 433 batch_loss: 0.22270923852920532\n",
      "training: 87 batch 434 batch_loss: 0.22623062133789062\n",
      "training: 87 batch 435 batch_loss: 0.22694706916809082\n",
      "training: 87 batch 436 batch_loss: 0.22694680094718933\n",
      "training: 87 batch 437 batch_loss: 0.23048508167266846\n",
      "training: 87 batch 438 batch_loss: 0.22997838258743286\n",
      "training: 87 batch 439 batch_loss: 0.22501793503761292\n",
      "training: 87 batch 440 batch_loss: 0.2293541431427002\n",
      "training: 87 batch 441 batch_loss: 0.22710031270980835\n",
      "training: 87 batch 442 batch_loss: 0.22574973106384277\n",
      "training: 87 batch 443 batch_loss: 0.22398120164871216\n",
      "training: 87 batch 444 batch_loss: 0.230902761220932\n",
      "training: 87 batch 445 batch_loss: 0.2275981307029724\n",
      "training: 87 batch 446 batch_loss: 0.2238660454750061\n",
      "training: 87 batch 447 batch_loss: 0.22660711407661438\n",
      "training: 87 batch 448 batch_loss: 0.22782158851623535\n",
      "training: 87 batch 449 batch_loss: 0.22165557742118835\n",
      "training: 87 batch 450 batch_loss: 0.22712311148643494\n",
      "training: 87 batch 451 batch_loss: 0.22729668021202087\n",
      "training: 87 batch 452 batch_loss: 0.2272091507911682\n",
      "training: 87 batch 453 batch_loss: 0.22469815611839294\n",
      "training: 87 batch 454 batch_loss: 0.2247757613658905\n",
      "training: 87 batch 455 batch_loss: 0.22717463970184326\n",
      "training: 87 batch 456 batch_loss: 0.22779422998428345\n",
      "training: 87 batch 457 batch_loss: 0.22312510013580322\n",
      "training: 87 batch 458 batch_loss: 0.22833538055419922\n",
      "training: 87 batch 459 batch_loss: 0.22781896591186523\n",
      "training: 87 batch 460 batch_loss: 0.2279163897037506\n",
      "training: 87 batch 461 batch_loss: 0.22731664776802063\n",
      "training: 87 batch 462 batch_loss: 0.2211875021457672\n",
      "training: 87 batch 463 batch_loss: 0.2265033721923828\n",
      "training: 87 batch 464 batch_loss: 0.2277282476425171\n",
      "training: 87 batch 465 batch_loss: 0.22802412509918213\n",
      "training: 87 batch 466 batch_loss: 0.23285871744155884\n",
      "training: 87 batch 467 batch_loss: 0.22663477063179016\n",
      "training: 87 batch 468 batch_loss: 0.22374430298805237\n",
      "training: 87 batch 469 batch_loss: 0.22544273734092712\n",
      "training: 87 batch 470 batch_loss: 0.22806662321090698\n",
      "training: 87 batch 471 batch_loss: 0.22140294313430786\n",
      "training: 87 batch 472 batch_loss: 0.22535809874534607\n",
      "training: 87 batch 473 batch_loss: 0.22786754369735718\n",
      "training: 87 batch 474 batch_loss: 0.22681471705436707\n",
      "training: 87 batch 475 batch_loss: 0.22622400522232056\n",
      "training: 87 batch 476 batch_loss: 0.23238888382911682\n",
      "training: 87 batch 477 batch_loss: 0.23162901401519775\n",
      "training: 87 batch 478 batch_loss: 0.22299110889434814\n",
      "training: 87 batch 479 batch_loss: 0.2236308455467224\n",
      "training: 87 batch 480 batch_loss: 0.22549736499786377\n",
      "training: 87 batch 481 batch_loss: 0.22721070051193237\n",
      "training: 87 batch 482 batch_loss: 0.22792693972587585\n",
      "training: 87 batch 483 batch_loss: 0.22893354296684265\n",
      "training: 87 batch 484 batch_loss: 0.22927194833755493\n",
      "training: 87 batch 485 batch_loss: 0.2274201214313507\n",
      "training: 87 batch 486 batch_loss: 0.22559413313865662\n",
      "training: 87 batch 487 batch_loss: 0.22282570600509644\n",
      "training: 87 batch 488 batch_loss: 0.22610491514205933\n",
      "training: 87 batch 489 batch_loss: 0.22503697872161865\n",
      "training: 87 batch 490 batch_loss: 0.22663861513137817\n",
      "training: 87 batch 491 batch_loss: 0.22975492477416992\n",
      "training: 87 batch 492 batch_loss: 0.22582358121871948\n",
      "training: 87 batch 493 batch_loss: 0.22610637545585632\n",
      "training: 87 batch 494 batch_loss: 0.22749415040016174\n",
      "training: 87 batch 495 batch_loss: 0.224879652261734\n",
      "training: 87 batch 496 batch_loss: 0.22408288717269897\n",
      "training: 87 batch 497 batch_loss: 0.23018652200698853\n",
      "training: 87 batch 498 batch_loss: 0.22483041882514954\n",
      "training: 87 batch 499 batch_loss: 0.2267693281173706\n",
      "training: 87 batch 500 batch_loss: 0.2283342480659485\n",
      "training: 87 batch 501 batch_loss: 0.22230985760688782\n",
      "training: 87 batch 502 batch_loss: 0.2268204689025879\n",
      "training: 87 batch 503 batch_loss: 0.22699856758117676\n",
      "training: 87 batch 504 batch_loss: 0.22633907198905945\n",
      "training: 87 batch 505 batch_loss: 0.22909143567085266\n",
      "training: 87 batch 506 batch_loss: 0.2277834713459015\n",
      "training: 87 batch 507 batch_loss: 0.22143292427062988\n",
      "training: 87 batch 508 batch_loss: 0.22746732831001282\n",
      "training: 87 batch 509 batch_loss: 0.22506266832351685\n",
      "training: 87 batch 510 batch_loss: 0.224168598651886\n",
      "training: 87 batch 511 batch_loss: 0.22840720415115356\n",
      "training: 87 batch 512 batch_loss: 0.22679901123046875\n",
      "training: 87 batch 513 batch_loss: 0.2249247431755066\n",
      "training: 87 batch 514 batch_loss: 0.2251218855381012\n",
      "training: 87 batch 515 batch_loss: 0.22644788026809692\n",
      "training: 87 batch 516 batch_loss: 0.2329198718070984\n",
      "training: 87 batch 517 batch_loss: 0.23082968592643738\n",
      "training: 87 batch 518 batch_loss: 0.22558677196502686\n",
      "training: 87 batch 519 batch_loss: 0.22271683812141418\n",
      "training: 87 batch 520 batch_loss: 0.227081298828125\n",
      "training: 87 batch 521 batch_loss: 0.2245791256427765\n",
      "training: 87 batch 522 batch_loss: 0.22804099321365356\n",
      "training: 87 batch 523 batch_loss: 0.22551193833351135\n",
      "training: 87 batch 524 batch_loss: 0.2235892415046692\n",
      "training: 87 batch 525 batch_loss: 0.22894015908241272\n",
      "training: 87 batch 526 batch_loss: 0.22294336557388306\n",
      "training: 87 batch 527 batch_loss: 0.22243568301200867\n",
      "training: 87 batch 528 batch_loss: 0.22827118635177612\n",
      "training: 87 batch 529 batch_loss: 0.2293810248374939\n",
      "training: 87 batch 530 batch_loss: 0.22510501742362976\n",
      "training: 87 batch 531 batch_loss: 0.2264620065689087\n",
      "training: 87 batch 532 batch_loss: 0.227741539478302\n",
      "training: 87 batch 533 batch_loss: 0.22154846787452698\n",
      "training: 87 batch 534 batch_loss: 0.2259359061717987\n",
      "training: 87 batch 535 batch_loss: 0.23050710558891296\n",
      "training: 87 batch 536 batch_loss: 0.22468873858451843\n",
      "training: 87 batch 537 batch_loss: 0.2257729470729828\n",
      "training: 87 batch 538 batch_loss: 0.22203540802001953\n",
      "training: 87 batch 539 batch_loss: 0.22925317287445068\n",
      "training: 87 batch 540 batch_loss: 0.23049873113632202\n",
      "training: 87 batch 541 batch_loss: 0.229965478181839\n",
      "training: 87 batch 542 batch_loss: 0.22611624002456665\n",
      "training: 87 batch 543 batch_loss: 0.22463420033454895\n",
      "training: 87 batch 544 batch_loss: 0.22287797927856445\n",
      "training: 87 batch 545 batch_loss: 0.22414815425872803\n",
      "training: 87 batch 546 batch_loss: 0.23095491528511047\n",
      "training: 87 batch 547 batch_loss: 0.22606825828552246\n",
      "training: 87 batch 548 batch_loss: 0.23211434483528137\n",
      "training: 87 batch 549 batch_loss: 0.22339510917663574\n",
      "training: 87 batch 550 batch_loss: 0.22458794713020325\n",
      "training: 87 batch 551 batch_loss: 0.22546979784965515\n",
      "training: 87 batch 552 batch_loss: 0.22549238801002502\n",
      "training: 87 batch 553 batch_loss: 0.22719734907150269\n",
      "training: 87 batch 554 batch_loss: 0.22833237051963806\n",
      "training: 87 batch 555 batch_loss: 0.23016172647476196\n",
      "training: 87 batch 556 batch_loss: 0.2282385528087616\n",
      "training: 87 batch 557 batch_loss: 0.22419127821922302\n",
      "training: 87 batch 558 batch_loss: 0.2264663577079773\n",
      "training: 87 batch 559 batch_loss: 0.2246589958667755\n",
      "training: 87 batch 560 batch_loss: 0.22617846727371216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 87 batch 561 batch_loss: 0.2267448902130127\n",
      "training: 87 batch 562 batch_loss: 0.22792276740074158\n",
      "training: 87 batch 563 batch_loss: 0.22706764936447144\n",
      "training: 87 batch 564 batch_loss: 0.22859978675842285\n",
      "training: 87 batch 565 batch_loss: 0.2281131148338318\n",
      "training: 87 batch 566 batch_loss: 0.2241162359714508\n",
      "training: 87 batch 567 batch_loss: 0.22529372572898865\n",
      "training: 87 batch 568 batch_loss: 0.22897246479988098\n",
      "training: 87 batch 569 batch_loss: 0.22848328948020935\n",
      "training: 87 batch 570 batch_loss: 0.22672945261001587\n",
      "training: 87 batch 571 batch_loss: 0.22677826881408691\n",
      "training: 87 batch 572 batch_loss: 0.22298812866210938\n",
      "training: 87 batch 573 batch_loss: 0.2252848744392395\n",
      "training: 87 batch 574 batch_loss: 0.22521841526031494\n",
      "training: 87 batch 575 batch_loss: 0.22536465525627136\n",
      "training: 87 batch 576 batch_loss: 0.2297154664993286\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 87, Hit Ratio:0.03409140119856226 | Precision:0.050299813231101935 | Recall:0.06623474467749231 | NDCG:0.06544512792267772\n",
      "*Best Performance* \n",
      "Epoch: 87, Hit Ratio:0.03409140119856226 | Precision:0.050299813231101935 | Recall:0.06623474467749231 | MDCG:0.06544512792267772\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 88 batch 0 batch_loss: 0.22352337837219238\n",
      "training: 88 batch 1 batch_loss: 0.22318223118782043\n",
      "training: 88 batch 2 batch_loss: 0.22554877400398254\n",
      "training: 88 batch 3 batch_loss: 0.2269292175769806\n",
      "training: 88 batch 4 batch_loss: 0.22542929649353027\n",
      "training: 88 batch 5 batch_loss: 0.22416338324546814\n",
      "training: 88 batch 6 batch_loss: 0.22634199261665344\n",
      "training: 88 batch 7 batch_loss: 0.22825369238853455\n",
      "training: 88 batch 8 batch_loss: 0.2263297438621521\n",
      "training: 88 batch 9 batch_loss: 0.22476348280906677\n",
      "training: 88 batch 10 batch_loss: 0.22019612789154053\n",
      "training: 88 batch 11 batch_loss: 0.227118581533432\n",
      "training: 88 batch 12 batch_loss: 0.234308660030365\n",
      "training: 88 batch 13 batch_loss: 0.22385457158088684\n",
      "training: 88 batch 14 batch_loss: 0.22187384963035583\n",
      "training: 88 batch 15 batch_loss: 0.22533011436462402\n",
      "training: 88 batch 16 batch_loss: 0.22644290328025818\n",
      "training: 88 batch 17 batch_loss: 0.22734257578849792\n",
      "training: 88 batch 18 batch_loss: 0.22457465529441833\n",
      "training: 88 batch 19 batch_loss: 0.223751038312912\n",
      "training: 88 batch 20 batch_loss: 0.2306729257106781\n",
      "training: 88 batch 21 batch_loss: 0.22845134139060974\n",
      "training: 88 batch 22 batch_loss: 0.22482934594154358\n",
      "training: 88 batch 23 batch_loss: 0.22901281714439392\n",
      "training: 88 batch 24 batch_loss: 0.2249535322189331\n",
      "training: 88 batch 25 batch_loss: 0.2214098870754242\n",
      "training: 88 batch 26 batch_loss: 0.22258463501930237\n",
      "training: 88 batch 27 batch_loss: 0.22875750064849854\n",
      "training: 88 batch 28 batch_loss: 0.22242066264152527\n",
      "training: 88 batch 29 batch_loss: 0.22750377655029297\n",
      "training: 88 batch 30 batch_loss: 0.22588300704956055\n",
      "training: 88 batch 31 batch_loss: 0.22562995553016663\n",
      "training: 88 batch 32 batch_loss: 0.22973549365997314\n",
      "training: 88 batch 33 batch_loss: 0.22622805833816528\n",
      "training: 88 batch 34 batch_loss: 0.22297269105911255\n",
      "training: 88 batch 35 batch_loss: 0.22300398349761963\n",
      "training: 88 batch 36 batch_loss: 0.22858941555023193\n",
      "training: 88 batch 37 batch_loss: 0.22511038184165955\n",
      "training: 88 batch 38 batch_loss: 0.2255176305770874\n",
      "training: 88 batch 39 batch_loss: 0.2238156497478485\n",
      "training: 88 batch 40 batch_loss: 0.21795347332954407\n",
      "training: 88 batch 41 batch_loss: 0.22955715656280518\n",
      "training: 88 batch 42 batch_loss: 0.2304133176803589\n",
      "training: 88 batch 43 batch_loss: 0.22923094034194946\n",
      "training: 88 batch 44 batch_loss: 0.22475886344909668\n",
      "training: 88 batch 45 batch_loss: 0.21847352385520935\n",
      "training: 88 batch 46 batch_loss: 0.22979819774627686\n",
      "training: 88 batch 47 batch_loss: 0.22478649020195007\n",
      "training: 88 batch 48 batch_loss: 0.22666773200035095\n",
      "training: 88 batch 49 batch_loss: 0.2268003523349762\n",
      "training: 88 batch 50 batch_loss: 0.22582191228866577\n",
      "training: 88 batch 51 batch_loss: 0.22859320044517517\n",
      "training: 88 batch 52 batch_loss: 0.22693583369255066\n",
      "training: 88 batch 53 batch_loss: 0.22496768832206726\n",
      "training: 88 batch 54 batch_loss: 0.2255164384841919\n",
      "training: 88 batch 55 batch_loss: 0.2245950698852539\n",
      "training: 88 batch 56 batch_loss: 0.22784560918807983\n",
      "training: 88 batch 57 batch_loss: 0.22100138664245605\n",
      "training: 88 batch 58 batch_loss: 0.22465944290161133\n",
      "training: 88 batch 59 batch_loss: 0.22659265995025635\n",
      "training: 88 batch 60 batch_loss: 0.2236064076423645\n",
      "training: 88 batch 61 batch_loss: 0.22408327460289001\n",
      "training: 88 batch 62 batch_loss: 0.22291576862335205\n",
      "training: 88 batch 63 batch_loss: 0.22852039337158203\n",
      "training: 88 batch 64 batch_loss: 0.2241990566253662\n",
      "training: 88 batch 65 batch_loss: 0.22275400161743164\n",
      "training: 88 batch 66 batch_loss: 0.22639837861061096\n",
      "training: 88 batch 67 batch_loss: 0.22334933280944824\n",
      "training: 88 batch 68 batch_loss: 0.22436285018920898\n",
      "training: 88 batch 69 batch_loss: 0.22526031732559204\n",
      "training: 88 batch 70 batch_loss: 0.22364717721939087\n",
      "training: 88 batch 71 batch_loss: 0.22926932573318481\n",
      "training: 88 batch 72 batch_loss: 0.22775155305862427\n",
      "training: 88 batch 73 batch_loss: 0.2209671139717102\n",
      "training: 88 batch 74 batch_loss: 0.22499138116836548\n",
      "training: 88 batch 75 batch_loss: 0.22873133420944214\n",
      "training: 88 batch 76 batch_loss: 0.22295740246772766\n",
      "training: 88 batch 77 batch_loss: 0.22632423043251038\n",
      "training: 88 batch 78 batch_loss: 0.2226894497871399\n",
      "training: 88 batch 79 batch_loss: 0.22505122423171997\n",
      "training: 88 batch 80 batch_loss: 0.22370171546936035\n",
      "training: 88 batch 81 batch_loss: 0.22792494297027588\n",
      "training: 88 batch 82 batch_loss: 0.22383257746696472\n",
      "training: 88 batch 83 batch_loss: 0.2264053225517273\n",
      "training: 88 batch 84 batch_loss: 0.2258266806602478\n",
      "training: 88 batch 85 batch_loss: 0.22390508651733398\n",
      "training: 88 batch 86 batch_loss: 0.22508922219276428\n",
      "training: 88 batch 87 batch_loss: 0.2279478907585144\n",
      "training: 88 batch 88 batch_loss: 0.22678977251052856\n",
      "training: 88 batch 89 batch_loss: 0.22558510303497314\n",
      "training: 88 batch 90 batch_loss: 0.22556018829345703\n",
      "training: 88 batch 91 batch_loss: 0.22642242908477783\n",
      "training: 88 batch 92 batch_loss: 0.22036832571029663\n",
      "training: 88 batch 93 batch_loss: 0.22859841585159302\n",
      "training: 88 batch 94 batch_loss: 0.22216948866844177\n",
      "training: 88 batch 95 batch_loss: 0.2267240583896637\n",
      "training: 88 batch 96 batch_loss: 0.2221684455871582\n",
      "training: 88 batch 97 batch_loss: 0.2239176630973816\n",
      "training: 88 batch 98 batch_loss: 0.2240251898765564\n",
      "training: 88 batch 99 batch_loss: 0.22849857807159424\n",
      "training: 88 batch 100 batch_loss: 0.22988289594650269\n",
      "training: 88 batch 101 batch_loss: 0.2234153151512146\n",
      "training: 88 batch 102 batch_loss: 0.22383368015289307\n",
      "training: 88 batch 103 batch_loss: 0.22131499648094177\n",
      "training: 88 batch 104 batch_loss: 0.22433149814605713\n",
      "training: 88 batch 105 batch_loss: 0.22478356957435608\n",
      "training: 88 batch 106 batch_loss: 0.22728562355041504\n",
      "training: 88 batch 107 batch_loss: 0.22685149312019348\n",
      "training: 88 batch 108 batch_loss: 0.2254456877708435\n",
      "training: 88 batch 109 batch_loss: 0.23051047325134277\n",
      "training: 88 batch 110 batch_loss: 0.2219252586364746\n",
      "training: 88 batch 111 batch_loss: 0.22580239176750183\n",
      "training: 88 batch 112 batch_loss: 0.2257080078125\n",
      "training: 88 batch 113 batch_loss: 0.2216755449771881\n",
      "training: 88 batch 114 batch_loss: 0.23206785321235657\n",
      "training: 88 batch 115 batch_loss: 0.22390088438987732\n",
      "training: 88 batch 116 batch_loss: 0.2253718078136444\n",
      "training: 88 batch 117 batch_loss: 0.22450217604637146\n",
      "training: 88 batch 118 batch_loss: 0.22963759303092957\n",
      "training: 88 batch 119 batch_loss: 0.2249603569507599\n",
      "training: 88 batch 120 batch_loss: 0.22442284226417542\n",
      "training: 88 batch 121 batch_loss: 0.2224951684474945\n",
      "training: 88 batch 122 batch_loss: 0.22885382175445557\n",
      "training: 88 batch 123 batch_loss: 0.22320342063903809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 88 batch 124 batch_loss: 0.22683411836624146\n",
      "training: 88 batch 125 batch_loss: 0.2241830825805664\n",
      "training: 88 batch 126 batch_loss: 0.23009413480758667\n",
      "training: 88 batch 127 batch_loss: 0.22425708174705505\n",
      "training: 88 batch 128 batch_loss: 0.2243366539478302\n",
      "training: 88 batch 129 batch_loss: 0.225661963224411\n",
      "training: 88 batch 130 batch_loss: 0.2219114601612091\n",
      "training: 88 batch 131 batch_loss: 0.22226467728614807\n",
      "training: 88 batch 132 batch_loss: 0.22656357288360596\n",
      "training: 88 batch 133 batch_loss: 0.22626763582229614\n",
      "training: 88 batch 134 batch_loss: 0.2255917191505432\n",
      "training: 88 batch 135 batch_loss: 0.22928613424301147\n",
      "training: 88 batch 136 batch_loss: 0.22220122814178467\n",
      "training: 88 batch 137 batch_loss: 0.22580868005752563\n",
      "training: 88 batch 138 batch_loss: 0.22117796540260315\n",
      "training: 88 batch 139 batch_loss: 0.22217732667922974\n",
      "training: 88 batch 140 batch_loss: 0.22489550709724426\n",
      "training: 88 batch 141 batch_loss: 0.23019203543663025\n",
      "training: 88 batch 142 batch_loss: 0.22642666101455688\n",
      "training: 88 batch 143 batch_loss: 0.22705334424972534\n",
      "training: 88 batch 144 batch_loss: 0.22435659170150757\n",
      "training: 88 batch 145 batch_loss: 0.22010138630867004\n",
      "training: 88 batch 146 batch_loss: 0.2307952642440796\n",
      "training: 88 batch 147 batch_loss: 0.2265535295009613\n",
      "training: 88 batch 148 batch_loss: 0.22532203793525696\n",
      "training: 88 batch 149 batch_loss: 0.23091930150985718\n",
      "training: 88 batch 150 batch_loss: 0.22881147265434265\n",
      "training: 88 batch 151 batch_loss: 0.22858038544654846\n",
      "training: 88 batch 152 batch_loss: 0.22905132174491882\n",
      "training: 88 batch 153 batch_loss: 0.2234843373298645\n",
      "training: 88 batch 154 batch_loss: 0.22983777523040771\n",
      "training: 88 batch 155 batch_loss: 0.22372427582740784\n",
      "training: 88 batch 156 batch_loss: 0.22585636377334595\n",
      "training: 88 batch 157 batch_loss: 0.22267842292785645\n",
      "training: 88 batch 158 batch_loss: 0.22233304381370544\n",
      "training: 88 batch 159 batch_loss: 0.22190281748771667\n",
      "training: 88 batch 160 batch_loss: 0.22276481986045837\n",
      "training: 88 batch 161 batch_loss: 0.22231191396713257\n",
      "training: 88 batch 162 batch_loss: 0.22650068998336792\n",
      "training: 88 batch 163 batch_loss: 0.22850996255874634\n",
      "training: 88 batch 164 batch_loss: 0.22713500261306763\n",
      "training: 88 batch 165 batch_loss: 0.2244754135608673\n",
      "training: 88 batch 166 batch_loss: 0.22925156354904175\n",
      "training: 88 batch 167 batch_loss: 0.22593477368354797\n",
      "training: 88 batch 168 batch_loss: 0.22531753778457642\n",
      "training: 88 batch 169 batch_loss: 0.22303736209869385\n",
      "training: 88 batch 170 batch_loss: 0.22577747702598572\n",
      "training: 88 batch 171 batch_loss: 0.22723019123077393\n",
      "training: 88 batch 172 batch_loss: 0.22043555974960327\n",
      "training: 88 batch 173 batch_loss: 0.22131305932998657\n",
      "training: 88 batch 174 batch_loss: 0.2278614044189453\n",
      "training: 88 batch 175 batch_loss: 0.22956213355064392\n",
      "training: 88 batch 176 batch_loss: 0.22406995296478271\n",
      "training: 88 batch 177 batch_loss: 0.22411561012268066\n",
      "training: 88 batch 178 batch_loss: 0.22881418466567993\n",
      "training: 88 batch 179 batch_loss: 0.23023980855941772\n",
      "training: 88 batch 180 batch_loss: 0.2285664677619934\n",
      "training: 88 batch 181 batch_loss: 0.22490155696868896\n",
      "training: 88 batch 182 batch_loss: 0.225330650806427\n",
      "training: 88 batch 183 batch_loss: 0.22456970810890198\n",
      "training: 88 batch 184 batch_loss: 0.22290202975273132\n",
      "training: 88 batch 185 batch_loss: 0.22582551836967468\n",
      "training: 88 batch 186 batch_loss: 0.2282242774963379\n",
      "training: 88 batch 187 batch_loss: 0.22873014211654663\n",
      "training: 88 batch 188 batch_loss: 0.2224416434764862\n",
      "training: 88 batch 189 batch_loss: 0.22840121388435364\n",
      "training: 88 batch 190 batch_loss: 0.2221321165561676\n",
      "training: 88 batch 191 batch_loss: 0.22189116477966309\n",
      "training: 88 batch 192 batch_loss: 0.22755777835845947\n",
      "training: 88 batch 193 batch_loss: 0.23085474967956543\n",
      "training: 88 batch 194 batch_loss: 0.22593629360198975\n",
      "training: 88 batch 195 batch_loss: 0.22532528638839722\n",
      "training: 88 batch 196 batch_loss: 0.22747507691383362\n",
      "training: 88 batch 197 batch_loss: 0.22313779592514038\n",
      "training: 88 batch 198 batch_loss: 0.22718903422355652\n",
      "training: 88 batch 199 batch_loss: 0.2243812084197998\n",
      "training: 88 batch 200 batch_loss: 0.22879502177238464\n",
      "training: 88 batch 201 batch_loss: 0.22740793228149414\n",
      "training: 88 batch 202 batch_loss: 0.22387248277664185\n",
      "training: 88 batch 203 batch_loss: 0.22424116730690002\n",
      "training: 88 batch 204 batch_loss: 0.23040097951889038\n",
      "training: 88 batch 205 batch_loss: 0.22540774941444397\n",
      "training: 88 batch 206 batch_loss: 0.22724807262420654\n",
      "training: 88 batch 207 batch_loss: 0.2267627716064453\n",
      "training: 88 batch 208 batch_loss: 0.22346872091293335\n",
      "training: 88 batch 209 batch_loss: 0.22412440180778503\n",
      "training: 88 batch 210 batch_loss: 0.2293987274169922\n",
      "training: 88 batch 211 batch_loss: 0.22889301180839539\n",
      "training: 88 batch 212 batch_loss: 0.22767329216003418\n",
      "training: 88 batch 213 batch_loss: 0.227726012468338\n",
      "training: 88 batch 214 batch_loss: 0.22251054644584656\n",
      "training: 88 batch 215 batch_loss: 0.2256467044353485\n",
      "training: 88 batch 216 batch_loss: 0.22573870420455933\n",
      "training: 88 batch 217 batch_loss: 0.2242465615272522\n",
      "training: 88 batch 218 batch_loss: 0.22937235236167908\n",
      "training: 88 batch 219 batch_loss: 0.22686520218849182\n",
      "training: 88 batch 220 batch_loss: 0.22407442331314087\n",
      "training: 88 batch 221 batch_loss: 0.2243438959121704\n",
      "training: 88 batch 222 batch_loss: 0.22872287034988403\n",
      "training: 88 batch 223 batch_loss: 0.22468125820159912\n",
      "training: 88 batch 224 batch_loss: 0.22480624914169312\n",
      "training: 88 batch 225 batch_loss: 0.22507625818252563\n",
      "training: 88 batch 226 batch_loss: 0.22495180368423462\n",
      "training: 88 batch 227 batch_loss: 0.22804522514343262\n",
      "training: 88 batch 228 batch_loss: 0.2281402051448822\n",
      "training: 88 batch 229 batch_loss: 0.22614642977714539\n",
      "training: 88 batch 230 batch_loss: 0.22894585132598877\n",
      "training: 88 batch 231 batch_loss: 0.22647294402122498\n",
      "training: 88 batch 232 batch_loss: 0.22442394495010376\n",
      "training: 88 batch 233 batch_loss: 0.2231718897819519\n",
      "training: 88 batch 234 batch_loss: 0.2240023910999298\n",
      "training: 88 batch 235 batch_loss: 0.224429190158844\n",
      "training: 88 batch 236 batch_loss: 0.2249966561794281\n",
      "training: 88 batch 237 batch_loss: 0.22653698921203613\n",
      "training: 88 batch 238 batch_loss: 0.2254766821861267\n",
      "training: 88 batch 239 batch_loss: 0.22568738460540771\n",
      "training: 88 batch 240 batch_loss: 0.22827252745628357\n",
      "training: 88 batch 241 batch_loss: 0.22539538145065308\n",
      "training: 88 batch 242 batch_loss: 0.22366783022880554\n",
      "training: 88 batch 243 batch_loss: 0.22607457637786865\n",
      "training: 88 batch 244 batch_loss: 0.2274807095527649\n",
      "training: 88 batch 245 batch_loss: 0.22786295413970947\n",
      "training: 88 batch 246 batch_loss: 0.22884640097618103\n",
      "training: 88 batch 247 batch_loss: 0.22566312551498413\n",
      "training: 88 batch 248 batch_loss: 0.2207111418247223\n",
      "training: 88 batch 249 batch_loss: 0.22700220346450806\n",
      "training: 88 batch 250 batch_loss: 0.22430652379989624\n",
      "training: 88 batch 251 batch_loss: 0.21906393766403198\n",
      "training: 88 batch 252 batch_loss: 0.22635656595230103\n",
      "training: 88 batch 253 batch_loss: 0.21917298436164856\n",
      "training: 88 batch 254 batch_loss: 0.22791829705238342\n",
      "training: 88 batch 255 batch_loss: 0.22512668371200562\n",
      "training: 88 batch 256 batch_loss: 0.23293974995613098\n",
      "training: 88 batch 257 batch_loss: 0.2231091558933258\n",
      "training: 88 batch 258 batch_loss: 0.22745320200920105\n",
      "training: 88 batch 259 batch_loss: 0.22906571626663208\n",
      "training: 88 batch 260 batch_loss: 0.2239566445350647\n",
      "training: 88 batch 261 batch_loss: 0.22266101837158203\n",
      "training: 88 batch 262 batch_loss: 0.22658878564834595\n",
      "training: 88 batch 263 batch_loss: 0.2233874797821045\n",
      "training: 88 batch 264 batch_loss: 0.22700440883636475\n",
      "training: 88 batch 265 batch_loss: 0.22582915425300598\n",
      "training: 88 batch 266 batch_loss: 0.21960192918777466\n",
      "training: 88 batch 267 batch_loss: 0.22613388299942017\n",
      "training: 88 batch 268 batch_loss: 0.22701972723007202\n",
      "training: 88 batch 269 batch_loss: 0.22782829403877258\n",
      "training: 88 batch 270 batch_loss: 0.22506457567214966\n",
      "training: 88 batch 271 batch_loss: 0.22901085019111633\n",
      "training: 88 batch 272 batch_loss: 0.22734755277633667\n",
      "training: 88 batch 273 batch_loss: 0.22726845741271973\n",
      "training: 88 batch 274 batch_loss: 0.225286066532135\n",
      "training: 88 batch 275 batch_loss: 0.22214767336845398\n",
      "training: 88 batch 276 batch_loss: 0.22699615359306335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 88 batch 277 batch_loss: 0.22714263200759888\n",
      "training: 88 batch 278 batch_loss: 0.2244279980659485\n",
      "training: 88 batch 279 batch_loss: 0.22390216588974\n",
      "training: 88 batch 280 batch_loss: 0.22713953256607056\n",
      "training: 88 batch 281 batch_loss: 0.2259616255760193\n",
      "training: 88 batch 282 batch_loss: 0.23241457343101501\n",
      "training: 88 batch 283 batch_loss: 0.22782760858535767\n",
      "training: 88 batch 284 batch_loss: 0.22643011808395386\n",
      "training: 88 batch 285 batch_loss: 0.223873108625412\n",
      "training: 88 batch 286 batch_loss: 0.22397160530090332\n",
      "training: 88 batch 287 batch_loss: 0.22768256068229675\n",
      "training: 88 batch 288 batch_loss: 0.223968505859375\n",
      "training: 88 batch 289 batch_loss: 0.2314389944076538\n",
      "training: 88 batch 290 batch_loss: 0.22926348447799683\n",
      "training: 88 batch 291 batch_loss: 0.22135746479034424\n",
      "training: 88 batch 292 batch_loss: 0.22512638568878174\n",
      "training: 88 batch 293 batch_loss: 0.22805237770080566\n",
      "training: 88 batch 294 batch_loss: 0.2322540581226349\n",
      "training: 88 batch 295 batch_loss: 0.22859787940979004\n",
      "training: 88 batch 296 batch_loss: 0.2285478711128235\n",
      "training: 88 batch 297 batch_loss: 0.22136342525482178\n",
      "training: 88 batch 298 batch_loss: 0.22792500257492065\n",
      "training: 88 batch 299 batch_loss: 0.22513532638549805\n",
      "training: 88 batch 300 batch_loss: 0.2289554476737976\n",
      "training: 88 batch 301 batch_loss: 0.22873950004577637\n",
      "training: 88 batch 302 batch_loss: 0.22750553488731384\n",
      "training: 88 batch 303 batch_loss: 0.22590860724449158\n",
      "training: 88 batch 304 batch_loss: 0.22695696353912354\n",
      "training: 88 batch 305 batch_loss: 0.2251003384590149\n",
      "training: 88 batch 306 batch_loss: 0.2248362898826599\n",
      "training: 88 batch 307 batch_loss: 0.22656109929084778\n",
      "training: 88 batch 308 batch_loss: 0.22569236159324646\n",
      "training: 88 batch 309 batch_loss: 0.22646835446357727\n",
      "training: 88 batch 310 batch_loss: 0.22325065732002258\n",
      "training: 88 batch 311 batch_loss: 0.2248057723045349\n",
      "training: 88 batch 312 batch_loss: 0.22514480352401733\n",
      "training: 88 batch 313 batch_loss: 0.22561395168304443\n",
      "training: 88 batch 314 batch_loss: 0.2257058322429657\n",
      "training: 88 batch 315 batch_loss: 0.2281557023525238\n",
      "training: 88 batch 316 batch_loss: 0.22518223524093628\n",
      "training: 88 batch 317 batch_loss: 0.22387781739234924\n",
      "training: 88 batch 318 batch_loss: 0.22287225723266602\n",
      "training: 88 batch 319 batch_loss: 0.22619888186454773\n",
      "training: 88 batch 320 batch_loss: 0.22476226091384888\n",
      "training: 88 batch 321 batch_loss: 0.2238357961177826\n",
      "training: 88 batch 322 batch_loss: 0.22910186648368835\n",
      "training: 88 batch 323 batch_loss: 0.23022261261940002\n",
      "training: 88 batch 324 batch_loss: 0.22969350218772888\n",
      "training: 88 batch 325 batch_loss: 0.22584837675094604\n",
      "training: 88 batch 326 batch_loss: 0.2237032651901245\n",
      "training: 88 batch 327 batch_loss: 0.23123276233673096\n",
      "training: 88 batch 328 batch_loss: 0.22833973169326782\n",
      "training: 88 batch 329 batch_loss: 0.22831237316131592\n",
      "training: 88 batch 330 batch_loss: 0.22723528742790222\n",
      "training: 88 batch 331 batch_loss: 0.2225002646446228\n",
      "training: 88 batch 332 batch_loss: 0.22804021835327148\n",
      "training: 88 batch 333 batch_loss: 0.22461289167404175\n",
      "training: 88 batch 334 batch_loss: 0.22602680325508118\n",
      "training: 88 batch 335 batch_loss: 0.22479167580604553\n",
      "training: 88 batch 336 batch_loss: 0.2222900390625\n",
      "training: 88 batch 337 batch_loss: 0.22538408637046814\n",
      "training: 88 batch 338 batch_loss: 0.22457876801490784\n",
      "training: 88 batch 339 batch_loss: 0.2286682426929474\n",
      "training: 88 batch 340 batch_loss: 0.22553643584251404\n",
      "training: 88 batch 341 batch_loss: 0.22903555631637573\n",
      "training: 88 batch 342 batch_loss: 0.2256825864315033\n",
      "training: 88 batch 343 batch_loss: 0.22505199909210205\n",
      "training: 88 batch 344 batch_loss: 0.23143014311790466\n",
      "training: 88 batch 345 batch_loss: 0.22441038489341736\n",
      "training: 88 batch 346 batch_loss: 0.22599953413009644\n",
      "training: 88 batch 347 batch_loss: 0.22321408987045288\n",
      "training: 88 batch 348 batch_loss: 0.2258991003036499\n",
      "training: 88 batch 349 batch_loss: 0.2272549271583557\n",
      "training: 88 batch 350 batch_loss: 0.22654080390930176\n",
      "training: 88 batch 351 batch_loss: 0.2210475504398346\n",
      "training: 88 batch 352 batch_loss: 0.22591513395309448\n",
      "training: 88 batch 353 batch_loss: 0.22239148616790771\n",
      "training: 88 batch 354 batch_loss: 0.22856473922729492\n",
      "training: 88 batch 355 batch_loss: 0.22376075387001038\n",
      "training: 88 batch 356 batch_loss: 0.230159193277359\n",
      "training: 88 batch 357 batch_loss: 0.22925573587417603\n",
      "training: 88 batch 358 batch_loss: 0.2238248586654663\n",
      "training: 88 batch 359 batch_loss: 0.2331317961215973\n",
      "training: 88 batch 360 batch_loss: 0.22558161616325378\n",
      "training: 88 batch 361 batch_loss: 0.23058876395225525\n",
      "training: 88 batch 362 batch_loss: 0.22598573565483093\n",
      "training: 88 batch 363 batch_loss: 0.22832703590393066\n",
      "training: 88 batch 364 batch_loss: 0.22661766409873962\n",
      "training: 88 batch 365 batch_loss: 0.22294607758522034\n",
      "training: 88 batch 366 batch_loss: 0.22838151454925537\n",
      "training: 88 batch 367 batch_loss: 0.22395041584968567\n",
      "training: 88 batch 368 batch_loss: 0.22942867875099182\n",
      "training: 88 batch 369 batch_loss: 0.22722899913787842\n",
      "training: 88 batch 370 batch_loss: 0.23021209239959717\n",
      "training: 88 batch 371 batch_loss: 0.2251327931880951\n",
      "training: 88 batch 372 batch_loss: 0.22932323813438416\n",
      "training: 88 batch 373 batch_loss: 0.22282186150550842\n",
      "training: 88 batch 374 batch_loss: 0.22602570056915283\n",
      "training: 88 batch 375 batch_loss: 0.226220041513443\n",
      "training: 88 batch 376 batch_loss: 0.2259160280227661\n",
      "training: 88 batch 377 batch_loss: 0.2294844090938568\n",
      "training: 88 batch 378 batch_loss: 0.22505104541778564\n",
      "training: 88 batch 379 batch_loss: 0.2286868393421173\n",
      "training: 88 batch 380 batch_loss: 0.22497212886810303\n",
      "training: 88 batch 381 batch_loss: 0.22797441482543945\n",
      "training: 88 batch 382 batch_loss: 0.22434163093566895\n",
      "training: 88 batch 383 batch_loss: 0.23101481795310974\n",
      "training: 88 batch 384 batch_loss: 0.22575408220291138\n",
      "training: 88 batch 385 batch_loss: 0.2260741889476776\n",
      "training: 88 batch 386 batch_loss: 0.22215330600738525\n",
      "training: 88 batch 387 batch_loss: 0.2299976646900177\n",
      "training: 88 batch 388 batch_loss: 0.22762879729270935\n",
      "training: 88 batch 389 batch_loss: 0.22696667909622192\n",
      "training: 88 batch 390 batch_loss: 0.2263692319393158\n",
      "training: 88 batch 391 batch_loss: 0.2281644642353058\n",
      "training: 88 batch 392 batch_loss: 0.2246435284614563\n",
      "training: 88 batch 393 batch_loss: 0.22363537549972534\n",
      "training: 88 batch 394 batch_loss: 0.23136338591575623\n",
      "training: 88 batch 395 batch_loss: 0.22730696201324463\n",
      "training: 88 batch 396 batch_loss: 0.2247883379459381\n",
      "training: 88 batch 397 batch_loss: 0.2240571677684784\n",
      "training: 88 batch 398 batch_loss: 0.22761619091033936\n",
      "training: 88 batch 399 batch_loss: 0.22285622358322144\n",
      "training: 88 batch 400 batch_loss: 0.22892865538597107\n",
      "training: 88 batch 401 batch_loss: 0.2283152937889099\n",
      "training: 88 batch 402 batch_loss: 0.22593435645103455\n",
      "training: 88 batch 403 batch_loss: 0.2244604229927063\n",
      "training: 88 batch 404 batch_loss: 0.2281787395477295\n",
      "training: 88 batch 405 batch_loss: 0.22534936666488647\n",
      "training: 88 batch 406 batch_loss: 0.23064225912094116\n",
      "training: 88 batch 407 batch_loss: 0.2258615791797638\n",
      "training: 88 batch 408 batch_loss: 0.23002904653549194\n",
      "training: 88 batch 409 batch_loss: 0.2255173623561859\n",
      "training: 88 batch 410 batch_loss: 0.23305505514144897\n",
      "training: 88 batch 411 batch_loss: 0.22543948888778687\n",
      "training: 88 batch 412 batch_loss: 0.22782474756240845\n",
      "training: 88 batch 413 batch_loss: 0.22761744260787964\n",
      "training: 88 batch 414 batch_loss: 0.2280167043209076\n",
      "training: 88 batch 415 batch_loss: 0.22364646196365356\n",
      "training: 88 batch 416 batch_loss: 0.22221282124519348\n",
      "training: 88 batch 417 batch_loss: 0.22881102561950684\n",
      "training: 88 batch 418 batch_loss: 0.2264779508113861\n",
      "training: 88 batch 419 batch_loss: 0.22741472721099854\n",
      "training: 88 batch 420 batch_loss: 0.22801411151885986\n",
      "training: 88 batch 421 batch_loss: 0.2312650978565216\n",
      "training: 88 batch 422 batch_loss: 0.22791719436645508\n",
      "training: 88 batch 423 batch_loss: 0.22611036896705627\n",
      "training: 88 batch 424 batch_loss: 0.22582581639289856\n",
      "training: 88 batch 425 batch_loss: 0.23267799615859985\n",
      "training: 88 batch 426 batch_loss: 0.22587960958480835\n",
      "training: 88 batch 427 batch_loss: 0.22799158096313477\n",
      "training: 88 batch 428 batch_loss: 0.2237609624862671\n",
      "training: 88 batch 429 batch_loss: 0.22990256547927856\n",
      "training: 88 batch 430 batch_loss: 0.22308063507080078\n",
      "training: 88 batch 431 batch_loss: 0.22710829973220825\n",
      "training: 88 batch 432 batch_loss: 0.22841328382492065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 88 batch 433 batch_loss: 0.22892025113105774\n",
      "training: 88 batch 434 batch_loss: 0.22817197442054749\n",
      "training: 88 batch 435 batch_loss: 0.22433027625083923\n",
      "training: 88 batch 436 batch_loss: 0.22505024075508118\n",
      "training: 88 batch 437 batch_loss: 0.22786736488342285\n",
      "training: 88 batch 438 batch_loss: 0.23013588786125183\n",
      "training: 88 batch 439 batch_loss: 0.2278631329536438\n",
      "training: 88 batch 440 batch_loss: 0.22283458709716797\n",
      "training: 88 batch 441 batch_loss: 0.22782781720161438\n",
      "training: 88 batch 442 batch_loss: 0.23265224695205688\n",
      "training: 88 batch 443 batch_loss: 0.2261982560157776\n",
      "training: 88 batch 444 batch_loss: 0.2258528470993042\n",
      "training: 88 batch 445 batch_loss: 0.22662976384162903\n",
      "training: 88 batch 446 batch_loss: 0.22669756412506104\n",
      "training: 88 batch 447 batch_loss: 0.22624483704566956\n",
      "training: 88 batch 448 batch_loss: 0.22553735971450806\n",
      "training: 88 batch 449 batch_loss: 0.22916960716247559\n",
      "training: 88 batch 450 batch_loss: 0.22753426432609558\n",
      "training: 88 batch 451 batch_loss: 0.23004955053329468\n",
      "training: 88 batch 452 batch_loss: 0.2284402847290039\n",
      "training: 88 batch 453 batch_loss: 0.22118717432022095\n",
      "training: 88 batch 454 batch_loss: 0.2322322428226471\n",
      "training: 88 batch 455 batch_loss: 0.22199809551239014\n",
      "training: 88 batch 456 batch_loss: 0.2287110984325409\n",
      "training: 88 batch 457 batch_loss: 0.23476773500442505\n",
      "training: 88 batch 458 batch_loss: 0.22755131125450134\n",
      "training: 88 batch 459 batch_loss: 0.22393059730529785\n",
      "training: 88 batch 460 batch_loss: 0.22792476415634155\n",
      "training: 88 batch 461 batch_loss: 0.23044592142105103\n",
      "training: 88 batch 462 batch_loss: 0.22207239270210266\n",
      "training: 88 batch 463 batch_loss: 0.22826248407363892\n",
      "training: 88 batch 464 batch_loss: 0.22833773493766785\n",
      "training: 88 batch 465 batch_loss: 0.22585302591323853\n",
      "training: 88 batch 466 batch_loss: 0.22605931758880615\n",
      "training: 88 batch 467 batch_loss: 0.22904041409492493\n",
      "training: 88 batch 468 batch_loss: 0.22732967138290405\n",
      "training: 88 batch 469 batch_loss: 0.23299595713615417\n",
      "training: 88 batch 470 batch_loss: 0.22961345314979553\n",
      "training: 88 batch 471 batch_loss: 0.2277534008026123\n",
      "training: 88 batch 472 batch_loss: 0.22469907999038696\n",
      "training: 88 batch 473 batch_loss: 0.22611451148986816\n",
      "training: 88 batch 474 batch_loss: 0.22649896144866943\n",
      "training: 88 batch 475 batch_loss: 0.2235516607761383\n",
      "training: 88 batch 476 batch_loss: 0.22765088081359863\n",
      "training: 88 batch 477 batch_loss: 0.22895008325576782\n",
      "training: 88 batch 478 batch_loss: 0.22848239541053772\n",
      "training: 88 batch 479 batch_loss: 0.22148683667182922\n",
      "training: 88 batch 480 batch_loss: 0.2248876690864563\n",
      "training: 88 batch 481 batch_loss: 0.22949349880218506\n",
      "training: 88 batch 482 batch_loss: 0.23600730299949646\n",
      "training: 88 batch 483 batch_loss: 0.2307519018650055\n",
      "training: 88 batch 484 batch_loss: 0.2247946858406067\n",
      "training: 88 batch 485 batch_loss: 0.22704023122787476\n",
      "training: 88 batch 486 batch_loss: 0.22389477491378784\n",
      "training: 88 batch 487 batch_loss: 0.22613483667373657\n",
      "training: 88 batch 488 batch_loss: 0.2288491427898407\n",
      "training: 88 batch 489 batch_loss: 0.22758162021636963\n",
      "training: 88 batch 490 batch_loss: 0.22934111952781677\n",
      "training: 88 batch 491 batch_loss: 0.2271222472190857\n",
      "training: 88 batch 492 batch_loss: 0.2254619598388672\n",
      "training: 88 batch 493 batch_loss: 0.22830867767333984\n",
      "training: 88 batch 494 batch_loss: 0.22898870706558228\n",
      "training: 88 batch 495 batch_loss: 0.22697705030441284\n",
      "training: 88 batch 496 batch_loss: 0.2281508445739746\n",
      "training: 88 batch 497 batch_loss: 0.22511309385299683\n",
      "training: 88 batch 498 batch_loss: 0.228182852268219\n",
      "training: 88 batch 499 batch_loss: 0.2324061095714569\n",
      "training: 88 batch 500 batch_loss: 0.2269388735294342\n",
      "training: 88 batch 501 batch_loss: 0.22675660252571106\n",
      "training: 88 batch 502 batch_loss: 0.22760295867919922\n",
      "training: 88 batch 503 batch_loss: 0.2250663936138153\n",
      "training: 88 batch 504 batch_loss: 0.22948965430259705\n",
      "training: 88 batch 505 batch_loss: 0.2295817732810974\n",
      "training: 88 batch 506 batch_loss: 0.22892606258392334\n",
      "training: 88 batch 507 batch_loss: 0.2299320101737976\n",
      "training: 88 batch 508 batch_loss: 0.22665968537330627\n",
      "training: 88 batch 509 batch_loss: 0.22605684399604797\n",
      "training: 88 batch 510 batch_loss: 0.2251015603542328\n",
      "training: 88 batch 511 batch_loss: 0.22777444124221802\n",
      "training: 88 batch 512 batch_loss: 0.2272118330001831\n",
      "training: 88 batch 513 batch_loss: 0.22660702466964722\n",
      "training: 88 batch 514 batch_loss: 0.2269095778465271\n",
      "training: 88 batch 515 batch_loss: 0.22183078527450562\n",
      "training: 88 batch 516 batch_loss: 0.22733795642852783\n",
      "training: 88 batch 517 batch_loss: 0.22396916151046753\n",
      "training: 88 batch 518 batch_loss: 0.22808796167373657\n",
      "training: 88 batch 519 batch_loss: 0.22378605604171753\n",
      "training: 88 batch 520 batch_loss: 0.22407475113868713\n",
      "training: 88 batch 521 batch_loss: 0.22643035650253296\n",
      "training: 88 batch 522 batch_loss: 0.23097237944602966\n",
      "training: 88 batch 523 batch_loss: 0.2274177372455597\n",
      "training: 88 batch 524 batch_loss: 0.22927173972129822\n",
      "training: 88 batch 525 batch_loss: 0.23010599613189697\n",
      "training: 88 batch 526 batch_loss: 0.2303522229194641\n",
      "training: 88 batch 527 batch_loss: 0.22846528887748718\n",
      "training: 88 batch 528 batch_loss: 0.2263762354850769\n",
      "training: 88 batch 529 batch_loss: 0.22864335775375366\n",
      "training: 88 batch 530 batch_loss: 0.2287074327468872\n",
      "training: 88 batch 531 batch_loss: 0.22541797161102295\n",
      "training: 88 batch 532 batch_loss: 0.2243880033493042\n",
      "training: 88 batch 533 batch_loss: 0.223213791847229\n",
      "training: 88 batch 534 batch_loss: 0.22448784112930298\n",
      "training: 88 batch 535 batch_loss: 0.226667582988739\n",
      "training: 88 batch 536 batch_loss: 0.22731256484985352\n",
      "training: 88 batch 537 batch_loss: 0.22698232531547546\n",
      "training: 88 batch 538 batch_loss: 0.22659534215927124\n",
      "training: 88 batch 539 batch_loss: 0.2252979576587677\n",
      "training: 88 batch 540 batch_loss: 0.22865062952041626\n",
      "training: 88 batch 541 batch_loss: 0.22752472758293152\n",
      "training: 88 batch 542 batch_loss: 0.2282431721687317\n",
      "training: 88 batch 543 batch_loss: 0.2287120223045349\n",
      "training: 88 batch 544 batch_loss: 0.22977659106254578\n",
      "training: 88 batch 545 batch_loss: 0.227744460105896\n",
      "training: 88 batch 546 batch_loss: 0.229709655046463\n",
      "training: 88 batch 547 batch_loss: 0.22471025586128235\n",
      "training: 88 batch 548 batch_loss: 0.23037773370742798\n",
      "training: 88 batch 549 batch_loss: 0.23034337162971497\n",
      "training: 88 batch 550 batch_loss: 0.22430893778800964\n",
      "training: 88 batch 551 batch_loss: 0.2296905815601349\n",
      "training: 88 batch 552 batch_loss: 0.2262762486934662\n",
      "training: 88 batch 553 batch_loss: 0.22269484400749207\n",
      "training: 88 batch 554 batch_loss: 0.22407525777816772\n",
      "training: 88 batch 555 batch_loss: 0.2298153042793274\n",
      "training: 88 batch 556 batch_loss: 0.22972381114959717\n",
      "training: 88 batch 557 batch_loss: 0.23258501291275024\n",
      "training: 88 batch 558 batch_loss: 0.22659555077552795\n",
      "training: 88 batch 559 batch_loss: 0.22629985213279724\n",
      "training: 88 batch 560 batch_loss: 0.23104700446128845\n",
      "training: 88 batch 561 batch_loss: 0.2296566367149353\n",
      "training: 88 batch 562 batch_loss: 0.22420653700828552\n",
      "training: 88 batch 563 batch_loss: 0.22521820664405823\n",
      "training: 88 batch 564 batch_loss: 0.22879183292388916\n",
      "training: 88 batch 565 batch_loss: 0.23028117418289185\n",
      "training: 88 batch 566 batch_loss: 0.23348578810691833\n",
      "training: 88 batch 567 batch_loss: 0.22778254747390747\n",
      "training: 88 batch 568 batch_loss: 0.22548654675483704\n",
      "training: 88 batch 569 batch_loss: 0.22918856143951416\n",
      "training: 88 batch 570 batch_loss: 0.2227463722229004\n",
      "training: 88 batch 571 batch_loss: 0.22541314363479614\n",
      "training: 88 batch 572 batch_loss: 0.2225586175918579\n",
      "training: 88 batch 573 batch_loss: 0.22370925545692444\n",
      "training: 88 batch 574 batch_loss: 0.2290748953819275\n",
      "training: 88 batch 575 batch_loss: 0.22749769687652588\n",
      "training: 88 batch 576 batch_loss: 0.22801437973976135\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 88, Hit Ratio:0.0338848674019714 | Precision:0.049995085028998326 | Recall:0.06620871181105277 | NDCG:0.0653191201193803\n",
      "*Best Performance* \n",
      "Epoch: 87, Hit Ratio:0.03409140119856226 | Precision:0.050299813231101935 | Recall:0.06623474467749231 | MDCG:0.06544512792267772\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 89 batch 0 batch_loss: 0.22141492366790771\n",
      "training: 89 batch 1 batch_loss: 0.22207382321357727\n",
      "training: 89 batch 2 batch_loss: 0.22760054469108582\n",
      "training: 89 batch 3 batch_loss: 0.22046387195587158\n",
      "training: 89 batch 4 batch_loss: 0.2213279902935028\n",
      "training: 89 batch 5 batch_loss: 0.22732016444206238\n",
      "training: 89 batch 6 batch_loss: 0.22612538933753967\n",
      "training: 89 batch 7 batch_loss: 0.22182822227478027\n",
      "training: 89 batch 8 batch_loss: 0.22731196880340576\n",
      "training: 89 batch 9 batch_loss: 0.2253240942955017\n",
      "training: 89 batch 10 batch_loss: 0.2236735224723816\n",
      "training: 89 batch 11 batch_loss: 0.22573977708816528\n",
      "training: 89 batch 12 batch_loss: 0.22096025943756104\n",
      "training: 89 batch 13 batch_loss: 0.22764155268669128\n",
      "training: 89 batch 14 batch_loss: 0.22512152791023254\n",
      "training: 89 batch 15 batch_loss: 0.2252715826034546\n",
      "training: 89 batch 16 batch_loss: 0.22707489132881165\n",
      "training: 89 batch 17 batch_loss: 0.2248147428035736\n",
      "training: 89 batch 18 batch_loss: 0.22282731533050537\n",
      "training: 89 batch 19 batch_loss: 0.22717708349227905\n",
      "training: 89 batch 20 batch_loss: 0.22392544150352478\n",
      "training: 89 batch 21 batch_loss: 0.22640857100486755\n",
      "training: 89 batch 22 batch_loss: 0.22711721062660217\n",
      "training: 89 batch 23 batch_loss: 0.22222739458084106\n",
      "training: 89 batch 24 batch_loss: 0.22941359877586365\n",
      "training: 89 batch 25 batch_loss: 0.22793471813201904\n",
      "training: 89 batch 26 batch_loss: 0.226437509059906\n",
      "training: 89 batch 27 batch_loss: 0.22295793890953064\n",
      "training: 89 batch 28 batch_loss: 0.21985530853271484\n",
      "training: 89 batch 29 batch_loss: 0.2241830825805664\n",
      "training: 89 batch 30 batch_loss: 0.22502431273460388\n",
      "training: 89 batch 31 batch_loss: 0.2260933518409729\n",
      "training: 89 batch 32 batch_loss: 0.22097426652908325\n",
      "training: 89 batch 33 batch_loss: 0.22314471006393433\n",
      "training: 89 batch 34 batch_loss: 0.22440269589424133\n",
      "training: 89 batch 35 batch_loss: 0.22330284118652344\n",
      "training: 89 batch 36 batch_loss: 0.22507333755493164\n",
      "training: 89 batch 37 batch_loss: 0.22455361485481262\n",
      "training: 89 batch 38 batch_loss: 0.225230872631073\n",
      "training: 89 batch 39 batch_loss: 0.2251172959804535\n",
      "training: 89 batch 40 batch_loss: 0.22780799865722656\n",
      "training: 89 batch 41 batch_loss: 0.22660791873931885\n",
      "training: 89 batch 42 batch_loss: 0.22643709182739258\n",
      "training: 89 batch 43 batch_loss: 0.22484180331230164\n",
      "training: 89 batch 44 batch_loss: 0.2253895103931427\n",
      "training: 89 batch 45 batch_loss: 0.22620272636413574\n",
      "training: 89 batch 46 batch_loss: 0.22526216506958008\n",
      "training: 89 batch 47 batch_loss: 0.2265905737876892\n",
      "training: 89 batch 48 batch_loss: 0.22481471300125122\n",
      "training: 89 batch 49 batch_loss: 0.22489789128303528\n",
      "training: 89 batch 50 batch_loss: 0.22045114636421204\n",
      "training: 89 batch 51 batch_loss: 0.22539597749710083\n",
      "training: 89 batch 52 batch_loss: 0.22583365440368652\n",
      "training: 89 batch 53 batch_loss: 0.2246423363685608\n",
      "training: 89 batch 54 batch_loss: 0.2266141176223755\n",
      "training: 89 batch 55 batch_loss: 0.22641271352767944\n",
      "training: 89 batch 56 batch_loss: 0.22551941871643066\n",
      "training: 89 batch 57 batch_loss: 0.22903716564178467\n",
      "training: 89 batch 58 batch_loss: 0.22703954577445984\n",
      "training: 89 batch 59 batch_loss: 0.22460347414016724\n",
      "training: 89 batch 60 batch_loss: 0.22276771068572998\n",
      "training: 89 batch 61 batch_loss: 0.22329872846603394\n",
      "training: 89 batch 62 batch_loss: 0.22196334600448608\n",
      "training: 89 batch 63 batch_loss: 0.22547784447669983\n",
      "training: 89 batch 64 batch_loss: 0.22628211975097656\n",
      "training: 89 batch 65 batch_loss: 0.22525739669799805\n",
      "training: 89 batch 66 batch_loss: 0.2239987850189209\n",
      "training: 89 batch 67 batch_loss: 0.2289721667766571\n",
      "training: 89 batch 68 batch_loss: 0.22781026363372803\n",
      "training: 89 batch 69 batch_loss: 0.22492524981498718\n",
      "training: 89 batch 70 batch_loss: 0.22340786457061768\n",
      "training: 89 batch 71 batch_loss: 0.22213324904441833\n",
      "training: 89 batch 72 batch_loss: 0.22400438785552979\n",
      "training: 89 batch 73 batch_loss: 0.22570902109146118\n",
      "training: 89 batch 74 batch_loss: 0.22608071565628052\n",
      "training: 89 batch 75 batch_loss: 0.22520223259925842\n",
      "training: 89 batch 76 batch_loss: 0.2275436520576477\n",
      "training: 89 batch 77 batch_loss: 0.22408288717269897\n",
      "training: 89 batch 78 batch_loss: 0.2284429967403412\n",
      "training: 89 batch 79 batch_loss: 0.2267266809940338\n",
      "training: 89 batch 80 batch_loss: 0.22739934921264648\n",
      "training: 89 batch 81 batch_loss: 0.22904297709465027\n",
      "training: 89 batch 82 batch_loss: 0.2238316833972931\n",
      "training: 89 batch 83 batch_loss: 0.2271045446395874\n",
      "training: 89 batch 84 batch_loss: 0.2269054651260376\n",
      "training: 89 batch 85 batch_loss: 0.2265312671661377\n",
      "training: 89 batch 86 batch_loss: 0.22434929013252258\n",
      "training: 89 batch 87 batch_loss: 0.22965461015701294\n",
      "training: 89 batch 88 batch_loss: 0.22479575872421265\n",
      "training: 89 batch 89 batch_loss: 0.22593778371810913\n",
      "training: 89 batch 90 batch_loss: 0.22700610756874084\n",
      "training: 89 batch 91 batch_loss: 0.22669526934623718\n",
      "training: 89 batch 92 batch_loss: 0.22637063264846802\n",
      "training: 89 batch 93 batch_loss: 0.22678688168525696\n",
      "training: 89 batch 94 batch_loss: 0.22498494386672974\n",
      "training: 89 batch 95 batch_loss: 0.2212921380996704\n",
      "training: 89 batch 96 batch_loss: 0.22965261340141296\n",
      "training: 89 batch 97 batch_loss: 0.23189830780029297\n",
      "training: 89 batch 98 batch_loss: 0.2247999608516693\n",
      "training: 89 batch 99 batch_loss: 0.22470998764038086\n",
      "training: 89 batch 100 batch_loss: 0.2298387885093689\n",
      "training: 89 batch 101 batch_loss: 0.2334752380847931\n",
      "training: 89 batch 102 batch_loss: 0.2231847047805786\n",
      "training: 89 batch 103 batch_loss: 0.22472548484802246\n",
      "training: 89 batch 104 batch_loss: 0.22642892599105835\n",
      "training: 89 batch 105 batch_loss: 0.22477564215660095\n",
      "training: 89 batch 106 batch_loss: 0.22698181867599487\n",
      "training: 89 batch 107 batch_loss: 0.22476482391357422\n",
      "training: 89 batch 108 batch_loss: 0.22204208374023438\n",
      "training: 89 batch 109 batch_loss: 0.22219908237457275\n",
      "training: 89 batch 110 batch_loss: 0.22914844751358032\n",
      "training: 89 batch 111 batch_loss: 0.22445911169052124\n",
      "training: 89 batch 112 batch_loss: 0.2278563380241394\n",
      "training: 89 batch 113 batch_loss: 0.22682401537895203\n",
      "training: 89 batch 114 batch_loss: 0.22266802191734314\n",
      "training: 89 batch 115 batch_loss: 0.22238463163375854\n",
      "training: 89 batch 116 batch_loss: 0.22418144345283508\n",
      "training: 89 batch 117 batch_loss: 0.2283954918384552\n",
      "training: 89 batch 118 batch_loss: 0.2256270945072174\n",
      "training: 89 batch 119 batch_loss: 0.22959941625595093\n",
      "training: 89 batch 120 batch_loss: 0.22470495104789734\n",
      "training: 89 batch 121 batch_loss: 0.22816202044487\n",
      "training: 89 batch 122 batch_loss: 0.2294626533985138\n",
      "training: 89 batch 123 batch_loss: 0.22577622532844543\n",
      "training: 89 batch 124 batch_loss: 0.23151862621307373\n",
      "training: 89 batch 125 batch_loss: 0.2272157073020935\n",
      "training: 89 batch 126 batch_loss: 0.22719669342041016\n",
      "training: 89 batch 127 batch_loss: 0.22509223222732544\n",
      "training: 89 batch 128 batch_loss: 0.22346439957618713\n",
      "training: 89 batch 129 batch_loss: 0.22849082946777344\n",
      "training: 89 batch 130 batch_loss: 0.22677236795425415\n",
      "training: 89 batch 131 batch_loss: 0.22955119609832764\n",
      "training: 89 batch 132 batch_loss: 0.22702059149742126\n",
      "training: 89 batch 133 batch_loss: 0.22102507948875427\n",
      "training: 89 batch 134 batch_loss: 0.22736838459968567\n",
      "training: 89 batch 135 batch_loss: 0.22786784172058105\n",
      "training: 89 batch 136 batch_loss: 0.22679957747459412\n",
      "training: 89 batch 137 batch_loss: 0.22399133443832397\n",
      "training: 89 batch 138 batch_loss: 0.22216445207595825\n",
      "training: 89 batch 139 batch_loss: 0.22691577672958374\n",
      "training: 89 batch 140 batch_loss: 0.22177979350090027\n",
      "training: 89 batch 141 batch_loss: 0.22352346777915955\n",
      "training: 89 batch 142 batch_loss: 0.22509771585464478\n",
      "training: 89 batch 143 batch_loss: 0.22428393363952637\n",
      "training: 89 batch 144 batch_loss: 0.2250211238861084\n",
      "training: 89 batch 145 batch_loss: 0.22803574800491333\n",
      "training: 89 batch 146 batch_loss: 0.23020219802856445\n",
      "training: 89 batch 147 batch_loss: 0.22201097011566162\n",
      "training: 89 batch 148 batch_loss: 0.23032450675964355\n",
      "training: 89 batch 149 batch_loss: 0.22361761331558228\n",
      "training: 89 batch 150 batch_loss: 0.22500404715538025\n",
      "training: 89 batch 151 batch_loss: 0.2239381968975067\n",
      "training: 89 batch 152 batch_loss: 0.22548580169677734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 89 batch 153 batch_loss: 0.22812548279762268\n",
      "training: 89 batch 154 batch_loss: 0.23105618357658386\n",
      "training: 89 batch 155 batch_loss: 0.22891908884048462\n",
      "training: 89 batch 156 batch_loss: 0.22967517375946045\n",
      "training: 89 batch 157 batch_loss: 0.23022258281707764\n",
      "training: 89 batch 158 batch_loss: 0.23160845041275024\n",
      "training: 89 batch 159 batch_loss: 0.22656559944152832\n",
      "training: 89 batch 160 batch_loss: 0.22730892896652222\n",
      "training: 89 batch 161 batch_loss: 0.22600296139717102\n",
      "training: 89 batch 162 batch_loss: 0.21945706009864807\n",
      "training: 89 batch 163 batch_loss: 0.2276727557182312\n",
      "training: 89 batch 164 batch_loss: 0.22749510407447815\n",
      "training: 89 batch 165 batch_loss: 0.2256154716014862\n",
      "training: 89 batch 166 batch_loss: 0.2244960367679596\n",
      "training: 89 batch 167 batch_loss: 0.2267787754535675\n",
      "training: 89 batch 168 batch_loss: 0.23065999150276184\n",
      "training: 89 batch 169 batch_loss: 0.2309260368347168\n",
      "training: 89 batch 170 batch_loss: 0.22382009029388428\n",
      "training: 89 batch 171 batch_loss: 0.2276483178138733\n",
      "training: 89 batch 172 batch_loss: 0.22255560755729675\n",
      "training: 89 batch 173 batch_loss: 0.2263721227645874\n",
      "training: 89 batch 174 batch_loss: 0.22660130262374878\n",
      "training: 89 batch 175 batch_loss: 0.22305971384048462\n",
      "training: 89 batch 176 batch_loss: 0.22985205054283142\n",
      "training: 89 batch 177 batch_loss: 0.22298607230186462\n",
      "training: 89 batch 178 batch_loss: 0.22132578492164612\n",
      "training: 89 batch 179 batch_loss: 0.22770223021507263\n",
      "training: 89 batch 180 batch_loss: 0.22969654202461243\n",
      "training: 89 batch 181 batch_loss: 0.2239387035369873\n",
      "training: 89 batch 182 batch_loss: 0.22929993271827698\n",
      "training: 89 batch 183 batch_loss: 0.22497117519378662\n",
      "training: 89 batch 184 batch_loss: 0.22761201858520508\n",
      "training: 89 batch 185 batch_loss: 0.22740134596824646\n",
      "training: 89 batch 186 batch_loss: 0.22278666496276855\n",
      "training: 89 batch 187 batch_loss: 0.2241307497024536\n",
      "training: 89 batch 188 batch_loss: 0.2237391471862793\n",
      "training: 89 batch 189 batch_loss: 0.2266969084739685\n",
      "training: 89 batch 190 batch_loss: 0.22693514823913574\n",
      "training: 89 batch 191 batch_loss: 0.2234046459197998\n",
      "training: 89 batch 192 batch_loss: 0.22239252924919128\n",
      "training: 89 batch 193 batch_loss: 0.22288089990615845\n",
      "training: 89 batch 194 batch_loss: 0.22985509037971497\n",
      "training: 89 batch 195 batch_loss: 0.22659099102020264\n",
      "training: 89 batch 196 batch_loss: 0.23168689012527466\n",
      "training: 89 batch 197 batch_loss: 0.2261098325252533\n",
      "training: 89 batch 198 batch_loss: 0.22676825523376465\n",
      "training: 89 batch 199 batch_loss: 0.2244078516960144\n",
      "training: 89 batch 200 batch_loss: 0.22822877764701843\n",
      "training: 89 batch 201 batch_loss: 0.22576376795768738\n",
      "training: 89 batch 202 batch_loss: 0.2260286808013916\n",
      "training: 89 batch 203 batch_loss: 0.2256564497947693\n",
      "training: 89 batch 204 batch_loss: 0.22651439905166626\n",
      "training: 89 batch 205 batch_loss: 0.2294309437274933\n",
      "training: 89 batch 206 batch_loss: 0.22686100006103516\n",
      "training: 89 batch 207 batch_loss: 0.22723475098609924\n",
      "training: 89 batch 208 batch_loss: 0.22984853386878967\n",
      "training: 89 batch 209 batch_loss: 0.2256726622581482\n",
      "training: 89 batch 210 batch_loss: 0.22776055335998535\n",
      "training: 89 batch 211 batch_loss: 0.2286248505115509\n",
      "training: 89 batch 212 batch_loss: 0.2263025939464569\n",
      "training: 89 batch 213 batch_loss: 0.2243260145187378\n",
      "training: 89 batch 214 batch_loss: 0.22458291053771973\n",
      "training: 89 batch 215 batch_loss: 0.22602665424346924\n",
      "training: 89 batch 216 batch_loss: 0.22810062766075134\n",
      "training: 89 batch 217 batch_loss: 0.22596853971481323\n",
      "training: 89 batch 218 batch_loss: 0.22629019618034363\n",
      "training: 89 batch 219 batch_loss: 0.2243313491344452\n",
      "training: 89 batch 220 batch_loss: 0.22404977679252625\n",
      "training: 89 batch 221 batch_loss: 0.2222748100757599\n",
      "training: 89 batch 222 batch_loss: 0.22698014974594116\n",
      "training: 89 batch 223 batch_loss: 0.22360822558403015\n",
      "training: 89 batch 224 batch_loss: 0.22731757164001465\n",
      "training: 89 batch 225 batch_loss: 0.22869694232940674\n",
      "training: 89 batch 226 batch_loss: 0.22495552897453308\n",
      "training: 89 batch 227 batch_loss: 0.22479599714279175\n",
      "training: 89 batch 228 batch_loss: 0.2235095500946045\n",
      "training: 89 batch 229 batch_loss: 0.22314947843551636\n",
      "training: 89 batch 230 batch_loss: 0.2293533980846405\n",
      "training: 89 batch 231 batch_loss: 0.22617802023887634\n",
      "training: 89 batch 232 batch_loss: 0.22405976057052612\n",
      "training: 89 batch 233 batch_loss: 0.22697830200195312\n",
      "training: 89 batch 234 batch_loss: 0.2239968478679657\n",
      "training: 89 batch 235 batch_loss: 0.22591835260391235\n",
      "training: 89 batch 236 batch_loss: 0.23465988039970398\n",
      "training: 89 batch 237 batch_loss: 0.22529089450836182\n",
      "training: 89 batch 238 batch_loss: 0.23095768690109253\n",
      "training: 89 batch 239 batch_loss: 0.2233370840549469\n",
      "training: 89 batch 240 batch_loss: 0.23075082898139954\n",
      "training: 89 batch 241 batch_loss: 0.22285181283950806\n",
      "training: 89 batch 242 batch_loss: 0.22694498300552368\n",
      "training: 89 batch 243 batch_loss: 0.2245229184627533\n",
      "training: 89 batch 244 batch_loss: 0.22792750597000122\n",
      "training: 89 batch 245 batch_loss: 0.22662585973739624\n",
      "training: 89 batch 246 batch_loss: 0.229263037443161\n",
      "training: 89 batch 247 batch_loss: 0.22555682063102722\n",
      "training: 89 batch 248 batch_loss: 0.2257230281829834\n",
      "training: 89 batch 249 batch_loss: 0.2236497402191162\n",
      "training: 89 batch 250 batch_loss: 0.22892361879348755\n",
      "training: 89 batch 251 batch_loss: 0.2252216339111328\n",
      "training: 89 batch 252 batch_loss: 0.227250337600708\n",
      "training: 89 batch 253 batch_loss: 0.22838616371154785\n",
      "training: 89 batch 254 batch_loss: 0.22848549485206604\n",
      "training: 89 batch 255 batch_loss: 0.2236858606338501\n",
      "training: 89 batch 256 batch_loss: 0.22550147771835327\n",
      "training: 89 batch 257 batch_loss: 0.23045793175697327\n",
      "training: 89 batch 258 batch_loss: 0.22924134135246277\n",
      "training: 89 batch 259 batch_loss: 0.23037603497505188\n",
      "training: 89 batch 260 batch_loss: 0.22673767805099487\n",
      "training: 89 batch 261 batch_loss: 0.22507449984550476\n",
      "training: 89 batch 262 batch_loss: 0.23005881905555725\n",
      "training: 89 batch 263 batch_loss: 0.22564858198165894\n",
      "training: 89 batch 264 batch_loss: 0.22890788316726685\n",
      "training: 89 batch 265 batch_loss: 0.23083198070526123\n",
      "training: 89 batch 266 batch_loss: 0.22796142101287842\n",
      "training: 89 batch 267 batch_loss: 0.22575676441192627\n",
      "training: 89 batch 268 batch_loss: 0.23028701543807983\n",
      "training: 89 batch 269 batch_loss: 0.2289423644542694\n",
      "training: 89 batch 270 batch_loss: 0.22784027457237244\n",
      "training: 89 batch 271 batch_loss: 0.2291654348373413\n",
      "training: 89 batch 272 batch_loss: 0.22796380519866943\n",
      "training: 89 batch 273 batch_loss: 0.22780799865722656\n",
      "training: 89 batch 274 batch_loss: 0.228701651096344\n",
      "training: 89 batch 275 batch_loss: 0.22898325324058533\n",
      "training: 89 batch 276 batch_loss: 0.22990357875823975\n",
      "training: 89 batch 277 batch_loss: 0.22641798853874207\n",
      "training: 89 batch 278 batch_loss: 0.22486263513565063\n",
      "training: 89 batch 279 batch_loss: 0.22570547461509705\n",
      "training: 89 batch 280 batch_loss: 0.222976416349411\n",
      "training: 89 batch 281 batch_loss: 0.22535794973373413\n",
      "training: 89 batch 282 batch_loss: 0.2292773425579071\n",
      "training: 89 batch 283 batch_loss: 0.22622579336166382\n",
      "training: 89 batch 284 batch_loss: 0.22709646821022034\n",
      "training: 89 batch 285 batch_loss: 0.22974151372909546\n",
      "training: 89 batch 286 batch_loss: 0.2249068021774292\n",
      "training: 89 batch 287 batch_loss: 0.22859954833984375\n",
      "training: 89 batch 288 batch_loss: 0.23241809010505676\n",
      "training: 89 batch 289 batch_loss: 0.2215009331703186\n",
      "training: 89 batch 290 batch_loss: 0.22983002662658691\n",
      "training: 89 batch 291 batch_loss: 0.22904270887374878\n",
      "training: 89 batch 292 batch_loss: 0.22423574328422546\n",
      "training: 89 batch 293 batch_loss: 0.22897091507911682\n",
      "training: 89 batch 294 batch_loss: 0.22331318259239197\n",
      "training: 89 batch 295 batch_loss: 0.22618317604064941\n",
      "training: 89 batch 296 batch_loss: 0.22706735134124756\n",
      "training: 89 batch 297 batch_loss: 0.22763589024543762\n",
      "training: 89 batch 298 batch_loss: 0.22499090433120728\n",
      "training: 89 batch 299 batch_loss: 0.22702205181121826\n",
      "training: 89 batch 300 batch_loss: 0.2287587821483612\n",
      "training: 89 batch 301 batch_loss: 0.2239568531513214\n",
      "training: 89 batch 302 batch_loss: 0.23111560940742493\n",
      "training: 89 batch 303 batch_loss: 0.2238290011882782\n",
      "training: 89 batch 304 batch_loss: 0.22514158487319946\n",
      "training: 89 batch 305 batch_loss: 0.2309545874595642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 89 batch 306 batch_loss: 0.2305140495300293\n",
      "training: 89 batch 307 batch_loss: 0.22217851877212524\n",
      "training: 89 batch 308 batch_loss: 0.22886615991592407\n",
      "training: 89 batch 309 batch_loss: 0.22817394137382507\n",
      "training: 89 batch 310 batch_loss: 0.22743552923202515\n",
      "training: 89 batch 311 batch_loss: 0.22954028844833374\n",
      "training: 89 batch 312 batch_loss: 0.22842109203338623\n",
      "training: 89 batch 313 batch_loss: 0.23008620738983154\n",
      "training: 89 batch 314 batch_loss: 0.22740280628204346\n",
      "training: 89 batch 315 batch_loss: 0.22633761167526245\n",
      "training: 89 batch 316 batch_loss: 0.22533300518989563\n",
      "training: 89 batch 317 batch_loss: 0.22853869199752808\n",
      "training: 89 batch 318 batch_loss: 0.22839507460594177\n",
      "training: 89 batch 319 batch_loss: 0.23009327054023743\n",
      "training: 89 batch 320 batch_loss: 0.2309175431728363\n",
      "training: 89 batch 321 batch_loss: 0.22666817903518677\n",
      "training: 89 batch 322 batch_loss: 0.22944411635398865\n",
      "training: 89 batch 323 batch_loss: 0.2322659194469452\n",
      "training: 89 batch 324 batch_loss: 0.22668451070785522\n",
      "training: 89 batch 325 batch_loss: 0.2302379012107849\n",
      "training: 89 batch 326 batch_loss: 0.22332030534744263\n",
      "training: 89 batch 327 batch_loss: 0.22463303804397583\n",
      "training: 89 batch 328 batch_loss: 0.22705397009849548\n",
      "training: 89 batch 329 batch_loss: 0.2273171842098236\n",
      "training: 89 batch 330 batch_loss: 0.22623759508132935\n",
      "training: 89 batch 331 batch_loss: 0.22608965635299683\n",
      "training: 89 batch 332 batch_loss: 0.22693279385566711\n",
      "training: 89 batch 333 batch_loss: 0.22944507002830505\n",
      "training: 89 batch 334 batch_loss: 0.22952550649642944\n",
      "training: 89 batch 335 batch_loss: 0.22533029317855835\n",
      "training: 89 batch 336 batch_loss: 0.22731047868728638\n",
      "training: 89 batch 337 batch_loss: 0.23384243249893188\n",
      "training: 89 batch 338 batch_loss: 0.22477471828460693\n",
      "training: 89 batch 339 batch_loss: 0.22547906637191772\n",
      "training: 89 batch 340 batch_loss: 0.2283938229084015\n",
      "training: 89 batch 341 batch_loss: 0.22765636444091797\n",
      "training: 89 batch 342 batch_loss: 0.22627651691436768\n",
      "training: 89 batch 343 batch_loss: 0.22426027059555054\n",
      "training: 89 batch 344 batch_loss: 0.23307007551193237\n",
      "training: 89 batch 345 batch_loss: 0.2236475646495819\n",
      "training: 89 batch 346 batch_loss: 0.22656399011611938\n",
      "training: 89 batch 347 batch_loss: 0.23140770196914673\n",
      "training: 89 batch 348 batch_loss: 0.22476261854171753\n",
      "training: 89 batch 349 batch_loss: 0.2253074049949646\n",
      "training: 89 batch 350 batch_loss: 0.2274918258190155\n",
      "training: 89 batch 351 batch_loss: 0.22785452008247375\n",
      "training: 89 batch 352 batch_loss: 0.22271737456321716\n",
      "training: 89 batch 353 batch_loss: 0.22795963287353516\n",
      "training: 89 batch 354 batch_loss: 0.2242889702320099\n",
      "training: 89 batch 355 batch_loss: 0.22883930802345276\n",
      "training: 89 batch 356 batch_loss: 0.22866836190223694\n",
      "training: 89 batch 357 batch_loss: 0.2264365255832672\n",
      "training: 89 batch 358 batch_loss: 0.22579368948936462\n",
      "training: 89 batch 359 batch_loss: 0.22940707206726074\n",
      "training: 89 batch 360 batch_loss: 0.22616451978683472\n",
      "training: 89 batch 361 batch_loss: 0.22955235838890076\n",
      "training: 89 batch 362 batch_loss: 0.22180500626564026\n",
      "training: 89 batch 363 batch_loss: 0.22900742292404175\n",
      "training: 89 batch 364 batch_loss: 0.233878493309021\n",
      "training: 89 batch 365 batch_loss: 0.22976961731910706\n",
      "training: 89 batch 366 batch_loss: 0.22954314947128296\n",
      "training: 89 batch 367 batch_loss: 0.23104801774024963\n",
      "training: 89 batch 368 batch_loss: 0.22697532176971436\n",
      "training: 89 batch 369 batch_loss: 0.2270139455795288\n",
      "training: 89 batch 370 batch_loss: 0.22693964838981628\n",
      "training: 89 batch 371 batch_loss: 0.22807550430297852\n",
      "training: 89 batch 372 batch_loss: 0.2302488386631012\n",
      "training: 89 batch 373 batch_loss: 0.2317315638065338\n",
      "training: 89 batch 374 batch_loss: 0.2237376570701599\n",
      "training: 89 batch 375 batch_loss: 0.22767946124076843\n",
      "training: 89 batch 376 batch_loss: 0.23252049088478088\n",
      "training: 89 batch 377 batch_loss: 0.22386690974235535\n",
      "training: 89 batch 378 batch_loss: 0.22400471568107605\n",
      "training: 89 batch 379 batch_loss: 0.22541791200637817\n",
      "training: 89 batch 380 batch_loss: 0.23129239678382874\n",
      "training: 89 batch 381 batch_loss: 0.23126506805419922\n",
      "training: 89 batch 382 batch_loss: 0.23017802834510803\n",
      "training: 89 batch 383 batch_loss: 0.2283022701740265\n",
      "training: 89 batch 384 batch_loss: 0.22639960050582886\n",
      "training: 89 batch 385 batch_loss: 0.22840428352355957\n",
      "training: 89 batch 386 batch_loss: 0.22608065605163574\n",
      "training: 89 batch 387 batch_loss: 0.2314034104347229\n",
      "training: 89 batch 388 batch_loss: 0.22826915979385376\n",
      "training: 89 batch 389 batch_loss: 0.23248326778411865\n",
      "training: 89 batch 390 batch_loss: 0.23113331198692322\n",
      "training: 89 batch 391 batch_loss: 0.23043453693389893\n",
      "training: 89 batch 392 batch_loss: 0.22063687443733215\n",
      "training: 89 batch 393 batch_loss: 0.22905057668685913\n",
      "training: 89 batch 394 batch_loss: 0.22794854640960693\n",
      "training: 89 batch 395 batch_loss: 0.22868195176124573\n",
      "training: 89 batch 396 batch_loss: 0.22314271330833435\n",
      "training: 89 batch 397 batch_loss: 0.2290153205394745\n",
      "training: 89 batch 398 batch_loss: 0.22746780514717102\n",
      "training: 89 batch 399 batch_loss: 0.2263297736644745\n",
      "training: 89 batch 400 batch_loss: 0.23355066776275635\n",
      "training: 89 batch 401 batch_loss: 0.22317162156105042\n",
      "training: 89 batch 402 batch_loss: 0.22822722792625427\n",
      "training: 89 batch 403 batch_loss: 0.2281586229801178\n",
      "training: 89 batch 404 batch_loss: 0.2278851866722107\n",
      "training: 89 batch 405 batch_loss: 0.23169457912445068\n",
      "training: 89 batch 406 batch_loss: 0.2273215353488922\n",
      "training: 89 batch 407 batch_loss: 0.22715246677398682\n",
      "training: 89 batch 408 batch_loss: 0.2285182774066925\n",
      "training: 89 batch 409 batch_loss: 0.2231806516647339\n",
      "training: 89 batch 410 batch_loss: 0.23198044300079346\n",
      "training: 89 batch 411 batch_loss: 0.22772377729415894\n",
      "training: 89 batch 412 batch_loss: 0.22528332471847534\n",
      "training: 89 batch 413 batch_loss: 0.22679919004440308\n",
      "training: 89 batch 414 batch_loss: 0.23062101006507874\n",
      "training: 89 batch 415 batch_loss: 0.22860631346702576\n",
      "training: 89 batch 416 batch_loss: 0.22830039262771606\n",
      "training: 89 batch 417 batch_loss: 0.22852522134780884\n",
      "training: 89 batch 418 batch_loss: 0.22515344619750977\n",
      "training: 89 batch 419 batch_loss: 0.22881323099136353\n",
      "training: 89 batch 420 batch_loss: 0.22989574074745178\n",
      "training: 89 batch 421 batch_loss: 0.22856533527374268\n",
      "training: 89 batch 422 batch_loss: 0.2265664041042328\n",
      "training: 89 batch 423 batch_loss: 0.23332631587982178\n",
      "training: 89 batch 424 batch_loss: 0.22760751843452454\n",
      "training: 89 batch 425 batch_loss: 0.22502759099006653\n",
      "training: 89 batch 426 batch_loss: 0.23054680228233337\n",
      "training: 89 batch 427 batch_loss: 0.2242583930492401\n",
      "training: 89 batch 428 batch_loss: 0.2280929684638977\n",
      "training: 89 batch 429 batch_loss: 0.22880032658576965\n",
      "training: 89 batch 430 batch_loss: 0.2256861925125122\n",
      "training: 89 batch 431 batch_loss: 0.22504547238349915\n",
      "training: 89 batch 432 batch_loss: 0.23093384504318237\n",
      "training: 89 batch 433 batch_loss: 0.22989463806152344\n",
      "training: 89 batch 434 batch_loss: 0.23014426231384277\n",
      "training: 89 batch 435 batch_loss: 0.2266557216644287\n",
      "training: 89 batch 436 batch_loss: 0.22075551748275757\n",
      "training: 89 batch 437 batch_loss: 0.22348687052726746\n",
      "training: 89 batch 438 batch_loss: 0.22347992658615112\n",
      "training: 89 batch 439 batch_loss: 0.23128372430801392\n",
      "training: 89 batch 440 batch_loss: 0.23039710521697998\n",
      "training: 89 batch 441 batch_loss: 0.22634825110435486\n",
      "training: 89 batch 442 batch_loss: 0.22545558214187622\n",
      "training: 89 batch 443 batch_loss: 0.23179790377616882\n",
      "training: 89 batch 444 batch_loss: 0.22684839367866516\n",
      "training: 89 batch 445 batch_loss: 0.23005983233451843\n",
      "training: 89 batch 446 batch_loss: 0.23157057166099548\n",
      "training: 89 batch 447 batch_loss: 0.22583690285682678\n",
      "training: 89 batch 448 batch_loss: 0.22945833206176758\n",
      "training: 89 batch 449 batch_loss: 0.22664111852645874\n",
      "training: 89 batch 450 batch_loss: 0.22848764061927795\n",
      "training: 89 batch 451 batch_loss: 0.22382217645645142\n",
      "training: 89 batch 452 batch_loss: 0.22905376553535461\n",
      "training: 89 batch 453 batch_loss: 0.230772465467453\n",
      "training: 89 batch 454 batch_loss: 0.23123827576637268\n",
      "training: 89 batch 455 batch_loss: 0.22332113981246948\n",
      "training: 89 batch 456 batch_loss: 0.22746562957763672\n",
      "training: 89 batch 457 batch_loss: 0.2273198366165161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 89 batch 458 batch_loss: 0.23200935125350952\n",
      "training: 89 batch 459 batch_loss: 0.22802478075027466\n",
      "training: 89 batch 460 batch_loss: 0.2307078242301941\n",
      "training: 89 batch 461 batch_loss: 0.2337643802165985\n",
      "training: 89 batch 462 batch_loss: 0.22701367735862732\n",
      "training: 89 batch 463 batch_loss: 0.22813335061073303\n",
      "training: 89 batch 464 batch_loss: 0.22953209280967712\n",
      "training: 89 batch 465 batch_loss: 0.23013481497764587\n",
      "training: 89 batch 466 batch_loss: 0.22671425342559814\n",
      "training: 89 batch 467 batch_loss: 0.2307141125202179\n",
      "training: 89 batch 468 batch_loss: 0.22503191232681274\n",
      "training: 89 batch 469 batch_loss: 0.22894138097763062\n",
      "training: 89 batch 470 batch_loss: 0.2253517508506775\n",
      "training: 89 batch 471 batch_loss: 0.22828102111816406\n",
      "training: 89 batch 472 batch_loss: 0.22659286856651306\n",
      "training: 89 batch 473 batch_loss: 0.22863048315048218\n",
      "training: 89 batch 474 batch_loss: 0.22405177354812622\n",
      "training: 89 batch 475 batch_loss: 0.2225334644317627\n",
      "training: 89 batch 476 batch_loss: 0.23073402047157288\n",
      "training: 89 batch 477 batch_loss: 0.23045554757118225\n",
      "training: 89 batch 478 batch_loss: 0.2286691963672638\n",
      "training: 89 batch 479 batch_loss: 0.22242221236228943\n",
      "training: 89 batch 480 batch_loss: 0.22803866863250732\n",
      "training: 89 batch 481 batch_loss: 0.22691017389297485\n",
      "training: 89 batch 482 batch_loss: 0.22474601864814758\n",
      "training: 89 batch 483 batch_loss: 0.22291767597198486\n",
      "training: 89 batch 484 batch_loss: 0.23025429248809814\n",
      "training: 89 batch 485 batch_loss: 0.22616785764694214\n",
      "training: 89 batch 486 batch_loss: 0.2329212725162506\n",
      "training: 89 batch 487 batch_loss: 0.2273540496826172\n",
      "training: 89 batch 488 batch_loss: 0.22501766681671143\n",
      "training: 89 batch 489 batch_loss: 0.23022830486297607\n",
      "training: 89 batch 490 batch_loss: 0.22828984260559082\n",
      "training: 89 batch 491 batch_loss: 0.2239702343940735\n",
      "training: 89 batch 492 batch_loss: 0.23013317584991455\n",
      "training: 89 batch 493 batch_loss: 0.2261989414691925\n",
      "training: 89 batch 494 batch_loss: 0.228106290102005\n",
      "training: 89 batch 495 batch_loss: 0.2270125448703766\n",
      "training: 89 batch 496 batch_loss: 0.22538530826568604\n",
      "training: 89 batch 497 batch_loss: 0.22906532883644104\n",
      "training: 89 batch 498 batch_loss: 0.23114514350891113\n",
      "training: 89 batch 499 batch_loss: 0.2322010099887848\n",
      "training: 89 batch 500 batch_loss: 0.22690564393997192\n",
      "training: 89 batch 501 batch_loss: 0.2273038923740387\n",
      "training: 89 batch 502 batch_loss: 0.22759994864463806\n",
      "training: 89 batch 503 batch_loss: 0.22687238454818726\n",
      "training: 89 batch 504 batch_loss: 0.22532719373703003\n",
      "training: 89 batch 505 batch_loss: 0.22938674688339233\n",
      "training: 89 batch 506 batch_loss: 0.2274450957775116\n",
      "training: 89 batch 507 batch_loss: 0.23172274231910706\n",
      "training: 89 batch 508 batch_loss: 0.224929541349411\n",
      "training: 89 batch 509 batch_loss: 0.23065394163131714\n",
      "training: 89 batch 510 batch_loss: 0.22404330968856812\n",
      "training: 89 batch 511 batch_loss: 0.22581747174263\n",
      "training: 89 batch 512 batch_loss: 0.23296350240707397\n",
      "training: 89 batch 513 batch_loss: 0.22472023963928223\n",
      "training: 89 batch 514 batch_loss: 0.22990235686302185\n",
      "training: 89 batch 515 batch_loss: 0.22610464692115784\n",
      "training: 89 batch 516 batch_loss: 0.22791850566864014\n",
      "training: 89 batch 517 batch_loss: 0.22974255681037903\n",
      "training: 89 batch 518 batch_loss: 0.2274061143398285\n",
      "training: 89 batch 519 batch_loss: 0.2256336808204651\n",
      "training: 89 batch 520 batch_loss: 0.22478771209716797\n",
      "training: 89 batch 521 batch_loss: 0.2260703444480896\n",
      "training: 89 batch 522 batch_loss: 0.22760885953903198\n",
      "training: 89 batch 523 batch_loss: 0.22421151399612427\n",
      "training: 89 batch 524 batch_loss: 0.23409965634346008\n",
      "training: 89 batch 525 batch_loss: 0.23345723748207092\n",
      "training: 89 batch 526 batch_loss: 0.2285422384738922\n",
      "training: 89 batch 527 batch_loss: 0.22972020506858826\n",
      "training: 89 batch 528 batch_loss: 0.23007076978683472\n",
      "training: 89 batch 529 batch_loss: 0.22206854820251465\n",
      "training: 89 batch 530 batch_loss: 0.2238498330116272\n",
      "training: 89 batch 531 batch_loss: 0.23211824893951416\n",
      "training: 89 batch 532 batch_loss: 0.22545799612998962\n",
      "training: 89 batch 533 batch_loss: 0.22903475165367126\n",
      "training: 89 batch 534 batch_loss: 0.22967711091041565\n",
      "training: 89 batch 535 batch_loss: 0.22901171445846558\n",
      "training: 89 batch 536 batch_loss: 0.2300489842891693\n",
      "training: 89 batch 537 batch_loss: 0.22708141803741455\n",
      "training: 89 batch 538 batch_loss: 0.23325097560882568\n",
      "training: 89 batch 539 batch_loss: 0.22678083181381226\n",
      "training: 89 batch 540 batch_loss: 0.22813326120376587\n",
      "training: 89 batch 541 batch_loss: 0.2292560338973999\n",
      "training: 89 batch 542 batch_loss: 0.2275732159614563\n",
      "training: 89 batch 543 batch_loss: 0.2299240231513977\n",
      "training: 89 batch 544 batch_loss: 0.22592580318450928\n",
      "training: 89 batch 545 batch_loss: 0.22331702709197998\n",
      "training: 89 batch 546 batch_loss: 0.22819918394088745\n",
      "training: 89 batch 547 batch_loss: 0.22720584273338318\n",
      "training: 89 batch 548 batch_loss: 0.22850754857063293\n",
      "training: 89 batch 549 batch_loss: 0.22501546144485474\n",
      "training: 89 batch 550 batch_loss: 0.23129487037658691\n",
      "training: 89 batch 551 batch_loss: 0.23150107264518738\n",
      "training: 89 batch 552 batch_loss: 0.23095399141311646\n",
      "training: 89 batch 553 batch_loss: 0.22451984882354736\n",
      "training: 89 batch 554 batch_loss: 0.2273866832256317\n",
      "training: 89 batch 555 batch_loss: 0.22718673944473267\n",
      "training: 89 batch 556 batch_loss: 0.22968775033950806\n",
      "training: 89 batch 557 batch_loss: 0.2262379229068756\n",
      "training: 89 batch 558 batch_loss: 0.23188218474388123\n",
      "training: 89 batch 559 batch_loss: 0.23094868659973145\n",
      "training: 89 batch 560 batch_loss: 0.22955858707427979\n",
      "training: 89 batch 561 batch_loss: 0.22858911752700806\n",
      "training: 89 batch 562 batch_loss: 0.22900348901748657\n",
      "training: 89 batch 563 batch_loss: 0.22694101929664612\n",
      "training: 89 batch 564 batch_loss: 0.2311813235282898\n",
      "training: 89 batch 565 batch_loss: 0.22709348797798157\n",
      "training: 89 batch 566 batch_loss: 0.2288302779197693\n",
      "training: 89 batch 567 batch_loss: 0.23094356060028076\n",
      "training: 89 batch 568 batch_loss: 0.2269364595413208\n",
      "training: 89 batch 569 batch_loss: 0.23047146201133728\n",
      "training: 89 batch 570 batch_loss: 0.228378027677536\n",
      "training: 89 batch 571 batch_loss: 0.2281607985496521\n",
      "training: 89 batch 572 batch_loss: 0.224331796169281\n",
      "training: 89 batch 573 batch_loss: 0.22771099209785461\n",
      "training: 89 batch 574 batch_loss: 0.22577691078186035\n",
      "training: 89 batch 575 batch_loss: 0.23028424382209778\n",
      "training: 89 batch 576 batch_loss: 0.23120757937431335\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 89, Hit Ratio:0.03403810215428075 | Precision:0.0502211736950752 | Recall:0.06611383554232865 | NDCG:0.06552353565670299\n",
      "*Best Performance* \n",
      "Epoch: 87, Hit Ratio:0.03409140119856226 | Precision:0.050299813231101935 | Recall:0.06623474467749231 | MDCG:0.06544512792267772\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 90 batch 0 batch_loss: 0.22775942087173462\n",
      "training: 90 batch 1 batch_loss: 0.22518295049667358\n",
      "training: 90 batch 2 batch_loss: 0.22979283332824707\n",
      "training: 90 batch 3 batch_loss: 0.22499623894691467\n",
      "training: 90 batch 4 batch_loss: 0.22965306043624878\n",
      "training: 90 batch 5 batch_loss: 0.22707027196884155\n",
      "training: 90 batch 6 batch_loss: 0.22848224639892578\n",
      "training: 90 batch 7 batch_loss: 0.22380119562149048\n",
      "training: 90 batch 8 batch_loss: 0.22807911038398743\n",
      "training: 90 batch 9 batch_loss: 0.22476953268051147\n",
      "training: 90 batch 10 batch_loss: 0.22566643357276917\n",
      "training: 90 batch 11 batch_loss: 0.22702959179878235\n",
      "training: 90 batch 12 batch_loss: 0.2262171506881714\n",
      "training: 90 batch 13 batch_loss: 0.22933030128479004\n",
      "training: 90 batch 14 batch_loss: 0.2261216938495636\n",
      "training: 90 batch 15 batch_loss: 0.22836613655090332\n",
      "training: 90 batch 16 batch_loss: 0.2232111692428589\n",
      "training: 90 batch 17 batch_loss: 0.2228544056415558\n",
      "training: 90 batch 18 batch_loss: 0.2260698676109314\n",
      "training: 90 batch 19 batch_loss: 0.22386980056762695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 90 batch 20 batch_loss: 0.2242206335067749\n",
      "training: 90 batch 21 batch_loss: 0.22290292382240295\n",
      "training: 90 batch 22 batch_loss: 0.22582241892814636\n",
      "training: 90 batch 23 batch_loss: 0.22935190796852112\n",
      "training: 90 batch 24 batch_loss: 0.2285040020942688\n",
      "training: 90 batch 25 batch_loss: 0.22353893518447876\n",
      "training: 90 batch 26 batch_loss: 0.22731751203536987\n",
      "training: 90 batch 27 batch_loss: 0.22506508231163025\n",
      "training: 90 batch 28 batch_loss: 0.22308349609375\n",
      "training: 90 batch 29 batch_loss: 0.22668367624282837\n",
      "training: 90 batch 30 batch_loss: 0.22820279002189636\n",
      "training: 90 batch 31 batch_loss: 0.22453749179840088\n",
      "training: 90 batch 32 batch_loss: 0.22703427076339722\n",
      "training: 90 batch 33 batch_loss: 0.22473302483558655\n",
      "training: 90 batch 34 batch_loss: 0.23435714840888977\n",
      "training: 90 batch 35 batch_loss: 0.22827193140983582\n",
      "training: 90 batch 36 batch_loss: 0.22486266493797302\n",
      "training: 90 batch 37 batch_loss: 0.22377872467041016\n",
      "training: 90 batch 38 batch_loss: 0.22872021794319153\n",
      "training: 90 batch 39 batch_loss: 0.2270987629890442\n",
      "training: 90 batch 40 batch_loss: 0.22518867254257202\n",
      "training: 90 batch 41 batch_loss: 0.22507360577583313\n",
      "training: 90 batch 42 batch_loss: 0.22944962978363037\n",
      "training: 90 batch 43 batch_loss: 0.2277684211730957\n",
      "training: 90 batch 44 batch_loss: 0.22286027669906616\n",
      "training: 90 batch 45 batch_loss: 0.22237974405288696\n",
      "training: 90 batch 46 batch_loss: 0.22809091210365295\n",
      "training: 90 batch 47 batch_loss: 0.22128865122795105\n",
      "training: 90 batch 48 batch_loss: 0.22315829992294312\n",
      "training: 90 batch 49 batch_loss: 0.22407853603363037\n",
      "training: 90 batch 50 batch_loss: 0.2288408875465393\n",
      "training: 90 batch 51 batch_loss: 0.22487172484397888\n",
      "training: 90 batch 52 batch_loss: 0.22850516438484192\n",
      "training: 90 batch 53 batch_loss: 0.2241266369819641\n",
      "training: 90 batch 54 batch_loss: 0.22216325998306274\n",
      "training: 90 batch 55 batch_loss: 0.22849738597869873\n",
      "training: 90 batch 56 batch_loss: 0.2220238745212555\n",
      "training: 90 batch 57 batch_loss: 0.2292805016040802\n",
      "training: 90 batch 58 batch_loss: 0.22881558537483215\n",
      "training: 90 batch 59 batch_loss: 0.22673934698104858\n",
      "training: 90 batch 60 batch_loss: 0.2260858714580536\n",
      "training: 90 batch 61 batch_loss: 0.22961869835853577\n",
      "training: 90 batch 62 batch_loss: 0.22273457050323486\n",
      "training: 90 batch 63 batch_loss: 0.22974342107772827\n",
      "training: 90 batch 64 batch_loss: 0.22919946908950806\n",
      "training: 90 batch 65 batch_loss: 0.22368279099464417\n",
      "training: 90 batch 66 batch_loss: 0.22676420211791992\n",
      "training: 90 batch 67 batch_loss: 0.22573989629745483\n",
      "training: 90 batch 68 batch_loss: 0.224776029586792\n",
      "training: 90 batch 69 batch_loss: 0.2252364158630371\n",
      "training: 90 batch 70 batch_loss: 0.22847363352775574\n",
      "training: 90 batch 71 batch_loss: 0.22248408198356628\n",
      "training: 90 batch 72 batch_loss: 0.22677835822105408\n",
      "training: 90 batch 73 batch_loss: 0.22365865111351013\n",
      "training: 90 batch 74 batch_loss: 0.22480779886245728\n",
      "training: 90 batch 75 batch_loss: 0.22630399465560913\n",
      "training: 90 batch 76 batch_loss: 0.22112298011779785\n",
      "training: 90 batch 77 batch_loss: 0.22865360975265503\n",
      "training: 90 batch 78 batch_loss: 0.22718223929405212\n",
      "training: 90 batch 79 batch_loss: 0.22915580868721008\n",
      "training: 90 batch 80 batch_loss: 0.22748154401779175\n",
      "training: 90 batch 81 batch_loss: 0.227992445230484\n",
      "training: 90 batch 82 batch_loss: 0.22120633721351624\n",
      "training: 90 batch 83 batch_loss: 0.2243470549583435\n",
      "training: 90 batch 84 batch_loss: 0.22834572196006775\n",
      "training: 90 batch 85 batch_loss: 0.23054182529449463\n",
      "training: 90 batch 86 batch_loss: 0.22914475202560425\n",
      "training: 90 batch 87 batch_loss: 0.22684085369110107\n",
      "training: 90 batch 88 batch_loss: 0.22743093967437744\n",
      "training: 90 batch 89 batch_loss: 0.22195899486541748\n",
      "training: 90 batch 90 batch_loss: 0.2279212474822998\n",
      "training: 90 batch 91 batch_loss: 0.2320348024368286\n",
      "training: 90 batch 92 batch_loss: 0.228010356426239\n",
      "training: 90 batch 93 batch_loss: 0.22248905897140503\n",
      "training: 90 batch 94 batch_loss: 0.23008573055267334\n",
      "training: 90 batch 95 batch_loss: 0.22529208660125732\n",
      "training: 90 batch 96 batch_loss: 0.22312182188034058\n",
      "training: 90 batch 97 batch_loss: 0.23059964179992676\n",
      "training: 90 batch 98 batch_loss: 0.22538506984710693\n",
      "training: 90 batch 99 batch_loss: 0.22818857431411743\n",
      "training: 90 batch 100 batch_loss: 0.22478777170181274\n",
      "training: 90 batch 101 batch_loss: 0.22332125902175903\n",
      "training: 90 batch 102 batch_loss: 0.22668766975402832\n",
      "training: 90 batch 103 batch_loss: 0.22878411412239075\n",
      "training: 90 batch 104 batch_loss: 0.2236720323562622\n",
      "training: 90 batch 105 batch_loss: 0.22925397753715515\n",
      "training: 90 batch 106 batch_loss: 0.2328527867794037\n",
      "training: 90 batch 107 batch_loss: 0.22500425577163696\n",
      "training: 90 batch 108 batch_loss: 0.22730836272239685\n",
      "training: 90 batch 109 batch_loss: 0.22656309604644775\n",
      "training: 90 batch 110 batch_loss: 0.2293848991394043\n",
      "training: 90 batch 111 batch_loss: 0.22664648294448853\n",
      "training: 90 batch 112 batch_loss: 0.2277180254459381\n",
      "training: 90 batch 113 batch_loss: 0.22659048438072205\n",
      "training: 90 batch 114 batch_loss: 0.2260657548904419\n",
      "training: 90 batch 115 batch_loss: 0.22549590468406677\n",
      "training: 90 batch 116 batch_loss: 0.22928684949874878\n",
      "training: 90 batch 117 batch_loss: 0.2282264232635498\n",
      "training: 90 batch 118 batch_loss: 0.22422271966934204\n",
      "training: 90 batch 119 batch_loss: 0.22883117198944092\n",
      "training: 90 batch 120 batch_loss: 0.2223493754863739\n",
      "training: 90 batch 121 batch_loss: 0.2291620671749115\n",
      "training: 90 batch 122 batch_loss: 0.2294832468032837\n",
      "training: 90 batch 123 batch_loss: 0.22292447090148926\n",
      "training: 90 batch 124 batch_loss: 0.2270146906375885\n",
      "training: 90 batch 125 batch_loss: 0.2257726490497589\n",
      "training: 90 batch 126 batch_loss: 0.23190784454345703\n",
      "training: 90 batch 127 batch_loss: 0.22519341111183167\n",
      "training: 90 batch 128 batch_loss: 0.2242012619972229\n",
      "training: 90 batch 129 batch_loss: 0.22328326106071472\n",
      "training: 90 batch 130 batch_loss: 0.22743481397628784\n",
      "training: 90 batch 131 batch_loss: 0.22936615347862244\n",
      "training: 90 batch 132 batch_loss: 0.22979533672332764\n",
      "training: 90 batch 133 batch_loss: 0.2320590317249298\n",
      "training: 90 batch 134 batch_loss: 0.2289825677871704\n",
      "training: 90 batch 135 batch_loss: 0.22709780931472778\n",
      "training: 90 batch 136 batch_loss: 0.23127055168151855\n",
      "training: 90 batch 137 batch_loss: 0.2238054871559143\n",
      "training: 90 batch 138 batch_loss: 0.22217321395874023\n",
      "training: 90 batch 139 batch_loss: 0.22888466715812683\n",
      "training: 90 batch 140 batch_loss: 0.22946205735206604\n",
      "training: 90 batch 141 batch_loss: 0.22420597076416016\n",
      "training: 90 batch 142 batch_loss: 0.2211642563343048\n",
      "training: 90 batch 143 batch_loss: 0.22886738181114197\n",
      "training: 90 batch 144 batch_loss: 0.22550272941589355\n",
      "training: 90 batch 145 batch_loss: 0.22547000646591187\n",
      "training: 90 batch 146 batch_loss: 0.22841891646385193\n",
      "training: 90 batch 147 batch_loss: 0.2247522473335266\n",
      "training: 90 batch 148 batch_loss: 0.2291528582572937\n",
      "training: 90 batch 149 batch_loss: 0.2275559902191162\n",
      "training: 90 batch 150 batch_loss: 0.23016449809074402\n",
      "training: 90 batch 151 batch_loss: 0.2312111258506775\n",
      "training: 90 batch 152 batch_loss: 0.2288101613521576\n",
      "training: 90 batch 153 batch_loss: 0.22563400864601135\n",
      "training: 90 batch 154 batch_loss: 0.22774147987365723\n",
      "training: 90 batch 155 batch_loss: 0.2271554172039032\n",
      "training: 90 batch 156 batch_loss: 0.22627180814743042\n",
      "training: 90 batch 157 batch_loss: 0.2303953468799591\n",
      "training: 90 batch 158 batch_loss: 0.22624650597572327\n",
      "training: 90 batch 159 batch_loss: 0.22301095724105835\n",
      "training: 90 batch 160 batch_loss: 0.22507411241531372\n",
      "training: 90 batch 161 batch_loss: 0.22779032588005066\n",
      "training: 90 batch 162 batch_loss: 0.22738945484161377\n",
      "training: 90 batch 163 batch_loss: 0.22570088505744934\n",
      "training: 90 batch 164 batch_loss: 0.22801268100738525\n",
      "training: 90 batch 165 batch_loss: 0.22278133034706116\n",
      "training: 90 batch 166 batch_loss: 0.2255292534828186\n",
      "training: 90 batch 167 batch_loss: 0.22472476959228516\n",
      "training: 90 batch 168 batch_loss: 0.22962161898612976\n",
      "training: 90 batch 169 batch_loss: 0.22806331515312195\n",
      "training: 90 batch 170 batch_loss: 0.22734016180038452\n",
      "training: 90 batch 171 batch_loss: 0.22439894080162048\n",
      "training: 90 batch 172 batch_loss: 0.22969010472297668\n",
      "training: 90 batch 173 batch_loss: 0.22687989473342896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 90 batch 174 batch_loss: 0.2265857756137848\n",
      "training: 90 batch 175 batch_loss: 0.2252926528453827\n",
      "training: 90 batch 176 batch_loss: 0.22865945100784302\n",
      "training: 90 batch 177 batch_loss: 0.22146564722061157\n",
      "training: 90 batch 178 batch_loss: 0.2259732484817505\n",
      "training: 90 batch 179 batch_loss: 0.2234804332256317\n",
      "training: 90 batch 180 batch_loss: 0.22485443949699402\n",
      "training: 90 batch 181 batch_loss: 0.22603046894073486\n",
      "training: 90 batch 182 batch_loss: 0.22486740350723267\n",
      "training: 90 batch 183 batch_loss: 0.2302638590335846\n",
      "training: 90 batch 184 batch_loss: 0.22506508231163025\n",
      "training: 90 batch 185 batch_loss: 0.22554326057434082\n",
      "training: 90 batch 186 batch_loss: 0.22614657878875732\n",
      "training: 90 batch 187 batch_loss: 0.2283926010131836\n",
      "training: 90 batch 188 batch_loss: 0.2284117341041565\n",
      "training: 90 batch 189 batch_loss: 0.22679230570793152\n",
      "training: 90 batch 190 batch_loss: 0.23282384872436523\n",
      "training: 90 batch 191 batch_loss: 0.2306564450263977\n",
      "training: 90 batch 192 batch_loss: 0.22835099697113037\n",
      "training: 90 batch 193 batch_loss: 0.22628381848335266\n",
      "training: 90 batch 194 batch_loss: 0.2240646481513977\n",
      "training: 90 batch 195 batch_loss: 0.23330619931221008\n",
      "training: 90 batch 196 batch_loss: 0.22656866908073425\n",
      "training: 90 batch 197 batch_loss: 0.2267179787158966\n",
      "training: 90 batch 198 batch_loss: 0.23052167892456055\n",
      "training: 90 batch 199 batch_loss: 0.22813791036605835\n",
      "training: 90 batch 200 batch_loss: 0.22709593176841736\n",
      "training: 90 batch 201 batch_loss: 0.22953829169273376\n",
      "training: 90 batch 202 batch_loss: 0.22911864519119263\n",
      "training: 90 batch 203 batch_loss: 0.2285614013671875\n",
      "training: 90 batch 204 batch_loss: 0.22656002640724182\n",
      "training: 90 batch 205 batch_loss: 0.22653302550315857\n",
      "training: 90 batch 206 batch_loss: 0.22781866788864136\n",
      "training: 90 batch 207 batch_loss: 0.22907251119613647\n",
      "training: 90 batch 208 batch_loss: 0.22260311245918274\n",
      "training: 90 batch 209 batch_loss: 0.22739741206169128\n",
      "training: 90 batch 210 batch_loss: 0.22382396459579468\n",
      "training: 90 batch 211 batch_loss: 0.23023787140846252\n",
      "training: 90 batch 212 batch_loss: 0.22990089654922485\n",
      "training: 90 batch 213 batch_loss: 0.22790157794952393\n",
      "training: 90 batch 214 batch_loss: 0.22116783261299133\n",
      "training: 90 batch 215 batch_loss: 0.2257893681526184\n",
      "training: 90 batch 216 batch_loss: 0.2269286811351776\n",
      "training: 90 batch 217 batch_loss: 0.22360241413116455\n",
      "training: 90 batch 218 batch_loss: 0.23182356357574463\n",
      "training: 90 batch 219 batch_loss: 0.2280598282814026\n",
      "training: 90 batch 220 batch_loss: 0.22424304485321045\n",
      "training: 90 batch 221 batch_loss: 0.23186150193214417\n",
      "training: 90 batch 222 batch_loss: 0.2277495265007019\n",
      "training: 90 batch 223 batch_loss: 0.22136926651000977\n",
      "training: 90 batch 224 batch_loss: 0.23123157024383545\n",
      "training: 90 batch 225 batch_loss: 0.22865557670593262\n",
      "training: 90 batch 226 batch_loss: 0.22573956847190857\n",
      "training: 90 batch 227 batch_loss: 0.22619083523750305\n",
      "training: 90 batch 228 batch_loss: 0.22261181473731995\n",
      "training: 90 batch 229 batch_loss: 0.2259594202041626\n",
      "training: 90 batch 230 batch_loss: 0.22892293334007263\n",
      "training: 90 batch 231 batch_loss: 0.22679924964904785\n",
      "training: 90 batch 232 batch_loss: 0.2291688323020935\n",
      "training: 90 batch 233 batch_loss: 0.23043900728225708\n",
      "training: 90 batch 234 batch_loss: 0.22661945223808289\n",
      "training: 90 batch 235 batch_loss: 0.2238692045211792\n",
      "training: 90 batch 236 batch_loss: 0.22563588619232178\n",
      "training: 90 batch 237 batch_loss: 0.22746074199676514\n",
      "training: 90 batch 238 batch_loss: 0.23119935393333435\n",
      "training: 90 batch 239 batch_loss: 0.22653642296791077\n",
      "training: 90 batch 240 batch_loss: 0.2287653684616089\n",
      "training: 90 batch 241 batch_loss: 0.23017922043800354\n",
      "training: 90 batch 242 batch_loss: 0.2236989140510559\n",
      "training: 90 batch 243 batch_loss: 0.22598135471343994\n",
      "training: 90 batch 244 batch_loss: 0.22700265049934387\n",
      "training: 90 batch 245 batch_loss: 0.23013970255851746\n",
      "training: 90 batch 246 batch_loss: 0.22796335816383362\n",
      "training: 90 batch 247 batch_loss: 0.2271730899810791\n",
      "training: 90 batch 248 batch_loss: 0.2246265411376953\n",
      "training: 90 batch 249 batch_loss: 0.23177611827850342\n",
      "training: 90 batch 250 batch_loss: 0.22851765155792236\n",
      "training: 90 batch 251 batch_loss: 0.22491148114204407\n",
      "training: 90 batch 252 batch_loss: 0.22602275013923645\n",
      "training: 90 batch 253 batch_loss: 0.22625243663787842\n",
      "training: 90 batch 254 batch_loss: 0.22851240634918213\n",
      "training: 90 batch 255 batch_loss: 0.22572866082191467\n",
      "training: 90 batch 256 batch_loss: 0.23135876655578613\n",
      "training: 90 batch 257 batch_loss: 0.2268335521221161\n",
      "training: 90 batch 258 batch_loss: 0.2269098162651062\n",
      "training: 90 batch 259 batch_loss: 0.22722679376602173\n",
      "training: 90 batch 260 batch_loss: 0.2284383475780487\n",
      "training: 90 batch 261 batch_loss: 0.22691607475280762\n",
      "training: 90 batch 262 batch_loss: 0.23252251744270325\n",
      "training: 90 batch 263 batch_loss: 0.23071804642677307\n",
      "training: 90 batch 264 batch_loss: 0.2268553078174591\n",
      "training: 90 batch 265 batch_loss: 0.22978469729423523\n",
      "training: 90 batch 266 batch_loss: 0.22419315576553345\n",
      "training: 90 batch 267 batch_loss: 0.226577490568161\n",
      "training: 90 batch 268 batch_loss: 0.22905772924423218\n",
      "training: 90 batch 269 batch_loss: 0.22585487365722656\n",
      "training: 90 batch 270 batch_loss: 0.22926411032676697\n",
      "training: 90 batch 271 batch_loss: 0.2300061583518982\n",
      "training: 90 batch 272 batch_loss: 0.22688159346580505\n",
      "training: 90 batch 273 batch_loss: 0.2288132905960083\n",
      "training: 90 batch 274 batch_loss: 0.2280387282371521\n",
      "training: 90 batch 275 batch_loss: 0.22749793529510498\n",
      "training: 90 batch 276 batch_loss: 0.2268730103969574\n",
      "training: 90 batch 277 batch_loss: 0.23565268516540527\n",
      "training: 90 batch 278 batch_loss: 0.22908157110214233\n",
      "training: 90 batch 279 batch_loss: 0.2219873070716858\n",
      "training: 90 batch 280 batch_loss: 0.22734659910202026\n",
      "training: 90 batch 281 batch_loss: 0.2282574474811554\n",
      "training: 90 batch 282 batch_loss: 0.22849583625793457\n",
      "training: 90 batch 283 batch_loss: 0.22864949703216553\n",
      "training: 90 batch 284 batch_loss: 0.23306137323379517\n",
      "training: 90 batch 285 batch_loss: 0.22469648718833923\n",
      "training: 90 batch 286 batch_loss: 0.22765308618545532\n",
      "training: 90 batch 287 batch_loss: 0.22541242837905884\n",
      "training: 90 batch 288 batch_loss: 0.22686123847961426\n",
      "training: 90 batch 289 batch_loss: 0.22920483350753784\n",
      "training: 90 batch 290 batch_loss: 0.2252388596534729\n",
      "training: 90 batch 291 batch_loss: 0.22642391920089722\n",
      "training: 90 batch 292 batch_loss: 0.22902771830558777\n",
      "training: 90 batch 293 batch_loss: 0.23037460446357727\n",
      "training: 90 batch 294 batch_loss: 0.22404813766479492\n",
      "training: 90 batch 295 batch_loss: 0.22807863354682922\n",
      "training: 90 batch 296 batch_loss: 0.22618556022644043\n",
      "training: 90 batch 297 batch_loss: 0.22542813420295715\n",
      "training: 90 batch 298 batch_loss: 0.22742292284965515\n",
      "training: 90 batch 299 batch_loss: 0.227350115776062\n",
      "training: 90 batch 300 batch_loss: 0.23124754428863525\n",
      "training: 90 batch 301 batch_loss: 0.22995597124099731\n",
      "training: 90 batch 302 batch_loss: 0.2239232361316681\n",
      "training: 90 batch 303 batch_loss: 0.22735074162483215\n",
      "training: 90 batch 304 batch_loss: 0.2323383092880249\n",
      "training: 90 batch 305 batch_loss: 0.22809955477714539\n",
      "training: 90 batch 306 batch_loss: 0.22813093662261963\n",
      "training: 90 batch 307 batch_loss: 0.22890251874923706\n",
      "training: 90 batch 308 batch_loss: 0.2250041365623474\n",
      "training: 90 batch 309 batch_loss: 0.2293623983860016\n",
      "training: 90 batch 310 batch_loss: 0.2249225378036499\n",
      "training: 90 batch 311 batch_loss: 0.22856813669204712\n",
      "training: 90 batch 312 batch_loss: 0.22151368856430054\n",
      "training: 90 batch 313 batch_loss: 0.22809508442878723\n",
      "training: 90 batch 314 batch_loss: 0.22869235277175903\n",
      "training: 90 batch 315 batch_loss: 0.22335785627365112\n",
      "training: 90 batch 316 batch_loss: 0.2274332344532013\n",
      "training: 90 batch 317 batch_loss: 0.2286478877067566\n",
      "training: 90 batch 318 batch_loss: 0.22849041223526\n",
      "training: 90 batch 319 batch_loss: 0.22809621691703796\n",
      "training: 90 batch 320 batch_loss: 0.2281726598739624\n",
      "training: 90 batch 321 batch_loss: 0.22557735443115234\n",
      "training: 90 batch 322 batch_loss: 0.22609087824821472\n",
      "training: 90 batch 323 batch_loss: 0.22830680012702942\n",
      "training: 90 batch 324 batch_loss: 0.23143035173416138\n",
      "training: 90 batch 325 batch_loss: 0.22772958874702454\n",
      "training: 90 batch 326 batch_loss: 0.22685766220092773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 90 batch 327 batch_loss: 0.2272789478302002\n",
      "training: 90 batch 328 batch_loss: 0.22910356521606445\n",
      "training: 90 batch 329 batch_loss: 0.22829091548919678\n",
      "training: 90 batch 330 batch_loss: 0.2255287766456604\n",
      "training: 90 batch 331 batch_loss: 0.2245643436908722\n",
      "training: 90 batch 332 batch_loss: 0.2238645851612091\n",
      "training: 90 batch 333 batch_loss: 0.23395732045173645\n",
      "training: 90 batch 334 batch_loss: 0.2294994592666626\n",
      "training: 90 batch 335 batch_loss: 0.22642922401428223\n",
      "training: 90 batch 336 batch_loss: 0.22315791249275208\n",
      "training: 90 batch 337 batch_loss: 0.22664082050323486\n",
      "training: 90 batch 338 batch_loss: 0.2271803617477417\n",
      "training: 90 batch 339 batch_loss: 0.228697270154953\n",
      "training: 90 batch 340 batch_loss: 0.2326638102531433\n",
      "training: 90 batch 341 batch_loss: 0.22701993584632874\n",
      "training: 90 batch 342 batch_loss: 0.22803962230682373\n",
      "training: 90 batch 343 batch_loss: 0.22855281829833984\n",
      "training: 90 batch 344 batch_loss: 0.22970134019851685\n",
      "training: 90 batch 345 batch_loss: 0.2272282838821411\n",
      "training: 90 batch 346 batch_loss: 0.22862732410430908\n",
      "training: 90 batch 347 batch_loss: 0.23174485564231873\n",
      "training: 90 batch 348 batch_loss: 0.2306302785873413\n",
      "training: 90 batch 349 batch_loss: 0.23053908348083496\n",
      "training: 90 batch 350 batch_loss: 0.2264900505542755\n",
      "training: 90 batch 351 batch_loss: 0.22870498895645142\n",
      "training: 90 batch 352 batch_loss: 0.22347158193588257\n",
      "training: 90 batch 353 batch_loss: 0.22518855333328247\n",
      "training: 90 batch 354 batch_loss: 0.22618305683135986\n",
      "training: 90 batch 355 batch_loss: 0.23028254508972168\n",
      "training: 90 batch 356 batch_loss: 0.22253024578094482\n",
      "training: 90 batch 357 batch_loss: 0.2266022264957428\n",
      "training: 90 batch 358 batch_loss: 0.2295568883419037\n",
      "training: 90 batch 359 batch_loss: 0.2281647026538849\n",
      "training: 90 batch 360 batch_loss: 0.2293260097503662\n",
      "training: 90 batch 361 batch_loss: 0.2278730273246765\n",
      "training: 90 batch 362 batch_loss: 0.22683095932006836\n",
      "training: 90 batch 363 batch_loss: 0.22947421669960022\n",
      "training: 90 batch 364 batch_loss: 0.22690340876579285\n",
      "training: 90 batch 365 batch_loss: 0.229606032371521\n",
      "training: 90 batch 366 batch_loss: 0.22800442576408386\n",
      "training: 90 batch 367 batch_loss: 0.23085999488830566\n",
      "training: 90 batch 368 batch_loss: 0.2300952970981598\n",
      "training: 90 batch 369 batch_loss: 0.23017194867134094\n",
      "training: 90 batch 370 batch_loss: 0.22405993938446045\n",
      "training: 90 batch 371 batch_loss: 0.2268873155117035\n",
      "training: 90 batch 372 batch_loss: 0.22405749559402466\n",
      "training: 90 batch 373 batch_loss: 0.22670653462409973\n",
      "training: 90 batch 374 batch_loss: 0.2314220368862152\n",
      "training: 90 batch 375 batch_loss: 0.22722157835960388\n",
      "training: 90 batch 376 batch_loss: 0.2279534637928009\n",
      "training: 90 batch 377 batch_loss: 0.22296756505966187\n",
      "training: 90 batch 378 batch_loss: 0.228628009557724\n",
      "training: 90 batch 379 batch_loss: 0.22725769877433777\n",
      "training: 90 batch 380 batch_loss: 0.23000961542129517\n",
      "training: 90 batch 381 batch_loss: 0.23123672604560852\n",
      "training: 90 batch 382 batch_loss: 0.23331046104431152\n",
      "training: 90 batch 383 batch_loss: 0.22968155145645142\n",
      "training: 90 batch 384 batch_loss: 0.22816509008407593\n",
      "training: 90 batch 385 batch_loss: 0.22601938247680664\n",
      "training: 90 batch 386 batch_loss: 0.2259637713432312\n",
      "training: 90 batch 387 batch_loss: 0.22716984152793884\n",
      "training: 90 batch 388 batch_loss: 0.23118719458580017\n",
      "training: 90 batch 389 batch_loss: 0.22732803225517273\n",
      "training: 90 batch 390 batch_loss: 0.22836250066757202\n",
      "training: 90 batch 391 batch_loss: 0.22892263531684875\n",
      "training: 90 batch 392 batch_loss: 0.22655442357063293\n",
      "training: 90 batch 393 batch_loss: 0.23350080847740173\n",
      "training: 90 batch 394 batch_loss: 0.22831302881240845\n",
      "training: 90 batch 395 batch_loss: 0.23016616702079773\n",
      "training: 90 batch 396 batch_loss: 0.22894343733787537\n",
      "training: 90 batch 397 batch_loss: 0.22980740666389465\n",
      "training: 90 batch 398 batch_loss: 0.22952118515968323\n",
      "training: 90 batch 399 batch_loss: 0.22432255744934082\n",
      "training: 90 batch 400 batch_loss: 0.2336123287677765\n",
      "training: 90 batch 401 batch_loss: 0.23096469044685364\n",
      "training: 90 batch 402 batch_loss: 0.22776144742965698\n",
      "training: 90 batch 403 batch_loss: 0.22614255547523499\n",
      "training: 90 batch 404 batch_loss: 0.2262125015258789\n",
      "training: 90 batch 405 batch_loss: 0.22892361879348755\n",
      "training: 90 batch 406 batch_loss: 0.2266109585762024\n",
      "training: 90 batch 407 batch_loss: 0.22390130162239075\n",
      "training: 90 batch 408 batch_loss: 0.22826465964317322\n",
      "training: 90 batch 409 batch_loss: 0.22956788539886475\n",
      "training: 90 batch 410 batch_loss: 0.22931957244873047\n",
      "training: 90 batch 411 batch_loss: 0.2279457151889801\n",
      "training: 90 batch 412 batch_loss: 0.22496938705444336\n",
      "training: 90 batch 413 batch_loss: 0.2292320430278778\n",
      "training: 90 batch 414 batch_loss: 0.23002415895462036\n",
      "training: 90 batch 415 batch_loss: 0.23278120160102844\n",
      "training: 90 batch 416 batch_loss: 0.22597968578338623\n",
      "training: 90 batch 417 batch_loss: 0.2241268754005432\n",
      "training: 90 batch 418 batch_loss: 0.22926020622253418\n",
      "training: 90 batch 419 batch_loss: 0.2284354567527771\n",
      "training: 90 batch 420 batch_loss: 0.22856977581977844\n",
      "training: 90 batch 421 batch_loss: 0.22298386693000793\n",
      "training: 90 batch 422 batch_loss: 0.22394877672195435\n",
      "training: 90 batch 423 batch_loss: 0.22236156463623047\n",
      "training: 90 batch 424 batch_loss: 0.2276611328125\n",
      "training: 90 batch 425 batch_loss: 0.22769510746002197\n",
      "training: 90 batch 426 batch_loss: 0.22453123331069946\n",
      "training: 90 batch 427 batch_loss: 0.2279602289199829\n",
      "training: 90 batch 428 batch_loss: 0.2279127836227417\n",
      "training: 90 batch 429 batch_loss: 0.2305414378643036\n",
      "training: 90 batch 430 batch_loss: 0.22708958387374878\n",
      "training: 90 batch 431 batch_loss: 0.23007744550704956\n",
      "training: 90 batch 432 batch_loss: 0.23026010394096375\n",
      "training: 90 batch 433 batch_loss: 0.22384196519851685\n",
      "training: 90 batch 434 batch_loss: 0.2255554497241974\n",
      "training: 90 batch 435 batch_loss: 0.22523945569992065\n",
      "training: 90 batch 436 batch_loss: 0.22228959202766418\n",
      "training: 90 batch 437 batch_loss: 0.2264927625656128\n",
      "training: 90 batch 438 batch_loss: 0.22762206196784973\n",
      "training: 90 batch 439 batch_loss: 0.2289615273475647\n",
      "training: 90 batch 440 batch_loss: 0.23033958673477173\n",
      "training: 90 batch 441 batch_loss: 0.22383299469947815\n",
      "training: 90 batch 442 batch_loss: 0.22863346338272095\n",
      "training: 90 batch 443 batch_loss: 0.22846662998199463\n",
      "training: 90 batch 444 batch_loss: 0.22524380683898926\n",
      "training: 90 batch 445 batch_loss: 0.22958385944366455\n",
      "training: 90 batch 446 batch_loss: 0.22797781229019165\n",
      "training: 90 batch 447 batch_loss: 0.22778481245040894\n",
      "training: 90 batch 448 batch_loss: 0.2305775284767151\n",
      "training: 90 batch 449 batch_loss: 0.22703447937965393\n",
      "training: 90 batch 450 batch_loss: 0.22972416877746582\n",
      "training: 90 batch 451 batch_loss: 0.22894859313964844\n",
      "training: 90 batch 452 batch_loss: 0.2281017005443573\n",
      "training: 90 batch 453 batch_loss: 0.23056378960609436\n",
      "training: 90 batch 454 batch_loss: 0.2280798852443695\n",
      "training: 90 batch 455 batch_loss: 0.23000356554985046\n",
      "training: 90 batch 456 batch_loss: 0.22775331139564514\n",
      "training: 90 batch 457 batch_loss: 0.22555425763130188\n",
      "training: 90 batch 458 batch_loss: 0.22530889511108398\n",
      "training: 90 batch 459 batch_loss: 0.22973507642745972\n",
      "training: 90 batch 460 batch_loss: 0.22812026739120483\n",
      "training: 90 batch 461 batch_loss: 0.23325437307357788\n",
      "training: 90 batch 462 batch_loss: 0.22828763723373413\n",
      "training: 90 batch 463 batch_loss: 0.23249530792236328\n",
      "training: 90 batch 464 batch_loss: 0.22445756196975708\n",
      "training: 90 batch 465 batch_loss: 0.23108011484146118\n",
      "training: 90 batch 466 batch_loss: 0.23172903060913086\n",
      "training: 90 batch 467 batch_loss: 0.2283705770969391\n",
      "training: 90 batch 468 batch_loss: 0.22650623321533203\n",
      "training: 90 batch 469 batch_loss: 0.22671639919281006\n",
      "training: 90 batch 470 batch_loss: 0.22693192958831787\n",
      "training: 90 batch 471 batch_loss: 0.22893333435058594\n",
      "training: 90 batch 472 batch_loss: 0.23062312602996826\n",
      "training: 90 batch 473 batch_loss: 0.22655269503593445\n",
      "training: 90 batch 474 batch_loss: 0.22954171895980835\n",
      "training: 90 batch 475 batch_loss: 0.22844308614730835\n",
      "training: 90 batch 476 batch_loss: 0.23035818338394165\n",
      "training: 90 batch 477 batch_loss: 0.22966411709785461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 90 batch 478 batch_loss: 0.22677326202392578\n",
      "training: 90 batch 479 batch_loss: 0.23192459344863892\n",
      "training: 90 batch 480 batch_loss: 0.2283606231212616\n",
      "training: 90 batch 481 batch_loss: 0.22956806421279907\n",
      "training: 90 batch 482 batch_loss: 0.22864267230033875\n",
      "training: 90 batch 483 batch_loss: 0.2312021255493164\n",
      "training: 90 batch 484 batch_loss: 0.2278483510017395\n",
      "training: 90 batch 485 batch_loss: 0.22901266813278198\n",
      "training: 90 batch 486 batch_loss: 0.23020011186599731\n",
      "training: 90 batch 487 batch_loss: 0.22471603751182556\n",
      "training: 90 batch 488 batch_loss: 0.22381800413131714\n",
      "training: 90 batch 489 batch_loss: 0.23188504576683044\n",
      "training: 90 batch 490 batch_loss: 0.22726869583129883\n",
      "training: 90 batch 491 batch_loss: 0.2320801019668579\n",
      "training: 90 batch 492 batch_loss: 0.2303992211818695\n",
      "training: 90 batch 493 batch_loss: 0.22882553935050964\n",
      "training: 90 batch 494 batch_loss: 0.22516852617263794\n",
      "training: 90 batch 495 batch_loss: 0.22656992077827454\n",
      "training: 90 batch 496 batch_loss: 0.22832155227661133\n",
      "training: 90 batch 497 batch_loss: 0.22861576080322266\n",
      "training: 90 batch 498 batch_loss: 0.23173528909683228\n",
      "training: 90 batch 499 batch_loss: 0.2277645468711853\n",
      "training: 90 batch 500 batch_loss: 0.22736874222755432\n",
      "training: 90 batch 501 batch_loss: 0.22982847690582275\n",
      "training: 90 batch 502 batch_loss: 0.22750455141067505\n",
      "training: 90 batch 503 batch_loss: 0.22696495056152344\n",
      "training: 90 batch 504 batch_loss: 0.228672057390213\n",
      "training: 90 batch 505 batch_loss: 0.227659672498703\n",
      "training: 90 batch 506 batch_loss: 0.22659063339233398\n",
      "training: 90 batch 507 batch_loss: 0.2334984540939331\n",
      "training: 90 batch 508 batch_loss: 0.22821491956710815\n",
      "training: 90 batch 509 batch_loss: 0.23468399047851562\n",
      "training: 90 batch 510 batch_loss: 0.2243315577507019\n",
      "training: 90 batch 511 batch_loss: 0.22606706619262695\n",
      "training: 90 batch 512 batch_loss: 0.2252119779586792\n",
      "training: 90 batch 513 batch_loss: 0.2317657768726349\n",
      "training: 90 batch 514 batch_loss: 0.22792279720306396\n",
      "training: 90 batch 515 batch_loss: 0.22689086198806763\n",
      "training: 90 batch 516 batch_loss: 0.23361855745315552\n",
      "training: 90 batch 517 batch_loss: 0.23199421167373657\n",
      "training: 90 batch 518 batch_loss: 0.2267093062400818\n",
      "training: 90 batch 519 batch_loss: 0.2274288535118103\n",
      "training: 90 batch 520 batch_loss: 0.23142141103744507\n",
      "training: 90 batch 521 batch_loss: 0.23218446969985962\n",
      "training: 90 batch 522 batch_loss: 0.2264825999736786\n",
      "training: 90 batch 523 batch_loss: 0.2280833125114441\n",
      "training: 90 batch 524 batch_loss: 0.23235633969306946\n",
      "training: 90 batch 525 batch_loss: 0.2303493320941925\n",
      "training: 90 batch 526 batch_loss: 0.22518688440322876\n",
      "training: 90 batch 527 batch_loss: 0.23106881976127625\n",
      "training: 90 batch 528 batch_loss: 0.23184669017791748\n",
      "training: 90 batch 529 batch_loss: 0.2300700843334198\n",
      "training: 90 batch 530 batch_loss: 0.22952142357826233\n",
      "training: 90 batch 531 batch_loss: 0.22971433401107788\n",
      "training: 90 batch 532 batch_loss: 0.2269485890865326\n",
      "training: 90 batch 533 batch_loss: 0.2271716594696045\n",
      "training: 90 batch 534 batch_loss: 0.22687336802482605\n",
      "training: 90 batch 535 batch_loss: 0.23626244068145752\n",
      "training: 90 batch 536 batch_loss: 0.23083266615867615\n",
      "training: 90 batch 537 batch_loss: 0.2275524139404297\n",
      "training: 90 batch 538 batch_loss: 0.23175466060638428\n",
      "training: 90 batch 539 batch_loss: 0.23343589901924133\n",
      "training: 90 batch 540 batch_loss: 0.22761112451553345\n",
      "training: 90 batch 541 batch_loss: 0.22879737615585327\n",
      "training: 90 batch 542 batch_loss: 0.22841373085975647\n",
      "training: 90 batch 543 batch_loss: 0.23274481296539307\n",
      "training: 90 batch 544 batch_loss: 0.2271936535835266\n",
      "training: 90 batch 545 batch_loss: 0.22641703486442566\n",
      "training: 90 batch 546 batch_loss: 0.22588467597961426\n",
      "training: 90 batch 547 batch_loss: 0.2241804003715515\n",
      "training: 90 batch 548 batch_loss: 0.22831538319587708\n",
      "training: 90 batch 549 batch_loss: 0.22784855961799622\n",
      "training: 90 batch 550 batch_loss: 0.22605592012405396\n",
      "training: 90 batch 551 batch_loss: 0.23187601566314697\n",
      "training: 90 batch 552 batch_loss: 0.2260027527809143\n",
      "training: 90 batch 553 batch_loss: 0.23195809125900269\n",
      "training: 90 batch 554 batch_loss: 0.23001980781555176\n",
      "training: 90 batch 555 batch_loss: 0.226706862449646\n",
      "training: 90 batch 556 batch_loss: 0.2306654453277588\n",
      "training: 90 batch 557 batch_loss: 0.2291688621044159\n",
      "training: 90 batch 558 batch_loss: 0.2277889847755432\n",
      "training: 90 batch 559 batch_loss: 0.22810900211334229\n",
      "training: 90 batch 560 batch_loss: 0.22635534405708313\n",
      "training: 90 batch 561 batch_loss: 0.22762838006019592\n",
      "training: 90 batch 562 batch_loss: 0.22974121570587158\n",
      "training: 90 batch 563 batch_loss: 0.2281644344329834\n",
      "training: 90 batch 564 batch_loss: 0.22435984015464783\n",
      "training: 90 batch 565 batch_loss: 0.23484620451927185\n",
      "training: 90 batch 566 batch_loss: 0.22712752223014832\n",
      "training: 90 batch 567 batch_loss: 0.22921940684318542\n",
      "training: 90 batch 568 batch_loss: 0.2337878942489624\n",
      "training: 90 batch 569 batch_loss: 0.23165971040725708\n",
      "training: 90 batch 570 batch_loss: 0.229143887758255\n",
      "training: 90 batch 571 batch_loss: 0.22876277565956116\n",
      "training: 90 batch 572 batch_loss: 0.22718456387519836\n",
      "training: 90 batch 573 batch_loss: 0.22698909044265747\n",
      "training: 90 batch 574 batch_loss: 0.23347148299217224\n",
      "training: 90 batch 575 batch_loss: 0.2274494171142578\n",
      "training: 90 batch 576 batch_loss: 0.22545191645622253\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 90, Hit Ratio:0.033641690512437 | Precision:0.04963629214587634 | Recall:0.0652685509474984 | NDCG:0.0646054515498804\n",
      "*Best Performance* \n",
      "Epoch: 87, Hit Ratio:0.03409140119856226 | Precision:0.050299813231101935 | Recall:0.06623474467749231 | MDCG:0.06544512792267772\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 91 batch 0 batch_loss: 0.22673863172531128\n",
      "training: 91 batch 1 batch_loss: 0.23061251640319824\n",
      "training: 91 batch 2 batch_loss: 0.22797903418540955\n",
      "training: 91 batch 3 batch_loss: 0.22883710265159607\n",
      "training: 91 batch 4 batch_loss: 0.2206323742866516\n",
      "training: 91 batch 5 batch_loss: 0.22661948204040527\n",
      "training: 91 batch 6 batch_loss: 0.2290235161781311\n",
      "training: 91 batch 7 batch_loss: 0.22709238529205322\n",
      "training: 91 batch 8 batch_loss: 0.22895067930221558\n",
      "training: 91 batch 9 batch_loss: 0.2211107313632965\n",
      "training: 91 batch 10 batch_loss: 0.22137925028800964\n",
      "training: 91 batch 11 batch_loss: 0.22931921482086182\n",
      "training: 91 batch 12 batch_loss: 0.22842857241630554\n",
      "training: 91 batch 13 batch_loss: 0.226739764213562\n",
      "training: 91 batch 14 batch_loss: 0.22176474332809448\n",
      "training: 91 batch 15 batch_loss: 0.22632494568824768\n",
      "training: 91 batch 16 batch_loss: 0.22923630475997925\n",
      "training: 91 batch 17 batch_loss: 0.2276914417743683\n",
      "training: 91 batch 18 batch_loss: 0.22418126463890076\n",
      "training: 91 batch 19 batch_loss: 0.2214064598083496\n",
      "training: 91 batch 20 batch_loss: 0.22515273094177246\n",
      "training: 91 batch 21 batch_loss: 0.22864198684692383\n",
      "training: 91 batch 22 batch_loss: 0.22954320907592773\n",
      "training: 91 batch 23 batch_loss: 0.22726774215698242\n",
      "training: 91 batch 24 batch_loss: 0.22453323006629944\n",
      "training: 91 batch 25 batch_loss: 0.2299070656299591\n",
      "training: 91 batch 26 batch_loss: 0.2297581136226654\n",
      "training: 91 batch 27 batch_loss: 0.22308585047721863\n",
      "training: 91 batch 28 batch_loss: 0.2264186143875122\n",
      "training: 91 batch 29 batch_loss: 0.22327014803886414\n",
      "training: 91 batch 30 batch_loss: 0.2250860631465912\n",
      "training: 91 batch 31 batch_loss: 0.2256757616996765\n",
      "training: 91 batch 32 batch_loss: 0.2289164662361145\n",
      "training: 91 batch 33 batch_loss: 0.2292078137397766\n",
      "training: 91 batch 34 batch_loss: 0.22524911165237427\n",
      "training: 91 batch 35 batch_loss: 0.2215212881565094\n",
      "training: 91 batch 36 batch_loss: 0.22900736331939697\n",
      "training: 91 batch 37 batch_loss: 0.22564762830734253\n",
      "training: 91 batch 38 batch_loss: 0.22816422581672668\n",
      "training: 91 batch 39 batch_loss: 0.22385182976722717\n",
      "training: 91 batch 40 batch_loss: 0.2289714813232422\n",
      "training: 91 batch 41 batch_loss: 0.2272787094116211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 91 batch 42 batch_loss: 0.22582900524139404\n",
      "training: 91 batch 43 batch_loss: 0.22556105256080627\n",
      "training: 91 batch 44 batch_loss: 0.22491198778152466\n",
      "training: 91 batch 45 batch_loss: 0.22975093126296997\n",
      "training: 91 batch 46 batch_loss: 0.2250811755657196\n",
      "training: 91 batch 47 batch_loss: 0.2249654233455658\n",
      "training: 91 batch 48 batch_loss: 0.23056581616401672\n",
      "training: 91 batch 49 batch_loss: 0.22449392080307007\n",
      "training: 91 batch 50 batch_loss: 0.2277669906616211\n",
      "training: 91 batch 51 batch_loss: 0.22581011056900024\n",
      "training: 91 batch 52 batch_loss: 0.22620907425880432\n",
      "training: 91 batch 53 batch_loss: 0.2259850800037384\n",
      "training: 91 batch 54 batch_loss: 0.22841906547546387\n",
      "training: 91 batch 55 batch_loss: 0.2286028265953064\n",
      "training: 91 batch 56 batch_loss: 0.22402462363243103\n",
      "training: 91 batch 57 batch_loss: 0.23479732871055603\n",
      "training: 91 batch 58 batch_loss: 0.2298107147216797\n",
      "training: 91 batch 59 batch_loss: 0.23133498430252075\n",
      "training: 91 batch 60 batch_loss: 0.22599291801452637\n",
      "training: 91 batch 61 batch_loss: 0.22273895144462585\n",
      "training: 91 batch 62 batch_loss: 0.2262800931930542\n",
      "training: 91 batch 63 batch_loss: 0.22640979290008545\n",
      "training: 91 batch 64 batch_loss: 0.2263575792312622\n",
      "training: 91 batch 65 batch_loss: 0.2266334891319275\n",
      "training: 91 batch 66 batch_loss: 0.22903892397880554\n",
      "training: 91 batch 67 batch_loss: 0.22631314396858215\n",
      "training: 91 batch 68 batch_loss: 0.22223761677742004\n",
      "training: 91 batch 69 batch_loss: 0.22611001133918762\n",
      "training: 91 batch 70 batch_loss: 0.2235584557056427\n",
      "training: 91 batch 71 batch_loss: 0.22896340489387512\n",
      "training: 91 batch 72 batch_loss: 0.22951751947402954\n",
      "training: 91 batch 73 batch_loss: 0.227623850107193\n",
      "training: 91 batch 74 batch_loss: 0.22422757744789124\n",
      "training: 91 batch 75 batch_loss: 0.2297465205192566\n",
      "training: 91 batch 76 batch_loss: 0.22558057308197021\n",
      "training: 91 batch 77 batch_loss: 0.2259644865989685\n",
      "training: 91 batch 78 batch_loss: 0.2241581380367279\n",
      "training: 91 batch 79 batch_loss: 0.23154640197753906\n",
      "training: 91 batch 80 batch_loss: 0.22382792830467224\n",
      "training: 91 batch 81 batch_loss: 0.2264239490032196\n",
      "training: 91 batch 82 batch_loss: 0.22585958242416382\n",
      "training: 91 batch 83 batch_loss: 0.227608323097229\n",
      "training: 91 batch 84 batch_loss: 0.22578665614128113\n",
      "training: 91 batch 85 batch_loss: 0.23315280675888062\n",
      "training: 91 batch 86 batch_loss: 0.22892841696739197\n",
      "training: 91 batch 87 batch_loss: 0.22778180241584778\n",
      "training: 91 batch 88 batch_loss: 0.2245761752128601\n",
      "training: 91 batch 89 batch_loss: 0.22758281230926514\n",
      "training: 91 batch 90 batch_loss: 0.22937172651290894\n",
      "training: 91 batch 91 batch_loss: 0.22937968373298645\n",
      "training: 91 batch 92 batch_loss: 0.2242649495601654\n",
      "training: 91 batch 93 batch_loss: 0.22685691714286804\n",
      "training: 91 batch 94 batch_loss: 0.22514259815216064\n",
      "training: 91 batch 95 batch_loss: 0.22491151094436646\n",
      "training: 91 batch 96 batch_loss: 0.22704270482063293\n",
      "training: 91 batch 97 batch_loss: 0.22723323106765747\n",
      "training: 91 batch 98 batch_loss: 0.22597229480743408\n",
      "training: 91 batch 99 batch_loss: 0.2262301743030548\n",
      "training: 91 batch 100 batch_loss: 0.22523421049118042\n",
      "training: 91 batch 101 batch_loss: 0.2277068793773651\n",
      "training: 91 batch 102 batch_loss: 0.223019540309906\n",
      "training: 91 batch 103 batch_loss: 0.22678622603416443\n",
      "training: 91 batch 104 batch_loss: 0.22264039516448975\n",
      "training: 91 batch 105 batch_loss: 0.22277453541755676\n",
      "training: 91 batch 106 batch_loss: 0.22762921452522278\n",
      "training: 91 batch 107 batch_loss: 0.22629466652870178\n",
      "training: 91 batch 108 batch_loss: 0.22752174735069275\n",
      "training: 91 batch 109 batch_loss: 0.22258442640304565\n",
      "training: 91 batch 110 batch_loss: 0.2252587080001831\n",
      "training: 91 batch 111 batch_loss: 0.2231856882572174\n",
      "training: 91 batch 112 batch_loss: 0.22823438048362732\n",
      "training: 91 batch 113 batch_loss: 0.22599968314170837\n",
      "training: 91 batch 114 batch_loss: 0.2298581600189209\n",
      "training: 91 batch 115 batch_loss: 0.2275746464729309\n",
      "training: 91 batch 116 batch_loss: 0.225857675075531\n",
      "training: 91 batch 117 batch_loss: 0.22809383273124695\n",
      "training: 91 batch 118 batch_loss: 0.22777250409126282\n",
      "training: 91 batch 119 batch_loss: 0.2254715859889984\n",
      "training: 91 batch 120 batch_loss: 0.2258477807044983\n",
      "training: 91 batch 121 batch_loss: 0.2270432710647583\n",
      "training: 91 batch 122 batch_loss: 0.23124995827674866\n",
      "training: 91 batch 123 batch_loss: 0.23158147931098938\n",
      "training: 91 batch 124 batch_loss: 0.2298334240913391\n",
      "training: 91 batch 125 batch_loss: 0.23196741938591003\n",
      "training: 91 batch 126 batch_loss: 0.23167988657951355\n",
      "training: 91 batch 127 batch_loss: 0.2309395968914032\n",
      "training: 91 batch 128 batch_loss: 0.22252869606018066\n",
      "training: 91 batch 129 batch_loss: 0.22285214066505432\n",
      "training: 91 batch 130 batch_loss: 0.228326678276062\n",
      "training: 91 batch 131 batch_loss: 0.2311767041683197\n",
      "training: 91 batch 132 batch_loss: 0.226756751537323\n",
      "training: 91 batch 133 batch_loss: 0.22986996173858643\n",
      "training: 91 batch 134 batch_loss: 0.23051413893699646\n",
      "training: 91 batch 135 batch_loss: 0.22657188773155212\n",
      "training: 91 batch 136 batch_loss: 0.22941553592681885\n",
      "training: 91 batch 137 batch_loss: 0.2287886142730713\n",
      "training: 91 batch 138 batch_loss: 0.227777898311615\n",
      "training: 91 batch 139 batch_loss: 0.2229864001274109\n",
      "training: 91 batch 140 batch_loss: 0.22337070107460022\n",
      "training: 91 batch 141 batch_loss: 0.22584488987922668\n",
      "training: 91 batch 142 batch_loss: 0.22636839747428894\n",
      "training: 91 batch 143 batch_loss: 0.2287488579750061\n",
      "training: 91 batch 144 batch_loss: 0.22587361931800842\n",
      "training: 91 batch 145 batch_loss: 0.22498756647109985\n",
      "training: 91 batch 146 batch_loss: 0.23280581831932068\n",
      "training: 91 batch 147 batch_loss: 0.2289629578590393\n",
      "training: 91 batch 148 batch_loss: 0.23041686415672302\n",
      "training: 91 batch 149 batch_loss: 0.2264891266822815\n",
      "training: 91 batch 150 batch_loss: 0.22946882247924805\n",
      "training: 91 batch 151 batch_loss: 0.23258253931999207\n",
      "training: 91 batch 152 batch_loss: 0.2245180308818817\n",
      "training: 91 batch 153 batch_loss: 0.22875726222991943\n",
      "training: 91 batch 154 batch_loss: 0.22856265306472778\n",
      "training: 91 batch 155 batch_loss: 0.22602272033691406\n",
      "training: 91 batch 156 batch_loss: 0.22337964177131653\n",
      "training: 91 batch 157 batch_loss: 0.22564512491226196\n",
      "training: 91 batch 158 batch_loss: 0.22968575358390808\n",
      "training: 91 batch 159 batch_loss: 0.23229408264160156\n",
      "training: 91 batch 160 batch_loss: 0.2280643880367279\n",
      "training: 91 batch 161 batch_loss: 0.23073098063468933\n",
      "training: 91 batch 162 batch_loss: 0.22784101963043213\n",
      "training: 91 batch 163 batch_loss: 0.23023578524589539\n",
      "training: 91 batch 164 batch_loss: 0.22699281573295593\n",
      "training: 91 batch 165 batch_loss: 0.2303837239742279\n",
      "training: 91 batch 166 batch_loss: 0.22683486342430115\n",
      "training: 91 batch 167 batch_loss: 0.22891736030578613\n",
      "training: 91 batch 168 batch_loss: 0.23212343454360962\n",
      "training: 91 batch 169 batch_loss: 0.22429704666137695\n",
      "training: 91 batch 170 batch_loss: 0.23006609082221985\n",
      "training: 91 batch 171 batch_loss: 0.22719982266426086\n",
      "training: 91 batch 172 batch_loss: 0.23023837804794312\n",
      "training: 91 batch 173 batch_loss: 0.22211724519729614\n",
      "training: 91 batch 174 batch_loss: 0.22724151611328125\n",
      "training: 91 batch 175 batch_loss: 0.2247997522354126\n",
      "training: 91 batch 176 batch_loss: 0.22895967960357666\n",
      "training: 91 batch 177 batch_loss: 0.22485262155532837\n",
      "training: 91 batch 178 batch_loss: 0.2276543378829956\n",
      "training: 91 batch 179 batch_loss: 0.23091834783554077\n",
      "training: 91 batch 180 batch_loss: 0.22987258434295654\n",
      "training: 91 batch 181 batch_loss: 0.22798216342926025\n",
      "training: 91 batch 182 batch_loss: 0.2328709363937378\n",
      "training: 91 batch 183 batch_loss: 0.23134371638298035\n",
      "training: 91 batch 184 batch_loss: 0.22788900136947632\n",
      "training: 91 batch 185 batch_loss: 0.23050358891487122\n",
      "training: 91 batch 186 batch_loss: 0.22876214981079102\n",
      "training: 91 batch 187 batch_loss: 0.2340192198753357\n",
      "training: 91 batch 188 batch_loss: 0.22800147533416748\n",
      "training: 91 batch 189 batch_loss: 0.22703540325164795\n",
      "training: 91 batch 190 batch_loss: 0.23144182562828064\n",
      "training: 91 batch 191 batch_loss: 0.22821813821792603\n",
      "training: 91 batch 192 batch_loss: 0.23054206371307373\n",
      "training: 91 batch 193 batch_loss: 0.22900158166885376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 91 batch 194 batch_loss: 0.227365642786026\n",
      "training: 91 batch 195 batch_loss: 0.2292836308479309\n",
      "training: 91 batch 196 batch_loss: 0.23051616549491882\n",
      "training: 91 batch 197 batch_loss: 0.2306365966796875\n",
      "training: 91 batch 198 batch_loss: 0.22701194882392883\n",
      "training: 91 batch 199 batch_loss: 0.2257751226425171\n",
      "training: 91 batch 200 batch_loss: 0.22801363468170166\n",
      "training: 91 batch 201 batch_loss: 0.22635823488235474\n",
      "training: 91 batch 202 batch_loss: 0.2274734377861023\n",
      "training: 91 batch 203 batch_loss: 0.23359230160713196\n",
      "training: 91 batch 204 batch_loss: 0.2308354675769806\n",
      "training: 91 batch 205 batch_loss: 0.22764641046524048\n",
      "training: 91 batch 206 batch_loss: 0.22365906834602356\n",
      "training: 91 batch 207 batch_loss: 0.22664296627044678\n",
      "training: 91 batch 208 batch_loss: 0.22934913635253906\n",
      "training: 91 batch 209 batch_loss: 0.22689077258110046\n",
      "training: 91 batch 210 batch_loss: 0.22895631194114685\n",
      "training: 91 batch 211 batch_loss: 0.22599488496780396\n",
      "training: 91 batch 212 batch_loss: 0.22752171754837036\n",
      "training: 91 batch 213 batch_loss: 0.22683274745941162\n",
      "training: 91 batch 214 batch_loss: 0.22892746329307556\n",
      "training: 91 batch 215 batch_loss: 0.22410935163497925\n",
      "training: 91 batch 216 batch_loss: 0.22757232189178467\n",
      "training: 91 batch 217 batch_loss: 0.22541925311088562\n",
      "training: 91 batch 218 batch_loss: 0.22649529576301575\n",
      "training: 91 batch 219 batch_loss: 0.22727838158607483\n",
      "training: 91 batch 220 batch_loss: 0.2297065556049347\n",
      "training: 91 batch 221 batch_loss: 0.22737330198287964\n",
      "training: 91 batch 222 batch_loss: 0.22632601857185364\n",
      "training: 91 batch 223 batch_loss: 0.23060578107833862\n",
      "training: 91 batch 224 batch_loss: 0.22603288292884827\n",
      "training: 91 batch 225 batch_loss: 0.22538557648658752\n",
      "training: 91 batch 226 batch_loss: 0.22527974843978882\n",
      "training: 91 batch 227 batch_loss: 0.22720494866371155\n",
      "training: 91 batch 228 batch_loss: 0.22237426042556763\n",
      "training: 91 batch 229 batch_loss: 0.22968709468841553\n",
      "training: 91 batch 230 batch_loss: 0.22762048244476318\n",
      "training: 91 batch 231 batch_loss: 0.23374032974243164\n",
      "training: 91 batch 232 batch_loss: 0.2328513264656067\n",
      "training: 91 batch 233 batch_loss: 0.2289537787437439\n",
      "training: 91 batch 234 batch_loss: 0.22526472806930542\n",
      "training: 91 batch 235 batch_loss: 0.2260584831237793\n",
      "training: 91 batch 236 batch_loss: 0.22790426015853882\n",
      "training: 91 batch 237 batch_loss: 0.22390753030776978\n",
      "training: 91 batch 238 batch_loss: 0.22259128093719482\n",
      "training: 91 batch 239 batch_loss: 0.23218387365341187\n",
      "training: 91 batch 240 batch_loss: 0.22507980465888977\n",
      "training: 91 batch 241 batch_loss: 0.2266642153263092\n",
      "training: 91 batch 242 batch_loss: 0.2268422544002533\n",
      "training: 91 batch 243 batch_loss: 0.23181354999542236\n",
      "training: 91 batch 244 batch_loss: 0.22907352447509766\n",
      "training: 91 batch 245 batch_loss: 0.22653555870056152\n",
      "training: 91 batch 246 batch_loss: 0.23132476210594177\n",
      "training: 91 batch 247 batch_loss: 0.23289793729782104\n",
      "training: 91 batch 248 batch_loss: 0.23229217529296875\n",
      "training: 91 batch 249 batch_loss: 0.22995087504386902\n",
      "training: 91 batch 250 batch_loss: 0.2297324538230896\n",
      "training: 91 batch 251 batch_loss: 0.22958359122276306\n",
      "training: 91 batch 252 batch_loss: 0.2234041690826416\n",
      "training: 91 batch 253 batch_loss: 0.22735711932182312\n",
      "training: 91 batch 254 batch_loss: 0.23168396949768066\n",
      "training: 91 batch 255 batch_loss: 0.22569045424461365\n",
      "training: 91 batch 256 batch_loss: 0.22373533248901367\n",
      "training: 91 batch 257 batch_loss: 0.23013201355934143\n",
      "training: 91 batch 258 batch_loss: 0.2231987714767456\n",
      "training: 91 batch 259 batch_loss: 0.22327101230621338\n",
      "training: 91 batch 260 batch_loss: 0.2260313630104065\n",
      "training: 91 batch 261 batch_loss: 0.22900253534317017\n",
      "training: 91 batch 262 batch_loss: 0.22970610857009888\n",
      "training: 91 batch 263 batch_loss: 0.22889289259910583\n",
      "training: 91 batch 264 batch_loss: 0.22899913787841797\n",
      "training: 91 batch 265 batch_loss: 0.22475218772888184\n",
      "training: 91 batch 266 batch_loss: 0.23258894681930542\n",
      "training: 91 batch 267 batch_loss: 0.22841450572013855\n",
      "training: 91 batch 268 batch_loss: 0.2312355637550354\n",
      "training: 91 batch 269 batch_loss: 0.22582557797431946\n",
      "training: 91 batch 270 batch_loss: 0.2287766933441162\n",
      "training: 91 batch 271 batch_loss: 0.2291908860206604\n",
      "training: 91 batch 272 batch_loss: 0.22691065073013306\n",
      "training: 91 batch 273 batch_loss: 0.2220952808856964\n",
      "training: 91 batch 274 batch_loss: 0.2261641025543213\n",
      "training: 91 batch 275 batch_loss: 0.22699731588363647\n",
      "training: 91 batch 276 batch_loss: 0.227434903383255\n",
      "training: 91 batch 277 batch_loss: 0.23048704862594604\n",
      "training: 91 batch 278 batch_loss: 0.2300100326538086\n",
      "training: 91 batch 279 batch_loss: 0.23274728655815125\n",
      "training: 91 batch 280 batch_loss: 0.22843486070632935\n",
      "training: 91 batch 281 batch_loss: 0.22750753164291382\n",
      "training: 91 batch 282 batch_loss: 0.23061603307724\n",
      "training: 91 batch 283 batch_loss: 0.229813814163208\n",
      "training: 91 batch 284 batch_loss: 0.23080772161483765\n",
      "training: 91 batch 285 batch_loss: 0.22727960348129272\n",
      "training: 91 batch 286 batch_loss: 0.2292262613773346\n",
      "training: 91 batch 287 batch_loss: 0.22928059101104736\n",
      "training: 91 batch 288 batch_loss: 0.2281557023525238\n",
      "training: 91 batch 289 batch_loss: 0.2281811535358429\n",
      "training: 91 batch 290 batch_loss: 0.23547598719596863\n",
      "training: 91 batch 291 batch_loss: 0.2302374243736267\n",
      "training: 91 batch 292 batch_loss: 0.23313376307487488\n",
      "training: 91 batch 293 batch_loss: 0.2262192964553833\n",
      "training: 91 batch 294 batch_loss: 0.22663062810897827\n",
      "training: 91 batch 295 batch_loss: 0.22990906238555908\n",
      "training: 91 batch 296 batch_loss: 0.2254413366317749\n",
      "training: 91 batch 297 batch_loss: 0.2321884036064148\n",
      "training: 91 batch 298 batch_loss: 0.22688814997673035\n",
      "training: 91 batch 299 batch_loss: 0.22761008143424988\n",
      "training: 91 batch 300 batch_loss: 0.22946584224700928\n",
      "training: 91 batch 301 batch_loss: 0.22882461547851562\n",
      "training: 91 batch 302 batch_loss: 0.23009711503982544\n",
      "training: 91 batch 303 batch_loss: 0.22978618741035461\n",
      "training: 91 batch 304 batch_loss: 0.22972053289413452\n",
      "training: 91 batch 305 batch_loss: 0.23341915011405945\n",
      "training: 91 batch 306 batch_loss: 0.22413986921310425\n",
      "training: 91 batch 307 batch_loss: 0.22577479481697083\n",
      "training: 91 batch 308 batch_loss: 0.2259262204170227\n",
      "training: 91 batch 309 batch_loss: 0.22950172424316406\n",
      "training: 91 batch 310 batch_loss: 0.2259763777256012\n",
      "training: 91 batch 311 batch_loss: 0.2268175482749939\n",
      "training: 91 batch 312 batch_loss: 0.22753700613975525\n",
      "training: 91 batch 313 batch_loss: 0.23541313409805298\n",
      "training: 91 batch 314 batch_loss: 0.22536924481391907\n",
      "training: 91 batch 315 batch_loss: 0.22830477356910706\n",
      "training: 91 batch 316 batch_loss: 0.2309635877609253\n",
      "training: 91 batch 317 batch_loss: 0.22610247135162354\n",
      "training: 91 batch 318 batch_loss: 0.22905585169792175\n",
      "training: 91 batch 319 batch_loss: 0.22649890184402466\n",
      "training: 91 batch 320 batch_loss: 0.23028564453125\n",
      "training: 91 batch 321 batch_loss: 0.22660794854164124\n",
      "training: 91 batch 322 batch_loss: 0.23261740803718567\n",
      "training: 91 batch 323 batch_loss: 0.22522574663162231\n",
      "training: 91 batch 324 batch_loss: 0.22971871495246887\n",
      "training: 91 batch 325 batch_loss: 0.22743386030197144\n",
      "training: 91 batch 326 batch_loss: 0.22659623622894287\n",
      "training: 91 batch 327 batch_loss: 0.22513717412948608\n",
      "training: 91 batch 328 batch_loss: 0.22556933760643005\n",
      "training: 91 batch 329 batch_loss: 0.22797325253486633\n",
      "training: 91 batch 330 batch_loss: 0.22633951902389526\n",
      "training: 91 batch 331 batch_loss: 0.2294824719429016\n",
      "training: 91 batch 332 batch_loss: 0.22847390174865723\n",
      "training: 91 batch 333 batch_loss: 0.2258683443069458\n",
      "training: 91 batch 334 batch_loss: 0.22537773847579956\n",
      "training: 91 batch 335 batch_loss: 0.22873616218566895\n",
      "training: 91 batch 336 batch_loss: 0.22455522418022156\n",
      "training: 91 batch 337 batch_loss: 0.22825640439987183\n",
      "training: 91 batch 338 batch_loss: 0.22965502738952637\n",
      "training: 91 batch 339 batch_loss: 0.23110371828079224\n",
      "training: 91 batch 340 batch_loss: 0.23086446523666382\n",
      "training: 91 batch 341 batch_loss: 0.2285703718662262\n",
      "training: 91 batch 342 batch_loss: 0.22803843021392822\n",
      "training: 91 batch 343 batch_loss: 0.22628608345985413\n",
      "training: 91 batch 344 batch_loss: 0.22979730367660522\n",
      "training: 91 batch 345 batch_loss: 0.22929614782333374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 91 batch 346 batch_loss: 0.22191166877746582\n",
      "training: 91 batch 347 batch_loss: 0.22640037536621094\n",
      "training: 91 batch 348 batch_loss: 0.22886192798614502\n",
      "training: 91 batch 349 batch_loss: 0.22750172019004822\n",
      "training: 91 batch 350 batch_loss: 0.22977560758590698\n",
      "training: 91 batch 351 batch_loss: 0.22949856519699097\n",
      "training: 91 batch 352 batch_loss: 0.2308368682861328\n",
      "training: 91 batch 353 batch_loss: 0.22470247745513916\n",
      "training: 91 batch 354 batch_loss: 0.22705426812171936\n",
      "training: 91 batch 355 batch_loss: 0.23338079452514648\n",
      "training: 91 batch 356 batch_loss: 0.2294953167438507\n",
      "training: 91 batch 357 batch_loss: 0.22853899002075195\n",
      "training: 91 batch 358 batch_loss: 0.23450839519500732\n",
      "training: 91 batch 359 batch_loss: 0.22662776708602905\n",
      "training: 91 batch 360 batch_loss: 0.23066163063049316\n",
      "training: 91 batch 361 batch_loss: 0.2299519181251526\n",
      "training: 91 batch 362 batch_loss: 0.22473779320716858\n",
      "training: 91 batch 363 batch_loss: 0.23020970821380615\n",
      "training: 91 batch 364 batch_loss: 0.2304544448852539\n",
      "training: 91 batch 365 batch_loss: 0.226526141166687\n",
      "training: 91 batch 366 batch_loss: 0.22504061460494995\n",
      "training: 91 batch 367 batch_loss: 0.22910994291305542\n",
      "training: 91 batch 368 batch_loss: 0.22889360785484314\n",
      "training: 91 batch 369 batch_loss: 0.225021630525589\n",
      "training: 91 batch 370 batch_loss: 0.22855979204177856\n",
      "training: 91 batch 371 batch_loss: 0.23001793026924133\n",
      "training: 91 batch 372 batch_loss: 0.2271173894405365\n",
      "training: 91 batch 373 batch_loss: 0.22697684168815613\n",
      "training: 91 batch 374 batch_loss: 0.23136651515960693\n",
      "training: 91 batch 375 batch_loss: 0.2320231795310974\n",
      "training: 91 batch 376 batch_loss: 0.23019906878471375\n",
      "training: 91 batch 377 batch_loss: 0.225949227809906\n",
      "training: 91 batch 378 batch_loss: 0.2328614890575409\n",
      "training: 91 batch 379 batch_loss: 0.2287587821483612\n",
      "training: 91 batch 380 batch_loss: 0.22549733519554138\n",
      "training: 91 batch 381 batch_loss: 0.2303084433078766\n",
      "training: 91 batch 382 batch_loss: 0.2265404462814331\n",
      "training: 91 batch 383 batch_loss: 0.2314336895942688\n",
      "training: 91 batch 384 batch_loss: 0.22800126671791077\n",
      "training: 91 batch 385 batch_loss: 0.22765761613845825\n",
      "training: 91 batch 386 batch_loss: 0.22878336906433105\n",
      "training: 91 batch 387 batch_loss: 0.235100656747818\n",
      "training: 91 batch 388 batch_loss: 0.2314237654209137\n",
      "training: 91 batch 389 batch_loss: 0.22849664092063904\n",
      "training: 91 batch 390 batch_loss: 0.22762641310691833\n",
      "training: 91 batch 391 batch_loss: 0.2277393639087677\n",
      "training: 91 batch 392 batch_loss: 0.23008260130882263\n",
      "training: 91 batch 393 batch_loss: 0.22734099626541138\n",
      "training: 91 batch 394 batch_loss: 0.22972610592842102\n",
      "training: 91 batch 395 batch_loss: 0.23011618852615356\n",
      "training: 91 batch 396 batch_loss: 0.2331448495388031\n",
      "training: 91 batch 397 batch_loss: 0.22859525680541992\n",
      "training: 91 batch 398 batch_loss: 0.2321602702140808\n",
      "training: 91 batch 399 batch_loss: 0.2313571572303772\n",
      "training: 91 batch 400 batch_loss: 0.22486287355422974\n",
      "training: 91 batch 401 batch_loss: 0.2306780219078064\n",
      "training: 91 batch 402 batch_loss: 0.23019322752952576\n",
      "training: 91 batch 403 batch_loss: 0.2280709147453308\n",
      "training: 91 batch 404 batch_loss: 0.22887063026428223\n",
      "training: 91 batch 405 batch_loss: 0.22679901123046875\n",
      "training: 91 batch 406 batch_loss: 0.2323397994041443\n",
      "training: 91 batch 407 batch_loss: 0.2301052212715149\n",
      "training: 91 batch 408 batch_loss: 0.22895976901054382\n",
      "training: 91 batch 409 batch_loss: 0.22337868809700012\n",
      "training: 91 batch 410 batch_loss: 0.22693437337875366\n",
      "training: 91 batch 411 batch_loss: 0.2306382954120636\n",
      "training: 91 batch 412 batch_loss: 0.22875678539276123\n",
      "training: 91 batch 413 batch_loss: 0.23076093196868896\n",
      "training: 91 batch 414 batch_loss: 0.2263391613960266\n",
      "training: 91 batch 415 batch_loss: 0.22592708468437195\n",
      "training: 91 batch 416 batch_loss: 0.22080615162849426\n",
      "training: 91 batch 417 batch_loss: 0.22983011603355408\n",
      "training: 91 batch 418 batch_loss: 0.23047521710395813\n",
      "training: 91 batch 419 batch_loss: 0.22965162992477417\n",
      "training: 91 batch 420 batch_loss: 0.22807154059410095\n",
      "training: 91 batch 421 batch_loss: 0.22503027319908142\n",
      "training: 91 batch 422 batch_loss: 0.23420855402946472\n",
      "training: 91 batch 423 batch_loss: 0.22936105728149414\n",
      "training: 91 batch 424 batch_loss: 0.22990158200263977\n",
      "training: 91 batch 425 batch_loss: 0.22836151719093323\n",
      "training: 91 batch 426 batch_loss: 0.2275107502937317\n",
      "training: 91 batch 427 batch_loss: 0.22883445024490356\n",
      "training: 91 batch 428 batch_loss: 0.22489404678344727\n",
      "training: 91 batch 429 batch_loss: 0.2324201762676239\n",
      "training: 91 batch 430 batch_loss: 0.22257179021835327\n",
      "training: 91 batch 431 batch_loss: 0.22750353813171387\n",
      "training: 91 batch 432 batch_loss: 0.22901439666748047\n",
      "training: 91 batch 433 batch_loss: 0.22964835166931152\n",
      "training: 91 batch 434 batch_loss: 0.227918803691864\n",
      "training: 91 batch 435 batch_loss: 0.2260546088218689\n",
      "training: 91 batch 436 batch_loss: 0.2277512550354004\n",
      "training: 91 batch 437 batch_loss: 0.22882062196731567\n",
      "training: 91 batch 438 batch_loss: 0.23141559958457947\n",
      "training: 91 batch 439 batch_loss: 0.2260410189628601\n",
      "training: 91 batch 440 batch_loss: 0.23136687278747559\n",
      "training: 91 batch 441 batch_loss: 0.22944995760917664\n",
      "training: 91 batch 442 batch_loss: 0.22817635536193848\n",
      "training: 91 batch 443 batch_loss: 0.23126313090324402\n",
      "training: 91 batch 444 batch_loss: 0.23042166233062744\n",
      "training: 91 batch 445 batch_loss: 0.23215055465698242\n",
      "training: 91 batch 446 batch_loss: 0.23015591502189636\n",
      "training: 91 batch 447 batch_loss: 0.22731664776802063\n",
      "training: 91 batch 448 batch_loss: 0.22654813528060913\n",
      "training: 91 batch 449 batch_loss: 0.2264554798603058\n",
      "training: 91 batch 450 batch_loss: 0.23110094666481018\n",
      "training: 91 batch 451 batch_loss: 0.22493481636047363\n",
      "training: 91 batch 452 batch_loss: 0.23279836773872375\n",
      "training: 91 batch 453 batch_loss: 0.2311888337135315\n",
      "training: 91 batch 454 batch_loss: 0.22987520694732666\n",
      "training: 91 batch 455 batch_loss: 0.2272336483001709\n",
      "training: 91 batch 456 batch_loss: 0.22995120286941528\n",
      "training: 91 batch 457 batch_loss: 0.23111698031425476\n",
      "training: 91 batch 458 batch_loss: 0.22697344422340393\n",
      "training: 91 batch 459 batch_loss: 0.2285226583480835\n",
      "training: 91 batch 460 batch_loss: 0.230046808719635\n",
      "training: 91 batch 461 batch_loss: 0.22722327709197998\n",
      "training: 91 batch 462 batch_loss: 0.2258804440498352\n",
      "training: 91 batch 463 batch_loss: 0.22775936126708984\n",
      "training: 91 batch 464 batch_loss: 0.22988054156303406\n",
      "training: 91 batch 465 batch_loss: 0.23221582174301147\n",
      "training: 91 batch 466 batch_loss: 0.2296660840511322\n",
      "training: 91 batch 467 batch_loss: 0.2247149646282196\n",
      "training: 91 batch 468 batch_loss: 0.23284536600112915\n",
      "training: 91 batch 469 batch_loss: 0.22467714548110962\n",
      "training: 91 batch 470 batch_loss: 0.2243686020374298\n",
      "training: 91 batch 471 batch_loss: 0.22801867127418518\n",
      "training: 91 batch 472 batch_loss: 0.2279013693332672\n",
      "training: 91 batch 473 batch_loss: 0.23024815320968628\n",
      "training: 91 batch 474 batch_loss: 0.23139876127243042\n",
      "training: 91 batch 475 batch_loss: 0.2275804877281189\n",
      "training: 91 batch 476 batch_loss: 0.23077335953712463\n",
      "training: 91 batch 477 batch_loss: 0.23212993144989014\n",
      "training: 91 batch 478 batch_loss: 0.2315579056739807\n",
      "training: 91 batch 479 batch_loss: 0.22541946172714233\n",
      "training: 91 batch 480 batch_loss: 0.23402062058448792\n",
      "training: 91 batch 481 batch_loss: 0.22912999987602234\n",
      "training: 91 batch 482 batch_loss: 0.22606146335601807\n",
      "training: 91 batch 483 batch_loss: 0.22987604141235352\n",
      "training: 91 batch 484 batch_loss: 0.22744089365005493\n",
      "training: 91 batch 485 batch_loss: 0.23225253820419312\n",
      "training: 91 batch 486 batch_loss: 0.22963324189186096\n",
      "training: 91 batch 487 batch_loss: 0.22360065579414368\n",
      "training: 91 batch 488 batch_loss: 0.22639790177345276\n",
      "training: 91 batch 489 batch_loss: 0.22832617163658142\n",
      "training: 91 batch 490 batch_loss: 0.23331016302108765\n",
      "training: 91 batch 491 batch_loss: 0.2274368703365326\n",
      "training: 91 batch 492 batch_loss: 0.23150289058685303\n",
      "training: 91 batch 493 batch_loss: 0.22853046655654907\n",
      "training: 91 batch 494 batch_loss: 0.23510372638702393\n",
      "training: 91 batch 495 batch_loss: 0.22721320390701294\n",
      "training: 91 batch 496 batch_loss: 0.23255330324172974\n",
      "training: 91 batch 497 batch_loss: 0.23212802410125732\n",
      "training: 91 batch 498 batch_loss: 0.23461365699768066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 91 batch 499 batch_loss: 0.22783949971199036\n",
      "training: 91 batch 500 batch_loss: 0.22791296243667603\n",
      "training: 91 batch 501 batch_loss: 0.22824716567993164\n",
      "training: 91 batch 502 batch_loss: 0.23067837953567505\n",
      "training: 91 batch 503 batch_loss: 0.2330007255077362\n",
      "training: 91 batch 504 batch_loss: 0.23298513889312744\n",
      "training: 91 batch 505 batch_loss: 0.2324371337890625\n",
      "training: 91 batch 506 batch_loss: 0.22552746534347534\n",
      "training: 91 batch 507 batch_loss: 0.22699588537216187\n",
      "training: 91 batch 508 batch_loss: 0.22964584827423096\n",
      "training: 91 batch 509 batch_loss: 0.2294248342514038\n",
      "training: 91 batch 510 batch_loss: 0.2301221489906311\n",
      "training: 91 batch 511 batch_loss: 0.23103418946266174\n",
      "training: 91 batch 512 batch_loss: 0.23242834210395813\n",
      "training: 91 batch 513 batch_loss: 0.22571343183517456\n",
      "training: 91 batch 514 batch_loss: 0.2313755750656128\n",
      "training: 91 batch 515 batch_loss: 0.22938179969787598\n",
      "training: 91 batch 516 batch_loss: 0.23103761672973633\n",
      "training: 91 batch 517 batch_loss: 0.22782450914382935\n",
      "training: 91 batch 518 batch_loss: 0.23172008991241455\n",
      "training: 91 batch 519 batch_loss: 0.23316329717636108\n",
      "training: 91 batch 520 batch_loss: 0.22747284173965454\n",
      "training: 91 batch 521 batch_loss: 0.2250264585018158\n",
      "training: 91 batch 522 batch_loss: 0.23099100589752197\n",
      "training: 91 batch 523 batch_loss: 0.22826892137527466\n",
      "training: 91 batch 524 batch_loss: 0.22900432348251343\n",
      "training: 91 batch 525 batch_loss: 0.22830414772033691\n",
      "training: 91 batch 526 batch_loss: 0.22727912664413452\n",
      "training: 91 batch 527 batch_loss: 0.22961503267288208\n",
      "training: 91 batch 528 batch_loss: 0.2321958839893341\n",
      "training: 91 batch 529 batch_loss: 0.2251078188419342\n",
      "training: 91 batch 530 batch_loss: 0.22764134407043457\n",
      "training: 91 batch 531 batch_loss: 0.2292580008506775\n",
      "training: 91 batch 532 batch_loss: 0.22834035754203796\n",
      "training: 91 batch 533 batch_loss: 0.23291829228401184\n",
      "training: 91 batch 534 batch_loss: 0.23147526383399963\n",
      "training: 91 batch 535 batch_loss: 0.22974836826324463\n",
      "training: 91 batch 536 batch_loss: 0.22472631931304932\n",
      "training: 91 batch 537 batch_loss: 0.2310924232006073\n",
      "training: 91 batch 538 batch_loss: 0.22832530736923218\n",
      "training: 91 batch 539 batch_loss: 0.2301476001739502\n",
      "training: 91 batch 540 batch_loss: 0.22680944204330444\n",
      "training: 91 batch 541 batch_loss: 0.22860008478164673\n",
      "training: 91 batch 542 batch_loss: 0.23118066787719727\n",
      "training: 91 batch 543 batch_loss: 0.23124033212661743\n",
      "training: 91 batch 544 batch_loss: 0.22916334867477417\n",
      "training: 91 batch 545 batch_loss: 0.2281404733657837\n",
      "training: 91 batch 546 batch_loss: 0.22743955254554749\n",
      "training: 91 batch 547 batch_loss: 0.2311098277568817\n",
      "training: 91 batch 548 batch_loss: 0.23084956407546997\n",
      "training: 91 batch 549 batch_loss: 0.2307189404964447\n",
      "training: 91 batch 550 batch_loss: 0.2253427505493164\n",
      "training: 91 batch 551 batch_loss: 0.22812849283218384\n",
      "training: 91 batch 552 batch_loss: 0.22809961438179016\n",
      "training: 91 batch 553 batch_loss: 0.23405671119689941\n",
      "training: 91 batch 554 batch_loss: 0.22983500361442566\n",
      "training: 91 batch 555 batch_loss: 0.23137950897216797\n",
      "training: 91 batch 556 batch_loss: 0.22986188530921936\n",
      "training: 91 batch 557 batch_loss: 0.2289465069770813\n",
      "training: 91 batch 558 batch_loss: 0.23056340217590332\n",
      "training: 91 batch 559 batch_loss: 0.22775378823280334\n",
      "training: 91 batch 560 batch_loss: 0.2294435203075409\n",
      "training: 91 batch 561 batch_loss: 0.2289450466632843\n",
      "training: 91 batch 562 batch_loss: 0.22629579901695251\n",
      "training: 91 batch 563 batch_loss: 0.23072189092636108\n",
      "training: 91 batch 564 batch_loss: 0.2294778823852539\n",
      "training: 91 batch 565 batch_loss: 0.22622418403625488\n",
      "training: 91 batch 566 batch_loss: 0.22904768586158752\n",
      "training: 91 batch 567 batch_loss: 0.22911858558654785\n",
      "training: 91 batch 568 batch_loss: 0.23012354969978333\n",
      "training: 91 batch 569 batch_loss: 0.22815155982971191\n",
      "training: 91 batch 570 batch_loss: 0.22657239437103271\n",
      "training: 91 batch 571 batch_loss: 0.22880417108535767\n",
      "training: 91 batch 572 batch_loss: 0.22981137037277222\n",
      "training: 91 batch 573 batch_loss: 0.2270917296409607\n",
      "training: 91 batch 574 batch_loss: 0.22520554065704346\n",
      "training: 91 batch 575 batch_loss: 0.23080086708068848\n",
      "training: 91 batch 576 batch_loss: 0.22859084606170654\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 91, Hit Ratio:0.03411471953043542 | Precision:0.050334218028113636 | Recall:0.065924760030861 | NDCG:0.06557326192202872\n",
      "*Best Performance* \n",
      "Epoch: 91, Hit Ratio:0.03411471953043542 | Precision:0.050334218028113636 | Recall:0.065924760030861 | MDCG:0.06557326192202872\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 92 batch 0 batch_loss: 0.227207213640213\n",
      "training: 92 batch 1 batch_loss: 0.2249472737312317\n",
      "training: 92 batch 2 batch_loss: 0.2249634861946106\n",
      "training: 92 batch 3 batch_loss: 0.2249547243118286\n",
      "training: 92 batch 4 batch_loss: 0.22613731026649475\n",
      "training: 92 batch 5 batch_loss: 0.2260713279247284\n",
      "training: 92 batch 6 batch_loss: 0.23189610242843628\n",
      "training: 92 batch 7 batch_loss: 0.22733435034751892\n",
      "training: 92 batch 8 batch_loss: 0.22524607181549072\n",
      "training: 92 batch 9 batch_loss: 0.2241095006465912\n",
      "training: 92 batch 10 batch_loss: 0.22946545481681824\n",
      "training: 92 batch 11 batch_loss: 0.22804412245750427\n",
      "training: 92 batch 12 batch_loss: 0.22992399334907532\n",
      "training: 92 batch 13 batch_loss: 0.22353509068489075\n",
      "training: 92 batch 14 batch_loss: 0.22430485486984253\n",
      "training: 92 batch 15 batch_loss: 0.22786739468574524\n",
      "training: 92 batch 16 batch_loss: 0.22758004069328308\n",
      "training: 92 batch 17 batch_loss: 0.2296883463859558\n",
      "training: 92 batch 18 batch_loss: 0.22660642862319946\n",
      "training: 92 batch 19 batch_loss: 0.2287602424621582\n",
      "training: 92 batch 20 batch_loss: 0.2315891981124878\n",
      "training: 92 batch 21 batch_loss: 0.23143482208251953\n",
      "training: 92 batch 22 batch_loss: 0.22822299599647522\n",
      "training: 92 batch 23 batch_loss: 0.2260562777519226\n",
      "training: 92 batch 24 batch_loss: 0.22883880138397217\n",
      "training: 92 batch 25 batch_loss: 0.22828558087348938\n",
      "training: 92 batch 26 batch_loss: 0.22869372367858887\n",
      "training: 92 batch 27 batch_loss: 0.22875657677650452\n",
      "training: 92 batch 28 batch_loss: 0.2334946095943451\n",
      "training: 92 batch 29 batch_loss: 0.2248671054840088\n",
      "training: 92 batch 30 batch_loss: 0.23155349493026733\n",
      "training: 92 batch 31 batch_loss: 0.2283480167388916\n",
      "training: 92 batch 32 batch_loss: 0.22574636340141296\n",
      "training: 92 batch 33 batch_loss: 0.23028218746185303\n",
      "training: 92 batch 34 batch_loss: 0.233961284160614\n",
      "training: 92 batch 35 batch_loss: 0.22758528590202332\n",
      "training: 92 batch 36 batch_loss: 0.22874584794044495\n",
      "training: 92 batch 37 batch_loss: 0.22326070070266724\n",
      "training: 92 batch 38 batch_loss: 0.22930973768234253\n",
      "training: 92 batch 39 batch_loss: 0.23078101873397827\n",
      "training: 92 batch 40 batch_loss: 0.22654062509536743\n",
      "training: 92 batch 41 batch_loss: 0.22835659980773926\n",
      "training: 92 batch 42 batch_loss: 0.2250538170337677\n",
      "training: 92 batch 43 batch_loss: 0.22689467668533325\n",
      "training: 92 batch 44 batch_loss: 0.23105210065841675\n",
      "training: 92 batch 45 batch_loss: 0.22818297147750854\n",
      "training: 92 batch 46 batch_loss: 0.2268146276473999\n",
      "training: 92 batch 47 batch_loss: 0.2286664843559265\n",
      "training: 92 batch 48 batch_loss: 0.22646528482437134\n",
      "training: 92 batch 49 batch_loss: 0.2305375039577484\n",
      "training: 92 batch 50 batch_loss: 0.22930240631103516\n",
      "training: 92 batch 51 batch_loss: 0.22936341166496277\n",
      "training: 92 batch 52 batch_loss: 0.2300395369529724\n",
      "training: 92 batch 53 batch_loss: 0.2291303277015686\n",
      "training: 92 batch 54 batch_loss: 0.22557035088539124\n",
      "training: 92 batch 55 batch_loss: 0.22705721855163574\n",
      "training: 92 batch 56 batch_loss: 0.2250795066356659\n",
      "training: 92 batch 57 batch_loss: 0.2274247109889984\n",
      "training: 92 batch 58 batch_loss: 0.22717803716659546\n",
      "training: 92 batch 59 batch_loss: 0.22935760021209717\n",
      "training: 92 batch 60 batch_loss: 0.2270435094833374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 92 batch 61 batch_loss: 0.22810685634613037\n",
      "training: 92 batch 62 batch_loss: 0.22667688131332397\n",
      "training: 92 batch 63 batch_loss: 0.22899800539016724\n",
      "training: 92 batch 64 batch_loss: 0.2282784879207611\n",
      "training: 92 batch 65 batch_loss: 0.22576898336410522\n",
      "training: 92 batch 66 batch_loss: 0.22753509879112244\n",
      "training: 92 batch 67 batch_loss: 0.22974127531051636\n",
      "training: 92 batch 68 batch_loss: 0.22775226831436157\n",
      "training: 92 batch 69 batch_loss: 0.2321990728378296\n",
      "training: 92 batch 70 batch_loss: 0.2291661500930786\n",
      "training: 92 batch 71 batch_loss: 0.22920730710029602\n",
      "training: 92 batch 72 batch_loss: 0.22400131821632385\n",
      "training: 92 batch 73 batch_loss: 0.22989913821220398\n",
      "training: 92 batch 74 batch_loss: 0.22388410568237305\n",
      "training: 92 batch 75 batch_loss: 0.223979651927948\n",
      "training: 92 batch 76 batch_loss: 0.22417008876800537\n",
      "training: 92 batch 77 batch_loss: 0.22976726293563843\n",
      "training: 92 batch 78 batch_loss: 0.22420430183410645\n",
      "training: 92 batch 79 batch_loss: 0.2286745011806488\n",
      "training: 92 batch 80 batch_loss: 0.22655487060546875\n",
      "training: 92 batch 81 batch_loss: 0.23384639620780945\n",
      "training: 92 batch 82 batch_loss: 0.22831928730010986\n",
      "training: 92 batch 83 batch_loss: 0.22801396250724792\n",
      "training: 92 batch 84 batch_loss: 0.23249351978302002\n",
      "training: 92 batch 85 batch_loss: 0.23300379514694214\n",
      "training: 92 batch 86 batch_loss: 0.2314647138118744\n",
      "training: 92 batch 87 batch_loss: 0.23119693994522095\n",
      "training: 92 batch 88 batch_loss: 0.22887259721755981\n",
      "training: 92 batch 89 batch_loss: 0.22991642355918884\n",
      "training: 92 batch 90 batch_loss: 0.23025241494178772\n",
      "training: 92 batch 91 batch_loss: 0.2304357886314392\n",
      "training: 92 batch 92 batch_loss: 0.230668842792511\n",
      "training: 92 batch 93 batch_loss: 0.2268311083316803\n",
      "training: 92 batch 94 batch_loss: 0.2271106243133545\n",
      "training: 92 batch 95 batch_loss: 0.22997456789016724\n",
      "training: 92 batch 96 batch_loss: 0.2258983552455902\n",
      "training: 92 batch 97 batch_loss: 0.23053085803985596\n",
      "training: 92 batch 98 batch_loss: 0.2250308096408844\n",
      "training: 92 batch 99 batch_loss: 0.23097512125968933\n",
      "training: 92 batch 100 batch_loss: 0.2278805673122406\n",
      "training: 92 batch 101 batch_loss: 0.2250044047832489\n",
      "training: 92 batch 102 batch_loss: 0.22860020399093628\n",
      "training: 92 batch 103 batch_loss: 0.2313992977142334\n",
      "training: 92 batch 104 batch_loss: 0.22729802131652832\n",
      "training: 92 batch 105 batch_loss: 0.22871500253677368\n",
      "training: 92 batch 106 batch_loss: 0.2279195487499237\n",
      "training: 92 batch 107 batch_loss: 0.2332097291946411\n",
      "training: 92 batch 108 batch_loss: 0.2292451560497284\n",
      "training: 92 batch 109 batch_loss: 0.227823406457901\n",
      "training: 92 batch 110 batch_loss: 0.22407686710357666\n",
      "training: 92 batch 111 batch_loss: 0.23029470443725586\n",
      "training: 92 batch 112 batch_loss: 0.22904819250106812\n",
      "training: 92 batch 113 batch_loss: 0.22952258586883545\n",
      "training: 92 batch 114 batch_loss: 0.2272489070892334\n",
      "training: 92 batch 115 batch_loss: 0.2278614044189453\n",
      "training: 92 batch 116 batch_loss: 0.23230716586112976\n",
      "training: 92 batch 117 batch_loss: 0.23042023181915283\n",
      "training: 92 batch 118 batch_loss: 0.2287481427192688\n",
      "training: 92 batch 119 batch_loss: 0.22614973783493042\n",
      "training: 92 batch 120 batch_loss: 0.22711950540542603\n",
      "training: 92 batch 121 batch_loss: 0.2234642505645752\n",
      "training: 92 batch 122 batch_loss: 0.22535163164138794\n",
      "training: 92 batch 123 batch_loss: 0.23100382089614868\n",
      "training: 92 batch 124 batch_loss: 0.2318422496318817\n",
      "training: 92 batch 125 batch_loss: 0.22790580987930298\n",
      "training: 92 batch 126 batch_loss: 0.23217642307281494\n",
      "training: 92 batch 127 batch_loss: 0.22789010405540466\n",
      "training: 92 batch 128 batch_loss: 0.23230934143066406\n",
      "training: 92 batch 129 batch_loss: 0.2279079556465149\n",
      "training: 92 batch 130 batch_loss: 0.22793146967887878\n",
      "training: 92 batch 131 batch_loss: 0.22848844528198242\n",
      "training: 92 batch 132 batch_loss: 0.22674202919006348\n",
      "training: 92 batch 133 batch_loss: 0.22750607132911682\n",
      "training: 92 batch 134 batch_loss: 0.224807471036911\n",
      "training: 92 batch 135 batch_loss: 0.22925877571105957\n",
      "training: 92 batch 136 batch_loss: 0.22622430324554443\n",
      "training: 92 batch 137 batch_loss: 0.22083374857902527\n",
      "training: 92 batch 138 batch_loss: 0.22848761081695557\n",
      "training: 92 batch 139 batch_loss: 0.2284409999847412\n",
      "training: 92 batch 140 batch_loss: 0.2259455919265747\n",
      "training: 92 batch 141 batch_loss: 0.22982871532440186\n",
      "training: 92 batch 142 batch_loss: 0.22803032398223877\n",
      "training: 92 batch 143 batch_loss: 0.23433804512023926\n",
      "training: 92 batch 144 batch_loss: 0.22899681329727173\n",
      "training: 92 batch 145 batch_loss: 0.22569310665130615\n",
      "training: 92 batch 146 batch_loss: 0.22900736331939697\n",
      "training: 92 batch 147 batch_loss: 0.22560790181159973\n",
      "training: 92 batch 148 batch_loss: 0.2243136465549469\n",
      "training: 92 batch 149 batch_loss: 0.2266339659690857\n",
      "training: 92 batch 150 batch_loss: 0.226771742105484\n",
      "training: 92 batch 151 batch_loss: 0.22444722056388855\n",
      "training: 92 batch 152 batch_loss: 0.22889170050621033\n",
      "training: 92 batch 153 batch_loss: 0.2254590392112732\n",
      "training: 92 batch 154 batch_loss: 0.22497397661209106\n",
      "training: 92 batch 155 batch_loss: 0.22997534275054932\n",
      "training: 92 batch 156 batch_loss: 0.2290630340576172\n",
      "training: 92 batch 157 batch_loss: 0.22904005646705627\n",
      "training: 92 batch 158 batch_loss: 0.2336271107196808\n",
      "training: 92 batch 159 batch_loss: 0.2273772656917572\n",
      "training: 92 batch 160 batch_loss: 0.22189930081367493\n",
      "training: 92 batch 161 batch_loss: 0.228684663772583\n",
      "training: 92 batch 162 batch_loss: 0.22583884000778198\n",
      "training: 92 batch 163 batch_loss: 0.23127400875091553\n",
      "training: 92 batch 164 batch_loss: 0.22686082124710083\n",
      "training: 92 batch 165 batch_loss: 0.22802680730819702\n",
      "training: 92 batch 166 batch_loss: 0.22896727919578552\n",
      "training: 92 batch 167 batch_loss: 0.22761279344558716\n",
      "training: 92 batch 168 batch_loss: 0.22414499521255493\n",
      "training: 92 batch 169 batch_loss: 0.22971779108047485\n",
      "training: 92 batch 170 batch_loss: 0.22701483964920044\n",
      "training: 92 batch 171 batch_loss: 0.23138749599456787\n",
      "training: 92 batch 172 batch_loss: 0.2232920527458191\n",
      "training: 92 batch 173 batch_loss: 0.22954103350639343\n",
      "training: 92 batch 174 batch_loss: 0.2306523323059082\n",
      "training: 92 batch 175 batch_loss: 0.2260059118270874\n",
      "training: 92 batch 176 batch_loss: 0.22842806577682495\n",
      "training: 92 batch 177 batch_loss: 0.2280542552471161\n",
      "training: 92 batch 178 batch_loss: 0.23039191961288452\n",
      "training: 92 batch 179 batch_loss: 0.22735479474067688\n",
      "training: 92 batch 180 batch_loss: 0.2299591302871704\n",
      "training: 92 batch 181 batch_loss: 0.2287222445011139\n",
      "training: 92 batch 182 batch_loss: 0.2290353775024414\n",
      "training: 92 batch 183 batch_loss: 0.2262839376926422\n",
      "training: 92 batch 184 batch_loss: 0.22394031286239624\n",
      "training: 92 batch 185 batch_loss: 0.2324000895023346\n",
      "training: 92 batch 186 batch_loss: 0.22748517990112305\n",
      "training: 92 batch 187 batch_loss: 0.2313973307609558\n",
      "training: 92 batch 188 batch_loss: 0.22841814160346985\n",
      "training: 92 batch 189 batch_loss: 0.22901859879493713\n",
      "training: 92 batch 190 batch_loss: 0.23053011298179626\n",
      "training: 92 batch 191 batch_loss: 0.23163649439811707\n",
      "training: 92 batch 192 batch_loss: 0.23127630352973938\n",
      "training: 92 batch 193 batch_loss: 0.22614187002182007\n",
      "training: 92 batch 194 batch_loss: 0.225620836019516\n",
      "training: 92 batch 195 batch_loss: 0.22915691137313843\n",
      "training: 92 batch 196 batch_loss: 0.231926828622818\n",
      "training: 92 batch 197 batch_loss: 0.23065882921218872\n",
      "training: 92 batch 198 batch_loss: 0.22449809312820435\n",
      "training: 92 batch 199 batch_loss: 0.22541609406471252\n",
      "training: 92 batch 200 batch_loss: 0.2289588451385498\n",
      "training: 92 batch 201 batch_loss: 0.2270166277885437\n",
      "training: 92 batch 202 batch_loss: 0.22956186532974243\n",
      "training: 92 batch 203 batch_loss: 0.22857502102851868\n",
      "training: 92 batch 204 batch_loss: 0.22872889041900635\n",
      "training: 92 batch 205 batch_loss: 0.2270936369895935\n",
      "training: 92 batch 206 batch_loss: 0.22256645560264587\n",
      "training: 92 batch 207 batch_loss: 0.2267310619354248\n",
      "training: 92 batch 208 batch_loss: 0.23198071122169495\n",
      "training: 92 batch 209 batch_loss: 0.23093467950820923\n",
      "training: 92 batch 210 batch_loss: 0.22674745321273804\n",
      "training: 92 batch 211 batch_loss: 0.2347542643547058\n",
      "training: 92 batch 212 batch_loss: 0.23247700929641724\n",
      "training: 92 batch 213 batch_loss: 0.22560104727745056\n",
      "training: 92 batch 214 batch_loss: 0.23005059361457825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 92 batch 215 batch_loss: 0.23066815733909607\n",
      "training: 92 batch 216 batch_loss: 0.22883492708206177\n",
      "training: 92 batch 217 batch_loss: 0.22275707125663757\n",
      "training: 92 batch 218 batch_loss: 0.22726616263389587\n",
      "training: 92 batch 219 batch_loss: 0.22693508863449097\n",
      "training: 92 batch 220 batch_loss: 0.2266099452972412\n",
      "training: 92 batch 221 batch_loss: 0.22800910472869873\n",
      "training: 92 batch 222 batch_loss: 0.22956141829490662\n",
      "training: 92 batch 223 batch_loss: 0.22987255454063416\n",
      "training: 92 batch 224 batch_loss: 0.23142367601394653\n",
      "training: 92 batch 225 batch_loss: 0.2277064323425293\n",
      "training: 92 batch 226 batch_loss: 0.22942721843719482\n",
      "training: 92 batch 227 batch_loss: 0.22943758964538574\n",
      "training: 92 batch 228 batch_loss: 0.22821369767189026\n",
      "training: 92 batch 229 batch_loss: 0.2276366651058197\n",
      "training: 92 batch 230 batch_loss: 0.22899717092514038\n",
      "training: 92 batch 231 batch_loss: 0.22284847497940063\n",
      "training: 92 batch 232 batch_loss: 0.2310613989830017\n",
      "training: 92 batch 233 batch_loss: 0.22878602147102356\n",
      "training: 92 batch 234 batch_loss: 0.23155266046524048\n",
      "training: 92 batch 235 batch_loss: 0.22720563411712646\n",
      "training: 92 batch 236 batch_loss: 0.2274768054485321\n",
      "training: 92 batch 237 batch_loss: 0.22794854640960693\n",
      "training: 92 batch 238 batch_loss: 0.22596371173858643\n",
      "training: 92 batch 239 batch_loss: 0.22845813632011414\n",
      "training: 92 batch 240 batch_loss: 0.22784128785133362\n",
      "training: 92 batch 241 batch_loss: 0.2303452491760254\n",
      "training: 92 batch 242 batch_loss: 0.23012584447860718\n",
      "training: 92 batch 243 batch_loss: 0.22942382097244263\n",
      "training: 92 batch 244 batch_loss: 0.2274475395679474\n",
      "training: 92 batch 245 batch_loss: 0.22511756420135498\n",
      "training: 92 batch 246 batch_loss: 0.229883074760437\n",
      "training: 92 batch 247 batch_loss: 0.23609191179275513\n",
      "training: 92 batch 248 batch_loss: 0.2298641800880432\n",
      "training: 92 batch 249 batch_loss: 0.2274933159351349\n",
      "training: 92 batch 250 batch_loss: 0.22944599390029907\n",
      "training: 92 batch 251 batch_loss: 0.22875350713729858\n",
      "training: 92 batch 252 batch_loss: 0.2267981767654419\n",
      "training: 92 batch 253 batch_loss: 0.23070448637008667\n",
      "training: 92 batch 254 batch_loss: 0.22911587357521057\n",
      "training: 92 batch 255 batch_loss: 0.2238454520702362\n",
      "training: 92 batch 256 batch_loss: 0.2301998734474182\n",
      "training: 92 batch 257 batch_loss: 0.2309616506099701\n",
      "training: 92 batch 258 batch_loss: 0.22673112154006958\n",
      "training: 92 batch 259 batch_loss: 0.22149866819381714\n",
      "training: 92 batch 260 batch_loss: 0.22769156098365784\n",
      "training: 92 batch 261 batch_loss: 0.22734785079956055\n",
      "training: 92 batch 262 batch_loss: 0.22950446605682373\n",
      "training: 92 batch 263 batch_loss: 0.22777938842773438\n",
      "training: 92 batch 264 batch_loss: 0.22906553745269775\n",
      "training: 92 batch 265 batch_loss: 0.22735384106636047\n",
      "training: 92 batch 266 batch_loss: 0.22556066513061523\n",
      "training: 92 batch 267 batch_loss: 0.2282547950744629\n",
      "training: 92 batch 268 batch_loss: 0.22595641016960144\n",
      "training: 92 batch 269 batch_loss: 0.2239057719707489\n",
      "training: 92 batch 270 batch_loss: 0.2295781970024109\n",
      "training: 92 batch 271 batch_loss: 0.23141586780548096\n",
      "training: 92 batch 272 batch_loss: 0.23020851612091064\n",
      "training: 92 batch 273 batch_loss: 0.22709381580352783\n",
      "training: 92 batch 274 batch_loss: 0.2309904396533966\n",
      "training: 92 batch 275 batch_loss: 0.22428634762763977\n",
      "training: 92 batch 276 batch_loss: 0.22790712118148804\n",
      "training: 92 batch 277 batch_loss: 0.23077112436294556\n",
      "training: 92 batch 278 batch_loss: 0.22499829530715942\n",
      "training: 92 batch 279 batch_loss: 0.22898060083389282\n",
      "training: 92 batch 280 batch_loss: 0.23119091987609863\n",
      "training: 92 batch 281 batch_loss: 0.2247428596019745\n",
      "training: 92 batch 282 batch_loss: 0.22784680128097534\n",
      "training: 92 batch 283 batch_loss: 0.22852957248687744\n",
      "training: 92 batch 284 batch_loss: 0.22666996717453003\n",
      "training: 92 batch 285 batch_loss: 0.23070096969604492\n",
      "training: 92 batch 286 batch_loss: 0.22912567853927612\n",
      "training: 92 batch 287 batch_loss: 0.22806420922279358\n",
      "training: 92 batch 288 batch_loss: 0.22680974006652832\n",
      "training: 92 batch 289 batch_loss: 0.22572341561317444\n",
      "training: 92 batch 290 batch_loss: 0.22701513767242432\n",
      "training: 92 batch 291 batch_loss: 0.2265019416809082\n",
      "training: 92 batch 292 batch_loss: 0.22616878151893616\n",
      "training: 92 batch 293 batch_loss: 0.23180103302001953\n",
      "training: 92 batch 294 batch_loss: 0.22494420409202576\n",
      "training: 92 batch 295 batch_loss: 0.227988600730896\n",
      "training: 92 batch 296 batch_loss: 0.230199933052063\n",
      "training: 92 batch 297 batch_loss: 0.22920870780944824\n",
      "training: 92 batch 298 batch_loss: 0.22544673085212708\n",
      "training: 92 batch 299 batch_loss: 0.230305016040802\n",
      "training: 92 batch 300 batch_loss: 0.22594010829925537\n",
      "training: 92 batch 301 batch_loss: 0.2307424545288086\n",
      "training: 92 batch 302 batch_loss: 0.22831404209136963\n",
      "training: 92 batch 303 batch_loss: 0.22735318541526794\n",
      "training: 92 batch 304 batch_loss: 0.22696146368980408\n",
      "training: 92 batch 305 batch_loss: 0.23102441430091858\n",
      "training: 92 batch 306 batch_loss: 0.23414728045463562\n",
      "training: 92 batch 307 batch_loss: 0.22661972045898438\n",
      "training: 92 batch 308 batch_loss: 0.2271350920200348\n",
      "training: 92 batch 309 batch_loss: 0.23200386762619019\n",
      "training: 92 batch 310 batch_loss: 0.2272213101387024\n",
      "training: 92 batch 311 batch_loss: 0.23083236813545227\n",
      "training: 92 batch 312 batch_loss: 0.23127499222755432\n",
      "training: 92 batch 313 batch_loss: 0.2349872589111328\n",
      "training: 92 batch 314 batch_loss: 0.22668680548667908\n",
      "training: 92 batch 315 batch_loss: 0.2290928065776825\n",
      "training: 92 batch 316 batch_loss: 0.229137122631073\n",
      "training: 92 batch 317 batch_loss: 0.2280055284500122\n",
      "training: 92 batch 318 batch_loss: 0.235076904296875\n",
      "training: 92 batch 319 batch_loss: 0.22993355989456177\n",
      "training: 92 batch 320 batch_loss: 0.22684115171432495\n",
      "training: 92 batch 321 batch_loss: 0.2266334593296051\n",
      "training: 92 batch 322 batch_loss: 0.22883811593055725\n",
      "training: 92 batch 323 batch_loss: 0.23170676827430725\n",
      "training: 92 batch 324 batch_loss: 0.22990581393241882\n",
      "training: 92 batch 325 batch_loss: 0.22908899188041687\n",
      "training: 92 batch 326 batch_loss: 0.227201908826828\n",
      "training: 92 batch 327 batch_loss: 0.22582125663757324\n",
      "training: 92 batch 328 batch_loss: 0.229458749294281\n",
      "training: 92 batch 329 batch_loss: 0.22893577814102173\n",
      "training: 92 batch 330 batch_loss: 0.22936493158340454\n",
      "training: 92 batch 331 batch_loss: 0.22864240407943726\n",
      "training: 92 batch 332 batch_loss: 0.23017734289169312\n",
      "training: 92 batch 333 batch_loss: 0.228883296251297\n",
      "training: 92 batch 334 batch_loss: 0.22759076952934265\n",
      "training: 92 batch 335 batch_loss: 0.22415077686309814\n",
      "training: 92 batch 336 batch_loss: 0.23020362854003906\n",
      "training: 92 batch 337 batch_loss: 0.22891923785209656\n",
      "training: 92 batch 338 batch_loss: 0.23099184036254883\n",
      "training: 92 batch 339 batch_loss: 0.232985258102417\n",
      "training: 92 batch 340 batch_loss: 0.23132473230361938\n",
      "training: 92 batch 341 batch_loss: 0.22434279322624207\n",
      "training: 92 batch 342 batch_loss: 0.23201796412467957\n",
      "training: 92 batch 343 batch_loss: 0.22700101137161255\n",
      "training: 92 batch 344 batch_loss: 0.2275504171848297\n",
      "training: 92 batch 345 batch_loss: 0.22514501214027405\n",
      "training: 92 batch 346 batch_loss: 0.22776803374290466\n",
      "training: 92 batch 347 batch_loss: 0.23323717713356018\n",
      "training: 92 batch 348 batch_loss: 0.23118487000465393\n",
      "training: 92 batch 349 batch_loss: 0.22991704940795898\n",
      "training: 92 batch 350 batch_loss: 0.22965490818023682\n",
      "training: 92 batch 351 batch_loss: 0.228469580411911\n",
      "training: 92 batch 352 batch_loss: 0.22807464003562927\n",
      "training: 92 batch 353 batch_loss: 0.22954797744750977\n",
      "training: 92 batch 354 batch_loss: 0.22815579175949097\n",
      "training: 92 batch 355 batch_loss: 0.22750899195671082\n",
      "training: 92 batch 356 batch_loss: 0.227751225233078\n",
      "training: 92 batch 357 batch_loss: 0.2293911576271057\n",
      "training: 92 batch 358 batch_loss: 0.23049694299697876\n",
      "training: 92 batch 359 batch_loss: 0.23250988125801086\n",
      "training: 92 batch 360 batch_loss: 0.22822505235671997\n",
      "training: 92 batch 361 batch_loss: 0.23273926973342896\n",
      "training: 92 batch 362 batch_loss: 0.22862255573272705\n",
      "training: 92 batch 363 batch_loss: 0.22576695680618286\n",
      "training: 92 batch 364 batch_loss: 0.22892141342163086\n",
      "training: 92 batch 365 batch_loss: 0.2308577597141266\n",
      "training: 92 batch 366 batch_loss: 0.2320842444896698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 92 batch 367 batch_loss: 0.2258128821849823\n",
      "training: 92 batch 368 batch_loss: 0.229379802942276\n",
      "training: 92 batch 369 batch_loss: 0.2276667356491089\n",
      "training: 92 batch 370 batch_loss: 0.2278066873550415\n",
      "training: 92 batch 371 batch_loss: 0.2268650233745575\n",
      "training: 92 batch 372 batch_loss: 0.23009204864501953\n",
      "training: 92 batch 373 batch_loss: 0.23073923587799072\n",
      "training: 92 batch 374 batch_loss: 0.22442671656608582\n",
      "training: 92 batch 375 batch_loss: 0.23032301664352417\n",
      "training: 92 batch 376 batch_loss: 0.2300131618976593\n",
      "training: 92 batch 377 batch_loss: 0.2339821457862854\n",
      "training: 92 batch 378 batch_loss: 0.22515755891799927\n",
      "training: 92 batch 379 batch_loss: 0.22609108686447144\n",
      "training: 92 batch 380 batch_loss: 0.2284945845603943\n",
      "training: 92 batch 381 batch_loss: 0.2324928641319275\n",
      "training: 92 batch 382 batch_loss: 0.23113614320755005\n",
      "training: 92 batch 383 batch_loss: 0.22927376627922058\n",
      "training: 92 batch 384 batch_loss: 0.22430136799812317\n",
      "training: 92 batch 385 batch_loss: 0.2304094433784485\n",
      "training: 92 batch 386 batch_loss: 0.23171520233154297\n",
      "training: 92 batch 387 batch_loss: 0.23002365231513977\n",
      "training: 92 batch 388 batch_loss: 0.23533675074577332\n",
      "training: 92 batch 389 batch_loss: 0.23055210709571838\n",
      "training: 92 batch 390 batch_loss: 0.225032240152359\n",
      "training: 92 batch 391 batch_loss: 0.23042982816696167\n",
      "training: 92 batch 392 batch_loss: 0.22830799221992493\n",
      "training: 92 batch 393 batch_loss: 0.23183614015579224\n",
      "training: 92 batch 394 batch_loss: 0.22458219528198242\n",
      "training: 92 batch 395 batch_loss: 0.23150330781936646\n",
      "training: 92 batch 396 batch_loss: 0.229106605052948\n",
      "training: 92 batch 397 batch_loss: 0.23221847414970398\n",
      "training: 92 batch 398 batch_loss: 0.2330096960067749\n",
      "training: 92 batch 399 batch_loss: 0.22867321968078613\n",
      "training: 92 batch 400 batch_loss: 0.22969508171081543\n",
      "training: 92 batch 401 batch_loss: 0.22783523797988892\n",
      "training: 92 batch 402 batch_loss: 0.22777777910232544\n",
      "training: 92 batch 403 batch_loss: 0.22695469856262207\n",
      "training: 92 batch 404 batch_loss: 0.22662851214408875\n",
      "training: 92 batch 405 batch_loss: 0.227891206741333\n",
      "training: 92 batch 406 batch_loss: 0.23212942481040955\n",
      "training: 92 batch 407 batch_loss: 0.22857218980789185\n",
      "training: 92 batch 408 batch_loss: 0.22805839776992798\n",
      "training: 92 batch 409 batch_loss: 0.22970768809318542\n",
      "training: 92 batch 410 batch_loss: 0.22671297192573547\n",
      "training: 92 batch 411 batch_loss: 0.22979912161827087\n",
      "training: 92 batch 412 batch_loss: 0.23112165927886963\n",
      "training: 92 batch 413 batch_loss: 0.23294249176979065\n",
      "training: 92 batch 414 batch_loss: 0.23099416494369507\n",
      "training: 92 batch 415 batch_loss: 0.22957566380500793\n",
      "training: 92 batch 416 batch_loss: 0.22987794876098633\n",
      "training: 92 batch 417 batch_loss: 0.22923216223716736\n",
      "training: 92 batch 418 batch_loss: 0.2369230091571808\n",
      "training: 92 batch 419 batch_loss: 0.2307443618774414\n",
      "training: 92 batch 420 batch_loss: 0.22772586345672607\n",
      "training: 92 batch 421 batch_loss: 0.22900840640068054\n",
      "training: 92 batch 422 batch_loss: 0.23097822070121765\n",
      "training: 92 batch 423 batch_loss: 0.23467162251472473\n",
      "training: 92 batch 424 batch_loss: 0.22651609778404236\n",
      "training: 92 batch 425 batch_loss: 0.22795593738555908\n",
      "training: 92 batch 426 batch_loss: 0.2296653389930725\n",
      "training: 92 batch 427 batch_loss: 0.23271867632865906\n",
      "training: 92 batch 428 batch_loss: 0.22853466868400574\n",
      "training: 92 batch 429 batch_loss: 0.23257985711097717\n",
      "training: 92 batch 430 batch_loss: 0.22814270853996277\n",
      "training: 92 batch 431 batch_loss: 0.22960218787193298\n",
      "training: 92 batch 432 batch_loss: 0.226333349943161\n",
      "training: 92 batch 433 batch_loss: 0.23172789812088013\n",
      "training: 92 batch 434 batch_loss: 0.2276894748210907\n",
      "training: 92 batch 435 batch_loss: 0.23144066333770752\n",
      "training: 92 batch 436 batch_loss: 0.22667813301086426\n",
      "training: 92 batch 437 batch_loss: 0.23320963978767395\n",
      "training: 92 batch 438 batch_loss: 0.22829389572143555\n",
      "training: 92 batch 439 batch_loss: 0.22771507501602173\n",
      "training: 92 batch 440 batch_loss: 0.22872385382652283\n",
      "training: 92 batch 441 batch_loss: 0.23177188634872437\n",
      "training: 92 batch 442 batch_loss: 0.22690171003341675\n",
      "training: 92 batch 443 batch_loss: 0.22555896639823914\n",
      "training: 92 batch 444 batch_loss: 0.22695088386535645\n",
      "training: 92 batch 445 batch_loss: 0.23088979721069336\n",
      "training: 92 batch 446 batch_loss: 0.22649678587913513\n",
      "training: 92 batch 447 batch_loss: 0.2290128767490387\n",
      "training: 92 batch 448 batch_loss: 0.23086899518966675\n",
      "training: 92 batch 449 batch_loss: 0.23220041394233704\n",
      "training: 92 batch 450 batch_loss: 0.2299506962299347\n",
      "training: 92 batch 451 batch_loss: 0.2287886142730713\n",
      "training: 92 batch 452 batch_loss: 0.23436596989631653\n",
      "training: 92 batch 453 batch_loss: 0.22885870933532715\n",
      "training: 92 batch 454 batch_loss: 0.22968724370002747\n",
      "training: 92 batch 455 batch_loss: 0.23285603523254395\n",
      "training: 92 batch 456 batch_loss: 0.2310417890548706\n",
      "training: 92 batch 457 batch_loss: 0.22985947132110596\n",
      "training: 92 batch 458 batch_loss: 0.2263830304145813\n",
      "training: 92 batch 459 batch_loss: 0.2306218147277832\n",
      "training: 92 batch 460 batch_loss: 0.22923043370246887\n",
      "training: 92 batch 461 batch_loss: 0.2297970950603485\n",
      "training: 92 batch 462 batch_loss: 0.229303777217865\n",
      "training: 92 batch 463 batch_loss: 0.22916877269744873\n",
      "training: 92 batch 464 batch_loss: 0.23018446564674377\n",
      "training: 92 batch 465 batch_loss: 0.2328025996685028\n",
      "training: 92 batch 466 batch_loss: 0.23098105192184448\n",
      "training: 92 batch 467 batch_loss: 0.22607892751693726\n",
      "training: 92 batch 468 batch_loss: 0.22653159499168396\n",
      "training: 92 batch 469 batch_loss: 0.2260255217552185\n",
      "training: 92 batch 470 batch_loss: 0.2291475534439087\n",
      "training: 92 batch 471 batch_loss: 0.22810804843902588\n",
      "training: 92 batch 472 batch_loss: 0.2260972261428833\n",
      "training: 92 batch 473 batch_loss: 0.2306242287158966\n",
      "training: 92 batch 474 batch_loss: 0.23132550716400146\n",
      "training: 92 batch 475 batch_loss: 0.22957420349121094\n",
      "training: 92 batch 476 batch_loss: 0.22632548213005066\n",
      "training: 92 batch 477 batch_loss: 0.227371484041214\n",
      "training: 92 batch 478 batch_loss: 0.22724395990371704\n",
      "training: 92 batch 479 batch_loss: 0.22833308577537537\n",
      "training: 92 batch 480 batch_loss: 0.23102229833602905\n",
      "training: 92 batch 481 batch_loss: 0.23141354322433472\n",
      "training: 92 batch 482 batch_loss: 0.23008525371551514\n",
      "training: 92 batch 483 batch_loss: 0.23071706295013428\n",
      "training: 92 batch 484 batch_loss: 0.22417986392974854\n",
      "training: 92 batch 485 batch_loss: 0.22946396470069885\n",
      "training: 92 batch 486 batch_loss: 0.23011311888694763\n",
      "training: 92 batch 487 batch_loss: 0.23344048857688904\n",
      "training: 92 batch 488 batch_loss: 0.23223328590393066\n",
      "training: 92 batch 489 batch_loss: 0.23607701063156128\n",
      "training: 92 batch 490 batch_loss: 0.22631484270095825\n",
      "training: 92 batch 491 batch_loss: 0.23507782816886902\n",
      "training: 92 batch 492 batch_loss: 0.2307882010936737\n",
      "training: 92 batch 493 batch_loss: 0.23241835832595825\n",
      "training: 92 batch 494 batch_loss: 0.22830650210380554\n",
      "training: 92 batch 495 batch_loss: 0.23279151320457458\n",
      "training: 92 batch 496 batch_loss: 0.22845101356506348\n",
      "training: 92 batch 497 batch_loss: 0.22954890131950378\n",
      "training: 92 batch 498 batch_loss: 0.23218059539794922\n",
      "training: 92 batch 499 batch_loss: 0.22862601280212402\n",
      "training: 92 batch 500 batch_loss: 0.22616130113601685\n",
      "training: 92 batch 501 batch_loss: 0.23445755243301392\n",
      "training: 92 batch 502 batch_loss: 0.23253214359283447\n",
      "training: 92 batch 503 batch_loss: 0.23009145259857178\n",
      "training: 92 batch 504 batch_loss: 0.23319143056869507\n",
      "training: 92 batch 505 batch_loss: 0.23133963346481323\n",
      "training: 92 batch 506 batch_loss: 0.22917652130126953\n",
      "training: 92 batch 507 batch_loss: 0.2275957465171814\n",
      "training: 92 batch 508 batch_loss: 0.22937428951263428\n",
      "training: 92 batch 509 batch_loss: 0.23042523860931396\n",
      "training: 92 batch 510 batch_loss: 0.23071503639221191\n",
      "training: 92 batch 511 batch_loss: 0.22737780213356018\n",
      "training: 92 batch 512 batch_loss: 0.22485071420669556\n",
      "training: 92 batch 513 batch_loss: 0.22960031032562256\n",
      "training: 92 batch 514 batch_loss: 0.23237234354019165\n",
      "training: 92 batch 515 batch_loss: 0.22929376363754272\n",
      "training: 92 batch 516 batch_loss: 0.23121356964111328\n",
      "training: 92 batch 517 batch_loss: 0.2267252802848816\n",
      "training: 92 batch 518 batch_loss: 0.22797316312789917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 92 batch 519 batch_loss: 0.22939690947532654\n",
      "training: 92 batch 520 batch_loss: 0.23347234725952148\n",
      "training: 92 batch 521 batch_loss: 0.225986510515213\n",
      "training: 92 batch 522 batch_loss: 0.2312387228012085\n",
      "training: 92 batch 523 batch_loss: 0.23287519812583923\n",
      "training: 92 batch 524 batch_loss: 0.22910982370376587\n",
      "training: 92 batch 525 batch_loss: 0.2288423776626587\n",
      "training: 92 batch 526 batch_loss: 0.22836515307426453\n",
      "training: 92 batch 527 batch_loss: 0.2314976453781128\n",
      "training: 92 batch 528 batch_loss: 0.22781476378440857\n",
      "training: 92 batch 529 batch_loss: 0.22988954186439514\n",
      "training: 92 batch 530 batch_loss: 0.2326100468635559\n",
      "training: 92 batch 531 batch_loss: 0.23507118225097656\n",
      "training: 92 batch 532 batch_loss: 0.23161008954048157\n",
      "training: 92 batch 533 batch_loss: 0.23080259561538696\n",
      "training: 92 batch 534 batch_loss: 0.22911745309829712\n",
      "training: 92 batch 535 batch_loss: 0.23124158382415771\n",
      "training: 92 batch 536 batch_loss: 0.2294360101222992\n",
      "training: 92 batch 537 batch_loss: 0.23001611232757568\n",
      "training: 92 batch 538 batch_loss: 0.2264847457408905\n",
      "training: 92 batch 539 batch_loss: 0.2334543764591217\n",
      "training: 92 batch 540 batch_loss: 0.23168310523033142\n",
      "training: 92 batch 541 batch_loss: 0.22652560472488403\n",
      "training: 92 batch 542 batch_loss: 0.23279356956481934\n",
      "training: 92 batch 543 batch_loss: 0.22627407312393188\n",
      "training: 92 batch 544 batch_loss: 0.22942474484443665\n",
      "training: 92 batch 545 batch_loss: 0.22782564163208008\n",
      "training: 92 batch 546 batch_loss: 0.23137909173965454\n",
      "training: 92 batch 547 batch_loss: 0.2282729148864746\n",
      "training: 92 batch 548 batch_loss: 0.23012155294418335\n",
      "training: 92 batch 549 batch_loss: 0.23145291209220886\n",
      "training: 92 batch 550 batch_loss: 0.22946399450302124\n",
      "training: 92 batch 551 batch_loss: 0.22921621799468994\n",
      "training: 92 batch 552 batch_loss: 0.23208585381507874\n",
      "training: 92 batch 553 batch_loss: 0.23295456171035767\n",
      "training: 92 batch 554 batch_loss: 0.22853833436965942\n",
      "training: 92 batch 555 batch_loss: 0.23377037048339844\n",
      "training: 92 batch 556 batch_loss: 0.23344916105270386\n",
      "training: 92 batch 557 batch_loss: 0.22519931197166443\n",
      "training: 92 batch 558 batch_loss: 0.22789150476455688\n",
      "training: 92 batch 559 batch_loss: 0.2292889952659607\n",
      "training: 92 batch 560 batch_loss: 0.22638043761253357\n",
      "training: 92 batch 561 batch_loss: 0.23105481266975403\n",
      "training: 92 batch 562 batch_loss: 0.2302296757698059\n",
      "training: 92 batch 563 batch_loss: 0.23142766952514648\n",
      "training: 92 batch 564 batch_loss: 0.2274135947227478\n",
      "training: 92 batch 565 batch_loss: 0.231539785861969\n",
      "training: 92 batch 566 batch_loss: 0.23095184564590454\n",
      "training: 92 batch 567 batch_loss: 0.2341545820236206\n",
      "training: 92 batch 568 batch_loss: 0.23110544681549072\n",
      "training: 92 batch 569 batch_loss: 0.2300226092338562\n",
      "training: 92 batch 570 batch_loss: 0.22943449020385742\n",
      "training: 92 batch 571 batch_loss: 0.22768297791481018\n",
      "training: 92 batch 572 batch_loss: 0.2272125780582428\n",
      "training: 92 batch 573 batch_loss: 0.2275564968585968\n",
      "training: 92 batch 574 batch_loss: 0.23219257593154907\n",
      "training: 92 batch 575 batch_loss: 0.2341625988483429\n",
      "training: 92 batch 576 batch_loss: 0.22978079319000244\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 92, Hit Ratio:0.033998127871069615 | Precision:0.05016219404305514 | Recall:0.06600136568340656 | NDCG:0.06533442119536378\n",
      "*Best Performance* \n",
      "Epoch: 91, Hit Ratio:0.03411471953043542 | Precision:0.050334218028113636 | Recall:0.065924760030861 | MDCG:0.06557326192202872\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 93 batch 0 batch_loss: 0.23327326774597168\n",
      "training: 93 batch 1 batch_loss: 0.22632789611816406\n",
      "training: 93 batch 2 batch_loss: 0.23026016354560852\n",
      "training: 93 batch 3 batch_loss: 0.22648707032203674\n",
      "training: 93 batch 4 batch_loss: 0.22966966032981873\n",
      "training: 93 batch 5 batch_loss: 0.22774142026901245\n",
      "training: 93 batch 6 batch_loss: 0.2250526249408722\n",
      "training: 93 batch 7 batch_loss: 0.22649073600769043\n",
      "training: 93 batch 8 batch_loss: 0.22540199756622314\n",
      "training: 93 batch 9 batch_loss: 0.22441360354423523\n",
      "training: 93 batch 10 batch_loss: 0.22387653589248657\n",
      "training: 93 batch 11 batch_loss: 0.22671827673912048\n",
      "training: 93 batch 12 batch_loss: 0.2269585132598877\n",
      "training: 93 batch 13 batch_loss: 0.23398655652999878\n",
      "training: 93 batch 14 batch_loss: 0.2285768985748291\n",
      "training: 93 batch 15 batch_loss: 0.22926700115203857\n",
      "training: 93 batch 16 batch_loss: 0.22951853275299072\n",
      "training: 93 batch 17 batch_loss: 0.2275797724723816\n",
      "training: 93 batch 18 batch_loss: 0.22771766781806946\n",
      "training: 93 batch 19 batch_loss: 0.2296936810016632\n",
      "training: 93 batch 20 batch_loss: 0.22698134183883667\n",
      "training: 93 batch 21 batch_loss: 0.22598481178283691\n",
      "training: 93 batch 22 batch_loss: 0.2319982647895813\n",
      "training: 93 batch 23 batch_loss: 0.22865986824035645\n",
      "training: 93 batch 24 batch_loss: 0.22991150617599487\n",
      "training: 93 batch 25 batch_loss: 0.22734335064888\n",
      "training: 93 batch 26 batch_loss: 0.23038870096206665\n",
      "training: 93 batch 27 batch_loss: 0.2276630401611328\n",
      "training: 93 batch 28 batch_loss: 0.22921183705329895\n",
      "training: 93 batch 29 batch_loss: 0.22841089963912964\n",
      "training: 93 batch 30 batch_loss: 0.23367616534233093\n",
      "training: 93 batch 31 batch_loss: 0.2266276776790619\n",
      "training: 93 batch 32 batch_loss: 0.2300022840499878\n",
      "training: 93 batch 33 batch_loss: 0.22471508383750916\n",
      "training: 93 batch 34 batch_loss: 0.2298544943332672\n",
      "training: 93 batch 35 batch_loss: 0.22677084803581238\n",
      "training: 93 batch 36 batch_loss: 0.22616654634475708\n",
      "training: 93 batch 37 batch_loss: 0.22934067249298096\n",
      "training: 93 batch 38 batch_loss: 0.23101866245269775\n",
      "training: 93 batch 39 batch_loss: 0.22687223553657532\n",
      "training: 93 batch 40 batch_loss: 0.22737979888916016\n",
      "training: 93 batch 41 batch_loss: 0.22585591673851013\n",
      "training: 93 batch 42 batch_loss: 0.23199331760406494\n",
      "training: 93 batch 43 batch_loss: 0.22461232542991638\n",
      "training: 93 batch 44 batch_loss: 0.23315373063087463\n",
      "training: 93 batch 45 batch_loss: 0.2280268669128418\n",
      "training: 93 batch 46 batch_loss: 0.22765856981277466\n",
      "training: 93 batch 47 batch_loss: 0.2330372929573059\n",
      "training: 93 batch 48 batch_loss: 0.2285834550857544\n",
      "training: 93 batch 49 batch_loss: 0.2326703667640686\n",
      "training: 93 batch 50 batch_loss: 0.22938215732574463\n",
      "training: 93 batch 51 batch_loss: 0.22380363941192627\n",
      "training: 93 batch 52 batch_loss: 0.2304707169532776\n",
      "training: 93 batch 53 batch_loss: 0.2296367883682251\n",
      "training: 93 batch 54 batch_loss: 0.2275148630142212\n",
      "training: 93 batch 55 batch_loss: 0.22792035341262817\n",
      "training: 93 batch 56 batch_loss: 0.2315070927143097\n",
      "training: 93 batch 57 batch_loss: 0.2329482138156891\n",
      "training: 93 batch 58 batch_loss: 0.2298564910888672\n",
      "training: 93 batch 59 batch_loss: 0.22781223058700562\n",
      "training: 93 batch 60 batch_loss: 0.22894468903541565\n",
      "training: 93 batch 61 batch_loss: 0.23192420601844788\n",
      "training: 93 batch 62 batch_loss: 0.2234976887702942\n",
      "training: 93 batch 63 batch_loss: 0.22958296537399292\n",
      "training: 93 batch 64 batch_loss: 0.23240089416503906\n",
      "training: 93 batch 65 batch_loss: 0.23193508386611938\n",
      "training: 93 batch 66 batch_loss: 0.22844943404197693\n",
      "training: 93 batch 67 batch_loss: 0.22154277563095093\n",
      "training: 93 batch 68 batch_loss: 0.2340589463710785\n",
      "training: 93 batch 69 batch_loss: 0.22795352339744568\n",
      "training: 93 batch 70 batch_loss: 0.22897952795028687\n",
      "training: 93 batch 71 batch_loss: 0.22157931327819824\n",
      "training: 93 batch 72 batch_loss: 0.227311909198761\n",
      "training: 93 batch 73 batch_loss: 0.22968259453773499\n",
      "training: 93 batch 74 batch_loss: 0.22496265172958374\n",
      "training: 93 batch 75 batch_loss: 0.22370955348014832\n",
      "training: 93 batch 76 batch_loss: 0.22675001621246338\n",
      "training: 93 batch 77 batch_loss: 0.22977107763290405\n",
      "training: 93 batch 78 batch_loss: 0.22440600395202637\n",
      "training: 93 batch 79 batch_loss: 0.22964411973953247\n",
      "training: 93 batch 80 batch_loss: 0.226268470287323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 93 batch 81 batch_loss: 0.23146867752075195\n",
      "training: 93 batch 82 batch_loss: 0.2304868996143341\n",
      "training: 93 batch 83 batch_loss: 0.22435849905014038\n",
      "training: 93 batch 84 batch_loss: 0.22421669960021973\n",
      "training: 93 batch 85 batch_loss: 0.23105508089065552\n",
      "training: 93 batch 86 batch_loss: 0.2242242991924286\n",
      "training: 93 batch 87 batch_loss: 0.22763434052467346\n",
      "training: 93 batch 88 batch_loss: 0.2283409833908081\n",
      "training: 93 batch 89 batch_loss: 0.22481432557106018\n",
      "training: 93 batch 90 batch_loss: 0.2276139259338379\n",
      "training: 93 batch 91 batch_loss: 0.22575396299362183\n",
      "training: 93 batch 92 batch_loss: 0.22711268067359924\n",
      "training: 93 batch 93 batch_loss: 0.22522574663162231\n",
      "training: 93 batch 94 batch_loss: 0.22772473096847534\n",
      "training: 93 batch 95 batch_loss: 0.23115286231040955\n",
      "training: 93 batch 96 batch_loss: 0.22696757316589355\n",
      "training: 93 batch 97 batch_loss: 0.23047614097595215\n",
      "training: 93 batch 98 batch_loss: 0.22700217366218567\n",
      "training: 93 batch 99 batch_loss: 0.22988110780715942\n",
      "training: 93 batch 100 batch_loss: 0.22832661867141724\n",
      "training: 93 batch 101 batch_loss: 0.23289716243743896\n",
      "training: 93 batch 102 batch_loss: 0.22550737857818604\n",
      "training: 93 batch 103 batch_loss: 0.22929051518440247\n",
      "training: 93 batch 104 batch_loss: 0.2331465482711792\n",
      "training: 93 batch 105 batch_loss: 0.23101377487182617\n",
      "training: 93 batch 106 batch_loss: 0.22962644696235657\n",
      "training: 93 batch 107 batch_loss: 0.23078292608261108\n",
      "training: 93 batch 108 batch_loss: 0.23035728931427002\n",
      "training: 93 batch 109 batch_loss: 0.22993865609169006\n",
      "training: 93 batch 110 batch_loss: 0.22604095935821533\n",
      "training: 93 batch 111 batch_loss: 0.22468098998069763\n",
      "training: 93 batch 112 batch_loss: 0.22654032707214355\n",
      "training: 93 batch 113 batch_loss: 0.22972822189331055\n",
      "training: 93 batch 114 batch_loss: 0.2349400818347931\n",
      "training: 93 batch 115 batch_loss: 0.22550076246261597\n",
      "training: 93 batch 116 batch_loss: 0.2267816662788391\n",
      "training: 93 batch 117 batch_loss: 0.23220711946487427\n",
      "training: 93 batch 118 batch_loss: 0.22616928815841675\n",
      "training: 93 batch 119 batch_loss: 0.2315915822982788\n",
      "training: 93 batch 120 batch_loss: 0.22977158427238464\n",
      "training: 93 batch 121 batch_loss: 0.22621136903762817\n",
      "training: 93 batch 122 batch_loss: 0.22495537996292114\n",
      "training: 93 batch 123 batch_loss: 0.22849217057228088\n",
      "training: 93 batch 124 batch_loss: 0.22745931148529053\n",
      "training: 93 batch 125 batch_loss: 0.22764411568641663\n",
      "training: 93 batch 126 batch_loss: 0.22636806964874268\n",
      "training: 93 batch 127 batch_loss: 0.23011204600334167\n",
      "training: 93 batch 128 batch_loss: 0.23210152983665466\n",
      "training: 93 batch 129 batch_loss: 0.22688862681388855\n",
      "training: 93 batch 130 batch_loss: 0.22660040855407715\n",
      "training: 93 batch 131 batch_loss: 0.23161375522613525\n",
      "training: 93 batch 132 batch_loss: 0.23079487681388855\n",
      "training: 93 batch 133 batch_loss: 0.22933033108711243\n",
      "training: 93 batch 134 batch_loss: 0.22760537266731262\n",
      "training: 93 batch 135 batch_loss: 0.22626370191574097\n",
      "training: 93 batch 136 batch_loss: 0.23011630773544312\n",
      "training: 93 batch 137 batch_loss: 0.23010942339897156\n",
      "training: 93 batch 138 batch_loss: 0.22505754232406616\n",
      "training: 93 batch 139 batch_loss: 0.22825294733047485\n",
      "training: 93 batch 140 batch_loss: 0.2260274589061737\n",
      "training: 93 batch 141 batch_loss: 0.22473591566085815\n",
      "training: 93 batch 142 batch_loss: 0.23227357864379883\n",
      "training: 93 batch 143 batch_loss: 0.2273610234260559\n",
      "training: 93 batch 144 batch_loss: 0.23294475674629211\n",
      "training: 93 batch 145 batch_loss: 0.23076772689819336\n",
      "training: 93 batch 146 batch_loss: 0.22534823417663574\n",
      "training: 93 batch 147 batch_loss: 0.22779768705368042\n",
      "training: 93 batch 148 batch_loss: 0.22440826892852783\n",
      "training: 93 batch 149 batch_loss: 0.22833818197250366\n",
      "training: 93 batch 150 batch_loss: 0.23130327463150024\n",
      "training: 93 batch 151 batch_loss: 0.2315012514591217\n",
      "training: 93 batch 152 batch_loss: 0.23154962062835693\n",
      "training: 93 batch 153 batch_loss: 0.22628524899482727\n",
      "training: 93 batch 154 batch_loss: 0.22745943069458008\n",
      "training: 93 batch 155 batch_loss: 0.22878706455230713\n",
      "training: 93 batch 156 batch_loss: 0.23050999641418457\n",
      "training: 93 batch 157 batch_loss: 0.22694113850593567\n",
      "training: 93 batch 158 batch_loss: 0.22777751088142395\n",
      "training: 93 batch 159 batch_loss: 0.22520804405212402\n",
      "training: 93 batch 160 batch_loss: 0.23027148842811584\n",
      "training: 93 batch 161 batch_loss: 0.23265895247459412\n",
      "training: 93 batch 162 batch_loss: 0.22695410251617432\n",
      "training: 93 batch 163 batch_loss: 0.22920721769332886\n",
      "training: 93 batch 164 batch_loss: 0.22624224424362183\n",
      "training: 93 batch 165 batch_loss: 0.22645285725593567\n",
      "training: 93 batch 166 batch_loss: 0.2262202501296997\n",
      "training: 93 batch 167 batch_loss: 0.22648316621780396\n",
      "training: 93 batch 168 batch_loss: 0.23283502459526062\n",
      "training: 93 batch 169 batch_loss: 0.2309151589870453\n",
      "training: 93 batch 170 batch_loss: 0.22903889417648315\n",
      "training: 93 batch 171 batch_loss: 0.2296057939529419\n",
      "training: 93 batch 172 batch_loss: 0.2291181981563568\n",
      "training: 93 batch 173 batch_loss: 0.23181888461112976\n",
      "training: 93 batch 174 batch_loss: 0.22956833243370056\n",
      "training: 93 batch 175 batch_loss: 0.22502440214157104\n",
      "training: 93 batch 176 batch_loss: 0.22840839624404907\n",
      "training: 93 batch 177 batch_loss: 0.22960114479064941\n",
      "training: 93 batch 178 batch_loss: 0.23145180940628052\n",
      "training: 93 batch 179 batch_loss: 0.2270108461380005\n",
      "training: 93 batch 180 batch_loss: 0.22846722602844238\n",
      "training: 93 batch 181 batch_loss: 0.22845062613487244\n",
      "training: 93 batch 182 batch_loss: 0.2325427234172821\n",
      "training: 93 batch 183 batch_loss: 0.2339029610157013\n",
      "training: 93 batch 184 batch_loss: 0.22875458002090454\n",
      "training: 93 batch 185 batch_loss: 0.22734200954437256\n",
      "training: 93 batch 186 batch_loss: 0.23408570885658264\n",
      "training: 93 batch 187 batch_loss: 0.22469773888587952\n",
      "training: 93 batch 188 batch_loss: 0.22690951824188232\n",
      "training: 93 batch 189 batch_loss: 0.2284032702445984\n",
      "training: 93 batch 190 batch_loss: 0.2325257658958435\n",
      "training: 93 batch 191 batch_loss: 0.23175126314163208\n",
      "training: 93 batch 192 batch_loss: 0.22876369953155518\n",
      "training: 93 batch 193 batch_loss: 0.22480159997940063\n",
      "training: 93 batch 194 batch_loss: 0.2295074164867401\n",
      "training: 93 batch 195 batch_loss: 0.23041847348213196\n",
      "training: 93 batch 196 batch_loss: 0.22918984293937683\n",
      "training: 93 batch 197 batch_loss: 0.22773239016532898\n",
      "training: 93 batch 198 batch_loss: 0.22860723733901978\n",
      "training: 93 batch 199 batch_loss: 0.22756516933441162\n",
      "training: 93 batch 200 batch_loss: 0.22711586952209473\n",
      "training: 93 batch 201 batch_loss: 0.2269797921180725\n",
      "training: 93 batch 202 batch_loss: 0.22878369688987732\n",
      "training: 93 batch 203 batch_loss: 0.22446715831756592\n",
      "training: 93 batch 204 batch_loss: 0.2300945520401001\n",
      "training: 93 batch 205 batch_loss: 0.23057830333709717\n",
      "training: 93 batch 206 batch_loss: 0.22941750288009644\n",
      "training: 93 batch 207 batch_loss: 0.23011574149131775\n",
      "training: 93 batch 208 batch_loss: 0.22699061036109924\n",
      "training: 93 batch 209 batch_loss: 0.22850948572158813\n",
      "training: 93 batch 210 batch_loss: 0.23210254311561584\n",
      "training: 93 batch 211 batch_loss: 0.2315375804901123\n",
      "training: 93 batch 212 batch_loss: 0.22930732369422913\n",
      "training: 93 batch 213 batch_loss: 0.22965076565742493\n",
      "training: 93 batch 214 batch_loss: 0.23113206028938293\n",
      "training: 93 batch 215 batch_loss: 0.22561505436897278\n",
      "training: 93 batch 216 batch_loss: 0.23065468668937683\n",
      "training: 93 batch 217 batch_loss: 0.2286664843559265\n",
      "training: 93 batch 218 batch_loss: 0.22987061738967896\n",
      "training: 93 batch 219 batch_loss: 0.23134690523147583\n",
      "training: 93 batch 220 batch_loss: 0.23034507036209106\n",
      "training: 93 batch 221 batch_loss: 0.2287498116493225\n",
      "training: 93 batch 222 batch_loss: 0.23035496473312378\n",
      "training: 93 batch 223 batch_loss: 0.22889643907546997\n",
      "training: 93 batch 224 batch_loss: 0.2288888394832611\n",
      "training: 93 batch 225 batch_loss: 0.22936472296714783\n",
      "training: 93 batch 226 batch_loss: 0.23115944862365723\n",
      "training: 93 batch 227 batch_loss: 0.2277783751487732\n",
      "training: 93 batch 228 batch_loss: 0.2330125868320465\n",
      "training: 93 batch 229 batch_loss: 0.23227065801620483\n",
      "training: 93 batch 230 batch_loss: 0.22680652141571045\n",
      "training: 93 batch 231 batch_loss: 0.2237851917743683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 93 batch 232 batch_loss: 0.23136577010154724\n",
      "training: 93 batch 233 batch_loss: 0.23623359203338623\n",
      "training: 93 batch 234 batch_loss: 0.23191088438034058\n",
      "training: 93 batch 235 batch_loss: 0.22913026809692383\n",
      "training: 93 batch 236 batch_loss: 0.22814860939979553\n",
      "training: 93 batch 237 batch_loss: 0.2265995740890503\n",
      "training: 93 batch 238 batch_loss: 0.23299890756607056\n",
      "training: 93 batch 239 batch_loss: 0.23336279392242432\n",
      "training: 93 batch 240 batch_loss: 0.22710013389587402\n",
      "training: 93 batch 241 batch_loss: 0.22961562871932983\n",
      "training: 93 batch 242 batch_loss: 0.2285768985748291\n",
      "training: 93 batch 243 batch_loss: 0.23080316185951233\n",
      "training: 93 batch 244 batch_loss: 0.22813284397125244\n",
      "training: 93 batch 245 batch_loss: 0.22868633270263672\n",
      "training: 93 batch 246 batch_loss: 0.23218917846679688\n",
      "training: 93 batch 247 batch_loss: 0.22844642400741577\n",
      "training: 93 batch 248 batch_loss: 0.22633051872253418\n",
      "training: 93 batch 249 batch_loss: 0.22938615083694458\n",
      "training: 93 batch 250 batch_loss: 0.2286079227924347\n",
      "training: 93 batch 251 batch_loss: 0.230226069688797\n",
      "training: 93 batch 252 batch_loss: 0.22906452417373657\n",
      "training: 93 batch 253 batch_loss: 0.23088091611862183\n",
      "training: 93 batch 254 batch_loss: 0.23080849647521973\n",
      "training: 93 batch 255 batch_loss: 0.23231512308120728\n",
      "training: 93 batch 256 batch_loss: 0.22657662630081177\n",
      "training: 93 batch 257 batch_loss: 0.2308466136455536\n",
      "training: 93 batch 258 batch_loss: 0.22934317588806152\n",
      "training: 93 batch 259 batch_loss: 0.230302631855011\n",
      "training: 93 batch 260 batch_loss: 0.23376873135566711\n",
      "training: 93 batch 261 batch_loss: 0.2270403802394867\n",
      "training: 93 batch 262 batch_loss: 0.22480303049087524\n",
      "training: 93 batch 263 batch_loss: 0.22932648658752441\n",
      "training: 93 batch 264 batch_loss: 0.22480273246765137\n",
      "training: 93 batch 265 batch_loss: 0.23022201657295227\n",
      "training: 93 batch 266 batch_loss: 0.23044076561927795\n",
      "training: 93 batch 267 batch_loss: 0.2250726819038391\n",
      "training: 93 batch 268 batch_loss: 0.2333882451057434\n",
      "training: 93 batch 269 batch_loss: 0.2323562502861023\n",
      "training: 93 batch 270 batch_loss: 0.22767260670661926\n",
      "training: 93 batch 271 batch_loss: 0.23074716329574585\n",
      "training: 93 batch 272 batch_loss: 0.23199981451034546\n",
      "training: 93 batch 273 batch_loss: 0.231694757938385\n",
      "training: 93 batch 274 batch_loss: 0.22748833894729614\n",
      "training: 93 batch 275 batch_loss: 0.22680839896202087\n",
      "training: 93 batch 276 batch_loss: 0.22675779461860657\n",
      "training: 93 batch 277 batch_loss: 0.2298089563846588\n",
      "training: 93 batch 278 batch_loss: 0.2314211130142212\n",
      "training: 93 batch 279 batch_loss: 0.22598794102668762\n",
      "training: 93 batch 280 batch_loss: 0.23224610090255737\n",
      "training: 93 batch 281 batch_loss: 0.22860044240951538\n",
      "training: 93 batch 282 batch_loss: 0.22897261381149292\n",
      "training: 93 batch 283 batch_loss: 0.23024392127990723\n",
      "training: 93 batch 284 batch_loss: 0.22796368598937988\n",
      "training: 93 batch 285 batch_loss: 0.23060700297355652\n",
      "training: 93 batch 286 batch_loss: 0.2312551736831665\n",
      "training: 93 batch 287 batch_loss: 0.2283346951007843\n",
      "training: 93 batch 288 batch_loss: 0.2333422601222992\n",
      "training: 93 batch 289 batch_loss: 0.2277158498764038\n",
      "training: 93 batch 290 batch_loss: 0.2296769917011261\n",
      "training: 93 batch 291 batch_loss: 0.2322821319103241\n",
      "training: 93 batch 292 batch_loss: 0.22590994834899902\n",
      "training: 93 batch 293 batch_loss: 0.22646912932395935\n",
      "training: 93 batch 294 batch_loss: 0.23354890942573547\n",
      "training: 93 batch 295 batch_loss: 0.22647175192832947\n",
      "training: 93 batch 296 batch_loss: 0.22861137986183167\n",
      "training: 93 batch 297 batch_loss: 0.23038840293884277\n",
      "training: 93 batch 298 batch_loss: 0.22710999846458435\n",
      "training: 93 batch 299 batch_loss: 0.2252483069896698\n",
      "training: 93 batch 300 batch_loss: 0.22789883613586426\n",
      "training: 93 batch 301 batch_loss: 0.23262086510658264\n",
      "training: 93 batch 302 batch_loss: 0.22942215204238892\n",
      "training: 93 batch 303 batch_loss: 0.23000553250312805\n",
      "training: 93 batch 304 batch_loss: 0.23102200031280518\n",
      "training: 93 batch 305 batch_loss: 0.22949638962745667\n",
      "training: 93 batch 306 batch_loss: 0.23334527015686035\n",
      "training: 93 batch 307 batch_loss: 0.22894439101219177\n",
      "training: 93 batch 308 batch_loss: 0.22564426064491272\n",
      "training: 93 batch 309 batch_loss: 0.2274957299232483\n",
      "training: 93 batch 310 batch_loss: 0.2329162359237671\n",
      "training: 93 batch 311 batch_loss: 0.22835877537727356\n",
      "training: 93 batch 312 batch_loss: 0.2312205731868744\n",
      "training: 93 batch 313 batch_loss: 0.22746697068214417\n",
      "training: 93 batch 314 batch_loss: 0.23235923051834106\n",
      "training: 93 batch 315 batch_loss: 0.22815987467765808\n",
      "training: 93 batch 316 batch_loss: 0.23115280270576477\n",
      "training: 93 batch 317 batch_loss: 0.22628474235534668\n",
      "training: 93 batch 318 batch_loss: 0.2304692268371582\n",
      "training: 93 batch 319 batch_loss: 0.23488542437553406\n",
      "training: 93 batch 320 batch_loss: 0.22803303599357605\n",
      "training: 93 batch 321 batch_loss: 0.23208287358283997\n",
      "training: 93 batch 322 batch_loss: 0.23161417245864868\n",
      "training: 93 batch 323 batch_loss: 0.22780746221542358\n",
      "training: 93 batch 324 batch_loss: 0.22630876302719116\n",
      "training: 93 batch 325 batch_loss: 0.22335729002952576\n",
      "training: 93 batch 326 batch_loss: 0.23047560453414917\n",
      "training: 93 batch 327 batch_loss: 0.22843074798583984\n",
      "training: 93 batch 328 batch_loss: 0.23307454586029053\n",
      "training: 93 batch 329 batch_loss: 0.23163583874702454\n",
      "training: 93 batch 330 batch_loss: 0.23227274417877197\n",
      "training: 93 batch 331 batch_loss: 0.23206570744514465\n",
      "training: 93 batch 332 batch_loss: 0.22738829255104065\n",
      "training: 93 batch 333 batch_loss: 0.2319532334804535\n",
      "training: 93 batch 334 batch_loss: 0.2276652455329895\n",
      "training: 93 batch 335 batch_loss: 0.23176583647727966\n",
      "training: 93 batch 336 batch_loss: 0.22843056917190552\n",
      "training: 93 batch 337 batch_loss: 0.22993624210357666\n",
      "training: 93 batch 338 batch_loss: 0.23018288612365723\n",
      "training: 93 batch 339 batch_loss: 0.2322492003440857\n",
      "training: 93 batch 340 batch_loss: 0.22954490780830383\n",
      "training: 93 batch 341 batch_loss: 0.22638675570487976\n",
      "training: 93 batch 342 batch_loss: 0.2257746458053589\n",
      "training: 93 batch 343 batch_loss: 0.22928595542907715\n",
      "training: 93 batch 344 batch_loss: 0.2286071479320526\n",
      "training: 93 batch 345 batch_loss: 0.2314719557762146\n",
      "training: 93 batch 346 batch_loss: 0.23060214519500732\n",
      "training: 93 batch 347 batch_loss: 0.22725501656532288\n",
      "training: 93 batch 348 batch_loss: 0.23130464553833008\n",
      "training: 93 batch 349 batch_loss: 0.23057821393013\n",
      "training: 93 batch 350 batch_loss: 0.230962872505188\n",
      "training: 93 batch 351 batch_loss: 0.22752171754837036\n",
      "training: 93 batch 352 batch_loss: 0.23103410005569458\n",
      "training: 93 batch 353 batch_loss: 0.2273024618625641\n",
      "training: 93 batch 354 batch_loss: 0.22956600785255432\n",
      "training: 93 batch 355 batch_loss: 0.2304101586341858\n",
      "training: 93 batch 356 batch_loss: 0.22755485773086548\n",
      "training: 93 batch 357 batch_loss: 0.22852492332458496\n",
      "training: 93 batch 358 batch_loss: 0.23039233684539795\n",
      "training: 93 batch 359 batch_loss: 0.22903114557266235\n",
      "training: 93 batch 360 batch_loss: 0.228377103805542\n",
      "training: 93 batch 361 batch_loss: 0.23059093952178955\n",
      "training: 93 batch 362 batch_loss: 0.2282935380935669\n",
      "training: 93 batch 363 batch_loss: 0.22954943776130676\n",
      "training: 93 batch 364 batch_loss: 0.2291661500930786\n",
      "training: 93 batch 365 batch_loss: 0.22915750741958618\n",
      "training: 93 batch 366 batch_loss: 0.2304350733757019\n",
      "training: 93 batch 367 batch_loss: 0.22637838125228882\n",
      "training: 93 batch 368 batch_loss: 0.22573727369308472\n",
      "training: 93 batch 369 batch_loss: 0.23099803924560547\n",
      "training: 93 batch 370 batch_loss: 0.23245233297348022\n",
      "training: 93 batch 371 batch_loss: 0.2274899184703827\n",
      "training: 93 batch 372 batch_loss: 0.23471087217330933\n",
      "training: 93 batch 373 batch_loss: 0.2278347611427307\n",
      "training: 93 batch 374 batch_loss: 0.23466545343399048\n",
      "training: 93 batch 375 batch_loss: 0.2255297303199768\n",
      "training: 93 batch 376 batch_loss: 0.2325015664100647\n",
      "training: 93 batch 377 batch_loss: 0.2311263084411621\n",
      "training: 93 batch 378 batch_loss: 0.2319743037223816\n",
      "training: 93 batch 379 batch_loss: 0.23042023181915283\n",
      "training: 93 batch 380 batch_loss: 0.2282334268093109\n",
      "training: 93 batch 381 batch_loss: 0.2242181897163391\n",
      "training: 93 batch 382 batch_loss: 0.2274925708770752\n",
      "training: 93 batch 383 batch_loss: 0.23339572548866272\n",
      "training: 93 batch 384 batch_loss: 0.2305808961391449\n",
      "training: 93 batch 385 batch_loss: 0.2264867126941681\n",
      "training: 93 batch 386 batch_loss: 0.23096930980682373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 93 batch 387 batch_loss: 0.22980371117591858\n",
      "training: 93 batch 388 batch_loss: 0.23375463485717773\n",
      "training: 93 batch 389 batch_loss: 0.2302907109260559\n",
      "training: 93 batch 390 batch_loss: 0.22833195328712463\n",
      "training: 93 batch 391 batch_loss: 0.22855788469314575\n",
      "training: 93 batch 392 batch_loss: 0.2291281819343567\n",
      "training: 93 batch 393 batch_loss: 0.22623395919799805\n",
      "training: 93 batch 394 batch_loss: 0.22915691137313843\n",
      "training: 93 batch 395 batch_loss: 0.23015975952148438\n",
      "training: 93 batch 396 batch_loss: 0.22961172461509705\n",
      "training: 93 batch 397 batch_loss: 0.2282004952430725\n",
      "training: 93 batch 398 batch_loss: 0.23038458824157715\n",
      "training: 93 batch 399 batch_loss: 0.22535783052444458\n",
      "training: 93 batch 400 batch_loss: 0.22653576731681824\n",
      "training: 93 batch 401 batch_loss: 0.2287350296974182\n",
      "training: 93 batch 402 batch_loss: 0.22697442770004272\n",
      "training: 93 batch 403 batch_loss: 0.22645428776741028\n",
      "training: 93 batch 404 batch_loss: 0.22954782843589783\n",
      "training: 93 batch 405 batch_loss: 0.22934123873710632\n",
      "training: 93 batch 406 batch_loss: 0.23099708557128906\n",
      "training: 93 batch 407 batch_loss: 0.22971373796463013\n",
      "training: 93 batch 408 batch_loss: 0.23375195264816284\n",
      "training: 93 batch 409 batch_loss: 0.2277839183807373\n",
      "training: 93 batch 410 batch_loss: 0.23120194673538208\n",
      "training: 93 batch 411 batch_loss: 0.2249860167503357\n",
      "training: 93 batch 412 batch_loss: 0.2311001420021057\n",
      "training: 93 batch 413 batch_loss: 0.2342940866947174\n",
      "training: 93 batch 414 batch_loss: 0.2305438220500946\n",
      "training: 93 batch 415 batch_loss: 0.2274285852909088\n",
      "training: 93 batch 416 batch_loss: 0.2286117970943451\n",
      "training: 93 batch 417 batch_loss: 0.23367959260940552\n",
      "training: 93 batch 418 batch_loss: 0.2276095151901245\n",
      "training: 93 batch 419 batch_loss: 0.22948721051216125\n",
      "training: 93 batch 420 batch_loss: 0.22688323259353638\n",
      "training: 93 batch 421 batch_loss: 0.22863084077835083\n",
      "training: 93 batch 422 batch_loss: 0.22928482294082642\n",
      "training: 93 batch 423 batch_loss: 0.23062947392463684\n",
      "training: 93 batch 424 batch_loss: 0.22665736079216003\n",
      "training: 93 batch 425 batch_loss: 0.23427432775497437\n",
      "training: 93 batch 426 batch_loss: 0.23260390758514404\n",
      "training: 93 batch 427 batch_loss: 0.23039019107818604\n",
      "training: 93 batch 428 batch_loss: 0.22801348567008972\n",
      "training: 93 batch 429 batch_loss: 0.2309539020061493\n",
      "training: 93 batch 430 batch_loss: 0.2236214280128479\n",
      "training: 93 batch 431 batch_loss: 0.22781893610954285\n",
      "training: 93 batch 432 batch_loss: 0.2280232310295105\n",
      "training: 93 batch 433 batch_loss: 0.22998294234275818\n",
      "training: 93 batch 434 batch_loss: 0.22974491119384766\n",
      "training: 93 batch 435 batch_loss: 0.22991159558296204\n",
      "training: 93 batch 436 batch_loss: 0.2343948483467102\n",
      "training: 93 batch 437 batch_loss: 0.23181366920471191\n",
      "training: 93 batch 438 batch_loss: 0.23003187775611877\n",
      "training: 93 batch 439 batch_loss: 0.22931453585624695\n",
      "training: 93 batch 440 batch_loss: 0.22910887002944946\n",
      "training: 93 batch 441 batch_loss: 0.2275262176990509\n",
      "training: 93 batch 442 batch_loss: 0.2292519211769104\n",
      "training: 93 batch 443 batch_loss: 0.23273134231567383\n",
      "training: 93 batch 444 batch_loss: 0.23145946860313416\n",
      "training: 93 batch 445 batch_loss: 0.2301374077796936\n",
      "training: 93 batch 446 batch_loss: 0.2306770384311676\n",
      "training: 93 batch 447 batch_loss: 0.2346724271774292\n",
      "training: 93 batch 448 batch_loss: 0.2269982099533081\n",
      "training: 93 batch 449 batch_loss: 0.22854876518249512\n",
      "training: 93 batch 450 batch_loss: 0.2289636731147766\n",
      "training: 93 batch 451 batch_loss: 0.23234501481056213\n",
      "training: 93 batch 452 batch_loss: 0.2347521185874939\n",
      "training: 93 batch 453 batch_loss: 0.23135551810264587\n",
      "training: 93 batch 454 batch_loss: 0.22897771000862122\n",
      "training: 93 batch 455 batch_loss: 0.23296001553535461\n",
      "training: 93 batch 456 batch_loss: 0.23519688844680786\n",
      "training: 93 batch 457 batch_loss: 0.22791588306427002\n",
      "training: 93 batch 458 batch_loss: 0.2330256700515747\n",
      "training: 93 batch 459 batch_loss: 0.23300617933273315\n",
      "training: 93 batch 460 batch_loss: 0.23186567425727844\n",
      "training: 93 batch 461 batch_loss: 0.23269537091255188\n",
      "training: 93 batch 462 batch_loss: 0.2339400351047516\n",
      "training: 93 batch 463 batch_loss: 0.23350971937179565\n",
      "training: 93 batch 464 batch_loss: 0.22743090987205505\n",
      "training: 93 batch 465 batch_loss: 0.2321166694164276\n",
      "training: 93 batch 466 batch_loss: 0.22906649112701416\n",
      "training: 93 batch 467 batch_loss: 0.23052716255187988\n",
      "training: 93 batch 468 batch_loss: 0.22575488686561584\n",
      "training: 93 batch 469 batch_loss: 0.22740983963012695\n",
      "training: 93 batch 470 batch_loss: 0.2289028763771057\n",
      "training: 93 batch 471 batch_loss: 0.2234387993812561\n",
      "training: 93 batch 472 batch_loss: 0.22804436087608337\n",
      "training: 93 batch 473 batch_loss: 0.23091953992843628\n",
      "training: 93 batch 474 batch_loss: 0.23116764426231384\n",
      "training: 93 batch 475 batch_loss: 0.2268655002117157\n",
      "training: 93 batch 476 batch_loss: 0.23073738813400269\n",
      "training: 93 batch 477 batch_loss: 0.22806373238563538\n",
      "training: 93 batch 478 batch_loss: 0.22951465845108032\n",
      "training: 93 batch 479 batch_loss: 0.22898459434509277\n",
      "training: 93 batch 480 batch_loss: 0.23308369517326355\n",
      "training: 93 batch 481 batch_loss: 0.23072293400764465\n",
      "training: 93 batch 482 batch_loss: 0.23198163509368896\n",
      "training: 93 batch 483 batch_loss: 0.22791728377342224\n",
      "training: 93 batch 484 batch_loss: 0.23039451241493225\n",
      "training: 93 batch 485 batch_loss: 0.2354070246219635\n",
      "training: 93 batch 486 batch_loss: 0.2325323522090912\n",
      "training: 93 batch 487 batch_loss: 0.2302703559398651\n",
      "training: 93 batch 488 batch_loss: 0.22674334049224854\n",
      "training: 93 batch 489 batch_loss: 0.23031964898109436\n",
      "training: 93 batch 490 batch_loss: 0.2286686897277832\n",
      "training: 93 batch 491 batch_loss: 0.22964206337928772\n",
      "training: 93 batch 492 batch_loss: 0.23302075266838074\n",
      "training: 93 batch 493 batch_loss: 0.23205897212028503\n",
      "training: 93 batch 494 batch_loss: 0.23369210958480835\n",
      "training: 93 batch 495 batch_loss: 0.23448020219802856\n",
      "training: 93 batch 496 batch_loss: 0.23234844207763672\n",
      "training: 93 batch 497 batch_loss: 0.22743788361549377\n",
      "training: 93 batch 498 batch_loss: 0.2325330674648285\n",
      "training: 93 batch 499 batch_loss: 0.23119744658470154\n",
      "training: 93 batch 500 batch_loss: 0.22794094681739807\n",
      "training: 93 batch 501 batch_loss: 0.23148179054260254\n",
      "training: 93 batch 502 batch_loss: 0.2309250831604004\n",
      "training: 93 batch 503 batch_loss: 0.22977754473686218\n",
      "training: 93 batch 504 batch_loss: 0.23365157842636108\n",
      "training: 93 batch 505 batch_loss: 0.23188772797584534\n",
      "training: 93 batch 506 batch_loss: 0.2310873568058014\n",
      "training: 93 batch 507 batch_loss: 0.23045015335083008\n",
      "training: 93 batch 508 batch_loss: 0.23286551237106323\n",
      "training: 93 batch 509 batch_loss: 0.23040136694908142\n",
      "training: 93 batch 510 batch_loss: 0.23783749341964722\n",
      "training: 93 batch 511 batch_loss: 0.22497522830963135\n",
      "training: 93 batch 512 batch_loss: 0.23048552870750427\n",
      "training: 93 batch 513 batch_loss: 0.2346053123474121\n",
      "training: 93 batch 514 batch_loss: 0.2297624945640564\n",
      "training: 93 batch 515 batch_loss: 0.23249119520187378\n",
      "training: 93 batch 516 batch_loss: 0.22843390703201294\n",
      "training: 93 batch 517 batch_loss: 0.23150381445884705\n",
      "training: 93 batch 518 batch_loss: 0.22984224557876587\n",
      "training: 93 batch 519 batch_loss: 0.2269393801689148\n",
      "training: 93 batch 520 batch_loss: 0.2269708216190338\n",
      "training: 93 batch 521 batch_loss: 0.22422638535499573\n",
      "training: 93 batch 522 batch_loss: 0.2307898998260498\n",
      "training: 93 batch 523 batch_loss: 0.22834905982017517\n",
      "training: 93 batch 524 batch_loss: 0.2314818799495697\n",
      "training: 93 batch 525 batch_loss: 0.23182213306427002\n",
      "training: 93 batch 526 batch_loss: 0.22364461421966553\n",
      "training: 93 batch 527 batch_loss: 0.23182785511016846\n",
      "training: 93 batch 528 batch_loss: 0.22915294766426086\n",
      "training: 93 batch 529 batch_loss: 0.22946307063102722\n",
      "training: 93 batch 530 batch_loss: 0.2287493646144867\n",
      "training: 93 batch 531 batch_loss: 0.2317216694355011\n",
      "training: 93 batch 532 batch_loss: 0.23097795248031616\n",
      "training: 93 batch 533 batch_loss: 0.2294139862060547\n",
      "training: 93 batch 534 batch_loss: 0.2314015030860901\n",
      "training: 93 batch 535 batch_loss: 0.23151618242263794\n",
      "training: 93 batch 536 batch_loss: 0.22522258758544922\n",
      "training: 93 batch 537 batch_loss: 0.22942990064620972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 93 batch 538 batch_loss: 0.2278929054737091\n",
      "training: 93 batch 539 batch_loss: 0.22714468836784363\n",
      "training: 93 batch 540 batch_loss: 0.23106718063354492\n",
      "training: 93 batch 541 batch_loss: 0.23018482327461243\n",
      "training: 93 batch 542 batch_loss: 0.2319992482662201\n",
      "training: 93 batch 543 batch_loss: 0.22956252098083496\n",
      "training: 93 batch 544 batch_loss: 0.23308473825454712\n",
      "training: 93 batch 545 batch_loss: 0.22898104786872864\n",
      "training: 93 batch 546 batch_loss: 0.23481148481369019\n",
      "training: 93 batch 547 batch_loss: 0.23135215044021606\n",
      "training: 93 batch 548 batch_loss: 0.231536865234375\n",
      "training: 93 batch 549 batch_loss: 0.23010054230690002\n",
      "training: 93 batch 550 batch_loss: 0.23295417428016663\n",
      "training: 93 batch 551 batch_loss: 0.22864511609077454\n",
      "training: 93 batch 552 batch_loss: 0.23201251029968262\n",
      "training: 93 batch 553 batch_loss: 0.23172464966773987\n",
      "training: 93 batch 554 batch_loss: 0.23069709539413452\n",
      "training: 93 batch 555 batch_loss: 0.22765669226646423\n",
      "training: 93 batch 556 batch_loss: 0.22820359468460083\n",
      "training: 93 batch 557 batch_loss: 0.231716126203537\n",
      "training: 93 batch 558 batch_loss: 0.23055577278137207\n",
      "training: 93 batch 559 batch_loss: 0.231507807970047\n",
      "training: 93 batch 560 batch_loss: 0.2298487424850464\n",
      "training: 93 batch 561 batch_loss: 0.2331780195236206\n",
      "training: 93 batch 562 batch_loss: 0.22987061738967896\n",
      "training: 93 batch 563 batch_loss: 0.2319435477256775\n",
      "training: 93 batch 564 batch_loss: 0.2306438684463501\n",
      "training: 93 batch 565 batch_loss: 0.2299771010875702\n",
      "training: 93 batch 566 batch_loss: 0.22867915034294128\n",
      "training: 93 batch 567 batch_loss: 0.22855249047279358\n",
      "training: 93 batch 568 batch_loss: 0.23461177945137024\n",
      "training: 93 batch 569 batch_loss: 0.22522848844528198\n",
      "training: 93 batch 570 batch_loss: 0.22857517004013062\n",
      "training: 93 batch 571 batch_loss: 0.2334459125995636\n",
      "training: 93 batch 572 batch_loss: 0.23368224501609802\n",
      "training: 93 batch 573 batch_loss: 0.2297574281692505\n",
      "training: 93 batch 574 batch_loss: 0.22761213779449463\n",
      "training: 93 batch 575 batch_loss: 0.23246252536773682\n",
      "training: 93 batch 576 batch_loss: 0.22996696829795837\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 93, Hit Ratio:0.0342512983314068 | Precision:0.05053573183918215 | Recall:0.06654230327357293 | NDCG:0.06593679269218644\n",
      "*Best Performance* \n",
      "Epoch: 93, Hit Ratio:0.0342512983314068 | Precision:0.05053573183918215 | Recall:0.06654230327357293 | MDCG:0.06593679269218644\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 94 batch 0 batch_loss: 0.22611567378044128\n",
      "training: 94 batch 1 batch_loss: 0.2343602180480957\n",
      "training: 94 batch 2 batch_loss: 0.23045650124549866\n",
      "training: 94 batch 3 batch_loss: 0.22477662563323975\n",
      "training: 94 batch 4 batch_loss: 0.22245752811431885\n",
      "training: 94 batch 5 batch_loss: 0.2276228368282318\n",
      "training: 94 batch 6 batch_loss: 0.23047930002212524\n",
      "training: 94 batch 7 batch_loss: 0.22674861550331116\n",
      "training: 94 batch 8 batch_loss: 0.2266198992729187\n",
      "training: 94 batch 9 batch_loss: 0.22685810923576355\n",
      "training: 94 batch 10 batch_loss: 0.22688457369804382\n",
      "training: 94 batch 11 batch_loss: 0.22790783643722534\n",
      "training: 94 batch 12 batch_loss: 0.22977054119110107\n",
      "training: 94 batch 13 batch_loss: 0.23505061864852905\n",
      "training: 94 batch 14 batch_loss: 0.22679772973060608\n",
      "training: 94 batch 15 batch_loss: 0.225297212600708\n",
      "training: 94 batch 16 batch_loss: 0.2290937900543213\n",
      "training: 94 batch 17 batch_loss: 0.22844022512435913\n",
      "training: 94 batch 18 batch_loss: 0.22978010773658752\n",
      "training: 94 batch 19 batch_loss: 0.22595548629760742\n",
      "training: 94 batch 20 batch_loss: 0.22956296801567078\n",
      "training: 94 batch 21 batch_loss: 0.23053324222564697\n",
      "training: 94 batch 22 batch_loss: 0.22487950325012207\n",
      "training: 94 batch 23 batch_loss: 0.22683781385421753\n",
      "training: 94 batch 24 batch_loss: 0.2273266613483429\n",
      "training: 94 batch 25 batch_loss: 0.22551050782203674\n",
      "training: 94 batch 26 batch_loss: 0.2277582585811615\n",
      "training: 94 batch 27 batch_loss: 0.23193779587745667\n",
      "training: 94 batch 28 batch_loss: 0.23099511861801147\n",
      "training: 94 batch 29 batch_loss: 0.23207038640975952\n",
      "training: 94 batch 30 batch_loss: 0.22860607504844666\n",
      "training: 94 batch 31 batch_loss: 0.22860634326934814\n",
      "training: 94 batch 32 batch_loss: 0.22804886102676392\n",
      "training: 94 batch 33 batch_loss: 0.22858083248138428\n",
      "training: 94 batch 34 batch_loss: 0.23373278975486755\n",
      "training: 94 batch 35 batch_loss: 0.2293652594089508\n",
      "training: 94 batch 36 batch_loss: 0.23059025406837463\n",
      "training: 94 batch 37 batch_loss: 0.228160560131073\n",
      "training: 94 batch 38 batch_loss: 0.23313775658607483\n",
      "training: 94 batch 39 batch_loss: 0.22438064217567444\n",
      "training: 94 batch 40 batch_loss: 0.2291252613067627\n",
      "training: 94 batch 41 batch_loss: 0.2306201457977295\n",
      "training: 94 batch 42 batch_loss: 0.2275370955467224\n",
      "training: 94 batch 43 batch_loss: 0.2291867733001709\n",
      "training: 94 batch 44 batch_loss: 0.22978022694587708\n",
      "training: 94 batch 45 batch_loss: 0.23104676604270935\n",
      "training: 94 batch 46 batch_loss: 0.22600573301315308\n",
      "training: 94 batch 47 batch_loss: 0.23345226049423218\n",
      "training: 94 batch 48 batch_loss: 0.22660061717033386\n",
      "training: 94 batch 49 batch_loss: 0.23152944445610046\n",
      "training: 94 batch 50 batch_loss: 0.2230236828327179\n",
      "training: 94 batch 51 batch_loss: 0.22801852226257324\n",
      "training: 94 batch 52 batch_loss: 0.22777047753334045\n",
      "training: 94 batch 53 batch_loss: 0.2307620644569397\n",
      "training: 94 batch 54 batch_loss: 0.22755113244056702\n",
      "training: 94 batch 55 batch_loss: 0.22900861501693726\n",
      "training: 94 batch 56 batch_loss: 0.22765064239501953\n",
      "training: 94 batch 57 batch_loss: 0.22143560647964478\n",
      "training: 94 batch 58 batch_loss: 0.22656399011611938\n",
      "training: 94 batch 59 batch_loss: 0.22850632667541504\n",
      "training: 94 batch 60 batch_loss: 0.23006021976470947\n",
      "training: 94 batch 61 batch_loss: 0.22287794947624207\n",
      "training: 94 batch 62 batch_loss: 0.2339351773262024\n",
      "training: 94 batch 63 batch_loss: 0.22431185841560364\n",
      "training: 94 batch 64 batch_loss: 0.23224666714668274\n",
      "training: 94 batch 65 batch_loss: 0.23423266410827637\n",
      "training: 94 batch 66 batch_loss: 0.22886922955513\n",
      "training: 94 batch 67 batch_loss: 0.2281482219696045\n",
      "training: 94 batch 68 batch_loss: 0.22404435276985168\n",
      "training: 94 batch 69 batch_loss: 0.22868278622627258\n",
      "training: 94 batch 70 batch_loss: 0.23013940453529358\n",
      "training: 94 batch 71 batch_loss: 0.22738444805145264\n",
      "training: 94 batch 72 batch_loss: 0.22792693972587585\n",
      "training: 94 batch 73 batch_loss: 0.22954487800598145\n",
      "training: 94 batch 74 batch_loss: 0.22891592979431152\n",
      "training: 94 batch 75 batch_loss: 0.23104092478752136\n",
      "training: 94 batch 76 batch_loss: 0.22784528136253357\n",
      "training: 94 batch 77 batch_loss: 0.22866541147232056\n",
      "training: 94 batch 78 batch_loss: 0.22621667385101318\n",
      "training: 94 batch 79 batch_loss: 0.22700637578964233\n",
      "training: 94 batch 80 batch_loss: 0.22686919569969177\n",
      "training: 94 batch 81 batch_loss: 0.2287563681602478\n",
      "training: 94 batch 82 batch_loss: 0.2296055555343628\n",
      "training: 94 batch 83 batch_loss: 0.23052215576171875\n",
      "training: 94 batch 84 batch_loss: 0.22903621196746826\n",
      "training: 94 batch 85 batch_loss: 0.2298654019832611\n",
      "training: 94 batch 86 batch_loss: 0.23166993260383606\n",
      "training: 94 batch 87 batch_loss: 0.22862571477890015\n",
      "training: 94 batch 88 batch_loss: 0.2283262312412262\n",
      "training: 94 batch 89 batch_loss: 0.23159536719322205\n",
      "training: 94 batch 90 batch_loss: 0.23135900497436523\n",
      "training: 94 batch 91 batch_loss: 0.22839748859405518\n",
      "training: 94 batch 92 batch_loss: 0.22401970624923706\n",
      "training: 94 batch 93 batch_loss: 0.23034819960594177\n",
      "training: 94 batch 94 batch_loss: 0.2275114357471466\n",
      "training: 94 batch 95 batch_loss: 0.23120492696762085\n",
      "training: 94 batch 96 batch_loss: 0.22999125719070435\n",
      "training: 94 batch 97 batch_loss: 0.22929787635803223\n",
      "training: 94 batch 98 batch_loss: 0.23055654764175415\n",
      "training: 94 batch 99 batch_loss: 0.2251090705394745\n",
      "training: 94 batch 100 batch_loss: 0.2305368185043335\n",
      "training: 94 batch 101 batch_loss: 0.22987791895866394\n",
      "training: 94 batch 102 batch_loss: 0.22706356644630432\n",
      "training: 94 batch 103 batch_loss: 0.2295851707458496\n",
      "training: 94 batch 104 batch_loss: 0.2299765944480896\n",
      "training: 94 batch 105 batch_loss: 0.22712084650993347\n",
      "training: 94 batch 106 batch_loss: 0.22398275136947632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 94 batch 107 batch_loss: 0.23341208696365356\n",
      "training: 94 batch 108 batch_loss: 0.23254168033599854\n",
      "training: 94 batch 109 batch_loss: 0.22664666175842285\n",
      "training: 94 batch 110 batch_loss: 0.23211699724197388\n",
      "training: 94 batch 111 batch_loss: 0.22473344206809998\n",
      "training: 94 batch 112 batch_loss: 0.2301681637763977\n",
      "training: 94 batch 113 batch_loss: 0.22703316807746887\n",
      "training: 94 batch 114 batch_loss: 0.22997373342514038\n",
      "training: 94 batch 115 batch_loss: 0.2291901707649231\n",
      "training: 94 batch 116 batch_loss: 0.22208398580551147\n",
      "training: 94 batch 117 batch_loss: 0.2277235984802246\n",
      "training: 94 batch 118 batch_loss: 0.22810402512550354\n",
      "training: 94 batch 119 batch_loss: 0.22736838459968567\n",
      "training: 94 batch 120 batch_loss: 0.2286893129348755\n",
      "training: 94 batch 121 batch_loss: 0.2319304645061493\n",
      "training: 94 batch 122 batch_loss: 0.22859296202659607\n",
      "training: 94 batch 123 batch_loss: 0.2300659418106079\n",
      "training: 94 batch 124 batch_loss: 0.23184791207313538\n",
      "training: 94 batch 125 batch_loss: 0.23018765449523926\n",
      "training: 94 batch 126 batch_loss: 0.2271798551082611\n",
      "training: 94 batch 127 batch_loss: 0.22265520691871643\n",
      "training: 94 batch 128 batch_loss: 0.2297019064426422\n",
      "training: 94 batch 129 batch_loss: 0.2288970947265625\n",
      "training: 94 batch 130 batch_loss: 0.23236417770385742\n",
      "training: 94 batch 131 batch_loss: 0.230724036693573\n",
      "training: 94 batch 132 batch_loss: 0.22824057936668396\n",
      "training: 94 batch 133 batch_loss: 0.2278287410736084\n",
      "training: 94 batch 134 batch_loss: 0.2243443727493286\n",
      "training: 94 batch 135 batch_loss: 0.22788918018341064\n",
      "training: 94 batch 136 batch_loss: 0.2349153757095337\n",
      "training: 94 batch 137 batch_loss: 0.22907578945159912\n",
      "training: 94 batch 138 batch_loss: 0.2300763726234436\n",
      "training: 94 batch 139 batch_loss: 0.22832408547401428\n",
      "training: 94 batch 140 batch_loss: 0.22681614756584167\n",
      "training: 94 batch 141 batch_loss: 0.22831743955612183\n",
      "training: 94 batch 142 batch_loss: 0.23089590668678284\n",
      "training: 94 batch 143 batch_loss: 0.2299063801765442\n",
      "training: 94 batch 144 batch_loss: 0.2275705337524414\n",
      "training: 94 batch 145 batch_loss: 0.228979229927063\n",
      "training: 94 batch 146 batch_loss: 0.22712677717208862\n",
      "training: 94 batch 147 batch_loss: 0.22948580980300903\n",
      "training: 94 batch 148 batch_loss: 0.23048192262649536\n",
      "training: 94 batch 149 batch_loss: 0.2294272482395172\n",
      "training: 94 batch 150 batch_loss: 0.22816726565361023\n",
      "training: 94 batch 151 batch_loss: 0.22765207290649414\n",
      "training: 94 batch 152 batch_loss: 0.2282729148864746\n",
      "training: 94 batch 153 batch_loss: 0.23300635814666748\n",
      "training: 94 batch 154 batch_loss: 0.22755086421966553\n",
      "training: 94 batch 155 batch_loss: 0.22808301448822021\n",
      "training: 94 batch 156 batch_loss: 0.2280990481376648\n",
      "training: 94 batch 157 batch_loss: 0.2280023992061615\n",
      "training: 94 batch 158 batch_loss: 0.23118972778320312\n",
      "training: 94 batch 159 batch_loss: 0.22979730367660522\n",
      "training: 94 batch 160 batch_loss: 0.23022645711898804\n",
      "training: 94 batch 161 batch_loss: 0.22817111015319824\n",
      "training: 94 batch 162 batch_loss: 0.22443470358848572\n",
      "training: 94 batch 163 batch_loss: 0.23057174682617188\n",
      "training: 94 batch 164 batch_loss: 0.23093611001968384\n",
      "training: 94 batch 165 batch_loss: 0.2296958565711975\n",
      "training: 94 batch 166 batch_loss: 0.22796839475631714\n",
      "training: 94 batch 167 batch_loss: 0.22975164651870728\n",
      "training: 94 batch 168 batch_loss: 0.23079445958137512\n",
      "training: 94 batch 169 batch_loss: 0.2320418357849121\n",
      "training: 94 batch 170 batch_loss: 0.23088419437408447\n",
      "training: 94 batch 171 batch_loss: 0.2277774214744568\n",
      "training: 94 batch 172 batch_loss: 0.22598427534103394\n",
      "training: 94 batch 173 batch_loss: 0.2258467674255371\n",
      "training: 94 batch 174 batch_loss: 0.22962269186973572\n",
      "training: 94 batch 175 batch_loss: 0.234449565410614\n",
      "training: 94 batch 176 batch_loss: 0.2269415259361267\n",
      "training: 94 batch 177 batch_loss: 0.2325933277606964\n",
      "training: 94 batch 178 batch_loss: 0.2282285988330841\n",
      "training: 94 batch 179 batch_loss: 0.22721025347709656\n",
      "training: 94 batch 180 batch_loss: 0.22992274165153503\n",
      "training: 94 batch 181 batch_loss: 0.22966906428337097\n",
      "training: 94 batch 182 batch_loss: 0.22871097922325134\n",
      "training: 94 batch 183 batch_loss: 0.2295764684677124\n",
      "training: 94 batch 184 batch_loss: 0.23044386506080627\n",
      "training: 94 batch 185 batch_loss: 0.23374831676483154\n",
      "training: 94 batch 186 batch_loss: 0.22899672389030457\n",
      "training: 94 batch 187 batch_loss: 0.23211470246315002\n",
      "training: 94 batch 188 batch_loss: 0.2313023805618286\n",
      "training: 94 batch 189 batch_loss: 0.23015379905700684\n",
      "training: 94 batch 190 batch_loss: 0.2289484441280365\n",
      "training: 94 batch 191 batch_loss: 0.22771847248077393\n",
      "training: 94 batch 192 batch_loss: 0.22954002022743225\n",
      "training: 94 batch 193 batch_loss: 0.22793710231781006\n",
      "training: 94 batch 194 batch_loss: 0.2274044156074524\n",
      "training: 94 batch 195 batch_loss: 0.2269686460494995\n",
      "training: 94 batch 196 batch_loss: 0.22966289520263672\n",
      "training: 94 batch 197 batch_loss: 0.22976890206336975\n",
      "training: 94 batch 198 batch_loss: 0.2314489483833313\n",
      "training: 94 batch 199 batch_loss: 0.23161298036575317\n",
      "training: 94 batch 200 batch_loss: 0.22979223728179932\n",
      "training: 94 batch 201 batch_loss: 0.22863423824310303\n",
      "training: 94 batch 202 batch_loss: 0.23000594973564148\n",
      "training: 94 batch 203 batch_loss: 0.23167920112609863\n",
      "training: 94 batch 204 batch_loss: 0.23147663474082947\n",
      "training: 94 batch 205 batch_loss: 0.23059701919555664\n",
      "training: 94 batch 206 batch_loss: 0.2282046377658844\n",
      "training: 94 batch 207 batch_loss: 0.23097863793373108\n",
      "training: 94 batch 208 batch_loss: 0.2297869324684143\n",
      "training: 94 batch 209 batch_loss: 0.23365601897239685\n",
      "training: 94 batch 210 batch_loss: 0.23276180028915405\n",
      "training: 94 batch 211 batch_loss: 0.22879460453987122\n",
      "training: 94 batch 212 batch_loss: 0.22840046882629395\n",
      "training: 94 batch 213 batch_loss: 0.23416629433631897\n",
      "training: 94 batch 214 batch_loss: 0.23563039302825928\n",
      "training: 94 batch 215 batch_loss: 0.22999635338783264\n",
      "training: 94 batch 216 batch_loss: 0.2288849949836731\n",
      "training: 94 batch 217 batch_loss: 0.2297152578830719\n",
      "training: 94 batch 218 batch_loss: 0.2311808466911316\n",
      "training: 94 batch 219 batch_loss: 0.230955570936203\n",
      "training: 94 batch 220 batch_loss: 0.2315075397491455\n",
      "training: 94 batch 221 batch_loss: 0.22950464487075806\n",
      "training: 94 batch 222 batch_loss: 0.22938811779022217\n",
      "training: 94 batch 223 batch_loss: 0.22787657380104065\n",
      "training: 94 batch 224 batch_loss: 0.2290649116039276\n",
      "training: 94 batch 225 batch_loss: 0.2296147644519806\n",
      "training: 94 batch 226 batch_loss: 0.2336093783378601\n",
      "training: 94 batch 227 batch_loss: 0.22748279571533203\n",
      "training: 94 batch 228 batch_loss: 0.23513174057006836\n",
      "training: 94 batch 229 batch_loss: 0.22394883632659912\n",
      "training: 94 batch 230 batch_loss: 0.23209387063980103\n",
      "training: 94 batch 231 batch_loss: 0.22978034615516663\n",
      "training: 94 batch 232 batch_loss: 0.22973361611366272\n",
      "training: 94 batch 233 batch_loss: 0.23348432779312134\n",
      "training: 94 batch 234 batch_loss: 0.22814473509788513\n",
      "training: 94 batch 235 batch_loss: 0.22633197903633118\n",
      "training: 94 batch 236 batch_loss: 0.2277069091796875\n",
      "training: 94 batch 237 batch_loss: 0.23032182455062866\n",
      "training: 94 batch 238 batch_loss: 0.22992482781410217\n",
      "training: 94 batch 239 batch_loss: 0.23168152570724487\n",
      "training: 94 batch 240 batch_loss: 0.22915440797805786\n",
      "training: 94 batch 241 batch_loss: 0.2346823513507843\n",
      "training: 94 batch 242 batch_loss: 0.23049575090408325\n",
      "training: 94 batch 243 batch_loss: 0.22940045595169067\n",
      "training: 94 batch 244 batch_loss: 0.2329225242137909\n",
      "training: 94 batch 245 batch_loss: 0.23624616861343384\n",
      "training: 94 batch 246 batch_loss: 0.23212510347366333\n",
      "training: 94 batch 247 batch_loss: 0.23082536458969116\n",
      "training: 94 batch 248 batch_loss: 0.23147058486938477\n",
      "training: 94 batch 249 batch_loss: 0.22766488790512085\n",
      "training: 94 batch 250 batch_loss: 0.22653573751449585\n",
      "training: 94 batch 251 batch_loss: 0.22534537315368652\n",
      "training: 94 batch 252 batch_loss: 0.23522800207138062\n",
      "training: 94 batch 253 batch_loss: 0.23324772715568542\n",
      "training: 94 batch 254 batch_loss: 0.23005720973014832\n",
      "training: 94 batch 255 batch_loss: 0.22728687524795532\n",
      "training: 94 batch 256 batch_loss: 0.2321179211139679\n",
      "training: 94 batch 257 batch_loss: 0.2290104329586029\n",
      "training: 94 batch 258 batch_loss: 0.22790992259979248\n",
      "training: 94 batch 259 batch_loss: 0.23202115297317505\n",
      "training: 94 batch 260 batch_loss: 0.22854387760162354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 94 batch 261 batch_loss: 0.2280287742614746\n",
      "training: 94 batch 262 batch_loss: 0.22894874215126038\n",
      "training: 94 batch 263 batch_loss: 0.234904944896698\n",
      "training: 94 batch 264 batch_loss: 0.23353254795074463\n",
      "training: 94 batch 265 batch_loss: 0.23316949605941772\n",
      "training: 94 batch 266 batch_loss: 0.22813773155212402\n",
      "training: 94 batch 267 batch_loss: 0.22826868295669556\n",
      "training: 94 batch 268 batch_loss: 0.23065856099128723\n",
      "training: 94 batch 269 batch_loss: 0.23032832145690918\n",
      "training: 94 batch 270 batch_loss: 0.22734451293945312\n",
      "training: 94 batch 271 batch_loss: 0.2269386649131775\n",
      "training: 94 batch 272 batch_loss: 0.23017096519470215\n",
      "training: 94 batch 273 batch_loss: 0.23093733191490173\n",
      "training: 94 batch 274 batch_loss: 0.22705620527267456\n",
      "training: 94 batch 275 batch_loss: 0.23137658834457397\n",
      "training: 94 batch 276 batch_loss: 0.2276555597782135\n",
      "training: 94 batch 277 batch_loss: 0.23442435264587402\n",
      "training: 94 batch 278 batch_loss: 0.22937637567520142\n",
      "training: 94 batch 279 batch_loss: 0.2341178059577942\n",
      "training: 94 batch 280 batch_loss: 0.2285040318965912\n",
      "training: 94 batch 281 batch_loss: 0.23401212692260742\n",
      "training: 94 batch 282 batch_loss: 0.23016071319580078\n",
      "training: 94 batch 283 batch_loss: 0.22737687826156616\n",
      "training: 94 batch 284 batch_loss: 0.2312416434288025\n",
      "training: 94 batch 285 batch_loss: 0.2279251217842102\n",
      "training: 94 batch 286 batch_loss: 0.2347930669784546\n",
      "training: 94 batch 287 batch_loss: 0.23062089085578918\n",
      "training: 94 batch 288 batch_loss: 0.2305353283882141\n",
      "training: 94 batch 289 batch_loss: 0.2249702513217926\n",
      "training: 94 batch 290 batch_loss: 0.23104828596115112\n",
      "training: 94 batch 291 batch_loss: 0.22580480575561523\n",
      "training: 94 batch 292 batch_loss: 0.22983410954475403\n",
      "training: 94 batch 293 batch_loss: 0.23068496584892273\n",
      "training: 94 batch 294 batch_loss: 0.23112547397613525\n",
      "training: 94 batch 295 batch_loss: 0.22765439748764038\n",
      "training: 94 batch 296 batch_loss: 0.23548832535743713\n",
      "training: 94 batch 297 batch_loss: 0.2293863296508789\n",
      "training: 94 batch 298 batch_loss: 0.23291578888893127\n",
      "training: 94 batch 299 batch_loss: 0.22941875457763672\n",
      "training: 94 batch 300 batch_loss: 0.23270267248153687\n",
      "training: 94 batch 301 batch_loss: 0.2330254316329956\n",
      "training: 94 batch 302 batch_loss: 0.23088860511779785\n",
      "training: 94 batch 303 batch_loss: 0.22941729426383972\n",
      "training: 94 batch 304 batch_loss: 0.22769838571548462\n",
      "training: 94 batch 305 batch_loss: 0.23040786385536194\n",
      "training: 94 batch 306 batch_loss: 0.23106089234352112\n",
      "training: 94 batch 307 batch_loss: 0.23033013939857483\n",
      "training: 94 batch 308 batch_loss: 0.2259444296360016\n",
      "training: 94 batch 309 batch_loss: 0.2322676181793213\n",
      "training: 94 batch 310 batch_loss: 0.22879621386528015\n",
      "training: 94 batch 311 batch_loss: 0.23096954822540283\n",
      "training: 94 batch 312 batch_loss: 0.23520973324775696\n",
      "training: 94 batch 313 batch_loss: 0.22777318954467773\n",
      "training: 94 batch 314 batch_loss: 0.23017719388008118\n",
      "training: 94 batch 315 batch_loss: 0.22938567399978638\n",
      "training: 94 batch 316 batch_loss: 0.2319205105304718\n",
      "training: 94 batch 317 batch_loss: 0.23113617300987244\n",
      "training: 94 batch 318 batch_loss: 0.22747185826301575\n",
      "training: 94 batch 319 batch_loss: 0.23232746124267578\n",
      "training: 94 batch 320 batch_loss: 0.23186054825782776\n",
      "training: 94 batch 321 batch_loss: 0.2260516881942749\n",
      "training: 94 batch 322 batch_loss: 0.231227844953537\n",
      "training: 94 batch 323 batch_loss: 0.22886225581169128\n",
      "training: 94 batch 324 batch_loss: 0.23048895597457886\n",
      "training: 94 batch 325 batch_loss: 0.2285340428352356\n",
      "training: 94 batch 326 batch_loss: 0.22652187943458557\n",
      "training: 94 batch 327 batch_loss: 0.22938752174377441\n",
      "training: 94 batch 328 batch_loss: 0.23481833934783936\n",
      "training: 94 batch 329 batch_loss: 0.2258622646331787\n",
      "training: 94 batch 330 batch_loss: 0.2297677993774414\n",
      "training: 94 batch 331 batch_loss: 0.23144590854644775\n",
      "training: 94 batch 332 batch_loss: 0.2307887077331543\n",
      "training: 94 batch 333 batch_loss: 0.23000764846801758\n",
      "training: 94 batch 334 batch_loss: 0.23511457443237305\n",
      "training: 94 batch 335 batch_loss: 0.23281490802764893\n",
      "training: 94 batch 336 batch_loss: 0.22853392362594604\n",
      "training: 94 batch 337 batch_loss: 0.2289862036705017\n",
      "training: 94 batch 338 batch_loss: 0.23133322596549988\n",
      "training: 94 batch 339 batch_loss: 0.22443151473999023\n",
      "training: 94 batch 340 batch_loss: 0.2326485514640808\n",
      "training: 94 batch 341 batch_loss: 0.23181721568107605\n",
      "training: 94 batch 342 batch_loss: 0.23517480492591858\n",
      "training: 94 batch 343 batch_loss: 0.23198190331459045\n",
      "training: 94 batch 344 batch_loss: 0.23121803998947144\n",
      "training: 94 batch 345 batch_loss: 0.2341490387916565\n",
      "training: 94 batch 346 batch_loss: 0.22879433631896973\n",
      "training: 94 batch 347 batch_loss: 0.22957637906074524\n",
      "training: 94 batch 348 batch_loss: 0.22830769419670105\n",
      "training: 94 batch 349 batch_loss: 0.23169025778770447\n",
      "training: 94 batch 350 batch_loss: 0.2292039394378662\n",
      "training: 94 batch 351 batch_loss: 0.23224323987960815\n",
      "training: 94 batch 352 batch_loss: 0.22965657711029053\n",
      "training: 94 batch 353 batch_loss: 0.22956562042236328\n",
      "training: 94 batch 354 batch_loss: 0.22651001811027527\n",
      "training: 94 batch 355 batch_loss: 0.22959372401237488\n",
      "training: 94 batch 356 batch_loss: 0.23267647624015808\n",
      "training: 94 batch 357 batch_loss: 0.23154538869857788\n",
      "training: 94 batch 358 batch_loss: 0.2350546419620514\n",
      "training: 94 batch 359 batch_loss: 0.22821778059005737\n",
      "training: 94 batch 360 batch_loss: 0.2346498966217041\n",
      "training: 94 batch 361 batch_loss: 0.23232626914978027\n",
      "training: 94 batch 362 batch_loss: 0.2272169589996338\n",
      "training: 94 batch 363 batch_loss: 0.23162025213241577\n",
      "training: 94 batch 364 batch_loss: 0.22816145420074463\n",
      "training: 94 batch 365 batch_loss: 0.23012158274650574\n",
      "training: 94 batch 366 batch_loss: 0.22986215353012085\n",
      "training: 94 batch 367 batch_loss: 0.2284727394580841\n",
      "training: 94 batch 368 batch_loss: 0.23202335834503174\n",
      "training: 94 batch 369 batch_loss: 0.23193493485450745\n",
      "training: 94 batch 370 batch_loss: 0.23341313004493713\n",
      "training: 94 batch 371 batch_loss: 0.23350852727890015\n",
      "training: 94 batch 372 batch_loss: 0.22778508067131042\n",
      "training: 94 batch 373 batch_loss: 0.23294490575790405\n",
      "training: 94 batch 374 batch_loss: 0.22894591093063354\n",
      "training: 94 batch 375 batch_loss: 0.23078584671020508\n",
      "training: 94 batch 376 batch_loss: 0.2282240092754364\n",
      "training: 94 batch 377 batch_loss: 0.22905117273330688\n",
      "training: 94 batch 378 batch_loss: 0.23036813735961914\n",
      "training: 94 batch 379 batch_loss: 0.23050197958946228\n",
      "training: 94 batch 380 batch_loss: 0.2328123152256012\n",
      "training: 94 batch 381 batch_loss: 0.22538864612579346\n",
      "training: 94 batch 382 batch_loss: 0.23035848140716553\n",
      "training: 94 batch 383 batch_loss: 0.2287370264530182\n",
      "training: 94 batch 384 batch_loss: 0.22647666931152344\n",
      "training: 94 batch 385 batch_loss: 0.22885555028915405\n",
      "training: 94 batch 386 batch_loss: 0.23103395104408264\n",
      "training: 94 batch 387 batch_loss: 0.23042204976081848\n",
      "training: 94 batch 388 batch_loss: 0.2303893268108368\n",
      "training: 94 batch 389 batch_loss: 0.23146116733551025\n",
      "training: 94 batch 390 batch_loss: 0.22879230976104736\n",
      "training: 94 batch 391 batch_loss: 0.22993409633636475\n",
      "training: 94 batch 392 batch_loss: 0.22930091619491577\n",
      "training: 94 batch 393 batch_loss: 0.23286786675453186\n",
      "training: 94 batch 394 batch_loss: 0.2352563738822937\n",
      "training: 94 batch 395 batch_loss: 0.23467788100242615\n",
      "training: 94 batch 396 batch_loss: 0.2293253242969513\n",
      "training: 94 batch 397 batch_loss: 0.2330477237701416\n",
      "training: 94 batch 398 batch_loss: 0.2286805808544159\n",
      "training: 94 batch 399 batch_loss: 0.22937333583831787\n",
      "training: 94 batch 400 batch_loss: 0.22921723127365112\n",
      "training: 94 batch 401 batch_loss: 0.22875234484672546\n",
      "training: 94 batch 402 batch_loss: 0.23228955268859863\n",
      "training: 94 batch 403 batch_loss: 0.22853687405586243\n",
      "training: 94 batch 404 batch_loss: 0.22849354147911072\n",
      "training: 94 batch 405 batch_loss: 0.22808536887168884\n",
      "training: 94 batch 406 batch_loss: 0.22977539896965027\n",
      "training: 94 batch 407 batch_loss: 0.22820276021957397\n",
      "training: 94 batch 408 batch_loss: 0.23403793573379517\n",
      "training: 94 batch 409 batch_loss: 0.23168829083442688\n",
      "training: 94 batch 410 batch_loss: 0.2275991439819336\n",
      "training: 94 batch 411 batch_loss: 0.23330307006835938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 94 batch 412 batch_loss: 0.22600799798965454\n",
      "training: 94 batch 413 batch_loss: 0.23040556907653809\n",
      "training: 94 batch 414 batch_loss: 0.23066958785057068\n",
      "training: 94 batch 415 batch_loss: 0.22393307089805603\n",
      "training: 94 batch 416 batch_loss: 0.22903689742088318\n",
      "training: 94 batch 417 batch_loss: 0.22781741619110107\n",
      "training: 94 batch 418 batch_loss: 0.2299884855747223\n",
      "training: 94 batch 419 batch_loss: 0.2250077724456787\n",
      "training: 94 batch 420 batch_loss: 0.22858524322509766\n",
      "training: 94 batch 421 batch_loss: 0.23005688190460205\n",
      "training: 94 batch 422 batch_loss: 0.2297825813293457\n",
      "training: 94 batch 423 batch_loss: 0.23463445901870728\n",
      "training: 94 batch 424 batch_loss: 0.2353493869304657\n",
      "training: 94 batch 425 batch_loss: 0.2303648293018341\n",
      "training: 94 batch 426 batch_loss: 0.22874897718429565\n",
      "training: 94 batch 427 batch_loss: 0.23253518342971802\n",
      "training: 94 batch 428 batch_loss: 0.2335500717163086\n",
      "training: 94 batch 429 batch_loss: 0.23184379935264587\n",
      "training: 94 batch 430 batch_loss: 0.229277104139328\n",
      "training: 94 batch 431 batch_loss: 0.2238326072692871\n",
      "training: 94 batch 432 batch_loss: 0.23172146081924438\n",
      "training: 94 batch 433 batch_loss: 0.2321457862854004\n",
      "training: 94 batch 434 batch_loss: 0.23050716519355774\n",
      "training: 94 batch 435 batch_loss: 0.22929540276527405\n",
      "training: 94 batch 436 batch_loss: 0.22883573174476624\n",
      "training: 94 batch 437 batch_loss: 0.2293400764465332\n",
      "training: 94 batch 438 batch_loss: 0.23030197620391846\n",
      "training: 94 batch 439 batch_loss: 0.22712436318397522\n",
      "training: 94 batch 440 batch_loss: 0.23239347338676453\n",
      "training: 94 batch 441 batch_loss: 0.23192209005355835\n",
      "training: 94 batch 442 batch_loss: 0.23324233293533325\n",
      "training: 94 batch 443 batch_loss: 0.22947508096694946\n",
      "training: 94 batch 444 batch_loss: 0.22901469469070435\n",
      "training: 94 batch 445 batch_loss: 0.2276974618434906\n",
      "training: 94 batch 446 batch_loss: 0.23106816411018372\n",
      "training: 94 batch 447 batch_loss: 0.23057883977890015\n",
      "training: 94 batch 448 batch_loss: 0.23032712936401367\n",
      "training: 94 batch 449 batch_loss: 0.23030197620391846\n",
      "training: 94 batch 450 batch_loss: 0.23263126611709595\n",
      "training: 94 batch 451 batch_loss: 0.2351691722869873\n",
      "training: 94 batch 452 batch_loss: 0.2301141321659088\n",
      "training: 94 batch 453 batch_loss: 0.23328310251235962\n",
      "training: 94 batch 454 batch_loss: 0.22546398639678955\n",
      "training: 94 batch 455 batch_loss: 0.2279629111289978\n",
      "training: 94 batch 456 batch_loss: 0.23407691717147827\n",
      "training: 94 batch 457 batch_loss: 0.22564345598220825\n",
      "training: 94 batch 458 batch_loss: 0.22810369729995728\n",
      "training: 94 batch 459 batch_loss: 0.23281046748161316\n",
      "training: 94 batch 460 batch_loss: 0.22780564427375793\n",
      "training: 94 batch 461 batch_loss: 0.2355794906616211\n",
      "training: 94 batch 462 batch_loss: 0.22920390963554382\n",
      "training: 94 batch 463 batch_loss: 0.22877919673919678\n",
      "training: 94 batch 464 batch_loss: 0.23359572887420654\n",
      "training: 94 batch 465 batch_loss: 0.23398682475090027\n",
      "training: 94 batch 466 batch_loss: 0.23083898425102234\n",
      "training: 94 batch 467 batch_loss: 0.23236390948295593\n",
      "training: 94 batch 468 batch_loss: 0.23236989974975586\n",
      "training: 94 batch 469 batch_loss: 0.22936773300170898\n",
      "training: 94 batch 470 batch_loss: 0.22931456565856934\n",
      "training: 94 batch 471 batch_loss: 0.23326367139816284\n",
      "training: 94 batch 472 batch_loss: 0.2272322177886963\n",
      "training: 94 batch 473 batch_loss: 0.23316389322280884\n",
      "training: 94 batch 474 batch_loss: 0.2344217300415039\n",
      "training: 94 batch 475 batch_loss: 0.23299598693847656\n",
      "training: 94 batch 476 batch_loss: 0.2277994453907013\n",
      "training: 94 batch 477 batch_loss: 0.2290453016757965\n",
      "training: 94 batch 478 batch_loss: 0.23092839121818542\n",
      "training: 94 batch 479 batch_loss: 0.2323283553123474\n",
      "training: 94 batch 480 batch_loss: 0.23137155175209045\n",
      "training: 94 batch 481 batch_loss: 0.23301899433135986\n",
      "training: 94 batch 482 batch_loss: 0.22765657305717468\n",
      "training: 94 batch 483 batch_loss: 0.23329997062683105\n",
      "training: 94 batch 484 batch_loss: 0.22796165943145752\n",
      "training: 94 batch 485 batch_loss: 0.2318333387374878\n",
      "training: 94 batch 486 batch_loss: 0.2314181923866272\n",
      "training: 94 batch 487 batch_loss: 0.22803744673728943\n",
      "training: 94 batch 488 batch_loss: 0.22689545154571533\n",
      "training: 94 batch 489 batch_loss: 0.2297942042350769\n",
      "training: 94 batch 490 batch_loss: 0.23174673318862915\n",
      "training: 94 batch 491 batch_loss: 0.22680208086967468\n",
      "training: 94 batch 492 batch_loss: 0.2333334982395172\n",
      "training: 94 batch 493 batch_loss: 0.23341864347457886\n",
      "training: 94 batch 494 batch_loss: 0.2319202721118927\n",
      "training: 94 batch 495 batch_loss: 0.23000356554985046\n",
      "training: 94 batch 496 batch_loss: 0.2296352982521057\n",
      "training: 94 batch 497 batch_loss: 0.23136195540428162\n",
      "training: 94 batch 498 batch_loss: 0.23083814978599548\n",
      "training: 94 batch 499 batch_loss: 0.23078611493110657\n",
      "training: 94 batch 500 batch_loss: 0.2328128218650818\n",
      "training: 94 batch 501 batch_loss: 0.23167064785957336\n",
      "training: 94 batch 502 batch_loss: 0.22839555144309998\n",
      "training: 94 batch 503 batch_loss: 0.2353443205356598\n",
      "training: 94 batch 504 batch_loss: 0.2333281934261322\n",
      "training: 94 batch 505 batch_loss: 0.23010283708572388\n",
      "training: 94 batch 506 batch_loss: 0.23019671440124512\n",
      "training: 94 batch 507 batch_loss: 0.23190563917160034\n",
      "training: 94 batch 508 batch_loss: 0.23392218351364136\n",
      "training: 94 batch 509 batch_loss: 0.23161566257476807\n",
      "training: 94 batch 510 batch_loss: 0.22643133997917175\n",
      "training: 94 batch 511 batch_loss: 0.23062726855278015\n",
      "training: 94 batch 512 batch_loss: 0.23038631677627563\n",
      "training: 94 batch 513 batch_loss: 0.22875729203224182\n",
      "training: 94 batch 514 batch_loss: 0.23041951656341553\n",
      "training: 94 batch 515 batch_loss: 0.23382824659347534\n",
      "training: 94 batch 516 batch_loss: 0.23215392231941223\n",
      "training: 94 batch 517 batch_loss: 0.23418062925338745\n",
      "training: 94 batch 518 batch_loss: 0.2324448823928833\n",
      "training: 94 batch 519 batch_loss: 0.23723161220550537\n",
      "training: 94 batch 520 batch_loss: 0.23283082246780396\n",
      "training: 94 batch 521 batch_loss: 0.23073503375053406\n",
      "training: 94 batch 522 batch_loss: 0.23177030682563782\n",
      "training: 94 batch 523 batch_loss: 0.23349738121032715\n",
      "training: 94 batch 524 batch_loss: 0.23288708925247192\n",
      "training: 94 batch 525 batch_loss: 0.23563575744628906\n",
      "training: 94 batch 526 batch_loss: 0.23241949081420898\n",
      "training: 94 batch 527 batch_loss: 0.23163732886314392\n",
      "training: 94 batch 528 batch_loss: 0.2319055199623108\n",
      "training: 94 batch 529 batch_loss: 0.23151090741157532\n",
      "training: 94 batch 530 batch_loss: 0.23091664910316467\n",
      "training: 94 batch 531 batch_loss: 0.23223233222961426\n",
      "training: 94 batch 532 batch_loss: 0.23127210140228271\n",
      "training: 94 batch 533 batch_loss: 0.2327136993408203\n",
      "training: 94 batch 534 batch_loss: 0.22694969177246094\n",
      "training: 94 batch 535 batch_loss: 0.2329336404800415\n",
      "training: 94 batch 536 batch_loss: 0.22892993688583374\n",
      "training: 94 batch 537 batch_loss: 0.23287642002105713\n",
      "training: 94 batch 538 batch_loss: 0.2352968156337738\n",
      "training: 94 batch 539 batch_loss: 0.22731834650039673\n",
      "training: 94 batch 540 batch_loss: 0.23094472289085388\n",
      "training: 94 batch 541 batch_loss: 0.2295817732810974\n",
      "training: 94 batch 542 batch_loss: 0.22789615392684937\n",
      "training: 94 batch 543 batch_loss: 0.2323264479637146\n",
      "training: 94 batch 544 batch_loss: 0.22811022400856018\n",
      "training: 94 batch 545 batch_loss: 0.23145952820777893\n",
      "training: 94 batch 546 batch_loss: 0.2282911241054535\n",
      "training: 94 batch 547 batch_loss: 0.2338663935661316\n",
      "training: 94 batch 548 batch_loss: 0.23473572731018066\n",
      "training: 94 batch 549 batch_loss: 0.22949808835983276\n",
      "training: 94 batch 550 batch_loss: 0.23079365491867065\n",
      "training: 94 batch 551 batch_loss: 0.2298215627670288\n",
      "training: 94 batch 552 batch_loss: 0.2360680103302002\n",
      "training: 94 batch 553 batch_loss: 0.22546029090881348\n",
      "training: 94 batch 554 batch_loss: 0.22897011041641235\n",
      "training: 94 batch 555 batch_loss: 0.2301083505153656\n",
      "training: 94 batch 556 batch_loss: 0.23630031943321228\n",
      "training: 94 batch 557 batch_loss: 0.23577255010604858\n",
      "training: 94 batch 558 batch_loss: 0.22507038712501526\n",
      "training: 94 batch 559 batch_loss: 0.23066425323486328\n",
      "training: 94 batch 560 batch_loss: 0.23208925127983093\n",
      "training: 94 batch 561 batch_loss: 0.23564526438713074\n",
      "training: 94 batch 562 batch_loss: 0.2278052568435669\n",
      "training: 94 batch 563 batch_loss: 0.23093104362487793\n",
      "training: 94 batch 564 batch_loss: 0.23069322109222412\n",
      "training: 94 batch 565 batch_loss: 0.22948047518730164\n",
      "training: 94 batch 566 batch_loss: 0.23043876886367798\n",
      "training: 94 batch 567 batch_loss: 0.23702013492584229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 94 batch 568 batch_loss: 0.23240453004837036\n",
      "training: 94 batch 569 batch_loss: 0.23389661312103271\n",
      "training: 94 batch 570 batch_loss: 0.2272741198539734\n",
      "training: 94 batch 571 batch_loss: 0.23048710823059082\n",
      "training: 94 batch 572 batch_loss: 0.22806456685066223\n",
      "training: 94 batch 573 batch_loss: 0.23619309067726135\n",
      "training: 94 batch 574 batch_loss: 0.23402667045593262\n",
      "training: 94 batch 575 batch_loss: 0.23703938722610474\n",
      "training: 94 batch 576 batch_loss: 0.22364023327827454\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 94, Hit Ratio:0.03397480953919645 | Precision:0.05012778924604345 | Recall:0.06589128661487881 | NDCG:0.06532062349675048\n",
      "*Best Performance* \n",
      "Epoch: 93, Hit Ratio:0.0342512983314068 | Precision:0.05053573183918215 | Recall:0.06654230327357293 | MDCG:0.06593679269218644\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 95 batch 0 batch_loss: 0.2262798547744751\n",
      "training: 95 batch 1 batch_loss: 0.23327583074569702\n",
      "training: 95 batch 2 batch_loss: 0.2241675853729248\n",
      "training: 95 batch 3 batch_loss: 0.227006196975708\n",
      "training: 95 batch 4 batch_loss: 0.23060372471809387\n",
      "training: 95 batch 5 batch_loss: 0.22825536131858826\n",
      "training: 95 batch 6 batch_loss: 0.22847890853881836\n",
      "training: 95 batch 7 batch_loss: 0.23235619068145752\n",
      "training: 95 batch 8 batch_loss: 0.2268044352531433\n",
      "training: 95 batch 9 batch_loss: 0.22893568873405457\n",
      "training: 95 batch 10 batch_loss: 0.2327558994293213\n",
      "training: 95 batch 11 batch_loss: 0.22710740566253662\n",
      "training: 95 batch 12 batch_loss: 0.22743332386016846\n",
      "training: 95 batch 13 batch_loss: 0.22939476370811462\n",
      "training: 95 batch 14 batch_loss: 0.22672346234321594\n",
      "training: 95 batch 15 batch_loss: 0.22721317410469055\n",
      "training: 95 batch 16 batch_loss: 0.23302873969078064\n",
      "training: 95 batch 17 batch_loss: 0.2335626482963562\n",
      "training: 95 batch 18 batch_loss: 0.22968927025794983\n",
      "training: 95 batch 19 batch_loss: 0.22786948084831238\n",
      "training: 95 batch 20 batch_loss: 0.2293717861175537\n",
      "training: 95 batch 21 batch_loss: 0.2289363145828247\n",
      "training: 95 batch 22 batch_loss: 0.23114663362503052\n",
      "training: 95 batch 23 batch_loss: 0.2288079559803009\n",
      "training: 95 batch 24 batch_loss: 0.22641423344612122\n",
      "training: 95 batch 25 batch_loss: 0.23260149359703064\n",
      "training: 95 batch 26 batch_loss: 0.23113784193992615\n",
      "training: 95 batch 27 batch_loss: 0.23232978582382202\n",
      "training: 95 batch 28 batch_loss: 0.23437702655792236\n",
      "training: 95 batch 29 batch_loss: 0.22788739204406738\n",
      "training: 95 batch 30 batch_loss: 0.22568431496620178\n",
      "training: 95 batch 31 batch_loss: 0.22530680894851685\n",
      "training: 95 batch 32 batch_loss: 0.2302340567111969\n",
      "training: 95 batch 33 batch_loss: 0.22714391350746155\n",
      "training: 95 batch 34 batch_loss: 0.22408369183540344\n",
      "training: 95 batch 35 batch_loss: 0.2312588095664978\n",
      "training: 95 batch 36 batch_loss: 0.228912353515625\n",
      "training: 95 batch 37 batch_loss: 0.22998929023742676\n",
      "training: 95 batch 38 batch_loss: 0.23060238361358643\n",
      "training: 95 batch 39 batch_loss: 0.23520338535308838\n",
      "training: 95 batch 40 batch_loss: 0.22961771488189697\n",
      "training: 95 batch 41 batch_loss: 0.23119157552719116\n",
      "training: 95 batch 42 batch_loss: 0.22808599472045898\n",
      "training: 95 batch 43 batch_loss: 0.2341252565383911\n",
      "training: 95 batch 44 batch_loss: 0.22832107543945312\n",
      "training: 95 batch 45 batch_loss: 0.23216047883033752\n",
      "training: 95 batch 46 batch_loss: 0.2280222475528717\n",
      "training: 95 batch 47 batch_loss: 0.23145383596420288\n",
      "training: 95 batch 48 batch_loss: 0.22917890548706055\n",
      "training: 95 batch 49 batch_loss: 0.22793135046958923\n",
      "training: 95 batch 50 batch_loss: 0.22902911901474\n",
      "training: 95 batch 51 batch_loss: 0.22535440325737\n",
      "training: 95 batch 52 batch_loss: 0.229789137840271\n",
      "training: 95 batch 53 batch_loss: 0.22986897826194763\n",
      "training: 95 batch 54 batch_loss: 0.22928142547607422\n",
      "training: 95 batch 55 batch_loss: 0.2260478436946869\n",
      "training: 95 batch 56 batch_loss: 0.23148179054260254\n",
      "training: 95 batch 57 batch_loss: 0.2265830934047699\n",
      "training: 95 batch 58 batch_loss: 0.23546341061592102\n",
      "training: 95 batch 59 batch_loss: 0.22973543405532837\n",
      "training: 95 batch 60 batch_loss: 0.23157605528831482\n",
      "training: 95 batch 61 batch_loss: 0.2259998619556427\n",
      "training: 95 batch 62 batch_loss: 0.2295728325843811\n",
      "training: 95 batch 63 batch_loss: 0.23069176077842712\n",
      "training: 95 batch 64 batch_loss: 0.22718530893325806\n",
      "training: 95 batch 65 batch_loss: 0.23297345638275146\n",
      "training: 95 batch 66 batch_loss: 0.2279481589794159\n",
      "training: 95 batch 67 batch_loss: 0.2252267599105835\n",
      "training: 95 batch 68 batch_loss: 0.22989100217819214\n",
      "training: 95 batch 69 batch_loss: 0.2336224615573883\n",
      "training: 95 batch 70 batch_loss: 0.23172152042388916\n",
      "training: 95 batch 71 batch_loss: 0.23092877864837646\n",
      "training: 95 batch 72 batch_loss: 0.2248578667640686\n",
      "training: 95 batch 73 batch_loss: 0.2238239049911499\n",
      "training: 95 batch 74 batch_loss: 0.23316410183906555\n",
      "training: 95 batch 75 batch_loss: 0.22353681921958923\n",
      "training: 95 batch 76 batch_loss: 0.22743138670921326\n",
      "training: 95 batch 77 batch_loss: 0.23078423738479614\n",
      "training: 95 batch 78 batch_loss: 0.22859540581703186\n",
      "training: 95 batch 79 batch_loss: 0.23536193370819092\n",
      "training: 95 batch 80 batch_loss: 0.23198175430297852\n",
      "training: 95 batch 81 batch_loss: 0.23121261596679688\n",
      "training: 95 batch 82 batch_loss: 0.22832095623016357\n",
      "training: 95 batch 83 batch_loss: 0.2270897924900055\n",
      "training: 95 batch 84 batch_loss: 0.2304685115814209\n",
      "training: 95 batch 85 batch_loss: 0.22723358869552612\n",
      "training: 95 batch 86 batch_loss: 0.22962123155593872\n",
      "training: 95 batch 87 batch_loss: 0.23098304867744446\n",
      "training: 95 batch 88 batch_loss: 0.23158305883407593\n",
      "training: 95 batch 89 batch_loss: 0.22825270891189575\n",
      "training: 95 batch 90 batch_loss: 0.22467634081840515\n",
      "training: 95 batch 91 batch_loss: 0.22642168402671814\n",
      "training: 95 batch 92 batch_loss: 0.22931846976280212\n",
      "training: 95 batch 93 batch_loss: 0.23417523503303528\n",
      "training: 95 batch 94 batch_loss: 0.23329851031303406\n",
      "training: 95 batch 95 batch_loss: 0.23025938868522644\n",
      "training: 95 batch 96 batch_loss: 0.22964173555374146\n",
      "training: 95 batch 97 batch_loss: 0.232140451669693\n",
      "training: 95 batch 98 batch_loss: 0.22893917560577393\n",
      "training: 95 batch 99 batch_loss: 0.23053759336471558\n",
      "training: 95 batch 100 batch_loss: 0.22865092754364014\n",
      "training: 95 batch 101 batch_loss: 0.22863492369651794\n",
      "training: 95 batch 102 batch_loss: 0.23113608360290527\n",
      "training: 95 batch 103 batch_loss: 0.23271453380584717\n",
      "training: 95 batch 104 batch_loss: 0.23114681243896484\n",
      "training: 95 batch 105 batch_loss: 0.22945088148117065\n",
      "training: 95 batch 106 batch_loss: 0.23048600554466248\n",
      "training: 95 batch 107 batch_loss: 0.2282210886478424\n",
      "training: 95 batch 108 batch_loss: 0.23182839155197144\n",
      "training: 95 batch 109 batch_loss: 0.23122042417526245\n",
      "training: 95 batch 110 batch_loss: 0.2314196527004242\n",
      "training: 95 batch 111 batch_loss: 0.22706228494644165\n",
      "training: 95 batch 112 batch_loss: 0.2304781675338745\n",
      "training: 95 batch 113 batch_loss: 0.2317177653312683\n",
      "training: 95 batch 114 batch_loss: 0.23060104250907898\n",
      "training: 95 batch 115 batch_loss: 0.23189610242843628\n",
      "training: 95 batch 116 batch_loss: 0.22868511080741882\n",
      "training: 95 batch 117 batch_loss: 0.22744446992874146\n",
      "training: 95 batch 118 batch_loss: 0.23015663027763367\n",
      "training: 95 batch 119 batch_loss: 0.22712209820747375\n",
      "training: 95 batch 120 batch_loss: 0.23377585411071777\n",
      "training: 95 batch 121 batch_loss: 0.229358971118927\n",
      "training: 95 batch 122 batch_loss: 0.23157134652137756\n",
      "training: 95 batch 123 batch_loss: 0.22905045747756958\n",
      "training: 95 batch 124 batch_loss: 0.2290281057357788\n",
      "training: 95 batch 125 batch_loss: 0.2316061556339264\n",
      "training: 95 batch 126 batch_loss: 0.2306222915649414\n",
      "training: 95 batch 127 batch_loss: 0.22770720720291138\n",
      "training: 95 batch 128 batch_loss: 0.22929739952087402\n",
      "training: 95 batch 129 batch_loss: 0.22930890321731567\n",
      "training: 95 batch 130 batch_loss: 0.22993344068527222\n",
      "training: 95 batch 131 batch_loss: 0.22398394346237183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 95 batch 132 batch_loss: 0.23032405972480774\n",
      "training: 95 batch 133 batch_loss: 0.2321692407131195\n",
      "training: 95 batch 134 batch_loss: 0.23249971866607666\n",
      "training: 95 batch 135 batch_loss: 0.23353153467178345\n",
      "training: 95 batch 136 batch_loss: 0.233234703540802\n",
      "training: 95 batch 137 batch_loss: 0.23344498872756958\n",
      "training: 95 batch 138 batch_loss: 0.22975468635559082\n",
      "training: 95 batch 139 batch_loss: 0.22682610154151917\n",
      "training: 95 batch 140 batch_loss: 0.23306643962860107\n",
      "training: 95 batch 141 batch_loss: 0.23010629415512085\n",
      "training: 95 batch 142 batch_loss: 0.23399335145950317\n",
      "training: 95 batch 143 batch_loss: 0.22978925704956055\n",
      "training: 95 batch 144 batch_loss: 0.22957032918930054\n",
      "training: 95 batch 145 batch_loss: 0.23032385110855103\n",
      "training: 95 batch 146 batch_loss: 0.22753006219863892\n",
      "training: 95 batch 147 batch_loss: 0.2314826250076294\n",
      "training: 95 batch 148 batch_loss: 0.22922664880752563\n",
      "training: 95 batch 149 batch_loss: 0.23125430941581726\n",
      "training: 95 batch 150 batch_loss: 0.23301362991333008\n",
      "training: 95 batch 151 batch_loss: 0.22973179817199707\n",
      "training: 95 batch 152 batch_loss: 0.23587346076965332\n",
      "training: 95 batch 153 batch_loss: 0.22889697551727295\n",
      "training: 95 batch 154 batch_loss: 0.2323310375213623\n",
      "training: 95 batch 155 batch_loss: 0.23110580444335938\n",
      "training: 95 batch 156 batch_loss: 0.2321329116821289\n",
      "training: 95 batch 157 batch_loss: 0.2279331386089325\n",
      "training: 95 batch 158 batch_loss: 0.23134219646453857\n",
      "training: 95 batch 159 batch_loss: 0.23155519366264343\n",
      "training: 95 batch 160 batch_loss: 0.23007932305335999\n",
      "training: 95 batch 161 batch_loss: 0.22431278228759766\n",
      "training: 95 batch 162 batch_loss: 0.22947368025779724\n",
      "training: 95 batch 163 batch_loss: 0.2287278175354004\n",
      "training: 95 batch 164 batch_loss: 0.23276236653327942\n",
      "training: 95 batch 165 batch_loss: 0.23113679885864258\n",
      "training: 95 batch 166 batch_loss: 0.2326246201992035\n",
      "training: 95 batch 167 batch_loss: 0.22900879383087158\n",
      "training: 95 batch 168 batch_loss: 0.22680750489234924\n",
      "training: 95 batch 169 batch_loss: 0.2301853597164154\n",
      "training: 95 batch 170 batch_loss: 0.23150929808616638\n",
      "training: 95 batch 171 batch_loss: 0.23308688402175903\n",
      "training: 95 batch 172 batch_loss: 0.23115640878677368\n",
      "training: 95 batch 173 batch_loss: 0.23097380995750427\n",
      "training: 95 batch 174 batch_loss: 0.23439759016036987\n",
      "training: 95 batch 175 batch_loss: 0.23019027709960938\n",
      "training: 95 batch 176 batch_loss: 0.22764387726783752\n",
      "training: 95 batch 177 batch_loss: 0.22820568084716797\n",
      "training: 95 batch 178 batch_loss: 0.23062926530838013\n",
      "training: 95 batch 179 batch_loss: 0.22826367616653442\n",
      "training: 95 batch 180 batch_loss: 0.23230010271072388\n",
      "training: 95 batch 181 batch_loss: 0.22854575514793396\n",
      "training: 95 batch 182 batch_loss: 0.2295905351638794\n",
      "training: 95 batch 183 batch_loss: 0.23018288612365723\n",
      "training: 95 batch 184 batch_loss: 0.23010677099227905\n",
      "training: 95 batch 185 batch_loss: 0.22783315181732178\n",
      "training: 95 batch 186 batch_loss: 0.23023515939712524\n",
      "training: 95 batch 187 batch_loss: 0.2320936918258667\n",
      "training: 95 batch 188 batch_loss: 0.22722992300987244\n",
      "training: 95 batch 189 batch_loss: 0.22943198680877686\n",
      "training: 95 batch 190 batch_loss: 0.231073796749115\n",
      "training: 95 batch 191 batch_loss: 0.22900596261024475\n",
      "training: 95 batch 192 batch_loss: 0.2312614917755127\n",
      "training: 95 batch 193 batch_loss: 0.23295116424560547\n",
      "training: 95 batch 194 batch_loss: 0.2257755994796753\n",
      "training: 95 batch 195 batch_loss: 0.2322903871536255\n",
      "training: 95 batch 196 batch_loss: 0.23305809497833252\n",
      "training: 95 batch 197 batch_loss: 0.229385644197464\n",
      "training: 95 batch 198 batch_loss: 0.22756528854370117\n",
      "training: 95 batch 199 batch_loss: 0.22630056738853455\n",
      "training: 95 batch 200 batch_loss: 0.2304147183895111\n",
      "training: 95 batch 201 batch_loss: 0.2319544553756714\n",
      "training: 95 batch 202 batch_loss: 0.2336762547492981\n",
      "training: 95 batch 203 batch_loss: 0.23542580008506775\n",
      "training: 95 batch 204 batch_loss: 0.2289772927761078\n",
      "training: 95 batch 205 batch_loss: 0.22819578647613525\n",
      "training: 95 batch 206 batch_loss: 0.23407092690467834\n",
      "training: 95 batch 207 batch_loss: 0.23461616039276123\n",
      "training: 95 batch 208 batch_loss: 0.23461046814918518\n",
      "training: 95 batch 209 batch_loss: 0.2333756983280182\n",
      "training: 95 batch 210 batch_loss: 0.2300461232662201\n",
      "training: 95 batch 211 batch_loss: 0.2289484143257141\n",
      "training: 95 batch 212 batch_loss: 0.2272188663482666\n",
      "training: 95 batch 213 batch_loss: 0.23116835951805115\n",
      "training: 95 batch 214 batch_loss: 0.23138165473937988\n",
      "training: 95 batch 215 batch_loss: 0.22969523072242737\n",
      "training: 95 batch 216 batch_loss: 0.23169052600860596\n",
      "training: 95 batch 217 batch_loss: 0.23406991362571716\n",
      "training: 95 batch 218 batch_loss: 0.231012761592865\n",
      "training: 95 batch 219 batch_loss: 0.2311263382434845\n",
      "training: 95 batch 220 batch_loss: 0.2292114496231079\n",
      "training: 95 batch 221 batch_loss: 0.22734057903289795\n",
      "training: 95 batch 222 batch_loss: 0.23150253295898438\n",
      "training: 95 batch 223 batch_loss: 0.22834405303001404\n",
      "training: 95 batch 224 batch_loss: 0.22862234711647034\n",
      "training: 95 batch 225 batch_loss: 0.2333604097366333\n",
      "training: 95 batch 226 batch_loss: 0.23168760538101196\n",
      "training: 95 batch 227 batch_loss: 0.22933506965637207\n",
      "training: 95 batch 228 batch_loss: 0.2280730903148651\n",
      "training: 95 batch 229 batch_loss: 0.2280387580394745\n",
      "training: 95 batch 230 batch_loss: 0.2362270951271057\n",
      "training: 95 batch 231 batch_loss: 0.2314533293247223\n",
      "training: 95 batch 232 batch_loss: 0.2330554723739624\n",
      "training: 95 batch 233 batch_loss: 0.22696006298065186\n",
      "training: 95 batch 234 batch_loss: 0.23141717910766602\n",
      "training: 95 batch 235 batch_loss: 0.228400319814682\n",
      "training: 95 batch 236 batch_loss: 0.23037949204444885\n",
      "training: 95 batch 237 batch_loss: 0.2258821427822113\n",
      "training: 95 batch 238 batch_loss: 0.2285367250442505\n",
      "training: 95 batch 239 batch_loss: 0.2299894094467163\n",
      "training: 95 batch 240 batch_loss: 0.2324714958667755\n",
      "training: 95 batch 241 batch_loss: 0.22987672686576843\n",
      "training: 95 batch 242 batch_loss: 0.22807294130325317\n",
      "training: 95 batch 243 batch_loss: 0.23050421476364136\n",
      "training: 95 batch 244 batch_loss: 0.23344701528549194\n",
      "training: 95 batch 245 batch_loss: 0.2306482195854187\n",
      "training: 95 batch 246 batch_loss: 0.22684794664382935\n",
      "training: 95 batch 247 batch_loss: 0.23054161667823792\n",
      "training: 95 batch 248 batch_loss: 0.22495338320732117\n",
      "training: 95 batch 249 batch_loss: 0.22928911447525024\n",
      "training: 95 batch 250 batch_loss: 0.23601466417312622\n",
      "training: 95 batch 251 batch_loss: 0.22768071293830872\n",
      "training: 95 batch 252 batch_loss: 0.23034211993217468\n",
      "training: 95 batch 253 batch_loss: 0.23100316524505615\n",
      "training: 95 batch 254 batch_loss: 0.22956785559654236\n",
      "training: 95 batch 255 batch_loss: 0.22863727807998657\n",
      "training: 95 batch 256 batch_loss: 0.23179888725280762\n",
      "training: 95 batch 257 batch_loss: 0.23307344317436218\n",
      "training: 95 batch 258 batch_loss: 0.23443785309791565\n",
      "training: 95 batch 259 batch_loss: 0.2316531538963318\n",
      "training: 95 batch 260 batch_loss: 0.23481929302215576\n",
      "training: 95 batch 261 batch_loss: 0.22937795519828796\n",
      "training: 95 batch 262 batch_loss: 0.23612642288208008\n",
      "training: 95 batch 263 batch_loss: 0.23104089498519897\n",
      "training: 95 batch 264 batch_loss: 0.23000842332839966\n",
      "training: 95 batch 265 batch_loss: 0.23122259974479675\n",
      "training: 95 batch 266 batch_loss: 0.23311680555343628\n",
      "training: 95 batch 267 batch_loss: 0.2337014377117157\n",
      "training: 95 batch 268 batch_loss: 0.22949501872062683\n",
      "training: 95 batch 269 batch_loss: 0.230228990316391\n",
      "training: 95 batch 270 batch_loss: 0.23694729804992676\n",
      "training: 95 batch 271 batch_loss: 0.22571757435798645\n",
      "training: 95 batch 272 batch_loss: 0.2288581132888794\n",
      "training: 95 batch 273 batch_loss: 0.23034727573394775\n",
      "training: 95 batch 274 batch_loss: 0.23376381397247314\n",
      "training: 95 batch 275 batch_loss: 0.2238437533378601\n",
      "training: 95 batch 276 batch_loss: 0.23104500770568848\n",
      "training: 95 batch 277 batch_loss: 0.23267877101898193\n",
      "training: 95 batch 278 batch_loss: 0.23197075724601746\n",
      "training: 95 batch 279 batch_loss: 0.23212271928787231\n",
      "training: 95 batch 280 batch_loss: 0.23325353860855103\n",
      "training: 95 batch 281 batch_loss: 0.22912654280662537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 95 batch 282 batch_loss: 0.23044875264167786\n",
      "training: 95 batch 283 batch_loss: 0.22919073700904846\n",
      "training: 95 batch 284 batch_loss: 0.23229241371154785\n",
      "training: 95 batch 285 batch_loss: 0.23111289739608765\n",
      "training: 95 batch 286 batch_loss: 0.23122170567512512\n",
      "training: 95 batch 287 batch_loss: 0.22640323638916016\n",
      "training: 95 batch 288 batch_loss: 0.23190954327583313\n",
      "training: 95 batch 289 batch_loss: 0.23146966099739075\n",
      "training: 95 batch 290 batch_loss: 0.22828298807144165\n",
      "training: 95 batch 291 batch_loss: 0.227125883102417\n",
      "training: 95 batch 292 batch_loss: 0.23057815432548523\n",
      "training: 95 batch 293 batch_loss: 0.23002466559410095\n",
      "training: 95 batch 294 batch_loss: 0.23215556144714355\n",
      "training: 95 batch 295 batch_loss: 0.23513975739479065\n",
      "training: 95 batch 296 batch_loss: 0.2313544750213623\n",
      "training: 95 batch 297 batch_loss: 0.22953122854232788\n",
      "training: 95 batch 298 batch_loss: 0.22469347715377808\n",
      "training: 95 batch 299 batch_loss: 0.2322980761528015\n",
      "training: 95 batch 300 batch_loss: 0.23301821947097778\n",
      "training: 95 batch 301 batch_loss: 0.2317674458026886\n",
      "training: 95 batch 302 batch_loss: 0.23523718118667603\n",
      "training: 95 batch 303 batch_loss: 0.2313544750213623\n",
      "training: 95 batch 304 batch_loss: 0.22440719604492188\n",
      "training: 95 batch 305 batch_loss: 0.23074305057525635\n",
      "training: 95 batch 306 batch_loss: 0.2261112928390503\n",
      "training: 95 batch 307 batch_loss: 0.23248454928398132\n",
      "training: 95 batch 308 batch_loss: 0.23070335388183594\n",
      "training: 95 batch 309 batch_loss: 0.23350068926811218\n",
      "training: 95 batch 310 batch_loss: 0.23529145121574402\n",
      "training: 95 batch 311 batch_loss: 0.23003655672073364\n",
      "training: 95 batch 312 batch_loss: 0.23018327355384827\n",
      "training: 95 batch 313 batch_loss: 0.23014411330223083\n",
      "training: 95 batch 314 batch_loss: 0.2299242913722992\n",
      "training: 95 batch 315 batch_loss: 0.23628634214401245\n",
      "training: 95 batch 316 batch_loss: 0.233973890542984\n",
      "training: 95 batch 317 batch_loss: 0.23204123973846436\n",
      "training: 95 batch 318 batch_loss: 0.23261749744415283\n",
      "training: 95 batch 319 batch_loss: 0.22923970222473145\n",
      "training: 95 batch 320 batch_loss: 0.22908902168273926\n",
      "training: 95 batch 321 batch_loss: 0.2369385063648224\n",
      "training: 95 batch 322 batch_loss: 0.228127121925354\n",
      "training: 95 batch 323 batch_loss: 0.23396417498588562\n",
      "training: 95 batch 324 batch_loss: 0.22945627570152283\n",
      "training: 95 batch 325 batch_loss: 0.2314910888671875\n",
      "training: 95 batch 326 batch_loss: 0.23069828748703003\n",
      "training: 95 batch 327 batch_loss: 0.22562700510025024\n",
      "training: 95 batch 328 batch_loss: 0.22954639792442322\n",
      "training: 95 batch 329 batch_loss: 0.23116061091423035\n",
      "training: 95 batch 330 batch_loss: 0.23604285717010498\n",
      "training: 95 batch 331 batch_loss: 0.2291455864906311\n",
      "training: 95 batch 332 batch_loss: 0.23272719979286194\n",
      "training: 95 batch 333 batch_loss: 0.2267184555530548\n",
      "training: 95 batch 334 batch_loss: 0.23393002152442932\n",
      "training: 95 batch 335 batch_loss: 0.23165464401245117\n",
      "training: 95 batch 336 batch_loss: 0.23184984922409058\n",
      "training: 95 batch 337 batch_loss: 0.22812220454216003\n",
      "training: 95 batch 338 batch_loss: 0.23125728964805603\n",
      "training: 95 batch 339 batch_loss: 0.23491844534873962\n",
      "training: 95 batch 340 batch_loss: 0.2312254011631012\n",
      "training: 95 batch 341 batch_loss: 0.22675976157188416\n",
      "training: 95 batch 342 batch_loss: 0.2316151261329651\n",
      "training: 95 batch 343 batch_loss: 0.22731727361679077\n",
      "training: 95 batch 344 batch_loss: 0.23097002506256104\n",
      "training: 95 batch 345 batch_loss: 0.23380878567695618\n",
      "training: 95 batch 346 batch_loss: 0.2339058220386505\n",
      "training: 95 batch 347 batch_loss: 0.23175668716430664\n",
      "training: 95 batch 348 batch_loss: 0.23407307267189026\n",
      "training: 95 batch 349 batch_loss: 0.2264925241470337\n",
      "training: 95 batch 350 batch_loss: 0.23455888032913208\n",
      "training: 95 batch 351 batch_loss: 0.23732131719589233\n",
      "training: 95 batch 352 batch_loss: 0.22845548391342163\n",
      "training: 95 batch 353 batch_loss: 0.23227131366729736\n",
      "training: 95 batch 354 batch_loss: 0.23299381136894226\n",
      "training: 95 batch 355 batch_loss: 0.2309863567352295\n",
      "training: 95 batch 356 batch_loss: 0.23177096247673035\n",
      "training: 95 batch 357 batch_loss: 0.22868716716766357\n",
      "training: 95 batch 358 batch_loss: 0.23182886838912964\n",
      "training: 95 batch 359 batch_loss: 0.23038426041603088\n",
      "training: 95 batch 360 batch_loss: 0.23163533210754395\n",
      "training: 95 batch 361 batch_loss: 0.23088335990905762\n",
      "training: 95 batch 362 batch_loss: 0.2262709140777588\n",
      "training: 95 batch 363 batch_loss: 0.22758767008781433\n",
      "training: 95 batch 364 batch_loss: 0.230098694562912\n",
      "training: 95 batch 365 batch_loss: 0.2329646348953247\n",
      "training: 95 batch 366 batch_loss: 0.23111358284950256\n",
      "training: 95 batch 367 batch_loss: 0.2286759316921234\n",
      "training: 95 batch 368 batch_loss: 0.2323002815246582\n",
      "training: 95 batch 369 batch_loss: 0.23015281558036804\n",
      "training: 95 batch 370 batch_loss: 0.23176777362823486\n",
      "training: 95 batch 371 batch_loss: 0.23027628660202026\n",
      "training: 95 batch 372 batch_loss: 0.22960856556892395\n",
      "training: 95 batch 373 batch_loss: 0.23102062940597534\n",
      "training: 95 batch 374 batch_loss: 0.23059624433517456\n",
      "training: 95 batch 375 batch_loss: 0.23287010192871094\n",
      "training: 95 batch 376 batch_loss: 0.2306312918663025\n",
      "training: 95 batch 377 batch_loss: 0.23047274351119995\n",
      "training: 95 batch 378 batch_loss: 0.23017022013664246\n",
      "training: 95 batch 379 batch_loss: 0.23261302709579468\n",
      "training: 95 batch 380 batch_loss: 0.23478910326957703\n",
      "training: 95 batch 381 batch_loss: 0.23030191659927368\n",
      "training: 95 batch 382 batch_loss: 0.23088344931602478\n",
      "training: 95 batch 383 batch_loss: 0.23354890942573547\n",
      "training: 95 batch 384 batch_loss: 0.22979572415351868\n",
      "training: 95 batch 385 batch_loss: 0.23597565293312073\n",
      "training: 95 batch 386 batch_loss: 0.2325412929058075\n",
      "training: 95 batch 387 batch_loss: 0.22888794541358948\n",
      "training: 95 batch 388 batch_loss: 0.22968199849128723\n",
      "training: 95 batch 389 batch_loss: 0.23336473107337952\n",
      "training: 95 batch 390 batch_loss: 0.2319737672805786\n",
      "training: 95 batch 391 batch_loss: 0.23168465495109558\n",
      "training: 95 batch 392 batch_loss: 0.23361515998840332\n",
      "training: 95 batch 393 batch_loss: 0.22866985201835632\n",
      "training: 95 batch 394 batch_loss: 0.23449939489364624\n",
      "training: 95 batch 395 batch_loss: 0.23229387402534485\n",
      "training: 95 batch 396 batch_loss: 0.22769394516944885\n",
      "training: 95 batch 397 batch_loss: 0.23422932624816895\n",
      "training: 95 batch 398 batch_loss: 0.22980588674545288\n",
      "training: 95 batch 399 batch_loss: 0.2303503453731537\n",
      "training: 95 batch 400 batch_loss: 0.23227375745773315\n",
      "training: 95 batch 401 batch_loss: 0.23419445753097534\n",
      "training: 95 batch 402 batch_loss: 0.23119086027145386\n",
      "training: 95 batch 403 batch_loss: 0.232185959815979\n",
      "training: 95 batch 404 batch_loss: 0.22958746552467346\n",
      "training: 95 batch 405 batch_loss: 0.23289209604263306\n",
      "training: 95 batch 406 batch_loss: 0.22932404279708862\n",
      "training: 95 batch 407 batch_loss: 0.23251304030418396\n",
      "training: 95 batch 408 batch_loss: 0.2297346293926239\n",
      "training: 95 batch 409 batch_loss: 0.23040243983268738\n",
      "training: 95 batch 410 batch_loss: 0.2339847981929779\n",
      "training: 95 batch 411 batch_loss: 0.23265594244003296\n",
      "training: 95 batch 412 batch_loss: 0.23230046033859253\n",
      "training: 95 batch 413 batch_loss: 0.22953948378562927\n",
      "training: 95 batch 414 batch_loss: 0.22850558161735535\n",
      "training: 95 batch 415 batch_loss: 0.229742169380188\n",
      "training: 95 batch 416 batch_loss: 0.2279319167137146\n",
      "training: 95 batch 417 batch_loss: 0.2320818305015564\n",
      "training: 95 batch 418 batch_loss: 0.23367443680763245\n",
      "training: 95 batch 419 batch_loss: 0.2257942259311676\n",
      "training: 95 batch 420 batch_loss: 0.22498011589050293\n",
      "training: 95 batch 421 batch_loss: 0.23530149459838867\n",
      "training: 95 batch 422 batch_loss: 0.2315782904624939\n",
      "training: 95 batch 423 batch_loss: 0.235260009765625\n",
      "training: 95 batch 424 batch_loss: 0.23002365231513977\n",
      "training: 95 batch 425 batch_loss: 0.2380407750606537\n",
      "training: 95 batch 426 batch_loss: 0.23163223266601562\n",
      "training: 95 batch 427 batch_loss: 0.23417928814888\n",
      "training: 95 batch 428 batch_loss: 0.2264653444290161\n",
      "training: 95 batch 429 batch_loss: 0.23146170377731323\n",
      "training: 95 batch 430 batch_loss: 0.2311442792415619\n",
      "training: 95 batch 431 batch_loss: 0.2312016487121582\n",
      "training: 95 batch 432 batch_loss: 0.22837674617767334\n",
      "training: 95 batch 433 batch_loss: 0.2377544343471527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 95 batch 434 batch_loss: 0.2338927686214447\n",
      "training: 95 batch 435 batch_loss: 0.234902024269104\n",
      "training: 95 batch 436 batch_loss: 0.2310786247253418\n",
      "training: 95 batch 437 batch_loss: 0.22951775789260864\n",
      "training: 95 batch 438 batch_loss: 0.23266029357910156\n",
      "training: 95 batch 439 batch_loss: 0.22760474681854248\n",
      "training: 95 batch 440 batch_loss: 0.23506775498390198\n",
      "training: 95 batch 441 batch_loss: 0.23010361194610596\n",
      "training: 95 batch 442 batch_loss: 0.23097917437553406\n",
      "training: 95 batch 443 batch_loss: 0.23045626282691956\n",
      "training: 95 batch 444 batch_loss: 0.22862637042999268\n",
      "training: 95 batch 445 batch_loss: 0.23084571957588196\n",
      "training: 95 batch 446 batch_loss: 0.23418313264846802\n",
      "training: 95 batch 447 batch_loss: 0.2286454141139984\n",
      "training: 95 batch 448 batch_loss: 0.2319607138633728\n",
      "training: 95 batch 449 batch_loss: 0.2293476164340973\n",
      "training: 95 batch 450 batch_loss: 0.2329573929309845\n",
      "training: 95 batch 451 batch_loss: 0.23044097423553467\n",
      "training: 95 batch 452 batch_loss: 0.22878718376159668\n",
      "training: 95 batch 453 batch_loss: 0.22814732789993286\n",
      "training: 95 batch 454 batch_loss: 0.22559189796447754\n",
      "training: 95 batch 455 batch_loss: 0.2344716489315033\n",
      "training: 95 batch 456 batch_loss: 0.23577526211738586\n",
      "training: 95 batch 457 batch_loss: 0.23246735334396362\n",
      "training: 95 batch 458 batch_loss: 0.23387154936790466\n",
      "training: 95 batch 459 batch_loss: 0.2331218421459198\n",
      "training: 95 batch 460 batch_loss: 0.23236870765686035\n",
      "training: 95 batch 461 batch_loss: 0.23190122842788696\n",
      "training: 95 batch 462 batch_loss: 0.22820699214935303\n",
      "training: 95 batch 463 batch_loss: 0.23483943939208984\n",
      "training: 95 batch 464 batch_loss: 0.2318808138370514\n",
      "training: 95 batch 465 batch_loss: 0.23506003618240356\n",
      "training: 95 batch 466 batch_loss: 0.2283501923084259\n",
      "training: 95 batch 467 batch_loss: 0.2277638018131256\n",
      "training: 95 batch 468 batch_loss: 0.2295461893081665\n",
      "training: 95 batch 469 batch_loss: 0.23394504189491272\n",
      "training: 95 batch 470 batch_loss: 0.23475593328475952\n",
      "training: 95 batch 471 batch_loss: 0.23126500844955444\n",
      "training: 95 batch 472 batch_loss: 0.23315951228141785\n",
      "training: 95 batch 473 batch_loss: 0.23051446676254272\n",
      "training: 95 batch 474 batch_loss: 0.23255839943885803\n",
      "training: 95 batch 475 batch_loss: 0.23250699043273926\n",
      "training: 95 batch 476 batch_loss: 0.23340213298797607\n",
      "training: 95 batch 477 batch_loss: 0.23263207077980042\n",
      "training: 95 batch 478 batch_loss: 0.22841742634773254\n",
      "training: 95 batch 479 batch_loss: 0.23594361543655396\n",
      "training: 95 batch 480 batch_loss: 0.2321338951587677\n",
      "training: 95 batch 481 batch_loss: 0.22998040914535522\n",
      "training: 95 batch 482 batch_loss: 0.23548004031181335\n",
      "training: 95 batch 483 batch_loss: 0.23603805899620056\n",
      "training: 95 batch 484 batch_loss: 0.2285730540752411\n",
      "training: 95 batch 485 batch_loss: 0.23049935698509216\n",
      "training: 95 batch 486 batch_loss: 0.2299025058746338\n",
      "training: 95 batch 487 batch_loss: 0.2320302128791809\n",
      "training: 95 batch 488 batch_loss: 0.23338723182678223\n",
      "training: 95 batch 489 batch_loss: 0.23380491137504578\n",
      "training: 95 batch 490 batch_loss: 0.23246991634368896\n",
      "training: 95 batch 491 batch_loss: 0.2299654483795166\n",
      "training: 95 batch 492 batch_loss: 0.22668814659118652\n",
      "training: 95 batch 493 batch_loss: 0.23222437500953674\n",
      "training: 95 batch 494 batch_loss: 0.22741049528121948\n",
      "training: 95 batch 495 batch_loss: 0.22968915104866028\n",
      "training: 95 batch 496 batch_loss: 0.2294086217880249\n",
      "training: 95 batch 497 batch_loss: 0.23073917627334595\n",
      "training: 95 batch 498 batch_loss: 0.23018941283226013\n",
      "training: 95 batch 499 batch_loss: 0.2328234314918518\n",
      "training: 95 batch 500 batch_loss: 0.2279713749885559\n",
      "training: 95 batch 501 batch_loss: 0.22857773303985596\n",
      "training: 95 batch 502 batch_loss: 0.23162847757339478\n",
      "training: 95 batch 503 batch_loss: 0.2272166609764099\n",
      "training: 95 batch 504 batch_loss: 0.23198291659355164\n",
      "training: 95 batch 505 batch_loss: 0.23326843976974487\n",
      "training: 95 batch 506 batch_loss: 0.23301127552986145\n",
      "training: 95 batch 507 batch_loss: 0.22946467995643616\n",
      "training: 95 batch 508 batch_loss: 0.22862401604652405\n",
      "training: 95 batch 509 batch_loss: 0.22390779852867126\n",
      "training: 95 batch 510 batch_loss: 0.23021075129508972\n",
      "training: 95 batch 511 batch_loss: 0.23228931427001953\n",
      "training: 95 batch 512 batch_loss: 0.23174035549163818\n",
      "training: 95 batch 513 batch_loss: 0.23232847452163696\n",
      "training: 95 batch 514 batch_loss: 0.23120591044425964\n",
      "training: 95 batch 515 batch_loss: 0.2271493673324585\n",
      "training: 95 batch 516 batch_loss: 0.2295188307762146\n",
      "training: 95 batch 517 batch_loss: 0.23219221830368042\n",
      "training: 95 batch 518 batch_loss: 0.23270702362060547\n",
      "training: 95 batch 519 batch_loss: 0.22892647981643677\n",
      "training: 95 batch 520 batch_loss: 0.23080787062644958\n",
      "training: 95 batch 521 batch_loss: 0.23294231295585632\n",
      "training: 95 batch 522 batch_loss: 0.23001718521118164\n",
      "training: 95 batch 523 batch_loss: 0.23073726892471313\n",
      "training: 95 batch 524 batch_loss: 0.2298458218574524\n",
      "training: 95 batch 525 batch_loss: 0.2350310981273651\n",
      "training: 95 batch 526 batch_loss: 0.23294603824615479\n",
      "training: 95 batch 527 batch_loss: 0.2294752597808838\n",
      "training: 95 batch 528 batch_loss: 0.23293748497962952\n",
      "training: 95 batch 529 batch_loss: 0.23141464591026306\n",
      "training: 95 batch 530 batch_loss: 0.23308464884757996\n",
      "training: 95 batch 531 batch_loss: 0.23483365774154663\n",
      "training: 95 batch 532 batch_loss: 0.22754612565040588\n",
      "training: 95 batch 533 batch_loss: 0.23257574439048767\n",
      "training: 95 batch 534 batch_loss: 0.23434463143348694\n",
      "training: 95 batch 535 batch_loss: 0.23064365983009338\n",
      "training: 95 batch 536 batch_loss: 0.23051726818084717\n",
      "training: 95 batch 537 batch_loss: 0.23371750116348267\n",
      "training: 95 batch 538 batch_loss: 0.23082169890403748\n",
      "training: 95 batch 539 batch_loss: 0.23197275400161743\n",
      "training: 95 batch 540 batch_loss: 0.2308182716369629\n",
      "training: 95 batch 541 batch_loss: 0.23171842098236084\n",
      "training: 95 batch 542 batch_loss: 0.23177358508110046\n",
      "training: 95 batch 543 batch_loss: 0.22991514205932617\n",
      "training: 95 batch 544 batch_loss: 0.23409605026245117\n",
      "training: 95 batch 545 batch_loss: 0.22734081745147705\n",
      "training: 95 batch 546 batch_loss: 0.22468319535255432\n",
      "training: 95 batch 547 batch_loss: 0.23329508304595947\n",
      "training: 95 batch 548 batch_loss: 0.22997421026229858\n",
      "training: 95 batch 549 batch_loss: 0.2307271659374237\n",
      "training: 95 batch 550 batch_loss: 0.23689183592796326\n",
      "training: 95 batch 551 batch_loss: 0.2320632040500641\n",
      "training: 95 batch 552 batch_loss: 0.22783708572387695\n",
      "training: 95 batch 553 batch_loss: 0.23389863967895508\n",
      "training: 95 batch 554 batch_loss: 0.2293795943260193\n",
      "training: 95 batch 555 batch_loss: 0.2324029505252838\n",
      "training: 95 batch 556 batch_loss: 0.23079374432563782\n",
      "training: 95 batch 557 batch_loss: 0.23166438937187195\n",
      "training: 95 batch 558 batch_loss: 0.227706640958786\n",
      "training: 95 batch 559 batch_loss: 0.2315828800201416\n",
      "training: 95 batch 560 batch_loss: 0.23542818427085876\n",
      "training: 95 batch 561 batch_loss: 0.23320835828781128\n",
      "training: 95 batch 562 batch_loss: 0.2291710078716278\n",
      "training: 95 batch 563 batch_loss: 0.2307978868484497\n",
      "training: 95 batch 564 batch_loss: 0.2293843924999237\n",
      "training: 95 batch 565 batch_loss: 0.22883805632591248\n",
      "training: 95 batch 566 batch_loss: 0.22877538204193115\n",
      "training: 95 batch 567 batch_loss: 0.23075136542320251\n",
      "training: 95 batch 568 batch_loss: 0.22911608219146729\n",
      "training: 95 batch 569 batch_loss: 0.2333441972732544\n",
      "training: 95 batch 570 batch_loss: 0.23388752341270447\n",
      "training: 95 batch 571 batch_loss: 0.23117488622665405\n",
      "training: 95 batch 572 batch_loss: 0.23291751742362976\n",
      "training: 95 batch 573 batch_loss: 0.22821205854415894\n",
      "training: 95 batch 574 batch_loss: 0.22632631659507751\n",
      "training: 95 batch 575 batch_loss: 0.2287784218788147\n",
      "training: 95 batch 576 batch_loss: 0.230202317237854\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 95, Hit Ratio:0.03391151692411216 | Precision:0.0500344047970117 | Recall:0.06610821342035737 | NDCG:0.06532757491228885\n",
      "*Best Performance* \n",
      "Epoch: 93, Hit Ratio:0.0342512983314068 | Precision:0.05053573183918215 | Recall:0.06654230327357293 | MDCG:0.06593679269218644\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 96 batch 0 batch_loss: 0.22514155507087708\n",
      "training: 96 batch 1 batch_loss: 0.22912412881851196\n",
      "training: 96 batch 2 batch_loss: 0.23343342542648315\n",
      "training: 96 batch 3 batch_loss: 0.2303200662136078\n",
      "training: 96 batch 4 batch_loss: 0.23058894276618958\n",
      "training: 96 batch 5 batch_loss: 0.22833344340324402\n",
      "training: 96 batch 6 batch_loss: 0.23200929164886475\n",
      "training: 96 batch 7 batch_loss: 0.2237139642238617\n",
      "training: 96 batch 8 batch_loss: 0.22768253087997437\n",
      "training: 96 batch 9 batch_loss: 0.23036617040634155\n",
      "training: 96 batch 10 batch_loss: 0.23263216018676758\n",
      "training: 96 batch 11 batch_loss: 0.22914594411849976\n",
      "training: 96 batch 12 batch_loss: 0.2333783209323883\n",
      "training: 96 batch 13 batch_loss: 0.23186641931533813\n",
      "training: 96 batch 14 batch_loss: 0.23347917199134827\n",
      "training: 96 batch 15 batch_loss: 0.23200267553329468\n",
      "training: 96 batch 16 batch_loss: 0.2326354682445526\n",
      "training: 96 batch 17 batch_loss: 0.22764387726783752\n",
      "training: 96 batch 18 batch_loss: 0.23345839977264404\n",
      "training: 96 batch 19 batch_loss: 0.22587841749191284\n",
      "training: 96 batch 20 batch_loss: 0.23243847489356995\n",
      "training: 96 batch 21 batch_loss: 0.22730153799057007\n",
      "training: 96 batch 22 batch_loss: 0.23132801055908203\n",
      "training: 96 batch 23 batch_loss: 0.2307170033454895\n",
      "training: 96 batch 24 batch_loss: 0.23087358474731445\n",
      "training: 96 batch 25 batch_loss: 0.2301521897315979\n",
      "training: 96 batch 26 batch_loss: 0.22959545254707336\n",
      "training: 96 batch 27 batch_loss: 0.22868657112121582\n",
      "training: 96 batch 28 batch_loss: 0.23055139183998108\n",
      "training: 96 batch 29 batch_loss: 0.22640323638916016\n",
      "training: 96 batch 30 batch_loss: 0.234760582447052\n",
      "training: 96 batch 31 batch_loss: 0.22706902027130127\n",
      "training: 96 batch 32 batch_loss: 0.23151540756225586\n",
      "training: 96 batch 33 batch_loss: 0.22803965210914612\n",
      "training: 96 batch 34 batch_loss: 0.23038241267204285\n",
      "training: 96 batch 35 batch_loss: 0.22859370708465576\n",
      "training: 96 batch 36 batch_loss: 0.2301119565963745\n",
      "training: 96 batch 37 batch_loss: 0.22947168350219727\n",
      "training: 96 batch 38 batch_loss: 0.23316550254821777\n",
      "training: 96 batch 39 batch_loss: 0.2303592562675476\n",
      "training: 96 batch 40 batch_loss: 0.23054468631744385\n",
      "training: 96 batch 41 batch_loss: 0.23169752955436707\n",
      "training: 96 batch 42 batch_loss: 0.22815755009651184\n",
      "training: 96 batch 43 batch_loss: 0.23093721270561218\n",
      "training: 96 batch 44 batch_loss: 0.2278449535369873\n",
      "training: 96 batch 45 batch_loss: 0.22711583971977234\n",
      "training: 96 batch 46 batch_loss: 0.22989371418952942\n",
      "training: 96 batch 47 batch_loss: 0.23259922862052917\n",
      "training: 96 batch 48 batch_loss: 0.22880610823631287\n",
      "training: 96 batch 49 batch_loss: 0.23015433549880981\n",
      "training: 96 batch 50 batch_loss: 0.23173049092292786\n",
      "training: 96 batch 51 batch_loss: 0.22895750403404236\n",
      "training: 96 batch 52 batch_loss: 0.22719913721084595\n",
      "training: 96 batch 53 batch_loss: 0.22978782653808594\n",
      "training: 96 batch 54 batch_loss: 0.22805100679397583\n",
      "training: 96 batch 55 batch_loss: 0.23127034306526184\n",
      "training: 96 batch 56 batch_loss: 0.2284252643585205\n",
      "training: 96 batch 57 batch_loss: 0.22703075408935547\n",
      "training: 96 batch 58 batch_loss: 0.22588807344436646\n",
      "training: 96 batch 59 batch_loss: 0.23102691769599915\n",
      "training: 96 batch 60 batch_loss: 0.2319677472114563\n",
      "training: 96 batch 61 batch_loss: 0.22842878103256226\n",
      "training: 96 batch 62 batch_loss: 0.23652765154838562\n",
      "training: 96 batch 63 batch_loss: 0.22899186611175537\n",
      "training: 96 batch 64 batch_loss: 0.22991937398910522\n",
      "training: 96 batch 65 batch_loss: 0.2280566692352295\n",
      "training: 96 batch 66 batch_loss: 0.2309127151966095\n",
      "training: 96 batch 67 batch_loss: 0.2317502796649933\n",
      "training: 96 batch 68 batch_loss: 0.23021823167800903\n",
      "training: 96 batch 69 batch_loss: 0.2343934178352356\n",
      "training: 96 batch 70 batch_loss: 0.2348954677581787\n",
      "training: 96 batch 71 batch_loss: 0.22969534993171692\n",
      "training: 96 batch 72 batch_loss: 0.2291887104511261\n",
      "training: 96 batch 73 batch_loss: 0.22698059678077698\n",
      "training: 96 batch 74 batch_loss: 0.23144733905792236\n",
      "training: 96 batch 75 batch_loss: 0.2359929084777832\n",
      "training: 96 batch 76 batch_loss: 0.23030218482017517\n",
      "training: 96 batch 77 batch_loss: 0.22925829887390137\n",
      "training: 96 batch 78 batch_loss: 0.23066231608390808\n",
      "training: 96 batch 79 batch_loss: 0.2312805950641632\n",
      "training: 96 batch 80 batch_loss: 0.2334423065185547\n",
      "training: 96 batch 81 batch_loss: 0.23730415105819702\n",
      "training: 96 batch 82 batch_loss: 0.22585207223892212\n",
      "training: 96 batch 83 batch_loss: 0.231903076171875\n",
      "training: 96 batch 84 batch_loss: 0.22831854224205017\n",
      "training: 96 batch 85 batch_loss: 0.22785964608192444\n",
      "training: 96 batch 86 batch_loss: 0.23059675097465515\n",
      "training: 96 batch 87 batch_loss: 0.2327079176902771\n",
      "training: 96 batch 88 batch_loss: 0.22740530967712402\n",
      "training: 96 batch 89 batch_loss: 0.23451226949691772\n",
      "training: 96 batch 90 batch_loss: 0.22763186693191528\n",
      "training: 96 batch 91 batch_loss: 0.22783809900283813\n",
      "training: 96 batch 92 batch_loss: 0.23120763897895813\n",
      "training: 96 batch 93 batch_loss: 0.2331920862197876\n",
      "training: 96 batch 94 batch_loss: 0.23296940326690674\n",
      "training: 96 batch 95 batch_loss: 0.22992342710494995\n",
      "training: 96 batch 96 batch_loss: 0.2282722294330597\n",
      "training: 96 batch 97 batch_loss: 0.2303982973098755\n",
      "training: 96 batch 98 batch_loss: 0.22999140620231628\n",
      "training: 96 batch 99 batch_loss: 0.233001708984375\n",
      "training: 96 batch 100 batch_loss: 0.23003217577934265\n",
      "training: 96 batch 101 batch_loss: 0.23010054230690002\n",
      "training: 96 batch 102 batch_loss: 0.23294895887374878\n",
      "training: 96 batch 103 batch_loss: 0.22633057832717896\n",
      "training: 96 batch 104 batch_loss: 0.23235353827476501\n",
      "training: 96 batch 105 batch_loss: 0.2271098792552948\n",
      "training: 96 batch 106 batch_loss: 0.23473778367042542\n",
      "training: 96 batch 107 batch_loss: 0.2329157590866089\n",
      "training: 96 batch 108 batch_loss: 0.23077821731567383\n",
      "training: 96 batch 109 batch_loss: 0.233549565076828\n",
      "training: 96 batch 110 batch_loss: 0.22824406623840332\n",
      "training: 96 batch 111 batch_loss: 0.23074659705162048\n",
      "training: 96 batch 112 batch_loss: 0.2319830060005188\n",
      "training: 96 batch 113 batch_loss: 0.2283596396446228\n",
      "training: 96 batch 114 batch_loss: 0.227308452129364\n",
      "training: 96 batch 115 batch_loss: 0.23021137714385986\n",
      "training: 96 batch 116 batch_loss: 0.23065593838691711\n",
      "training: 96 batch 117 batch_loss: 0.2301856279373169\n",
      "training: 96 batch 118 batch_loss: 0.23184290528297424\n",
      "training: 96 batch 119 batch_loss: 0.23036617040634155\n",
      "training: 96 batch 120 batch_loss: 0.23168742656707764\n",
      "training: 96 batch 121 batch_loss: 0.22899121046066284\n",
      "training: 96 batch 122 batch_loss: 0.2320031225681305\n",
      "training: 96 batch 123 batch_loss: 0.2305009365081787\n",
      "training: 96 batch 124 batch_loss: 0.23227417469024658\n",
      "training: 96 batch 125 batch_loss: 0.230229914188385\n",
      "training: 96 batch 126 batch_loss: 0.22778502106666565\n",
      "training: 96 batch 127 batch_loss: 0.2307228147983551\n",
      "training: 96 batch 128 batch_loss: 0.23028090596199036\n",
      "training: 96 batch 129 batch_loss: 0.23306876420974731\n",
      "training: 96 batch 130 batch_loss: 0.23032155632972717\n",
      "training: 96 batch 131 batch_loss: 0.22940191626548767\n",
      "training: 96 batch 132 batch_loss: 0.22830384969711304\n",
      "training: 96 batch 133 batch_loss: 0.2303282618522644\n",
      "training: 96 batch 134 batch_loss: 0.2364392876625061\n",
      "training: 96 batch 135 batch_loss: 0.2290952503681183\n",
      "training: 96 batch 136 batch_loss: 0.23266395926475525\n",
      "training: 96 batch 137 batch_loss: 0.23383086919784546\n",
      "training: 96 batch 138 batch_loss: 0.2280670702457428\n",
      "training: 96 batch 139 batch_loss: 0.2324543297290802\n",
      "training: 96 batch 140 batch_loss: 0.22880488634109497\n",
      "training: 96 batch 141 batch_loss: 0.22752457857131958\n",
      "training: 96 batch 142 batch_loss: 0.23008319735527039\n",
      "training: 96 batch 143 batch_loss: 0.23013833165168762\n",
      "training: 96 batch 144 batch_loss: 0.23051097989082336\n",
      "training: 96 batch 145 batch_loss: 0.23173436522483826\n",
      "training: 96 batch 146 batch_loss: 0.23153355717658997\n",
      "training: 96 batch 147 batch_loss: 0.230528324842453\n",
      "training: 96 batch 148 batch_loss: 0.23494940996170044\n",
      "training: 96 batch 149 batch_loss: 0.22821995615959167\n",
      "training: 96 batch 150 batch_loss: 0.23230984807014465\n",
      "training: 96 batch 151 batch_loss: 0.23161664605140686\n",
      "training: 96 batch 152 batch_loss: 0.2356751561164856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 96 batch 153 batch_loss: 0.22879180312156677\n",
      "training: 96 batch 154 batch_loss: 0.23053348064422607\n",
      "training: 96 batch 155 batch_loss: 0.227336585521698\n",
      "training: 96 batch 156 batch_loss: 0.22497650980949402\n",
      "training: 96 batch 157 batch_loss: 0.23195865750312805\n",
      "training: 96 batch 158 batch_loss: 0.2295036017894745\n",
      "training: 96 batch 159 batch_loss: 0.22638577222824097\n",
      "training: 96 batch 160 batch_loss: 0.22869497537612915\n",
      "training: 96 batch 161 batch_loss: 0.23138949275016785\n",
      "training: 96 batch 162 batch_loss: 0.23250102996826172\n",
      "training: 96 batch 163 batch_loss: 0.23138093948364258\n",
      "training: 96 batch 164 batch_loss: 0.2362803816795349\n",
      "training: 96 batch 165 batch_loss: 0.2316400408744812\n",
      "training: 96 batch 166 batch_loss: 0.2279016375541687\n",
      "training: 96 batch 167 batch_loss: 0.23070061206817627\n",
      "training: 96 batch 168 batch_loss: 0.22880324721336365\n",
      "training: 96 batch 169 batch_loss: 0.23125171661376953\n",
      "training: 96 batch 170 batch_loss: 0.23171654343605042\n",
      "training: 96 batch 171 batch_loss: 0.2352564036846161\n",
      "training: 96 batch 172 batch_loss: 0.23259806632995605\n",
      "training: 96 batch 173 batch_loss: 0.22951313853263855\n",
      "training: 96 batch 174 batch_loss: 0.2306605875492096\n",
      "training: 96 batch 175 batch_loss: 0.2298954427242279\n",
      "training: 96 batch 176 batch_loss: 0.2335749864578247\n",
      "training: 96 batch 177 batch_loss: 0.230568528175354\n",
      "training: 96 batch 178 batch_loss: 0.2334895133972168\n",
      "training: 96 batch 179 batch_loss: 0.23073187470436096\n",
      "training: 96 batch 180 batch_loss: 0.22888541221618652\n",
      "training: 96 batch 181 batch_loss: 0.22952985763549805\n",
      "training: 96 batch 182 batch_loss: 0.23043891787528992\n",
      "training: 96 batch 183 batch_loss: 0.2319185435771942\n",
      "training: 96 batch 184 batch_loss: 0.22901418805122375\n",
      "training: 96 batch 185 batch_loss: 0.22848302125930786\n",
      "training: 96 batch 186 batch_loss: 0.2331891655921936\n",
      "training: 96 batch 187 batch_loss: 0.23035630583763123\n",
      "training: 96 batch 188 batch_loss: 0.22728079557418823\n",
      "training: 96 batch 189 batch_loss: 0.23134911060333252\n",
      "training: 96 batch 190 batch_loss: 0.2309700846672058\n",
      "training: 96 batch 191 batch_loss: 0.23379847407341003\n",
      "training: 96 batch 192 batch_loss: 0.23498746752738953\n",
      "training: 96 batch 193 batch_loss: 0.2342228889465332\n",
      "training: 96 batch 194 batch_loss: 0.22782611846923828\n",
      "training: 96 batch 195 batch_loss: 0.23166412115097046\n",
      "training: 96 batch 196 batch_loss: 0.23560762405395508\n",
      "training: 96 batch 197 batch_loss: 0.23186272382736206\n",
      "training: 96 batch 198 batch_loss: 0.23122647404670715\n",
      "training: 96 batch 199 batch_loss: 0.23407047986984253\n",
      "training: 96 batch 200 batch_loss: 0.23443663120269775\n",
      "training: 96 batch 201 batch_loss: 0.2299477458000183\n",
      "training: 96 batch 202 batch_loss: 0.23171672224998474\n",
      "training: 96 batch 203 batch_loss: 0.23123714327812195\n",
      "training: 96 batch 204 batch_loss: 0.23487341403961182\n",
      "training: 96 batch 205 batch_loss: 0.23100882768630981\n",
      "training: 96 batch 206 batch_loss: 0.2312183380126953\n",
      "training: 96 batch 207 batch_loss: 0.23017466068267822\n",
      "training: 96 batch 208 batch_loss: 0.22822332382202148\n",
      "training: 96 batch 209 batch_loss: 0.22786158323287964\n",
      "training: 96 batch 210 batch_loss: 0.23139849305152893\n",
      "training: 96 batch 211 batch_loss: 0.23204615712165833\n",
      "training: 96 batch 212 batch_loss: 0.23042511940002441\n",
      "training: 96 batch 213 batch_loss: 0.23551702499389648\n",
      "training: 96 batch 214 batch_loss: 0.23122823238372803\n",
      "training: 96 batch 215 batch_loss: 0.2309519350528717\n",
      "training: 96 batch 216 batch_loss: 0.23391681909561157\n",
      "training: 96 batch 217 batch_loss: 0.23421192169189453\n",
      "training: 96 batch 218 batch_loss: 0.22918564081192017\n",
      "training: 96 batch 219 batch_loss: 0.22783297300338745\n",
      "training: 96 batch 220 batch_loss: 0.22886648774147034\n",
      "training: 96 batch 221 batch_loss: 0.2279752492904663\n",
      "training: 96 batch 222 batch_loss: 0.2340240478515625\n",
      "training: 96 batch 223 batch_loss: 0.22989869117736816\n",
      "training: 96 batch 224 batch_loss: 0.2320939600467682\n",
      "training: 96 batch 225 batch_loss: 0.22896450757980347\n",
      "training: 96 batch 226 batch_loss: 0.23109182715415955\n",
      "training: 96 batch 227 batch_loss: 0.23017632961273193\n",
      "training: 96 batch 228 batch_loss: 0.2333003282546997\n",
      "training: 96 batch 229 batch_loss: 0.23026415705680847\n",
      "training: 96 batch 230 batch_loss: 0.2337082028388977\n",
      "training: 96 batch 231 batch_loss: 0.2276502549648285\n",
      "training: 96 batch 232 batch_loss: 0.22812700271606445\n",
      "training: 96 batch 233 batch_loss: 0.2303299903869629\n",
      "training: 96 batch 234 batch_loss: 0.2283344566822052\n",
      "training: 96 batch 235 batch_loss: 0.23114937543869019\n",
      "training: 96 batch 236 batch_loss: 0.23176562786102295\n",
      "training: 96 batch 237 batch_loss: 0.2261320948600769\n",
      "training: 96 batch 238 batch_loss: 0.2318425178527832\n",
      "training: 96 batch 239 batch_loss: 0.2306097149848938\n",
      "training: 96 batch 240 batch_loss: 0.2316252887248993\n",
      "training: 96 batch 241 batch_loss: 0.23087641596794128\n",
      "training: 96 batch 242 batch_loss: 0.23049375414848328\n",
      "training: 96 batch 243 batch_loss: 0.23064404726028442\n",
      "training: 96 batch 244 batch_loss: 0.22780746221542358\n",
      "training: 96 batch 245 batch_loss: 0.2330816388130188\n",
      "training: 96 batch 246 batch_loss: 0.23541584610939026\n",
      "training: 96 batch 247 batch_loss: 0.23215097188949585\n",
      "training: 96 batch 248 batch_loss: 0.22482991218566895\n",
      "training: 96 batch 249 batch_loss: 0.23044398427009583\n",
      "training: 96 batch 250 batch_loss: 0.2319105863571167\n",
      "training: 96 batch 251 batch_loss: 0.22846397757530212\n",
      "training: 96 batch 252 batch_loss: 0.23373711109161377\n",
      "training: 96 batch 253 batch_loss: 0.22914433479309082\n",
      "training: 96 batch 254 batch_loss: 0.23569950461387634\n",
      "training: 96 batch 255 batch_loss: 0.2295290231704712\n",
      "training: 96 batch 256 batch_loss: 0.23066720366477966\n",
      "training: 96 batch 257 batch_loss: 0.2313963770866394\n",
      "training: 96 batch 258 batch_loss: 0.2294003665447235\n",
      "training: 96 batch 259 batch_loss: 0.2312413454055786\n",
      "training: 96 batch 260 batch_loss: 0.23101821541786194\n",
      "training: 96 batch 261 batch_loss: 0.23213601112365723\n",
      "training: 96 batch 262 batch_loss: 0.23110753297805786\n",
      "training: 96 batch 263 batch_loss: 0.23227155208587646\n",
      "training: 96 batch 264 batch_loss: 0.2305344045162201\n",
      "training: 96 batch 265 batch_loss: 0.23647081851959229\n",
      "training: 96 batch 266 batch_loss: 0.23304247856140137\n",
      "training: 96 batch 267 batch_loss: 0.23350650072097778\n",
      "training: 96 batch 268 batch_loss: 0.23517733812332153\n",
      "training: 96 batch 269 batch_loss: 0.23144957423210144\n",
      "training: 96 batch 270 batch_loss: 0.22886374592781067\n",
      "training: 96 batch 271 batch_loss: 0.2271740436553955\n",
      "training: 96 batch 272 batch_loss: 0.23188728094100952\n",
      "training: 96 batch 273 batch_loss: 0.2296563982963562\n",
      "training: 96 batch 274 batch_loss: 0.23278892040252686\n",
      "training: 96 batch 275 batch_loss: 0.22889301180839539\n",
      "training: 96 batch 276 batch_loss: 0.23115813732147217\n",
      "training: 96 batch 277 batch_loss: 0.2322172224521637\n",
      "training: 96 batch 278 batch_loss: 0.22955322265625\n",
      "training: 96 batch 279 batch_loss: 0.23359030485153198\n",
      "training: 96 batch 280 batch_loss: 0.2303633987903595\n",
      "training: 96 batch 281 batch_loss: 0.23216521739959717\n",
      "training: 96 batch 282 batch_loss: 0.2321147918701172\n",
      "training: 96 batch 283 batch_loss: 0.23154249787330627\n",
      "training: 96 batch 284 batch_loss: 0.22884225845336914\n",
      "training: 96 batch 285 batch_loss: 0.23220005631446838\n",
      "training: 96 batch 286 batch_loss: 0.2277444303035736\n",
      "training: 96 batch 287 batch_loss: 0.23300200700759888\n",
      "training: 96 batch 288 batch_loss: 0.23150399327278137\n",
      "training: 96 batch 289 batch_loss: 0.23442262411117554\n",
      "training: 96 batch 290 batch_loss: 0.22787389159202576\n",
      "training: 96 batch 291 batch_loss: 0.23192855715751648\n",
      "training: 96 batch 292 batch_loss: 0.2301580309867859\n",
      "training: 96 batch 293 batch_loss: 0.23251059651374817\n",
      "training: 96 batch 294 batch_loss: 0.2286476194858551\n",
      "training: 96 batch 295 batch_loss: 0.23003384470939636\n",
      "training: 96 batch 296 batch_loss: 0.22603130340576172\n",
      "training: 96 batch 297 batch_loss: 0.2316654920578003\n",
      "training: 96 batch 298 batch_loss: 0.23189586400985718\n",
      "training: 96 batch 299 batch_loss: 0.23365885019302368\n",
      "training: 96 batch 300 batch_loss: 0.23289889097213745\n",
      "training: 96 batch 301 batch_loss: 0.23530149459838867\n",
      "training: 96 batch 302 batch_loss: 0.23367401957511902\n",
      "training: 96 batch 303 batch_loss: 0.23137789964675903\n",
      "training: 96 batch 304 batch_loss: 0.23139411211013794\n",
      "training: 96 batch 305 batch_loss: 0.2324618697166443\n",
      "training: 96 batch 306 batch_loss: 0.23375573754310608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 96 batch 307 batch_loss: 0.22917518019676208\n",
      "training: 96 batch 308 batch_loss: 0.23786291480064392\n",
      "training: 96 batch 309 batch_loss: 0.22885850071907043\n",
      "training: 96 batch 310 batch_loss: 0.2315848469734192\n",
      "training: 96 batch 311 batch_loss: 0.2305927276611328\n",
      "training: 96 batch 312 batch_loss: 0.23417562246322632\n",
      "training: 96 batch 313 batch_loss: 0.2269885540008545\n",
      "training: 96 batch 314 batch_loss: 0.23037657141685486\n",
      "training: 96 batch 315 batch_loss: 0.22857430577278137\n",
      "training: 96 batch 316 batch_loss: 0.23192614316940308\n",
      "training: 96 batch 317 batch_loss: 0.23200708627700806\n",
      "training: 96 batch 318 batch_loss: 0.2337338924407959\n",
      "training: 96 batch 319 batch_loss: 0.23100075125694275\n",
      "training: 96 batch 320 batch_loss: 0.23394647240638733\n",
      "training: 96 batch 321 batch_loss: 0.22465765476226807\n",
      "training: 96 batch 322 batch_loss: 0.23345822095870972\n",
      "training: 96 batch 323 batch_loss: 0.23241987824440002\n",
      "training: 96 batch 324 batch_loss: 0.2313312590122223\n",
      "training: 96 batch 325 batch_loss: 0.2342585027217865\n",
      "training: 96 batch 326 batch_loss: 0.23118138313293457\n",
      "training: 96 batch 327 batch_loss: 0.23189660906791687\n",
      "training: 96 batch 328 batch_loss: 0.2308279275894165\n",
      "training: 96 batch 329 batch_loss: 0.22835439443588257\n",
      "training: 96 batch 330 batch_loss: 0.23072105646133423\n",
      "training: 96 batch 331 batch_loss: 0.23401427268981934\n",
      "training: 96 batch 332 batch_loss: 0.23240578174591064\n",
      "training: 96 batch 333 batch_loss: 0.23016685247421265\n",
      "training: 96 batch 334 batch_loss: 0.23346394300460815\n",
      "training: 96 batch 335 batch_loss: 0.2309150993824005\n",
      "training: 96 batch 336 batch_loss: 0.2331455945968628\n",
      "training: 96 batch 337 batch_loss: 0.23509567975997925\n",
      "training: 96 batch 338 batch_loss: 0.23335176706314087\n",
      "training: 96 batch 339 batch_loss: 0.23263117671012878\n",
      "training: 96 batch 340 batch_loss: 0.22730380296707153\n",
      "training: 96 batch 341 batch_loss: 0.23087406158447266\n",
      "training: 96 batch 342 batch_loss: 0.22624251246452332\n",
      "training: 96 batch 343 batch_loss: 0.22468236088752747\n",
      "training: 96 batch 344 batch_loss: 0.22983184456825256\n",
      "training: 96 batch 345 batch_loss: 0.23437225818634033\n",
      "training: 96 batch 346 batch_loss: 0.22947531938552856\n",
      "training: 96 batch 347 batch_loss: 0.2293846607208252\n",
      "training: 96 batch 348 batch_loss: 0.23142296075820923\n",
      "training: 96 batch 349 batch_loss: 0.2311301827430725\n",
      "training: 96 batch 350 batch_loss: 0.22997206449508667\n",
      "training: 96 batch 351 batch_loss: 0.22762250900268555\n",
      "training: 96 batch 352 batch_loss: 0.22830486297607422\n",
      "training: 96 batch 353 batch_loss: 0.23668000102043152\n",
      "training: 96 batch 354 batch_loss: 0.23132148385047913\n",
      "training: 96 batch 355 batch_loss: 0.23321902751922607\n",
      "training: 96 batch 356 batch_loss: 0.23428374528884888\n",
      "training: 96 batch 357 batch_loss: 0.23471584916114807\n",
      "training: 96 batch 358 batch_loss: 0.23631760478019714\n",
      "training: 96 batch 359 batch_loss: 0.23303550481796265\n",
      "training: 96 batch 360 batch_loss: 0.23053643107414246\n",
      "training: 96 batch 361 batch_loss: 0.23285526037216187\n",
      "training: 96 batch 362 batch_loss: 0.23160198330879211\n",
      "training: 96 batch 363 batch_loss: 0.23303839564323425\n",
      "training: 96 batch 364 batch_loss: 0.23543128371238708\n",
      "training: 96 batch 365 batch_loss: 0.23249056935310364\n",
      "training: 96 batch 366 batch_loss: 0.23134690523147583\n",
      "training: 96 batch 367 batch_loss: 0.2342408299446106\n",
      "training: 96 batch 368 batch_loss: 0.23306846618652344\n",
      "training: 96 batch 369 batch_loss: 0.2304929792881012\n",
      "training: 96 batch 370 batch_loss: 0.2303035855293274\n",
      "training: 96 batch 371 batch_loss: 0.2308695912361145\n",
      "training: 96 batch 372 batch_loss: 0.23090529441833496\n",
      "training: 96 batch 373 batch_loss: 0.22677147388458252\n",
      "training: 96 batch 374 batch_loss: 0.22845378518104553\n",
      "training: 96 batch 375 batch_loss: 0.2326517105102539\n",
      "training: 96 batch 376 batch_loss: 0.23426684737205505\n",
      "training: 96 batch 377 batch_loss: 0.22913193702697754\n",
      "training: 96 batch 378 batch_loss: 0.2328072488307953\n",
      "training: 96 batch 379 batch_loss: 0.23601743578910828\n",
      "training: 96 batch 380 batch_loss: 0.2285347580909729\n",
      "training: 96 batch 381 batch_loss: 0.228196382522583\n",
      "training: 96 batch 382 batch_loss: 0.23072052001953125\n",
      "training: 96 batch 383 batch_loss: 0.22841355204582214\n",
      "training: 96 batch 384 batch_loss: 0.23083770275115967\n",
      "training: 96 batch 385 batch_loss: 0.23131582140922546\n",
      "training: 96 batch 386 batch_loss: 0.23333030939102173\n",
      "training: 96 batch 387 batch_loss: 0.23036912083625793\n",
      "training: 96 batch 388 batch_loss: 0.22963076829910278\n",
      "training: 96 batch 389 batch_loss: 0.229453444480896\n",
      "training: 96 batch 390 batch_loss: 0.2311641275882721\n",
      "training: 96 batch 391 batch_loss: 0.23378020524978638\n",
      "training: 96 batch 392 batch_loss: 0.235240638256073\n",
      "training: 96 batch 393 batch_loss: 0.2306165099143982\n",
      "training: 96 batch 394 batch_loss: 0.2356749176979065\n",
      "training: 96 batch 395 batch_loss: 0.23446065187454224\n",
      "training: 96 batch 396 batch_loss: 0.23524585366249084\n",
      "training: 96 batch 397 batch_loss: 0.22904491424560547\n",
      "training: 96 batch 398 batch_loss: 0.23422273993492126\n",
      "training: 96 batch 399 batch_loss: 0.22905957698822021\n",
      "training: 96 batch 400 batch_loss: 0.23165631294250488\n",
      "training: 96 batch 401 batch_loss: 0.22763463854789734\n",
      "training: 96 batch 402 batch_loss: 0.2277798056602478\n",
      "training: 96 batch 403 batch_loss: 0.23081398010253906\n",
      "training: 96 batch 404 batch_loss: 0.23607391119003296\n",
      "training: 96 batch 405 batch_loss: 0.23067280650138855\n",
      "training: 96 batch 406 batch_loss: 0.22812199592590332\n",
      "training: 96 batch 407 batch_loss: 0.2320675551891327\n",
      "training: 96 batch 408 batch_loss: 0.2341594696044922\n",
      "training: 96 batch 409 batch_loss: 0.2333413064479828\n",
      "training: 96 batch 410 batch_loss: 0.23007839918136597\n",
      "training: 96 batch 411 batch_loss: 0.23076337575912476\n",
      "training: 96 batch 412 batch_loss: 0.2333112359046936\n",
      "training: 96 batch 413 batch_loss: 0.23326918482780457\n",
      "training: 96 batch 414 batch_loss: 0.23178750276565552\n",
      "training: 96 batch 415 batch_loss: 0.2314985990524292\n",
      "training: 96 batch 416 batch_loss: 0.2301407754421234\n",
      "training: 96 batch 417 batch_loss: 0.2320917844772339\n",
      "training: 96 batch 418 batch_loss: 0.22993192076683044\n",
      "training: 96 batch 419 batch_loss: 0.2309381067752838\n",
      "training: 96 batch 420 batch_loss: 0.22732597589492798\n",
      "training: 96 batch 421 batch_loss: 0.23642081022262573\n",
      "training: 96 batch 422 batch_loss: 0.23264670372009277\n",
      "training: 96 batch 423 batch_loss: 0.235071063041687\n",
      "training: 96 batch 424 batch_loss: 0.22972267866134644\n",
      "training: 96 batch 425 batch_loss: 0.23079681396484375\n",
      "training: 96 batch 426 batch_loss: 0.2249002456665039\n",
      "training: 96 batch 427 batch_loss: 0.23891854286193848\n",
      "training: 96 batch 428 batch_loss: 0.23405250906944275\n",
      "training: 96 batch 429 batch_loss: 0.23167219758033752\n",
      "training: 96 batch 430 batch_loss: 0.2332858145236969\n",
      "training: 96 batch 431 batch_loss: 0.23234212398529053\n",
      "training: 96 batch 432 batch_loss: 0.2335614562034607\n",
      "training: 96 batch 433 batch_loss: 0.23065441846847534\n",
      "training: 96 batch 434 batch_loss: 0.22962382435798645\n",
      "training: 96 batch 435 batch_loss: 0.23503372073173523\n",
      "training: 96 batch 436 batch_loss: 0.2311747670173645\n",
      "training: 96 batch 437 batch_loss: 0.23093006014823914\n",
      "training: 96 batch 438 batch_loss: 0.23324868083000183\n",
      "training: 96 batch 439 batch_loss: 0.22804978489875793\n",
      "training: 96 batch 440 batch_loss: 0.22952589392662048\n",
      "training: 96 batch 441 batch_loss: 0.22892460227012634\n",
      "training: 96 batch 442 batch_loss: 0.23028096556663513\n",
      "training: 96 batch 443 batch_loss: 0.2322165071964264\n",
      "training: 96 batch 444 batch_loss: 0.23128867149353027\n",
      "training: 96 batch 445 batch_loss: 0.23270070552825928\n",
      "training: 96 batch 446 batch_loss: 0.22980862855911255\n",
      "training: 96 batch 447 batch_loss: 0.22873616218566895\n",
      "training: 96 batch 448 batch_loss: 0.2292097508907318\n",
      "training: 96 batch 449 batch_loss: 0.23594069480895996\n",
      "training: 96 batch 450 batch_loss: 0.22812023758888245\n",
      "training: 96 batch 451 batch_loss: 0.22993624210357666\n",
      "training: 96 batch 452 batch_loss: 0.22793620824813843\n",
      "training: 96 batch 453 batch_loss: 0.2299841344356537\n",
      "training: 96 batch 454 batch_loss: 0.23649173974990845\n",
      "training: 96 batch 455 batch_loss: 0.23376980423927307\n",
      "training: 96 batch 456 batch_loss: 0.23415598273277283\n",
      "training: 96 batch 457 batch_loss: 0.23449409008026123\n",
      "training: 96 batch 458 batch_loss: 0.23008328676223755\n",
      "training: 96 batch 459 batch_loss: 0.23519223928451538\n",
      "training: 96 batch 460 batch_loss: 0.22760164737701416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 96 batch 461 batch_loss: 0.23638901114463806\n",
      "training: 96 batch 462 batch_loss: 0.2289053201675415\n",
      "training: 96 batch 463 batch_loss: 0.2283850908279419\n",
      "training: 96 batch 464 batch_loss: 0.23305600881576538\n",
      "training: 96 batch 465 batch_loss: 0.23665502667427063\n",
      "training: 96 batch 466 batch_loss: 0.23420679569244385\n",
      "training: 96 batch 467 batch_loss: 0.23368769884109497\n",
      "training: 96 batch 468 batch_loss: 0.2282436490058899\n",
      "training: 96 batch 469 batch_loss: 0.23189014196395874\n",
      "training: 96 batch 470 batch_loss: 0.23438912630081177\n",
      "training: 96 batch 471 batch_loss: 0.2304341197013855\n",
      "training: 96 batch 472 batch_loss: 0.22963547706604004\n",
      "training: 96 batch 473 batch_loss: 0.23280054330825806\n",
      "training: 96 batch 474 batch_loss: 0.2344645857810974\n",
      "training: 96 batch 475 batch_loss: 0.22956594824790955\n",
      "training: 96 batch 476 batch_loss: 0.22883474826812744\n",
      "training: 96 batch 477 batch_loss: 0.23002657294273376\n",
      "training: 96 batch 478 batch_loss: 0.23485276103019714\n",
      "training: 96 batch 479 batch_loss: 0.2303013801574707\n",
      "training: 96 batch 480 batch_loss: 0.2290753722190857\n",
      "training: 96 batch 481 batch_loss: 0.23132899403572083\n",
      "training: 96 batch 482 batch_loss: 0.23389044404029846\n",
      "training: 96 batch 483 batch_loss: 0.23397907614707947\n",
      "training: 96 batch 484 batch_loss: 0.2330845594406128\n",
      "training: 96 batch 485 batch_loss: 0.2305600345134735\n",
      "training: 96 batch 486 batch_loss: 0.23062846064567566\n",
      "training: 96 batch 487 batch_loss: 0.23475536704063416\n",
      "training: 96 batch 488 batch_loss: 0.2325928509235382\n",
      "training: 96 batch 489 batch_loss: 0.23043173551559448\n",
      "training: 96 batch 490 batch_loss: 0.23116618394851685\n",
      "training: 96 batch 491 batch_loss: 0.23517650365829468\n",
      "training: 96 batch 492 batch_loss: 0.23341774940490723\n",
      "training: 96 batch 493 batch_loss: 0.23251688480377197\n",
      "training: 96 batch 494 batch_loss: 0.23331379890441895\n",
      "training: 96 batch 495 batch_loss: 0.23631849884986877\n",
      "training: 96 batch 496 batch_loss: 0.23117804527282715\n",
      "training: 96 batch 497 batch_loss: 0.2321796417236328\n",
      "training: 96 batch 498 batch_loss: 0.23369085788726807\n",
      "training: 96 batch 499 batch_loss: 0.2347843050956726\n",
      "training: 96 batch 500 batch_loss: 0.23283475637435913\n",
      "training: 96 batch 501 batch_loss: 0.2344411015510559\n",
      "training: 96 batch 502 batch_loss: 0.23279064893722534\n",
      "training: 96 batch 503 batch_loss: 0.22573259472846985\n",
      "training: 96 batch 504 batch_loss: 0.23237723112106323\n",
      "training: 96 batch 505 batch_loss: 0.23259463906288147\n",
      "training: 96 batch 506 batch_loss: 0.22643783688545227\n",
      "training: 96 batch 507 batch_loss: 0.23632779717445374\n",
      "training: 96 batch 508 batch_loss: 0.2280140519142151\n",
      "training: 96 batch 509 batch_loss: 0.23348486423492432\n",
      "training: 96 batch 510 batch_loss: 0.23792293667793274\n",
      "training: 96 batch 511 batch_loss: 0.23821070790290833\n",
      "training: 96 batch 512 batch_loss: 0.23187944293022156\n",
      "training: 96 batch 513 batch_loss: 0.23345869779586792\n",
      "training: 96 batch 514 batch_loss: 0.23005327582359314\n",
      "training: 96 batch 515 batch_loss: 0.2308051884174347\n",
      "training: 96 batch 516 batch_loss: 0.23560220003128052\n",
      "training: 96 batch 517 batch_loss: 0.23337417840957642\n",
      "training: 96 batch 518 batch_loss: 0.23568928241729736\n",
      "training: 96 batch 519 batch_loss: 0.23109036684036255\n",
      "training: 96 batch 520 batch_loss: 0.23329773545265198\n",
      "training: 96 batch 521 batch_loss: 0.23863902688026428\n",
      "training: 96 batch 522 batch_loss: 0.22724604606628418\n",
      "training: 96 batch 523 batch_loss: 0.22600945830345154\n",
      "training: 96 batch 524 batch_loss: 0.22706633806228638\n",
      "training: 96 batch 525 batch_loss: 0.22655454277992249\n",
      "training: 96 batch 526 batch_loss: 0.2278899848461151\n",
      "training: 96 batch 527 batch_loss: 0.23124882578849792\n",
      "training: 96 batch 528 batch_loss: 0.23320841789245605\n",
      "training: 96 batch 529 batch_loss: 0.23131799697875977\n",
      "training: 96 batch 530 batch_loss: 0.232940673828125\n",
      "training: 96 batch 531 batch_loss: 0.23436659574508667\n",
      "training: 96 batch 532 batch_loss: 0.23294872045516968\n",
      "training: 96 batch 533 batch_loss: 0.23316651582717896\n",
      "training: 96 batch 534 batch_loss: 0.23479190468788147\n",
      "training: 96 batch 535 batch_loss: 0.2277282178401947\n",
      "training: 96 batch 536 batch_loss: 0.2312755286693573\n",
      "training: 96 batch 537 batch_loss: 0.23052018880844116\n",
      "training: 96 batch 538 batch_loss: 0.2312554121017456\n",
      "training: 96 batch 539 batch_loss: 0.23642444610595703\n",
      "training: 96 batch 540 batch_loss: 0.22936522960662842\n",
      "training: 96 batch 541 batch_loss: 0.23338237404823303\n",
      "training: 96 batch 542 batch_loss: 0.2352660894393921\n",
      "training: 96 batch 543 batch_loss: 0.23066869378089905\n",
      "training: 96 batch 544 batch_loss: 0.2305694818496704\n",
      "training: 96 batch 545 batch_loss: 0.23258453607559204\n",
      "training: 96 batch 546 batch_loss: 0.2299826443195343\n",
      "training: 96 batch 547 batch_loss: 0.23088252544403076\n",
      "training: 96 batch 548 batch_loss: 0.22912096977233887\n",
      "training: 96 batch 549 batch_loss: 0.23138582706451416\n",
      "training: 96 batch 550 batch_loss: 0.23130470514297485\n",
      "training: 96 batch 551 batch_loss: 0.23373517394065857\n",
      "training: 96 batch 552 batch_loss: 0.22917693853378296\n",
      "training: 96 batch 553 batch_loss: 0.2325441539287567\n",
      "training: 96 batch 554 batch_loss: 0.2275095283985138\n",
      "training: 96 batch 555 batch_loss: 0.23693719506263733\n",
      "training: 96 batch 556 batch_loss: 0.23118793964385986\n",
      "training: 96 batch 557 batch_loss: 0.2351176142692566\n",
      "training: 96 batch 558 batch_loss: 0.2301647961139679\n",
      "training: 96 batch 559 batch_loss: 0.22630277276039124\n",
      "training: 96 batch 560 batch_loss: 0.23599395155906677\n",
      "training: 96 batch 561 batch_loss: 0.23422390222549438\n",
      "training: 96 batch 562 batch_loss: 0.2338573932647705\n",
      "training: 96 batch 563 batch_loss: 0.23680534958839417\n",
      "training: 96 batch 564 batch_loss: 0.23615652322769165\n",
      "training: 96 batch 565 batch_loss: 0.23292014002799988\n",
      "training: 96 batch 566 batch_loss: 0.23162606358528137\n",
      "training: 96 batch 567 batch_loss: 0.22893041372299194\n",
      "training: 96 batch 568 batch_loss: 0.23132914304733276\n",
      "training: 96 batch 569 batch_loss: 0.23401373624801636\n",
      "training: 96 batch 570 batch_loss: 0.2328932285308838\n",
      "training: 96 batch 571 batch_loss: 0.2306894063949585\n",
      "training: 96 batch 572 batch_loss: 0.2321348786354065\n",
      "training: 96 batch 573 batch_loss: 0.2302231788635254\n",
      "training: 96 batch 574 batch_loss: 0.23234528303146362\n",
      "training: 96 batch 575 batch_loss: 0.234469473361969\n",
      "training: 96 batch 576 batch_loss: 0.2311992645263672\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 96, Hit Ratio:0.03402810858347796 | Precision:0.050206428782070184 | Recall:0.06600416579457385 | NDCG:0.06554562345933611\n",
      "*Best Performance* \n",
      "Epoch: 93, Hit Ratio:0.0342512983314068 | Precision:0.05053573183918215 | Recall:0.06654230327357293 | MDCG:0.06593679269218644\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 97 batch 0 batch_loss: 0.22974517941474915\n",
      "training: 97 batch 1 batch_loss: 0.22222179174423218\n",
      "training: 97 batch 2 batch_loss: 0.23219013214111328\n",
      "training: 97 batch 3 batch_loss: 0.22815600037574768\n",
      "training: 97 batch 4 batch_loss: 0.23203736543655396\n",
      "training: 97 batch 5 batch_loss: 0.2334458827972412\n",
      "training: 97 batch 6 batch_loss: 0.23307856917381287\n",
      "training: 97 batch 7 batch_loss: 0.23109221458435059\n",
      "training: 97 batch 8 batch_loss: 0.2330097258090973\n",
      "training: 97 batch 9 batch_loss: 0.22918379306793213\n",
      "training: 97 batch 10 batch_loss: 0.2294330596923828\n",
      "training: 97 batch 11 batch_loss: 0.22904491424560547\n",
      "training: 97 batch 12 batch_loss: 0.22801488637924194\n",
      "training: 97 batch 13 batch_loss: 0.22865495085716248\n",
      "training: 97 batch 14 batch_loss: 0.22994041442871094\n",
      "training: 97 batch 15 batch_loss: 0.2299974262714386\n",
      "training: 97 batch 16 batch_loss: 0.23189491033554077\n",
      "training: 97 batch 17 batch_loss: 0.23163342475891113\n",
      "training: 97 batch 18 batch_loss: 0.2309856414794922\n",
      "training: 97 batch 19 batch_loss: 0.22816535830497742\n",
      "training: 97 batch 20 batch_loss: 0.22851362824440002\n",
      "training: 97 batch 21 batch_loss: 0.23164477944374084\n",
      "training: 97 batch 22 batch_loss: 0.2328248918056488\n",
      "training: 97 batch 23 batch_loss: 0.23070481419563293\n",
      "training: 97 batch 24 batch_loss: 0.22758126258850098\n",
      "training: 97 batch 25 batch_loss: 0.23004713654518127\n",
      "training: 97 batch 26 batch_loss: 0.23194432258605957\n",
      "training: 97 batch 27 batch_loss: 0.23226606845855713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 97 batch 28 batch_loss: 0.2286180853843689\n",
      "training: 97 batch 29 batch_loss: 0.2317298948764801\n",
      "training: 97 batch 30 batch_loss: 0.23364228010177612\n",
      "training: 97 batch 31 batch_loss: 0.22954615950584412\n",
      "training: 97 batch 32 batch_loss: 0.22901484370231628\n",
      "training: 97 batch 33 batch_loss: 0.22855347394943237\n",
      "training: 97 batch 34 batch_loss: 0.2287781536579132\n",
      "training: 97 batch 35 batch_loss: 0.2299516201019287\n",
      "training: 97 batch 36 batch_loss: 0.2319151759147644\n",
      "training: 97 batch 37 batch_loss: 0.23133274912834167\n",
      "training: 97 batch 38 batch_loss: 0.2297399342060089\n",
      "training: 97 batch 39 batch_loss: 0.2301563322544098\n",
      "training: 97 batch 40 batch_loss: 0.23092135787010193\n",
      "training: 97 batch 41 batch_loss: 0.23051878809928894\n",
      "training: 97 batch 42 batch_loss: 0.23379069566726685\n",
      "training: 97 batch 43 batch_loss: 0.23401767015457153\n",
      "training: 97 batch 44 batch_loss: 0.23132014274597168\n",
      "training: 97 batch 45 batch_loss: 0.23099109530448914\n",
      "training: 97 batch 46 batch_loss: 0.23018839955329895\n",
      "training: 97 batch 47 batch_loss: 0.22589758038520813\n",
      "training: 97 batch 48 batch_loss: 0.23522955179214478\n",
      "training: 97 batch 49 batch_loss: 0.23267862200737\n",
      "training: 97 batch 50 batch_loss: 0.23160627484321594\n",
      "training: 97 batch 51 batch_loss: 0.23098152875900269\n",
      "training: 97 batch 52 batch_loss: 0.23161780834197998\n",
      "training: 97 batch 53 batch_loss: 0.23188525438308716\n",
      "training: 97 batch 54 batch_loss: 0.2315373420715332\n",
      "training: 97 batch 55 batch_loss: 0.2328869104385376\n",
      "training: 97 batch 56 batch_loss: 0.23227891325950623\n",
      "training: 97 batch 57 batch_loss: 0.2302723526954651\n",
      "training: 97 batch 58 batch_loss: 0.22675535082817078\n",
      "training: 97 batch 59 batch_loss: 0.22894591093063354\n",
      "training: 97 batch 60 batch_loss: 0.22907495498657227\n",
      "training: 97 batch 61 batch_loss: 0.23490077257156372\n",
      "training: 97 batch 62 batch_loss: 0.22697168588638306\n",
      "training: 97 batch 63 batch_loss: 0.23537200689315796\n",
      "training: 97 batch 64 batch_loss: 0.23070785403251648\n",
      "training: 97 batch 65 batch_loss: 0.23190364241600037\n",
      "training: 97 batch 66 batch_loss: 0.23260879516601562\n",
      "training: 97 batch 67 batch_loss: 0.23338186740875244\n",
      "training: 97 batch 68 batch_loss: 0.22964435815811157\n",
      "training: 97 batch 69 batch_loss: 0.23450636863708496\n",
      "training: 97 batch 70 batch_loss: 0.22997033596038818\n",
      "training: 97 batch 71 batch_loss: 0.23306554555892944\n",
      "training: 97 batch 72 batch_loss: 0.23059797286987305\n",
      "training: 97 batch 73 batch_loss: 0.22888794541358948\n",
      "training: 97 batch 74 batch_loss: 0.22880834341049194\n",
      "training: 97 batch 75 batch_loss: 0.23214784264564514\n",
      "training: 97 batch 76 batch_loss: 0.2290644347667694\n",
      "training: 97 batch 77 batch_loss: 0.23278477787971497\n",
      "training: 97 batch 78 batch_loss: 0.23037102818489075\n",
      "training: 97 batch 79 batch_loss: 0.2325705587863922\n",
      "training: 97 batch 80 batch_loss: 0.2287275791168213\n",
      "training: 97 batch 81 batch_loss: 0.23003602027893066\n",
      "training: 97 batch 82 batch_loss: 0.22732016444206238\n",
      "training: 97 batch 83 batch_loss: 0.23358896374702454\n",
      "training: 97 batch 84 batch_loss: 0.2328348457813263\n",
      "training: 97 batch 85 batch_loss: 0.23325282335281372\n",
      "training: 97 batch 86 batch_loss: 0.2325807511806488\n",
      "training: 97 batch 87 batch_loss: 0.23265719413757324\n",
      "training: 97 batch 88 batch_loss: 0.2322133183479309\n",
      "training: 97 batch 89 batch_loss: 0.23359894752502441\n",
      "training: 97 batch 90 batch_loss: 0.24065881967544556\n",
      "training: 97 batch 91 batch_loss: 0.23037350177764893\n",
      "training: 97 batch 92 batch_loss: 0.22982537746429443\n",
      "training: 97 batch 93 batch_loss: 0.23463881015777588\n",
      "training: 97 batch 94 batch_loss: 0.2288910448551178\n",
      "training: 97 batch 95 batch_loss: 0.22874316573143005\n",
      "training: 97 batch 96 batch_loss: 0.22999265789985657\n",
      "training: 97 batch 97 batch_loss: 0.22759073972702026\n",
      "training: 97 batch 98 batch_loss: 0.23387238383293152\n",
      "training: 97 batch 99 batch_loss: 0.2306547462940216\n",
      "training: 97 batch 100 batch_loss: 0.22925129532814026\n",
      "training: 97 batch 101 batch_loss: 0.22975745797157288\n",
      "training: 97 batch 102 batch_loss: 0.22969037294387817\n",
      "training: 97 batch 103 batch_loss: 0.2321872115135193\n",
      "training: 97 batch 104 batch_loss: 0.2349526286125183\n",
      "training: 97 batch 105 batch_loss: 0.23149234056472778\n",
      "training: 97 batch 106 batch_loss: 0.22857338190078735\n",
      "training: 97 batch 107 batch_loss: 0.23291939496994019\n",
      "training: 97 batch 108 batch_loss: 0.23461145162582397\n",
      "training: 97 batch 109 batch_loss: 0.2316131591796875\n",
      "training: 97 batch 110 batch_loss: 0.23255380988121033\n",
      "training: 97 batch 111 batch_loss: 0.23258084058761597\n",
      "training: 97 batch 112 batch_loss: 0.23258143663406372\n",
      "training: 97 batch 113 batch_loss: 0.22998309135437012\n",
      "training: 97 batch 114 batch_loss: 0.22961071133613586\n",
      "training: 97 batch 115 batch_loss: 0.2328318953514099\n",
      "training: 97 batch 116 batch_loss: 0.22896748781204224\n",
      "training: 97 batch 117 batch_loss: 0.22901135683059692\n",
      "training: 97 batch 118 batch_loss: 0.23148977756500244\n",
      "training: 97 batch 119 batch_loss: 0.23408028483390808\n",
      "training: 97 batch 120 batch_loss: 0.22971481084823608\n",
      "training: 97 batch 121 batch_loss: 0.2321619689464569\n",
      "training: 97 batch 122 batch_loss: 0.2260424792766571\n",
      "training: 97 batch 123 batch_loss: 0.2326015830039978\n",
      "training: 97 batch 124 batch_loss: 0.23523259162902832\n",
      "training: 97 batch 125 batch_loss: 0.2302948236465454\n",
      "training: 97 batch 126 batch_loss: 0.2354896366596222\n",
      "training: 97 batch 127 batch_loss: 0.23337948322296143\n",
      "training: 97 batch 128 batch_loss: 0.22923368215560913\n",
      "training: 97 batch 129 batch_loss: 0.23074737191200256\n",
      "training: 97 batch 130 batch_loss: 0.2312936782836914\n",
      "training: 97 batch 131 batch_loss: 0.23414677381515503\n",
      "training: 97 batch 132 batch_loss: 0.22914499044418335\n",
      "training: 97 batch 133 batch_loss: 0.23134464025497437\n",
      "training: 97 batch 134 batch_loss: 0.2300119698047638\n",
      "training: 97 batch 135 batch_loss: 0.23016300797462463\n",
      "training: 97 batch 136 batch_loss: 0.22796061635017395\n",
      "training: 97 batch 137 batch_loss: 0.22387513518333435\n",
      "training: 97 batch 138 batch_loss: 0.23066040873527527\n",
      "training: 97 batch 139 batch_loss: 0.2312282919883728\n",
      "training: 97 batch 140 batch_loss: 0.22973841428756714\n",
      "training: 97 batch 141 batch_loss: 0.2304512858390808\n",
      "training: 97 batch 142 batch_loss: 0.23091191053390503\n",
      "training: 97 batch 143 batch_loss: 0.22736206650733948\n",
      "training: 97 batch 144 batch_loss: 0.22998890280723572\n",
      "training: 97 batch 145 batch_loss: 0.22750145196914673\n",
      "training: 97 batch 146 batch_loss: 0.23396536707878113\n",
      "training: 97 batch 147 batch_loss: 0.22643116116523743\n",
      "training: 97 batch 148 batch_loss: 0.22991293668746948\n",
      "training: 97 batch 149 batch_loss: 0.2326602339744568\n",
      "training: 97 batch 150 batch_loss: 0.2314586043357849\n",
      "training: 97 batch 151 batch_loss: 0.22862166166305542\n",
      "training: 97 batch 152 batch_loss: 0.23570626974105835\n",
      "training: 97 batch 153 batch_loss: 0.2303733229637146\n",
      "training: 97 batch 154 batch_loss: 0.2342434525489807\n",
      "training: 97 batch 155 batch_loss: 0.22673135995864868\n",
      "training: 97 batch 156 batch_loss: 0.23325785994529724\n",
      "training: 97 batch 157 batch_loss: 0.23285868763923645\n",
      "training: 97 batch 158 batch_loss: 0.22930261492729187\n",
      "training: 97 batch 159 batch_loss: 0.22955888509750366\n",
      "training: 97 batch 160 batch_loss: 0.2300742268562317\n",
      "training: 97 batch 161 batch_loss: 0.22825554013252258\n",
      "training: 97 batch 162 batch_loss: 0.2322353720664978\n",
      "training: 97 batch 163 batch_loss: 0.22668641805648804\n",
      "training: 97 batch 164 batch_loss: 0.23071706295013428\n",
      "training: 97 batch 165 batch_loss: 0.23606997728347778\n",
      "training: 97 batch 166 batch_loss: 0.22462975978851318\n",
      "training: 97 batch 167 batch_loss: 0.23048332333564758\n",
      "training: 97 batch 168 batch_loss: 0.22964149713516235\n",
      "training: 97 batch 169 batch_loss: 0.2318004071712494\n",
      "training: 97 batch 170 batch_loss: 0.23135149478912354\n",
      "training: 97 batch 171 batch_loss: 0.23622113466262817\n",
      "training: 97 batch 172 batch_loss: 0.2315768003463745\n",
      "training: 97 batch 173 batch_loss: 0.23333707451820374\n",
      "training: 97 batch 174 batch_loss: 0.23468559980392456\n",
      "training: 97 batch 175 batch_loss: 0.22929251194000244\n",
      "training: 97 batch 176 batch_loss: 0.22988232970237732\n",
      "training: 97 batch 177 batch_loss: 0.23491644859313965\n",
      "training: 97 batch 178 batch_loss: 0.22903934121131897\n",
      "training: 97 batch 179 batch_loss: 0.2340826690196991\n",
      "training: 97 batch 180 batch_loss: 0.23035874962806702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 97 batch 181 batch_loss: 0.23209398984909058\n",
      "training: 97 batch 182 batch_loss: 0.22856789827346802\n",
      "training: 97 batch 183 batch_loss: 0.22880136966705322\n",
      "training: 97 batch 184 batch_loss: 0.22830480337142944\n",
      "training: 97 batch 185 batch_loss: 0.226656973361969\n",
      "training: 97 batch 186 batch_loss: 0.23236840963363647\n",
      "training: 97 batch 187 batch_loss: 0.23309561610221863\n",
      "training: 97 batch 188 batch_loss: 0.23109203577041626\n",
      "training: 97 batch 189 batch_loss: 0.23283159732818604\n",
      "training: 97 batch 190 batch_loss: 0.22847941517829895\n",
      "training: 97 batch 191 batch_loss: 0.22870111465454102\n",
      "training: 97 batch 192 batch_loss: 0.22616320848464966\n",
      "training: 97 batch 193 batch_loss: 0.23291218280792236\n",
      "training: 97 batch 194 batch_loss: 0.2355925440788269\n",
      "training: 97 batch 195 batch_loss: 0.23326298594474792\n",
      "training: 97 batch 196 batch_loss: 0.22852197289466858\n",
      "training: 97 batch 197 batch_loss: 0.2360459566116333\n",
      "training: 97 batch 198 batch_loss: 0.2324923276901245\n",
      "training: 97 batch 199 batch_loss: 0.23133385181427002\n",
      "training: 97 batch 200 batch_loss: 0.2331642508506775\n",
      "training: 97 batch 201 batch_loss: 0.22969302535057068\n",
      "training: 97 batch 202 batch_loss: 0.2263009250164032\n",
      "training: 97 batch 203 batch_loss: 0.23015353083610535\n",
      "training: 97 batch 204 batch_loss: 0.22883382439613342\n",
      "training: 97 batch 205 batch_loss: 0.23827162384986877\n",
      "training: 97 batch 206 batch_loss: 0.23491743206977844\n",
      "training: 97 batch 207 batch_loss: 0.23028329014778137\n",
      "training: 97 batch 208 batch_loss: 0.23108559846878052\n",
      "training: 97 batch 209 batch_loss: 0.23052039742469788\n",
      "training: 97 batch 210 batch_loss: 0.2328682243824005\n",
      "training: 97 batch 211 batch_loss: 0.23169466853141785\n",
      "training: 97 batch 212 batch_loss: 0.23302686214447021\n",
      "training: 97 batch 213 batch_loss: 0.23133981227874756\n",
      "training: 97 batch 214 batch_loss: 0.23036885261535645\n",
      "training: 97 batch 215 batch_loss: 0.23223966360092163\n",
      "training: 97 batch 216 batch_loss: 0.23398703336715698\n",
      "training: 97 batch 217 batch_loss: 0.23131150007247925\n",
      "training: 97 batch 218 batch_loss: 0.2281234860420227\n",
      "training: 97 batch 219 batch_loss: 0.2342197299003601\n",
      "training: 97 batch 220 batch_loss: 0.23457986116409302\n",
      "training: 97 batch 221 batch_loss: 0.2310504913330078\n",
      "training: 97 batch 222 batch_loss: 0.2299799621105194\n",
      "training: 97 batch 223 batch_loss: 0.23318558931350708\n",
      "training: 97 batch 224 batch_loss: 0.23596256971359253\n",
      "training: 97 batch 225 batch_loss: 0.23040926456451416\n",
      "training: 97 batch 226 batch_loss: 0.2297709584236145\n",
      "training: 97 batch 227 batch_loss: 0.23028221726417542\n",
      "training: 97 batch 228 batch_loss: 0.2317754030227661\n",
      "training: 97 batch 229 batch_loss: 0.2341686189174652\n",
      "training: 97 batch 230 batch_loss: 0.22936713695526123\n",
      "training: 97 batch 231 batch_loss: 0.22985807061195374\n",
      "training: 97 batch 232 batch_loss: 0.23144662380218506\n",
      "training: 97 batch 233 batch_loss: 0.2349100410938263\n",
      "training: 97 batch 234 batch_loss: 0.23094239830970764\n",
      "training: 97 batch 235 batch_loss: 0.23101073503494263\n",
      "training: 97 batch 236 batch_loss: 0.2291204333305359\n",
      "training: 97 batch 237 batch_loss: 0.23229175806045532\n",
      "training: 97 batch 238 batch_loss: 0.2348794937133789\n",
      "training: 97 batch 239 batch_loss: 0.2301693558692932\n",
      "training: 97 batch 240 batch_loss: 0.2305029332637787\n",
      "training: 97 batch 241 batch_loss: 0.23070451617240906\n",
      "training: 97 batch 242 batch_loss: 0.23152923583984375\n",
      "training: 97 batch 243 batch_loss: 0.2352515161037445\n",
      "training: 97 batch 244 batch_loss: 0.23190945386886597\n",
      "training: 97 batch 245 batch_loss: 0.23053643107414246\n",
      "training: 97 batch 246 batch_loss: 0.233530193567276\n",
      "training: 97 batch 247 batch_loss: 0.22857913374900818\n",
      "training: 97 batch 248 batch_loss: 0.2310413420200348\n",
      "training: 97 batch 249 batch_loss: 0.2289143204689026\n",
      "training: 97 batch 250 batch_loss: 0.2259119153022766\n",
      "training: 97 batch 251 batch_loss: 0.23603400588035583\n",
      "training: 97 batch 252 batch_loss: 0.23128429055213928\n",
      "training: 97 batch 253 batch_loss: 0.23213514685630798\n",
      "training: 97 batch 254 batch_loss: 0.2369103729724884\n",
      "training: 97 batch 255 batch_loss: 0.23463746905326843\n",
      "training: 97 batch 256 batch_loss: 0.23490822315216064\n",
      "training: 97 batch 257 batch_loss: 0.2331300973892212\n",
      "training: 97 batch 258 batch_loss: 0.22972151637077332\n",
      "training: 97 batch 259 batch_loss: 0.23573771119117737\n",
      "training: 97 batch 260 batch_loss: 0.2329673171043396\n",
      "training: 97 batch 261 batch_loss: 0.23611098527908325\n",
      "training: 97 batch 262 batch_loss: 0.23090916872024536\n",
      "training: 97 batch 263 batch_loss: 0.23116153478622437\n",
      "training: 97 batch 264 batch_loss: 0.2354121208190918\n",
      "training: 97 batch 265 batch_loss: 0.23391401767730713\n",
      "training: 97 batch 266 batch_loss: 0.23202452063560486\n",
      "training: 97 batch 267 batch_loss: 0.2334076166152954\n",
      "training: 97 batch 268 batch_loss: 0.22712063789367676\n",
      "training: 97 batch 269 batch_loss: 0.23203453421592712\n",
      "training: 97 batch 270 batch_loss: 0.22658288478851318\n",
      "training: 97 batch 271 batch_loss: 0.23074346780776978\n",
      "training: 97 batch 272 batch_loss: 0.23272183537483215\n",
      "training: 97 batch 273 batch_loss: 0.23308506608009338\n",
      "training: 97 batch 274 batch_loss: 0.23159170150756836\n",
      "training: 97 batch 275 batch_loss: 0.2348097860813141\n",
      "training: 97 batch 276 batch_loss: 0.23393160104751587\n",
      "training: 97 batch 277 batch_loss: 0.23054826259613037\n",
      "training: 97 batch 278 batch_loss: 0.23094260692596436\n",
      "training: 97 batch 279 batch_loss: 0.2359417974948883\n",
      "training: 97 batch 280 batch_loss: 0.23114651441574097\n",
      "training: 97 batch 281 batch_loss: 0.23067349195480347\n",
      "training: 97 batch 282 batch_loss: 0.23521724343299866\n",
      "training: 97 batch 283 batch_loss: 0.23222202062606812\n",
      "training: 97 batch 284 batch_loss: 0.23027002811431885\n",
      "training: 97 batch 285 batch_loss: 0.2361239790916443\n",
      "training: 97 batch 286 batch_loss: 0.23106813430786133\n",
      "training: 97 batch 287 batch_loss: 0.23209065198898315\n",
      "training: 97 batch 288 batch_loss: 0.23352304100990295\n",
      "training: 97 batch 289 batch_loss: 0.22818124294281006\n",
      "training: 97 batch 290 batch_loss: 0.23623326420783997\n",
      "training: 97 batch 291 batch_loss: 0.23469525575637817\n",
      "training: 97 batch 292 batch_loss: 0.23378479480743408\n",
      "training: 97 batch 293 batch_loss: 0.23241397738456726\n",
      "training: 97 batch 294 batch_loss: 0.23111480474472046\n",
      "training: 97 batch 295 batch_loss: 0.23139449954032898\n",
      "training: 97 batch 296 batch_loss: 0.2297358512878418\n",
      "training: 97 batch 297 batch_loss: 0.2324833869934082\n",
      "training: 97 batch 298 batch_loss: 0.23469257354736328\n",
      "training: 97 batch 299 batch_loss: 0.23177999258041382\n",
      "training: 97 batch 300 batch_loss: 0.23125946521759033\n",
      "training: 97 batch 301 batch_loss: 0.22936493158340454\n",
      "training: 97 batch 302 batch_loss: 0.22739502787590027\n",
      "training: 97 batch 303 batch_loss: 0.23299598693847656\n",
      "training: 97 batch 304 batch_loss: 0.22978609800338745\n",
      "training: 97 batch 305 batch_loss: 0.2290557324886322\n",
      "training: 97 batch 306 batch_loss: 0.232225239276886\n",
      "training: 97 batch 307 batch_loss: 0.23103532195091248\n",
      "training: 97 batch 308 batch_loss: 0.22745195031166077\n",
      "training: 97 batch 309 batch_loss: 0.23040854930877686\n",
      "training: 97 batch 310 batch_loss: 0.22452592849731445\n",
      "training: 97 batch 311 batch_loss: 0.22630900144577026\n",
      "training: 97 batch 312 batch_loss: 0.23032981157302856\n",
      "training: 97 batch 313 batch_loss: 0.23234641551971436\n",
      "training: 97 batch 314 batch_loss: 0.22724705934524536\n",
      "training: 97 batch 315 batch_loss: 0.2284611165523529\n",
      "training: 97 batch 316 batch_loss: 0.23369717597961426\n",
      "training: 97 batch 317 batch_loss: 0.23616009950637817\n",
      "training: 97 batch 318 batch_loss: 0.2284965217113495\n",
      "training: 97 batch 319 batch_loss: 0.22959694266319275\n",
      "training: 97 batch 320 batch_loss: 0.23478594422340393\n",
      "training: 97 batch 321 batch_loss: 0.23133128881454468\n",
      "training: 97 batch 322 batch_loss: 0.2322978973388672\n",
      "training: 97 batch 323 batch_loss: 0.23665755987167358\n",
      "training: 97 batch 324 batch_loss: 0.2304307520389557\n",
      "training: 97 batch 325 batch_loss: 0.23320981860160828\n",
      "training: 97 batch 326 batch_loss: 0.2307460904121399\n",
      "training: 97 batch 327 batch_loss: 0.22869595885276794\n",
      "training: 97 batch 328 batch_loss: 0.2320864200592041\n",
      "training: 97 batch 329 batch_loss: 0.22946485877037048\n",
      "training: 97 batch 330 batch_loss: 0.22804099321365356\n",
      "training: 97 batch 331 batch_loss: 0.23521721363067627\n",
      "training: 97 batch 332 batch_loss: 0.2264602780342102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 97 batch 333 batch_loss: 0.2357105016708374\n",
      "training: 97 batch 334 batch_loss: 0.23513710498809814\n",
      "training: 97 batch 335 batch_loss: 0.23136475682258606\n",
      "training: 97 batch 336 batch_loss: 0.22845494747161865\n",
      "training: 97 batch 337 batch_loss: 0.2326328456401825\n",
      "training: 97 batch 338 batch_loss: 0.2305673062801361\n",
      "training: 97 batch 339 batch_loss: 0.23203736543655396\n",
      "training: 97 batch 340 batch_loss: 0.2327195405960083\n",
      "training: 97 batch 341 batch_loss: 0.23661330342292786\n",
      "training: 97 batch 342 batch_loss: 0.23862981796264648\n",
      "training: 97 batch 343 batch_loss: 0.23390141129493713\n",
      "training: 97 batch 344 batch_loss: 0.2404625415802002\n",
      "training: 97 batch 345 batch_loss: 0.2339392900466919\n",
      "training: 97 batch 346 batch_loss: 0.23041296005249023\n",
      "training: 97 batch 347 batch_loss: 0.2368757724761963\n",
      "training: 97 batch 348 batch_loss: 0.22986367344856262\n",
      "training: 97 batch 349 batch_loss: 0.23323184251785278\n",
      "training: 97 batch 350 batch_loss: 0.23358303308486938\n",
      "training: 97 batch 351 batch_loss: 0.2343757450580597\n",
      "training: 97 batch 352 batch_loss: 0.23662498593330383\n",
      "training: 97 batch 353 batch_loss: 0.23240864276885986\n",
      "training: 97 batch 354 batch_loss: 0.23341044783592224\n",
      "training: 97 batch 355 batch_loss: 0.23546230792999268\n",
      "training: 97 batch 356 batch_loss: 0.23441347479820251\n",
      "training: 97 batch 357 batch_loss: 0.23099422454833984\n",
      "training: 97 batch 358 batch_loss: 0.23075252771377563\n",
      "training: 97 batch 359 batch_loss: 0.230360746383667\n",
      "training: 97 batch 360 batch_loss: 0.2281528115272522\n",
      "training: 97 batch 361 batch_loss: 0.23710572719573975\n",
      "training: 97 batch 362 batch_loss: 0.23299247026443481\n",
      "training: 97 batch 363 batch_loss: 0.2335186004638672\n",
      "training: 97 batch 364 batch_loss: 0.2373720407485962\n",
      "training: 97 batch 365 batch_loss: 0.23053810000419617\n",
      "training: 97 batch 366 batch_loss: 0.2308741807937622\n",
      "training: 97 batch 367 batch_loss: 0.23212605714797974\n",
      "training: 97 batch 368 batch_loss: 0.23185917735099792\n",
      "training: 97 batch 369 batch_loss: 0.2349468469619751\n",
      "training: 97 batch 370 batch_loss: 0.23197421431541443\n",
      "training: 97 batch 371 batch_loss: 0.22617366909980774\n",
      "training: 97 batch 372 batch_loss: 0.2321453094482422\n",
      "training: 97 batch 373 batch_loss: 0.22923415899276733\n",
      "training: 97 batch 374 batch_loss: 0.22934597730636597\n",
      "training: 97 batch 375 batch_loss: 0.23337304592132568\n",
      "training: 97 batch 376 batch_loss: 0.23416423797607422\n",
      "training: 97 batch 377 batch_loss: 0.22905564308166504\n",
      "training: 97 batch 378 batch_loss: 0.23451778292655945\n",
      "training: 97 batch 379 batch_loss: 0.2258177101612091\n",
      "training: 97 batch 380 batch_loss: 0.2296960949897766\n",
      "training: 97 batch 381 batch_loss: 0.22848713397979736\n",
      "training: 97 batch 382 batch_loss: 0.22761660814285278\n",
      "training: 97 batch 383 batch_loss: 0.23305213451385498\n",
      "training: 97 batch 384 batch_loss: 0.2329941987991333\n",
      "training: 97 batch 385 batch_loss: 0.23282089829444885\n",
      "training: 97 batch 386 batch_loss: 0.23058462142944336\n",
      "training: 97 batch 387 batch_loss: 0.22596126794815063\n",
      "training: 97 batch 388 batch_loss: 0.2324739396572113\n",
      "training: 97 batch 389 batch_loss: 0.23440852761268616\n",
      "training: 97 batch 390 batch_loss: 0.23431214690208435\n",
      "training: 97 batch 391 batch_loss: 0.2369489073753357\n",
      "training: 97 batch 392 batch_loss: 0.2311156988143921\n",
      "training: 97 batch 393 batch_loss: 0.23243170976638794\n",
      "training: 97 batch 394 batch_loss: 0.23197481036186218\n",
      "training: 97 batch 395 batch_loss: 0.2311369776725769\n",
      "training: 97 batch 396 batch_loss: 0.23311179876327515\n",
      "training: 97 batch 397 batch_loss: 0.22817203402519226\n",
      "training: 97 batch 398 batch_loss: 0.23000171780586243\n",
      "training: 97 batch 399 batch_loss: 0.23175513744354248\n",
      "training: 97 batch 400 batch_loss: 0.22844964265823364\n",
      "training: 97 batch 401 batch_loss: 0.23040878772735596\n",
      "training: 97 batch 402 batch_loss: 0.23431608080863953\n",
      "training: 97 batch 403 batch_loss: 0.2319498062133789\n",
      "training: 97 batch 404 batch_loss: 0.23586887121200562\n",
      "training: 97 batch 405 batch_loss: 0.2339688241481781\n",
      "training: 97 batch 406 batch_loss: 0.23396211862564087\n",
      "training: 97 batch 407 batch_loss: 0.23660680651664734\n",
      "training: 97 batch 408 batch_loss: 0.23825058341026306\n",
      "training: 97 batch 409 batch_loss: 0.2313593029975891\n",
      "training: 97 batch 410 batch_loss: 0.2273518443107605\n",
      "training: 97 batch 411 batch_loss: 0.2293379008769989\n",
      "training: 97 batch 412 batch_loss: 0.22673052549362183\n",
      "training: 97 batch 413 batch_loss: 0.22795262932777405\n",
      "training: 97 batch 414 batch_loss: 0.23297303915023804\n",
      "training: 97 batch 415 batch_loss: 0.23494482040405273\n",
      "training: 97 batch 416 batch_loss: 0.23546993732452393\n",
      "training: 97 batch 417 batch_loss: 0.2296467423439026\n",
      "training: 97 batch 418 batch_loss: 0.23476025462150574\n",
      "training: 97 batch 419 batch_loss: 0.23922693729400635\n",
      "training: 97 batch 420 batch_loss: 0.22787055373191833\n",
      "training: 97 batch 421 batch_loss: 0.23124349117279053\n",
      "training: 97 batch 422 batch_loss: 0.23056325316429138\n",
      "training: 97 batch 423 batch_loss: 0.2295016348361969\n",
      "training: 97 batch 424 batch_loss: 0.23192647099494934\n",
      "training: 97 batch 425 batch_loss: 0.2371838092803955\n",
      "training: 97 batch 426 batch_loss: 0.23590755462646484\n",
      "training: 97 batch 427 batch_loss: 0.2346167266368866\n",
      "training: 97 batch 428 batch_loss: 0.22942480444908142\n",
      "training: 97 batch 429 batch_loss: 0.23629698157310486\n",
      "training: 97 batch 430 batch_loss: 0.2301958203315735\n",
      "training: 97 batch 431 batch_loss: 0.23388928174972534\n",
      "training: 97 batch 432 batch_loss: 0.2319740653038025\n",
      "training: 97 batch 433 batch_loss: 0.23149076104164124\n",
      "training: 97 batch 434 batch_loss: 0.22992795705795288\n",
      "training: 97 batch 435 batch_loss: 0.23288819193840027\n",
      "training: 97 batch 436 batch_loss: 0.23240354657173157\n",
      "training: 97 batch 437 batch_loss: 0.2341463565826416\n",
      "training: 97 batch 438 batch_loss: 0.2273915410041809\n",
      "training: 97 batch 439 batch_loss: 0.2309228777885437\n",
      "training: 97 batch 440 batch_loss: 0.23048681020736694\n",
      "training: 97 batch 441 batch_loss: 0.23258230090141296\n",
      "training: 97 batch 442 batch_loss: 0.22944912314414978\n",
      "training: 97 batch 443 batch_loss: 0.2319345474243164\n",
      "training: 97 batch 444 batch_loss: 0.23037755489349365\n",
      "training: 97 batch 445 batch_loss: 0.23151695728302002\n",
      "training: 97 batch 446 batch_loss: 0.2334178388118744\n",
      "training: 97 batch 447 batch_loss: 0.23385953903198242\n",
      "training: 97 batch 448 batch_loss: 0.23271679878234863\n",
      "training: 97 batch 449 batch_loss: 0.23476797342300415\n",
      "training: 97 batch 450 batch_loss: 0.23873767256736755\n",
      "training: 97 batch 451 batch_loss: 0.23408710956573486\n",
      "training: 97 batch 452 batch_loss: 0.23313984274864197\n",
      "training: 97 batch 453 batch_loss: 0.2294403314590454\n",
      "training: 97 batch 454 batch_loss: 0.23025494813919067\n",
      "training: 97 batch 455 batch_loss: 0.22938045859336853\n",
      "training: 97 batch 456 batch_loss: 0.23492932319641113\n",
      "training: 97 batch 457 batch_loss: 0.2355792224407196\n",
      "training: 97 batch 458 batch_loss: 0.23148170113563538\n",
      "training: 97 batch 459 batch_loss: 0.23459643125534058\n",
      "training: 97 batch 460 batch_loss: 0.23299667239189148\n",
      "training: 97 batch 461 batch_loss: 0.23234370350837708\n",
      "training: 97 batch 462 batch_loss: 0.23098987340927124\n",
      "training: 97 batch 463 batch_loss: 0.2347450852394104\n",
      "training: 97 batch 464 batch_loss: 0.22943437099456787\n",
      "training: 97 batch 465 batch_loss: 0.23138824105262756\n",
      "training: 97 batch 466 batch_loss: 0.23355615139007568\n",
      "training: 97 batch 467 batch_loss: 0.23230254650115967\n",
      "training: 97 batch 468 batch_loss: 0.23516502976417542\n",
      "training: 97 batch 469 batch_loss: 0.2352985143661499\n",
      "training: 97 batch 470 batch_loss: 0.23597359657287598\n",
      "training: 97 batch 471 batch_loss: 0.23371711373329163\n",
      "training: 97 batch 472 batch_loss: 0.2371787130832672\n",
      "training: 97 batch 473 batch_loss: 0.23609107732772827\n",
      "training: 97 batch 474 batch_loss: 0.23468196392059326\n",
      "training: 97 batch 475 batch_loss: 0.233354389667511\n",
      "training: 97 batch 476 batch_loss: 0.23442167043685913\n",
      "training: 97 batch 477 batch_loss: 0.23304840922355652\n",
      "training: 97 batch 478 batch_loss: 0.23538365960121155\n",
      "training: 97 batch 479 batch_loss: 0.23360979557037354\n",
      "training: 97 batch 480 batch_loss: 0.23421034216880798\n",
      "training: 97 batch 481 batch_loss: 0.23584908246994019\n",
      "training: 97 batch 482 batch_loss: 0.23320388793945312\n",
      "training: 97 batch 483 batch_loss: 0.23168513178825378\n",
      "training: 97 batch 484 batch_loss: 0.23519501090049744\n",
      "training: 97 batch 485 batch_loss: 0.23299533128738403\n",
      "training: 97 batch 486 batch_loss: 0.23578596115112305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 97 batch 487 batch_loss: 0.23515629768371582\n",
      "training: 97 batch 488 batch_loss: 0.22893628478050232\n",
      "training: 97 batch 489 batch_loss: 0.23436415195465088\n",
      "training: 97 batch 490 batch_loss: 0.23099419474601746\n",
      "training: 97 batch 491 batch_loss: 0.23743391036987305\n",
      "training: 97 batch 492 batch_loss: 0.23191958665847778\n",
      "training: 97 batch 493 batch_loss: 0.23320680856704712\n",
      "training: 97 batch 494 batch_loss: 0.2387414276599884\n",
      "training: 97 batch 495 batch_loss: 0.22872507572174072\n",
      "training: 97 batch 496 batch_loss: 0.22991400957107544\n",
      "training: 97 batch 497 batch_loss: 0.2297174334526062\n",
      "training: 97 batch 498 batch_loss: 0.2307015359401703\n",
      "training: 97 batch 499 batch_loss: 0.23675477504730225\n",
      "training: 97 batch 500 batch_loss: 0.23421329259872437\n",
      "training: 97 batch 501 batch_loss: 0.2335321307182312\n",
      "training: 97 batch 502 batch_loss: 0.23613691329956055\n",
      "training: 97 batch 503 batch_loss: 0.22875332832336426\n",
      "training: 97 batch 504 batch_loss: 0.2336004674434662\n",
      "training: 97 batch 505 batch_loss: 0.23339977860450745\n",
      "training: 97 batch 506 batch_loss: 0.23310181498527527\n",
      "training: 97 batch 507 batch_loss: 0.23259788751602173\n",
      "training: 97 batch 508 batch_loss: 0.23228585720062256\n",
      "training: 97 batch 509 batch_loss: 0.22912892699241638\n",
      "training: 97 batch 510 batch_loss: 0.23487570881843567\n",
      "training: 97 batch 511 batch_loss: 0.23291027545928955\n",
      "training: 97 batch 512 batch_loss: 0.23455563187599182\n",
      "training: 97 batch 513 batch_loss: 0.23208457231521606\n",
      "training: 97 batch 514 batch_loss: 0.23268699645996094\n",
      "training: 97 batch 515 batch_loss: 0.23419886827468872\n",
      "training: 97 batch 516 batch_loss: 0.2306268811225891\n",
      "training: 97 batch 517 batch_loss: 0.2348995804786682\n",
      "training: 97 batch 518 batch_loss: 0.2338646650314331\n",
      "training: 97 batch 519 batch_loss: 0.23301467299461365\n",
      "training: 97 batch 520 batch_loss: 0.23205602169036865\n",
      "training: 97 batch 521 batch_loss: 0.23857733607292175\n",
      "training: 97 batch 522 batch_loss: 0.23115229606628418\n",
      "training: 97 batch 523 batch_loss: 0.23329481482505798\n",
      "training: 97 batch 524 batch_loss: 0.23233619332313538\n",
      "training: 97 batch 525 batch_loss: 0.23133477568626404\n",
      "training: 97 batch 526 batch_loss: 0.23624864220619202\n",
      "training: 97 batch 527 batch_loss: 0.23484206199645996\n",
      "training: 97 batch 528 batch_loss: 0.24167156219482422\n",
      "training: 97 batch 529 batch_loss: 0.23168879747390747\n",
      "training: 97 batch 530 batch_loss: 0.23417538404464722\n",
      "training: 97 batch 531 batch_loss: 0.23908618092536926\n",
      "training: 97 batch 532 batch_loss: 0.23123076558113098\n",
      "training: 97 batch 533 batch_loss: 0.23115593194961548\n",
      "training: 97 batch 534 batch_loss: 0.22961756587028503\n",
      "training: 97 batch 535 batch_loss: 0.2296377718448639\n",
      "training: 97 batch 536 batch_loss: 0.23052677512168884\n",
      "training: 97 batch 537 batch_loss: 0.22695043683052063\n",
      "training: 97 batch 538 batch_loss: 0.23523831367492676\n",
      "training: 97 batch 539 batch_loss: 0.23077479004859924\n",
      "training: 97 batch 540 batch_loss: 0.23737940192222595\n",
      "training: 97 batch 541 batch_loss: 0.23596185445785522\n",
      "training: 97 batch 542 batch_loss: 0.23430585861206055\n",
      "training: 97 batch 543 batch_loss: 0.2356467843055725\n",
      "training: 97 batch 544 batch_loss: 0.2290564775466919\n",
      "training: 97 batch 545 batch_loss: 0.2310299575328827\n",
      "training: 97 batch 546 batch_loss: 0.22684946656227112\n",
      "training: 97 batch 547 batch_loss: 0.23432761430740356\n",
      "training: 97 batch 548 batch_loss: 0.23202797770500183\n",
      "training: 97 batch 549 batch_loss: 0.2357003092765808\n",
      "training: 97 batch 550 batch_loss: 0.22986486554145813\n",
      "training: 97 batch 551 batch_loss: 0.23426848649978638\n",
      "training: 97 batch 552 batch_loss: 0.2332964837551117\n",
      "training: 97 batch 553 batch_loss: 0.22579887509346008\n",
      "training: 97 batch 554 batch_loss: 0.23184385895729065\n",
      "training: 97 batch 555 batch_loss: 0.23531237244606018\n",
      "training: 97 batch 556 batch_loss: 0.23437923192977905\n",
      "training: 97 batch 557 batch_loss: 0.23581653833389282\n",
      "training: 97 batch 558 batch_loss: 0.23084679245948792\n",
      "training: 97 batch 559 batch_loss: 0.23242753744125366\n",
      "training: 97 batch 560 batch_loss: 0.23441582918167114\n",
      "training: 97 batch 561 batch_loss: 0.23324334621429443\n",
      "training: 97 batch 562 batch_loss: 0.23079043626785278\n",
      "training: 97 batch 563 batch_loss: 0.23279699683189392\n",
      "training: 97 batch 564 batch_loss: 0.2314414381980896\n",
      "training: 97 batch 565 batch_loss: 0.23051291704177856\n",
      "training: 97 batch 566 batch_loss: 0.2349713146686554\n",
      "training: 97 batch 567 batch_loss: 0.23709842562675476\n",
      "training: 97 batch 568 batch_loss: 0.2303859293460846\n",
      "training: 97 batch 569 batch_loss: 0.23376509547233582\n",
      "training: 97 batch 570 batch_loss: 0.23043721914291382\n",
      "training: 97 batch 571 batch_loss: 0.23240509629249573\n",
      "training: 97 batch 572 batch_loss: 0.22920912504196167\n",
      "training: 97 batch 573 batch_loss: 0.23473691940307617\n",
      "training: 97 batch 574 batch_loss: 0.23496827483177185\n",
      "training: 97 batch 575 batch_loss: 0.22986844182014465\n",
      "training: 97 batch 576 batch_loss: 0.22898128628730774\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 97, Hit Ratio:0.03426795428274477 | Precision:0.0505603066941905 | Recall:0.06646838612031453 | NDCG:0.06605620545333978\n",
      "*Best Performance* \n",
      "Epoch: 97, Hit Ratio:0.03426795428274477 | Precision:0.0505603066941905 | Recall:0.06646838612031453 | MDCG:0.06605620545333978\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 98 batch 0 batch_loss: 0.22957760095596313\n",
      "training: 98 batch 1 batch_loss: 0.23309820890426636\n",
      "training: 98 batch 2 batch_loss: 0.23262664675712585\n",
      "training: 98 batch 3 batch_loss: 0.2314639389514923\n",
      "training: 98 batch 4 batch_loss: 0.22997766733169556\n",
      "training: 98 batch 5 batch_loss: 0.22961914539337158\n",
      "training: 98 batch 6 batch_loss: 0.2338143289089203\n",
      "training: 98 batch 7 batch_loss: 0.22955140471458435\n",
      "training: 98 batch 8 batch_loss: 0.2366553246974945\n",
      "training: 98 batch 9 batch_loss: 0.22781383991241455\n",
      "training: 98 batch 10 batch_loss: 0.23177790641784668\n",
      "training: 98 batch 11 batch_loss: 0.2279776930809021\n",
      "training: 98 batch 12 batch_loss: 0.22759175300598145\n",
      "training: 98 batch 13 batch_loss: 0.23602303862571716\n",
      "training: 98 batch 14 batch_loss: 0.23261696100234985\n",
      "training: 98 batch 15 batch_loss: 0.23083904385566711\n",
      "training: 98 batch 16 batch_loss: 0.23056751489639282\n",
      "training: 98 batch 17 batch_loss: 0.22863152623176575\n",
      "training: 98 batch 18 batch_loss: 0.2291121482849121\n",
      "training: 98 batch 19 batch_loss: 0.23474329710006714\n",
      "training: 98 batch 20 batch_loss: 0.23616600036621094\n",
      "training: 98 batch 21 batch_loss: 0.2319774627685547\n",
      "training: 98 batch 22 batch_loss: 0.22403571009635925\n",
      "training: 98 batch 23 batch_loss: 0.23164409399032593\n",
      "training: 98 batch 24 batch_loss: 0.22705945372581482\n",
      "training: 98 batch 25 batch_loss: 0.22846102714538574\n",
      "training: 98 batch 26 batch_loss: 0.23177385330200195\n",
      "training: 98 batch 27 batch_loss: 0.231251060962677\n",
      "training: 98 batch 28 batch_loss: 0.2284543514251709\n",
      "training: 98 batch 29 batch_loss: 0.23133257031440735\n",
      "training: 98 batch 30 batch_loss: 0.2332669198513031\n",
      "training: 98 batch 31 batch_loss: 0.23070028424263\n",
      "training: 98 batch 32 batch_loss: 0.230342835187912\n",
      "training: 98 batch 33 batch_loss: 0.2326977252960205\n",
      "training: 98 batch 34 batch_loss: 0.23124238848686218\n",
      "training: 98 batch 35 batch_loss: 0.22995957732200623\n",
      "training: 98 batch 36 batch_loss: 0.2353520691394806\n",
      "training: 98 batch 37 batch_loss: 0.22818654775619507\n",
      "training: 98 batch 38 batch_loss: 0.23062986135482788\n",
      "training: 98 batch 39 batch_loss: 0.23184621334075928\n",
      "training: 98 batch 40 batch_loss: 0.23069056868553162\n",
      "training: 98 batch 41 batch_loss: 0.23068088293075562\n",
      "training: 98 batch 42 batch_loss: 0.23166215419769287\n",
      "training: 98 batch 43 batch_loss: 0.23428255319595337\n",
      "training: 98 batch 44 batch_loss: 0.2269880175590515\n",
      "training: 98 batch 45 batch_loss: 0.23367470502853394\n",
      "training: 98 batch 46 batch_loss: 0.23475241661071777\n",
      "training: 98 batch 47 batch_loss: 0.23078468441963196\n",
      "training: 98 batch 48 batch_loss: 0.2329661250114441\n",
      "training: 98 batch 49 batch_loss: 0.23296156525611877\n",
      "training: 98 batch 50 batch_loss: 0.2314835786819458\n",
      "training: 98 batch 51 batch_loss: 0.23078256845474243\n",
      "training: 98 batch 52 batch_loss: 0.23460662364959717\n",
      "training: 98 batch 53 batch_loss: 0.231880784034729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 98 batch 54 batch_loss: 0.2310134470462799\n",
      "training: 98 batch 55 batch_loss: 0.23569518327713013\n",
      "training: 98 batch 56 batch_loss: 0.23387134075164795\n",
      "training: 98 batch 57 batch_loss: 0.22973006963729858\n",
      "training: 98 batch 58 batch_loss: 0.23063471913337708\n",
      "training: 98 batch 59 batch_loss: 0.23266828060150146\n",
      "training: 98 batch 60 batch_loss: 0.23039555549621582\n",
      "training: 98 batch 61 batch_loss: 0.2309950292110443\n",
      "training: 98 batch 62 batch_loss: 0.2303869128227234\n",
      "training: 98 batch 63 batch_loss: 0.22997644543647766\n",
      "training: 98 batch 64 batch_loss: 0.23358550667762756\n",
      "training: 98 batch 65 batch_loss: 0.2333149015903473\n",
      "training: 98 batch 66 batch_loss: 0.23460829257965088\n",
      "training: 98 batch 67 batch_loss: 0.23198407888412476\n",
      "training: 98 batch 68 batch_loss: 0.22964689135551453\n",
      "training: 98 batch 69 batch_loss: 0.2267015278339386\n",
      "training: 98 batch 70 batch_loss: 0.23289215564727783\n",
      "training: 98 batch 71 batch_loss: 0.23238009214401245\n",
      "training: 98 batch 72 batch_loss: 0.23271691799163818\n",
      "training: 98 batch 73 batch_loss: 0.23150485754013062\n",
      "training: 98 batch 74 batch_loss: 0.22887909412384033\n",
      "training: 98 batch 75 batch_loss: 0.23275870084762573\n",
      "training: 98 batch 76 batch_loss: 0.2332887053489685\n",
      "training: 98 batch 77 batch_loss: 0.23086774349212646\n",
      "training: 98 batch 78 batch_loss: 0.23358356952667236\n",
      "training: 98 batch 79 batch_loss: 0.23510146141052246\n",
      "training: 98 batch 80 batch_loss: 0.22787371277809143\n",
      "training: 98 batch 81 batch_loss: 0.23083436489105225\n",
      "training: 98 batch 82 batch_loss: 0.22960159182548523\n",
      "training: 98 batch 83 batch_loss: 0.23623394966125488\n",
      "training: 98 batch 84 batch_loss: 0.2305223047733307\n",
      "training: 98 batch 85 batch_loss: 0.2332700490951538\n",
      "training: 98 batch 86 batch_loss: 0.23181715607643127\n",
      "training: 98 batch 87 batch_loss: 0.2309434413909912\n",
      "training: 98 batch 88 batch_loss: 0.23450127243995667\n",
      "training: 98 batch 89 batch_loss: 0.2366999387741089\n",
      "training: 98 batch 90 batch_loss: 0.23153162002563477\n",
      "training: 98 batch 91 batch_loss: 0.23232519626617432\n",
      "training: 98 batch 92 batch_loss: 0.23238873481750488\n",
      "training: 98 batch 93 batch_loss: 0.23292237520217896\n",
      "training: 98 batch 94 batch_loss: 0.22888296842575073\n",
      "training: 98 batch 95 batch_loss: 0.23343193531036377\n",
      "training: 98 batch 96 batch_loss: 0.22978556156158447\n",
      "training: 98 batch 97 batch_loss: 0.2306385636329651\n",
      "training: 98 batch 98 batch_loss: 0.23285162448883057\n",
      "training: 98 batch 99 batch_loss: 0.23436954617500305\n",
      "training: 98 batch 100 batch_loss: 0.23286283016204834\n",
      "training: 98 batch 101 batch_loss: 0.2273748815059662\n",
      "training: 98 batch 102 batch_loss: 0.23114094138145447\n",
      "training: 98 batch 103 batch_loss: 0.22900280356407166\n",
      "training: 98 batch 104 batch_loss: 0.2344430685043335\n",
      "training: 98 batch 105 batch_loss: 0.22857904434204102\n",
      "training: 98 batch 106 batch_loss: 0.23201331496238708\n",
      "training: 98 batch 107 batch_loss: 0.23030632734298706\n",
      "training: 98 batch 108 batch_loss: 0.2291109263896942\n",
      "training: 98 batch 109 batch_loss: 0.22976765036582947\n",
      "training: 98 batch 110 batch_loss: 0.22957906126976013\n",
      "training: 98 batch 111 batch_loss: 0.22944632172584534\n",
      "training: 98 batch 112 batch_loss: 0.23228463530540466\n",
      "training: 98 batch 113 batch_loss: 0.2323618233203888\n",
      "training: 98 batch 114 batch_loss: 0.23256811499595642\n",
      "training: 98 batch 115 batch_loss: 0.22821566462516785\n",
      "training: 98 batch 116 batch_loss: 0.232735276222229\n",
      "training: 98 batch 117 batch_loss: 0.231051504611969\n",
      "training: 98 batch 118 batch_loss: 0.23191219568252563\n",
      "training: 98 batch 119 batch_loss: 0.2277541160583496\n",
      "training: 98 batch 120 batch_loss: 0.23237913846969604\n",
      "training: 98 batch 121 batch_loss: 0.22918587923049927\n",
      "training: 98 batch 122 batch_loss: 0.2303922176361084\n",
      "training: 98 batch 123 batch_loss: 0.2309439778327942\n",
      "training: 98 batch 124 batch_loss: 0.22782635688781738\n",
      "training: 98 batch 125 batch_loss: 0.2310420274734497\n",
      "training: 98 batch 126 batch_loss: 0.23366641998291016\n",
      "training: 98 batch 127 batch_loss: 0.22837162017822266\n",
      "training: 98 batch 128 batch_loss: 0.23051553964614868\n",
      "training: 98 batch 129 batch_loss: 0.23046526312828064\n",
      "training: 98 batch 130 batch_loss: 0.22426235675811768\n",
      "training: 98 batch 131 batch_loss: 0.23028481006622314\n",
      "training: 98 batch 132 batch_loss: 0.2301134467124939\n",
      "training: 98 batch 133 batch_loss: 0.23187890648841858\n",
      "training: 98 batch 134 batch_loss: 0.23123043775558472\n",
      "training: 98 batch 135 batch_loss: 0.22979047894477844\n",
      "training: 98 batch 136 batch_loss: 0.23127615451812744\n",
      "training: 98 batch 137 batch_loss: 0.23568090796470642\n",
      "training: 98 batch 138 batch_loss: 0.23871862888336182\n",
      "training: 98 batch 139 batch_loss: 0.23270612955093384\n",
      "training: 98 batch 140 batch_loss: 0.23204666376113892\n",
      "training: 98 batch 141 batch_loss: 0.23122242093086243\n",
      "training: 98 batch 142 batch_loss: 0.23169070482254028\n",
      "training: 98 batch 143 batch_loss: 0.23161494731903076\n",
      "training: 98 batch 144 batch_loss: 0.23128741979599\n",
      "training: 98 batch 145 batch_loss: 0.23754894733428955\n",
      "training: 98 batch 146 batch_loss: 0.23331958055496216\n",
      "training: 98 batch 147 batch_loss: 0.23451495170593262\n",
      "training: 98 batch 148 batch_loss: 0.23250141739845276\n",
      "training: 98 batch 149 batch_loss: 0.23085397481918335\n",
      "training: 98 batch 150 batch_loss: 0.23432669043540955\n",
      "training: 98 batch 151 batch_loss: 0.230782151222229\n",
      "training: 98 batch 152 batch_loss: 0.2345784306526184\n",
      "training: 98 batch 153 batch_loss: 0.23712927103042603\n",
      "training: 98 batch 154 batch_loss: 0.23747113347053528\n",
      "training: 98 batch 155 batch_loss: 0.23412659764289856\n",
      "training: 98 batch 156 batch_loss: 0.23005160689353943\n",
      "training: 98 batch 157 batch_loss: 0.2335742712020874\n",
      "training: 98 batch 158 batch_loss: 0.22937238216400146\n",
      "training: 98 batch 159 batch_loss: 0.23061954975128174\n",
      "training: 98 batch 160 batch_loss: 0.232087641954422\n",
      "training: 98 batch 161 batch_loss: 0.23013034462928772\n",
      "training: 98 batch 162 batch_loss: 0.2306826114654541\n",
      "training: 98 batch 163 batch_loss: 0.23464006185531616\n",
      "training: 98 batch 164 batch_loss: 0.2308012843132019\n",
      "training: 98 batch 165 batch_loss: 0.2298356294631958\n",
      "training: 98 batch 166 batch_loss: 0.2289314568042755\n",
      "training: 98 batch 167 batch_loss: 0.22945651412010193\n",
      "training: 98 batch 168 batch_loss: 0.2277769148349762\n",
      "training: 98 batch 169 batch_loss: 0.23562222719192505\n",
      "training: 98 batch 170 batch_loss: 0.22899013757705688\n",
      "training: 98 batch 171 batch_loss: 0.23009580373764038\n",
      "training: 98 batch 172 batch_loss: 0.23408719897270203\n",
      "training: 98 batch 173 batch_loss: 0.23691275715827942\n",
      "training: 98 batch 174 batch_loss: 0.23429599404335022\n",
      "training: 98 batch 175 batch_loss: 0.23123767971992493\n",
      "training: 98 batch 176 batch_loss: 0.23352259397506714\n",
      "training: 98 batch 177 batch_loss: 0.22691988945007324\n",
      "training: 98 batch 178 batch_loss: 0.23163965344429016\n",
      "training: 98 batch 179 batch_loss: 0.23151302337646484\n",
      "training: 98 batch 180 batch_loss: 0.2386012077331543\n",
      "training: 98 batch 181 batch_loss: 0.2281993329524994\n",
      "training: 98 batch 182 batch_loss: 0.23715054988861084\n",
      "training: 98 batch 183 batch_loss: 0.2321682572364807\n",
      "training: 98 batch 184 batch_loss: 0.23410946130752563\n",
      "training: 98 batch 185 batch_loss: 0.22975167632102966\n",
      "training: 98 batch 186 batch_loss: 0.23031720519065857\n",
      "training: 98 batch 187 batch_loss: 0.23003607988357544\n",
      "training: 98 batch 188 batch_loss: 0.23203828930854797\n",
      "training: 98 batch 189 batch_loss: 0.2264096736907959\n",
      "training: 98 batch 190 batch_loss: 0.2343410849571228\n",
      "training: 98 batch 191 batch_loss: 0.2312765121459961\n",
      "training: 98 batch 192 batch_loss: 0.23321974277496338\n",
      "training: 98 batch 193 batch_loss: 0.23426800966262817\n",
      "training: 98 batch 194 batch_loss: 0.23248809576034546\n",
      "training: 98 batch 195 batch_loss: 0.22995758056640625\n",
      "training: 98 batch 196 batch_loss: 0.23547649383544922\n",
      "training: 98 batch 197 batch_loss: 0.2289133071899414\n",
      "training: 98 batch 198 batch_loss: 0.2313207983970642\n",
      "training: 98 batch 199 batch_loss: 0.22974878549575806\n",
      "training: 98 batch 200 batch_loss: 0.23712879419326782\n",
      "training: 98 batch 201 batch_loss: 0.23224037885665894\n",
      "training: 98 batch 202 batch_loss: 0.22741883993148804\n",
      "training: 98 batch 203 batch_loss: 0.2373126745223999\n",
      "training: 98 batch 204 batch_loss: 0.23201200366020203\n",
      "training: 98 batch 205 batch_loss: 0.23077210783958435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 98 batch 206 batch_loss: 0.23144039511680603\n",
      "training: 98 batch 207 batch_loss: 0.23346394300460815\n",
      "training: 98 batch 208 batch_loss: 0.22921264171600342\n",
      "training: 98 batch 209 batch_loss: 0.23388522863388062\n",
      "training: 98 batch 210 batch_loss: 0.23158198595046997\n",
      "training: 98 batch 211 batch_loss: 0.23164045810699463\n",
      "training: 98 batch 212 batch_loss: 0.22731062769889832\n",
      "training: 98 batch 213 batch_loss: 0.2299928367137909\n",
      "training: 98 batch 214 batch_loss: 0.23479166626930237\n",
      "training: 98 batch 215 batch_loss: 0.23518076539039612\n",
      "training: 98 batch 216 batch_loss: 0.23217669129371643\n",
      "training: 98 batch 217 batch_loss: 0.23426899313926697\n",
      "training: 98 batch 218 batch_loss: 0.23691293597221375\n",
      "training: 98 batch 219 batch_loss: 0.23161357641220093\n",
      "training: 98 batch 220 batch_loss: 0.23245736956596375\n",
      "training: 98 batch 221 batch_loss: 0.2330823838710785\n",
      "training: 98 batch 222 batch_loss: 0.22931861877441406\n",
      "training: 98 batch 223 batch_loss: 0.22939598560333252\n",
      "training: 98 batch 224 batch_loss: 0.23640358448028564\n",
      "training: 98 batch 225 batch_loss: 0.23111215233802795\n",
      "training: 98 batch 226 batch_loss: 0.2348860204219818\n",
      "training: 98 batch 227 batch_loss: 0.23286563158035278\n",
      "training: 98 batch 228 batch_loss: 0.23136672377586365\n",
      "training: 98 batch 229 batch_loss: 0.23430320620536804\n",
      "training: 98 batch 230 batch_loss: 0.23270592093467712\n",
      "training: 98 batch 231 batch_loss: 0.23161917924880981\n",
      "training: 98 batch 232 batch_loss: 0.23201560974121094\n",
      "training: 98 batch 233 batch_loss: 0.23387488722801208\n",
      "training: 98 batch 234 batch_loss: 0.2299562692642212\n",
      "training: 98 batch 235 batch_loss: 0.23110124468803406\n",
      "training: 98 batch 236 batch_loss: 0.22965127229690552\n",
      "training: 98 batch 237 batch_loss: 0.23553389310836792\n",
      "training: 98 batch 238 batch_loss: 0.2312242090702057\n",
      "training: 98 batch 239 batch_loss: 0.2331908643245697\n",
      "training: 98 batch 240 batch_loss: 0.23587870597839355\n",
      "training: 98 batch 241 batch_loss: 0.23046621680259705\n",
      "training: 98 batch 242 batch_loss: 0.2344263792037964\n",
      "training: 98 batch 243 batch_loss: 0.23049700260162354\n",
      "training: 98 batch 244 batch_loss: 0.23340639472007751\n",
      "training: 98 batch 245 batch_loss: 0.231867253780365\n",
      "training: 98 batch 246 batch_loss: 0.22862344980239868\n",
      "training: 98 batch 247 batch_loss: 0.23124507069587708\n",
      "training: 98 batch 248 batch_loss: 0.235945463180542\n",
      "training: 98 batch 249 batch_loss: 0.23474544286727905\n",
      "training: 98 batch 250 batch_loss: 0.2298775315284729\n",
      "training: 98 batch 251 batch_loss: 0.23276963829994202\n",
      "training: 98 batch 252 batch_loss: 0.2325221300125122\n",
      "training: 98 batch 253 batch_loss: 0.2275974154472351\n",
      "training: 98 batch 254 batch_loss: 0.23313963413238525\n",
      "training: 98 batch 255 batch_loss: 0.23705455660820007\n",
      "training: 98 batch 256 batch_loss: 0.2356441617012024\n",
      "training: 98 batch 257 batch_loss: 0.23331129550933838\n",
      "training: 98 batch 258 batch_loss: 0.2326313853263855\n",
      "training: 98 batch 259 batch_loss: 0.22616925835609436\n",
      "training: 98 batch 260 batch_loss: 0.23177653551101685\n",
      "training: 98 batch 261 batch_loss: 0.23325157165527344\n",
      "training: 98 batch 262 batch_loss: 0.23073363304138184\n",
      "training: 98 batch 263 batch_loss: 0.23012423515319824\n",
      "training: 98 batch 264 batch_loss: 0.23132860660552979\n",
      "training: 98 batch 265 batch_loss: 0.2306571900844574\n",
      "training: 98 batch 266 batch_loss: 0.23329868912696838\n",
      "training: 98 batch 267 batch_loss: 0.22786623239517212\n",
      "training: 98 batch 268 batch_loss: 0.22888720035552979\n",
      "training: 98 batch 269 batch_loss: 0.23256030678749084\n",
      "training: 98 batch 270 batch_loss: 0.2292061448097229\n",
      "training: 98 batch 271 batch_loss: 0.22615301609039307\n",
      "training: 98 batch 272 batch_loss: 0.2366003394126892\n",
      "training: 98 batch 273 batch_loss: 0.233308345079422\n",
      "training: 98 batch 274 batch_loss: 0.234857976436615\n",
      "training: 98 batch 275 batch_loss: 0.23108765482902527\n",
      "training: 98 batch 276 batch_loss: 0.23091557621955872\n",
      "training: 98 batch 277 batch_loss: 0.23641738295555115\n",
      "training: 98 batch 278 batch_loss: 0.2350132167339325\n",
      "training: 98 batch 279 batch_loss: 0.23227345943450928\n",
      "training: 98 batch 280 batch_loss: 0.23454385995864868\n",
      "training: 98 batch 281 batch_loss: 0.2376135289669037\n",
      "training: 98 batch 282 batch_loss: 0.23368418216705322\n",
      "training: 98 batch 283 batch_loss: 0.2315797209739685\n",
      "training: 98 batch 284 batch_loss: 0.23019078373908997\n",
      "training: 98 batch 285 batch_loss: 0.23659047484397888\n",
      "training: 98 batch 286 batch_loss: 0.2336454689502716\n",
      "training: 98 batch 287 batch_loss: 0.23591607809066772\n",
      "training: 98 batch 288 batch_loss: 0.23094701766967773\n",
      "training: 98 batch 289 batch_loss: 0.23137891292572021\n",
      "training: 98 batch 290 batch_loss: 0.23857632279396057\n",
      "training: 98 batch 291 batch_loss: 0.23334574699401855\n",
      "training: 98 batch 292 batch_loss: 0.23006293177604675\n",
      "training: 98 batch 293 batch_loss: 0.23405826091766357\n",
      "training: 98 batch 294 batch_loss: 0.2352922558784485\n",
      "training: 98 batch 295 batch_loss: 0.23399734497070312\n",
      "training: 98 batch 296 batch_loss: 0.2354193925857544\n",
      "training: 98 batch 297 batch_loss: 0.236504465341568\n",
      "training: 98 batch 298 batch_loss: 0.2304406762123108\n",
      "training: 98 batch 299 batch_loss: 0.23322272300720215\n",
      "training: 98 batch 300 batch_loss: 0.2352571189403534\n",
      "training: 98 batch 301 batch_loss: 0.23439159989356995\n",
      "training: 98 batch 302 batch_loss: 0.2302563190460205\n",
      "training: 98 batch 303 batch_loss: 0.23826903104782104\n",
      "training: 98 batch 304 batch_loss: 0.23632383346557617\n",
      "training: 98 batch 305 batch_loss: 0.2346084713935852\n",
      "training: 98 batch 306 batch_loss: 0.22890910506248474\n",
      "training: 98 batch 307 batch_loss: 0.23525682091712952\n",
      "training: 98 batch 308 batch_loss: 0.2305137813091278\n",
      "training: 98 batch 309 batch_loss: 0.23598232865333557\n",
      "training: 98 batch 310 batch_loss: 0.2319263517856598\n",
      "training: 98 batch 311 batch_loss: 0.23273414373397827\n",
      "training: 98 batch 312 batch_loss: 0.23346033692359924\n",
      "training: 98 batch 313 batch_loss: 0.23021617531776428\n",
      "training: 98 batch 314 batch_loss: 0.22767281532287598\n",
      "training: 98 batch 315 batch_loss: 0.23678570985794067\n",
      "training: 98 batch 316 batch_loss: 0.22933241724967957\n",
      "training: 98 batch 317 batch_loss: 0.23064199090003967\n",
      "training: 98 batch 318 batch_loss: 0.23225927352905273\n",
      "training: 98 batch 319 batch_loss: 0.23128634691238403\n",
      "training: 98 batch 320 batch_loss: 0.23596248030662537\n",
      "training: 98 batch 321 batch_loss: 0.2316487431526184\n",
      "training: 98 batch 322 batch_loss: 0.23056095838546753\n",
      "training: 98 batch 323 batch_loss: 0.22914233803749084\n",
      "training: 98 batch 324 batch_loss: 0.2319508194923401\n",
      "training: 98 batch 325 batch_loss: 0.233440101146698\n",
      "training: 98 batch 326 batch_loss: 0.2326374053955078\n",
      "training: 98 batch 327 batch_loss: 0.2301313877105713\n",
      "training: 98 batch 328 batch_loss: 0.22898253798484802\n",
      "training: 98 batch 329 batch_loss: 0.23790723085403442\n",
      "training: 98 batch 330 batch_loss: 0.23389366269111633\n",
      "training: 98 batch 331 batch_loss: 0.23350226879119873\n",
      "training: 98 batch 332 batch_loss: 0.23426544666290283\n",
      "training: 98 batch 333 batch_loss: 0.23073312640190125\n",
      "training: 98 batch 334 batch_loss: 0.24026694893836975\n",
      "training: 98 batch 335 batch_loss: 0.23060745000839233\n",
      "training: 98 batch 336 batch_loss: 0.23355528712272644\n",
      "training: 98 batch 337 batch_loss: 0.23513349890708923\n",
      "training: 98 batch 338 batch_loss: 0.23311108350753784\n",
      "training: 98 batch 339 batch_loss: 0.23640084266662598\n",
      "training: 98 batch 340 batch_loss: 0.23189258575439453\n",
      "training: 98 batch 341 batch_loss: 0.23396340012550354\n",
      "training: 98 batch 342 batch_loss: 0.22692209482192993\n",
      "training: 98 batch 343 batch_loss: 0.2339315414428711\n",
      "training: 98 batch 344 batch_loss: 0.23262131214141846\n",
      "training: 98 batch 345 batch_loss: 0.23139524459838867\n",
      "training: 98 batch 346 batch_loss: 0.23439934849739075\n",
      "training: 98 batch 347 batch_loss: 0.2310461401939392\n",
      "training: 98 batch 348 batch_loss: 0.23476645350456238\n",
      "training: 98 batch 349 batch_loss: 0.2349783480167389\n",
      "training: 98 batch 350 batch_loss: 0.23516741394996643\n",
      "training: 98 batch 351 batch_loss: 0.23010671138763428\n",
      "training: 98 batch 352 batch_loss: 0.23447144031524658\n",
      "training: 98 batch 353 batch_loss: 0.2324654459953308\n",
      "training: 98 batch 354 batch_loss: 0.22861629724502563\n",
      "training: 98 batch 355 batch_loss: 0.2353023886680603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 98 batch 356 batch_loss: 0.23195451498031616\n",
      "training: 98 batch 357 batch_loss: 0.23520159721374512\n",
      "training: 98 batch 358 batch_loss: 0.233139306306839\n",
      "training: 98 batch 359 batch_loss: 0.22963222861289978\n",
      "training: 98 batch 360 batch_loss: 0.23169288039207458\n",
      "training: 98 batch 361 batch_loss: 0.23531430959701538\n",
      "training: 98 batch 362 batch_loss: 0.2296907603740692\n",
      "training: 98 batch 363 batch_loss: 0.2335166335105896\n",
      "training: 98 batch 364 batch_loss: 0.23416820168495178\n",
      "training: 98 batch 365 batch_loss: 0.23126015067100525\n",
      "training: 98 batch 366 batch_loss: 0.2339763045310974\n",
      "training: 98 batch 367 batch_loss: 0.2351691722869873\n",
      "training: 98 batch 368 batch_loss: 0.23127996921539307\n",
      "training: 98 batch 369 batch_loss: 0.23504772782325745\n",
      "training: 98 batch 370 batch_loss: 0.23408952355384827\n",
      "training: 98 batch 371 batch_loss: 0.23171788454055786\n",
      "training: 98 batch 372 batch_loss: 0.23441177606582642\n",
      "training: 98 batch 373 batch_loss: 0.23044586181640625\n",
      "training: 98 batch 374 batch_loss: 0.23415258526802063\n",
      "training: 98 batch 375 batch_loss: 0.23241788148880005\n",
      "training: 98 batch 376 batch_loss: 0.23260894417762756\n",
      "training: 98 batch 377 batch_loss: 0.23800641298294067\n",
      "training: 98 batch 378 batch_loss: 0.2340734601020813\n",
      "training: 98 batch 379 batch_loss: 0.23199725151062012\n",
      "training: 98 batch 380 batch_loss: 0.23332178592681885\n",
      "training: 98 batch 381 batch_loss: 0.23151516914367676\n",
      "training: 98 batch 382 batch_loss: 0.2284846305847168\n",
      "training: 98 batch 383 batch_loss: 0.23167437314987183\n",
      "training: 98 batch 384 batch_loss: 0.2331961691379547\n",
      "training: 98 batch 385 batch_loss: 0.23452436923980713\n",
      "training: 98 batch 386 batch_loss: 0.22938159108161926\n",
      "training: 98 batch 387 batch_loss: 0.23596680164337158\n",
      "training: 98 batch 388 batch_loss: 0.23409488797187805\n",
      "training: 98 batch 389 batch_loss: 0.23575317859649658\n",
      "training: 98 batch 390 batch_loss: 0.23240214586257935\n",
      "training: 98 batch 391 batch_loss: 0.23346441984176636\n",
      "training: 98 batch 392 batch_loss: 0.23308229446411133\n",
      "training: 98 batch 393 batch_loss: 0.23322102427482605\n",
      "training: 98 batch 394 batch_loss: 0.23169517517089844\n",
      "training: 98 batch 395 batch_loss: 0.23197713494300842\n",
      "training: 98 batch 396 batch_loss: 0.2279341220855713\n",
      "training: 98 batch 397 batch_loss: 0.2310916781425476\n",
      "training: 98 batch 398 batch_loss: 0.23103764653205872\n",
      "training: 98 batch 399 batch_loss: 0.22801288962364197\n",
      "training: 98 batch 400 batch_loss: 0.2292853593826294\n",
      "training: 98 batch 401 batch_loss: 0.2303946614265442\n",
      "training: 98 batch 402 batch_loss: 0.23238614201545715\n",
      "training: 98 batch 403 batch_loss: 0.22880041599273682\n",
      "training: 98 batch 404 batch_loss: 0.2350757122039795\n",
      "training: 98 batch 405 batch_loss: 0.23219099640846252\n",
      "training: 98 batch 406 batch_loss: 0.23136413097381592\n",
      "training: 98 batch 407 batch_loss: 0.23058801889419556\n",
      "training: 98 batch 408 batch_loss: 0.2330433428287506\n",
      "training: 98 batch 409 batch_loss: 0.2311401665210724\n",
      "training: 98 batch 410 batch_loss: 0.23041486740112305\n",
      "training: 98 batch 411 batch_loss: 0.23155555129051208\n",
      "training: 98 batch 412 batch_loss: 0.23365503549575806\n",
      "training: 98 batch 413 batch_loss: 0.2334725260734558\n",
      "training: 98 batch 414 batch_loss: 0.2368064820766449\n",
      "training: 98 batch 415 batch_loss: 0.23261043429374695\n",
      "training: 98 batch 416 batch_loss: 0.23785358667373657\n",
      "training: 98 batch 417 batch_loss: 0.23509055376052856\n",
      "training: 98 batch 418 batch_loss: 0.23234865069389343\n",
      "training: 98 batch 419 batch_loss: 0.2334015965461731\n",
      "training: 98 batch 420 batch_loss: 0.23305931687355042\n",
      "training: 98 batch 421 batch_loss: 0.23452109098434448\n",
      "training: 98 batch 422 batch_loss: 0.2277018427848816\n",
      "training: 98 batch 423 batch_loss: 0.23133862018585205\n",
      "training: 98 batch 424 batch_loss: 0.23421749472618103\n",
      "training: 98 batch 425 batch_loss: 0.23362520337104797\n",
      "training: 98 batch 426 batch_loss: 0.23137494921684265\n",
      "training: 98 batch 427 batch_loss: 0.23313391208648682\n",
      "training: 98 batch 428 batch_loss: 0.2326067090034485\n",
      "training: 98 batch 429 batch_loss: 0.2321608066558838\n",
      "training: 98 batch 430 batch_loss: 0.2294258177280426\n",
      "training: 98 batch 431 batch_loss: 0.23868584632873535\n",
      "training: 98 batch 432 batch_loss: 0.23356527090072632\n",
      "training: 98 batch 433 batch_loss: 0.23334869742393494\n",
      "training: 98 batch 434 batch_loss: 0.23217910528182983\n",
      "training: 98 batch 435 batch_loss: 0.22920408844947815\n",
      "training: 98 batch 436 batch_loss: 0.23698335886001587\n",
      "training: 98 batch 437 batch_loss: 0.23077723383903503\n",
      "training: 98 batch 438 batch_loss: 0.2314029335975647\n",
      "training: 98 batch 439 batch_loss: 0.23127001523971558\n",
      "training: 98 batch 440 batch_loss: 0.23663610219955444\n",
      "training: 98 batch 441 batch_loss: 0.23383301496505737\n",
      "training: 98 batch 442 batch_loss: 0.23245558142662048\n",
      "training: 98 batch 443 batch_loss: 0.23131731152534485\n",
      "training: 98 batch 444 batch_loss: 0.23283010721206665\n",
      "training: 98 batch 445 batch_loss: 0.23181653022766113\n",
      "training: 98 batch 446 batch_loss: 0.23700550198554993\n",
      "training: 98 batch 447 batch_loss: 0.2324027121067047\n",
      "training: 98 batch 448 batch_loss: 0.23063230514526367\n",
      "training: 98 batch 449 batch_loss: 0.2335708737373352\n",
      "training: 98 batch 450 batch_loss: 0.23611286282539368\n",
      "training: 98 batch 451 batch_loss: 0.2297632396221161\n",
      "training: 98 batch 452 batch_loss: 0.22926798462867737\n",
      "training: 98 batch 453 batch_loss: 0.2308650016784668\n",
      "training: 98 batch 454 batch_loss: 0.22894027829170227\n",
      "training: 98 batch 455 batch_loss: 0.2313801348209381\n",
      "training: 98 batch 456 batch_loss: 0.22840750217437744\n",
      "training: 98 batch 457 batch_loss: 0.2339903712272644\n",
      "training: 98 batch 458 batch_loss: 0.235524982213974\n",
      "training: 98 batch 459 batch_loss: 0.23346376419067383\n",
      "training: 98 batch 460 batch_loss: 0.23288509249687195\n",
      "training: 98 batch 461 batch_loss: 0.23458877205848694\n",
      "training: 98 batch 462 batch_loss: 0.2324034571647644\n",
      "training: 98 batch 463 batch_loss: 0.23181596398353577\n",
      "training: 98 batch 464 batch_loss: 0.22957858443260193\n",
      "training: 98 batch 465 batch_loss: 0.23038840293884277\n",
      "training: 98 batch 466 batch_loss: 0.2340434193611145\n",
      "training: 98 batch 467 batch_loss: 0.23172858357429504\n",
      "training: 98 batch 468 batch_loss: 0.23846960067749023\n",
      "training: 98 batch 469 batch_loss: 0.2317942976951599\n",
      "training: 98 batch 470 batch_loss: 0.225793719291687\n",
      "training: 98 batch 471 batch_loss: 0.23463740944862366\n",
      "training: 98 batch 472 batch_loss: 0.2347753643989563\n",
      "training: 98 batch 473 batch_loss: 0.2346874475479126\n",
      "training: 98 batch 474 batch_loss: 0.23383882641792297\n",
      "training: 98 batch 475 batch_loss: 0.23487746715545654\n",
      "training: 98 batch 476 batch_loss: 0.23641684651374817\n",
      "training: 98 batch 477 batch_loss: 0.23121574521064758\n",
      "training: 98 batch 478 batch_loss: 0.23249128460884094\n",
      "training: 98 batch 479 batch_loss: 0.2305470108985901\n",
      "training: 98 batch 480 batch_loss: 0.23373636603355408\n",
      "training: 98 batch 481 batch_loss: 0.23285621404647827\n",
      "training: 98 batch 482 batch_loss: 0.23646920919418335\n",
      "training: 98 batch 483 batch_loss: 0.2313138246536255\n",
      "training: 98 batch 484 batch_loss: 0.23336073756217957\n",
      "training: 98 batch 485 batch_loss: 0.2330697476863861\n",
      "training: 98 batch 486 batch_loss: 0.23426544666290283\n",
      "training: 98 batch 487 batch_loss: 0.2325657308101654\n",
      "training: 98 batch 488 batch_loss: 0.23410695791244507\n",
      "training: 98 batch 489 batch_loss: 0.2359752357006073\n",
      "training: 98 batch 490 batch_loss: 0.23463228344917297\n",
      "training: 98 batch 491 batch_loss: 0.23321115970611572\n",
      "training: 98 batch 492 batch_loss: 0.23109287023544312\n",
      "training: 98 batch 493 batch_loss: 0.2315000593662262\n",
      "training: 98 batch 494 batch_loss: 0.2377655804157257\n",
      "training: 98 batch 495 batch_loss: 0.23578959703445435\n",
      "training: 98 batch 496 batch_loss: 0.23758137226104736\n",
      "training: 98 batch 497 batch_loss: 0.2329234778881073\n",
      "training: 98 batch 498 batch_loss: 0.236715167760849\n",
      "training: 98 batch 499 batch_loss: 0.23469820618629456\n",
      "training: 98 batch 500 batch_loss: 0.23097720742225647\n",
      "training: 98 batch 501 batch_loss: 0.227992981672287\n",
      "training: 98 batch 502 batch_loss: 0.23370924592018127\n",
      "training: 98 batch 503 batch_loss: 0.23122352361679077\n",
      "training: 98 batch 504 batch_loss: 0.23622000217437744\n",
      "training: 98 batch 505 batch_loss: 0.2307446002960205\n",
      "training: 98 batch 506 batch_loss: 0.23576170206069946\n",
      "training: 98 batch 507 batch_loss: 0.23642268776893616\n",
      "training: 98 batch 508 batch_loss: 0.23145300149917603\n",
      "training: 98 batch 509 batch_loss: 0.2318432331085205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 98 batch 510 batch_loss: 0.2277294099330902\n",
      "training: 98 batch 511 batch_loss: 0.23149573802947998\n",
      "training: 98 batch 512 batch_loss: 0.22652539610862732\n",
      "training: 98 batch 513 batch_loss: 0.23339426517486572\n",
      "training: 98 batch 514 batch_loss: 0.23608049750328064\n",
      "training: 98 batch 515 batch_loss: 0.23377245664596558\n",
      "training: 98 batch 516 batch_loss: 0.235670268535614\n",
      "training: 98 batch 517 batch_loss: 0.22882398962974548\n",
      "training: 98 batch 518 batch_loss: 0.23471391201019287\n",
      "training: 98 batch 519 batch_loss: 0.2350904941558838\n",
      "training: 98 batch 520 batch_loss: 0.2357381284236908\n",
      "training: 98 batch 521 batch_loss: 0.2345947027206421\n",
      "training: 98 batch 522 batch_loss: 0.2377167046070099\n",
      "training: 98 batch 523 batch_loss: 0.23129507899284363\n",
      "training: 98 batch 524 batch_loss: 0.23648104071617126\n",
      "training: 98 batch 525 batch_loss: 0.23542743921279907\n",
      "training: 98 batch 526 batch_loss: 0.23257708549499512\n",
      "training: 98 batch 527 batch_loss: 0.2301616370677948\n",
      "training: 98 batch 528 batch_loss: 0.2351149022579193\n",
      "training: 98 batch 529 batch_loss: 0.23645389080047607\n",
      "training: 98 batch 530 batch_loss: 0.236556738615036\n",
      "training: 98 batch 531 batch_loss: 0.23672634363174438\n",
      "training: 98 batch 532 batch_loss: 0.2329755425453186\n",
      "training: 98 batch 533 batch_loss: 0.23543787002563477\n",
      "training: 98 batch 534 batch_loss: 0.23084047436714172\n",
      "training: 98 batch 535 batch_loss: 0.22850710153579712\n",
      "training: 98 batch 536 batch_loss: 0.2348191738128662\n",
      "training: 98 batch 537 batch_loss: 0.23411408066749573\n",
      "training: 98 batch 538 batch_loss: 0.23501303791999817\n",
      "training: 98 batch 539 batch_loss: 0.23363560438156128\n",
      "training: 98 batch 540 batch_loss: 0.23169434070587158\n",
      "training: 98 batch 541 batch_loss: 0.2319079041481018\n",
      "training: 98 batch 542 batch_loss: 0.23460546135902405\n",
      "training: 98 batch 543 batch_loss: 0.23506587743759155\n",
      "training: 98 batch 544 batch_loss: 0.23634234070777893\n",
      "training: 98 batch 545 batch_loss: 0.23159655928611755\n",
      "training: 98 batch 546 batch_loss: 0.229070246219635\n",
      "training: 98 batch 547 batch_loss: 0.2322973608970642\n",
      "training: 98 batch 548 batch_loss: 0.23474162817001343\n",
      "training: 98 batch 549 batch_loss: 0.228590726852417\n",
      "training: 98 batch 550 batch_loss: 0.2318492829799652\n",
      "training: 98 batch 551 batch_loss: 0.23091423511505127\n",
      "training: 98 batch 552 batch_loss: 0.24069520831108093\n",
      "training: 98 batch 553 batch_loss: 0.23445084691047668\n",
      "training: 98 batch 554 batch_loss: 0.23341143131256104\n",
      "training: 98 batch 555 batch_loss: 0.23456820845603943\n",
      "training: 98 batch 556 batch_loss: 0.2299092411994934\n",
      "training: 98 batch 557 batch_loss: 0.2372286319732666\n",
      "training: 98 batch 558 batch_loss: 0.23471584916114807\n",
      "training: 98 batch 559 batch_loss: 0.22799506783485413\n",
      "training: 98 batch 560 batch_loss: 0.23298263549804688\n",
      "training: 98 batch 561 batch_loss: 0.23675864934921265\n",
      "training: 98 batch 562 batch_loss: 0.23142963647842407\n",
      "training: 98 batch 563 batch_loss: 0.23490992188453674\n",
      "training: 98 batch 564 batch_loss: 0.22987866401672363\n",
      "training: 98 batch 565 batch_loss: 0.23069965839385986\n",
      "training: 98 batch 566 batch_loss: 0.2302926778793335\n",
      "training: 98 batch 567 batch_loss: 0.23139381408691406\n",
      "training: 98 batch 568 batch_loss: 0.2298009693622589\n",
      "training: 98 batch 569 batch_loss: 0.23941263556480408\n",
      "training: 98 batch 570 batch_loss: 0.2358531355857849\n",
      "training: 98 batch 571 batch_loss: 0.23675692081451416\n",
      "training: 98 batch 572 batch_loss: 0.2337445616722107\n",
      "training: 98 batch 573 batch_loss: 0.23240897059440613\n",
      "training: 98 batch 574 batch_loss: 0.2305864691734314\n",
      "training: 98 batch 575 batch_loss: 0.23290884494781494\n",
      "training: 98 batch 576 batch_loss: 0.22668209671974182\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 98, Hit Ratio:0.03408140762775948 | Precision:0.050285068318096926 | Recall:0.06662543667197422 | NDCG:0.0657590363835593\n",
      "*Best Performance* \n",
      "Epoch: 97, Hit Ratio:0.03426795428274477 | Precision:0.0505603066941905 | Recall:0.06646838612031453 | MDCG:0.06605620545333978\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 99 batch 0 batch_loss: 0.2305501103401184\n",
      "training: 99 batch 1 batch_loss: 0.22956332564353943\n",
      "training: 99 batch 2 batch_loss: 0.2287275791168213\n",
      "training: 99 batch 3 batch_loss: 0.2302408218383789\n",
      "training: 99 batch 4 batch_loss: 0.23312556743621826\n",
      "training: 99 batch 5 batch_loss: 0.23134580254554749\n",
      "training: 99 batch 6 batch_loss: 0.23226147890090942\n",
      "training: 99 batch 7 batch_loss: 0.23077654838562012\n",
      "training: 99 batch 8 batch_loss: 0.2314550280570984\n",
      "training: 99 batch 9 batch_loss: 0.23052287101745605\n",
      "training: 99 batch 10 batch_loss: 0.2339073121547699\n",
      "training: 99 batch 11 batch_loss: 0.23321932554244995\n",
      "training: 99 batch 12 batch_loss: 0.23470085859298706\n",
      "training: 99 batch 13 batch_loss: 0.23050308227539062\n",
      "training: 99 batch 14 batch_loss: 0.22440755367279053\n",
      "training: 99 batch 15 batch_loss: 0.2317540943622589\n",
      "training: 99 batch 16 batch_loss: 0.23014909029006958\n",
      "training: 99 batch 17 batch_loss: 0.23105719685554504\n",
      "training: 99 batch 18 batch_loss: 0.23386207222938538\n",
      "training: 99 batch 19 batch_loss: 0.2375805377960205\n",
      "training: 99 batch 20 batch_loss: 0.23410433530807495\n",
      "training: 99 batch 21 batch_loss: 0.23670902848243713\n",
      "training: 99 batch 22 batch_loss: 0.23934724926948547\n",
      "training: 99 batch 23 batch_loss: 0.23389500379562378\n",
      "training: 99 batch 24 batch_loss: 0.23324787616729736\n",
      "training: 99 batch 25 batch_loss: 0.2304704785346985\n",
      "training: 99 batch 26 batch_loss: 0.23516365885734558\n",
      "training: 99 batch 27 batch_loss: 0.2292269468307495\n",
      "training: 99 batch 28 batch_loss: 0.23426535725593567\n",
      "training: 99 batch 29 batch_loss: 0.2313598096370697\n",
      "training: 99 batch 30 batch_loss: 0.23187753558158875\n",
      "training: 99 batch 31 batch_loss: 0.23601588606834412\n",
      "training: 99 batch 32 batch_loss: 0.23093396425247192\n",
      "training: 99 batch 33 batch_loss: 0.22887152433395386\n",
      "training: 99 batch 34 batch_loss: 0.23246228694915771\n",
      "training: 99 batch 35 batch_loss: 0.23166415095329285\n",
      "training: 99 batch 36 batch_loss: 0.2339785099029541\n",
      "training: 99 batch 37 batch_loss: 0.23567169904708862\n",
      "training: 99 batch 38 batch_loss: 0.23149698972702026\n",
      "training: 99 batch 39 batch_loss: 0.2312580943107605\n",
      "training: 99 batch 40 batch_loss: 0.22870373725891113\n",
      "training: 99 batch 41 batch_loss: 0.23061057925224304\n",
      "training: 99 batch 42 batch_loss: 0.23138964176177979\n",
      "training: 99 batch 43 batch_loss: 0.2252366840839386\n",
      "training: 99 batch 44 batch_loss: 0.23108148574829102\n",
      "training: 99 batch 45 batch_loss: 0.23437196016311646\n",
      "training: 99 batch 46 batch_loss: 0.23149707913398743\n",
      "training: 99 batch 47 batch_loss: 0.22570878267288208\n",
      "training: 99 batch 48 batch_loss: 0.2322431206703186\n",
      "training: 99 batch 49 batch_loss: 0.22810763120651245\n",
      "training: 99 batch 50 batch_loss: 0.22651907801628113\n",
      "training: 99 batch 51 batch_loss: 0.22892600297927856\n",
      "training: 99 batch 52 batch_loss: 0.22734877467155457\n",
      "training: 99 batch 53 batch_loss: 0.23076754808425903\n",
      "training: 99 batch 54 batch_loss: 0.23368564248085022\n",
      "training: 99 batch 55 batch_loss: 0.236683189868927\n",
      "training: 99 batch 56 batch_loss: 0.236862450838089\n",
      "training: 99 batch 57 batch_loss: 0.23078668117523193\n",
      "training: 99 batch 58 batch_loss: 0.23215991258621216\n",
      "training: 99 batch 59 batch_loss: 0.22805947065353394\n",
      "training: 99 batch 60 batch_loss: 0.23113131523132324\n",
      "training: 99 batch 61 batch_loss: 0.2295578420162201\n",
      "training: 99 batch 62 batch_loss: 0.23364076018333435\n",
      "training: 99 batch 63 batch_loss: 0.22844499349594116\n",
      "training: 99 batch 64 batch_loss: 0.23078316450119019\n",
      "training: 99 batch 65 batch_loss: 0.23237362504005432\n",
      "training: 99 batch 66 batch_loss: 0.233832448720932\n",
      "training: 99 batch 67 batch_loss: 0.2355765700340271\n",
      "training: 99 batch 68 batch_loss: 0.2302342653274536\n",
      "training: 99 batch 69 batch_loss: 0.23350387811660767\n",
      "training: 99 batch 70 batch_loss: 0.23460513353347778\n",
      "training: 99 batch 71 batch_loss: 0.2275088131427765\n",
      "training: 99 batch 72 batch_loss: 0.23383653163909912\n",
      "training: 99 batch 73 batch_loss: 0.2322503626346588\n",
      "training: 99 batch 74 batch_loss: 0.23229125142097473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 99 batch 75 batch_loss: 0.2279067039489746\n",
      "training: 99 batch 76 batch_loss: 0.23138388991355896\n",
      "training: 99 batch 77 batch_loss: 0.23068031668663025\n",
      "training: 99 batch 78 batch_loss: 0.23643386363983154\n",
      "training: 99 batch 79 batch_loss: 0.235434889793396\n",
      "training: 99 batch 80 batch_loss: 0.2360859215259552\n",
      "training: 99 batch 81 batch_loss: 0.23156437277793884\n",
      "training: 99 batch 82 batch_loss: 0.23208558559417725\n",
      "training: 99 batch 83 batch_loss: 0.2317923605442047\n",
      "training: 99 batch 84 batch_loss: 0.2319374680519104\n",
      "training: 99 batch 85 batch_loss: 0.23248732089996338\n",
      "training: 99 batch 86 batch_loss: 0.23506900668144226\n",
      "training: 99 batch 87 batch_loss: 0.23175328969955444\n",
      "training: 99 batch 88 batch_loss: 0.232803612947464\n",
      "training: 99 batch 89 batch_loss: 0.23093485832214355\n",
      "training: 99 batch 90 batch_loss: 0.23039257526397705\n",
      "training: 99 batch 91 batch_loss: 0.23182767629623413\n",
      "training: 99 batch 92 batch_loss: 0.23591506481170654\n",
      "training: 99 batch 93 batch_loss: 0.23130598664283752\n",
      "training: 99 batch 94 batch_loss: 0.23660200834274292\n",
      "training: 99 batch 95 batch_loss: 0.23507159948349\n",
      "training: 99 batch 96 batch_loss: 0.2289676070213318\n",
      "training: 99 batch 97 batch_loss: 0.23165124654769897\n",
      "training: 99 batch 98 batch_loss: 0.23220792412757874\n",
      "training: 99 batch 99 batch_loss: 0.23298996686935425\n",
      "training: 99 batch 100 batch_loss: 0.23138520121574402\n",
      "training: 99 batch 101 batch_loss: 0.23335760831832886\n",
      "training: 99 batch 102 batch_loss: 0.23161903023719788\n",
      "training: 99 batch 103 batch_loss: 0.23267605900764465\n",
      "training: 99 batch 104 batch_loss: 0.22958692908287048\n",
      "training: 99 batch 105 batch_loss: 0.23199284076690674\n",
      "training: 99 batch 106 batch_loss: 0.23111581802368164\n",
      "training: 99 batch 107 batch_loss: 0.2280242145061493\n",
      "training: 99 batch 108 batch_loss: 0.237396240234375\n",
      "training: 99 batch 109 batch_loss: 0.2345033884048462\n",
      "training: 99 batch 110 batch_loss: 0.23189401626586914\n",
      "training: 99 batch 111 batch_loss: 0.23663246631622314\n",
      "training: 99 batch 112 batch_loss: 0.23123523592948914\n",
      "training: 99 batch 113 batch_loss: 0.2310311198234558\n",
      "training: 99 batch 114 batch_loss: 0.2310331165790558\n",
      "training: 99 batch 115 batch_loss: 0.232703298330307\n",
      "training: 99 batch 116 batch_loss: 0.22502702474594116\n",
      "training: 99 batch 117 batch_loss: 0.2316606342792511\n",
      "training: 99 batch 118 batch_loss: 0.2319101095199585\n",
      "training: 99 batch 119 batch_loss: 0.23403698205947876\n",
      "training: 99 batch 120 batch_loss: 0.2303120195865631\n",
      "training: 99 batch 121 batch_loss: 0.23274391889572144\n",
      "training: 99 batch 122 batch_loss: 0.23042726516723633\n",
      "training: 99 batch 123 batch_loss: 0.2322092056274414\n",
      "training: 99 batch 124 batch_loss: 0.23192253708839417\n",
      "training: 99 batch 125 batch_loss: 0.23282009363174438\n",
      "training: 99 batch 126 batch_loss: 0.23060649633407593\n",
      "training: 99 batch 127 batch_loss: 0.23204514384269714\n",
      "training: 99 batch 128 batch_loss: 0.23410427570343018\n",
      "training: 99 batch 129 batch_loss: 0.23264989256858826\n",
      "training: 99 batch 130 batch_loss: 0.2335713803768158\n",
      "training: 99 batch 131 batch_loss: 0.2314068078994751\n",
      "training: 99 batch 132 batch_loss: 0.22890838980674744\n",
      "training: 99 batch 133 batch_loss: 0.2329275906085968\n",
      "training: 99 batch 134 batch_loss: 0.2325843870639801\n",
      "training: 99 batch 135 batch_loss: 0.2285591959953308\n",
      "training: 99 batch 136 batch_loss: 0.23462504148483276\n",
      "training: 99 batch 137 batch_loss: 0.23008409142494202\n",
      "training: 99 batch 138 batch_loss: 0.2357710599899292\n",
      "training: 99 batch 139 batch_loss: 0.22892051935195923\n",
      "training: 99 batch 140 batch_loss: 0.2297224998474121\n",
      "training: 99 batch 141 batch_loss: 0.24034282565116882\n",
      "training: 99 batch 142 batch_loss: 0.23254355788230896\n",
      "training: 99 batch 143 batch_loss: 0.227924644947052\n",
      "training: 99 batch 144 batch_loss: 0.23167073726654053\n",
      "training: 99 batch 145 batch_loss: 0.22789615392684937\n",
      "training: 99 batch 146 batch_loss: 0.23371735215187073\n",
      "training: 99 batch 147 batch_loss: 0.232313871383667\n",
      "training: 99 batch 148 batch_loss: 0.23172545433044434\n",
      "training: 99 batch 149 batch_loss: 0.23082458972930908\n",
      "training: 99 batch 150 batch_loss: 0.2330912947654724\n",
      "training: 99 batch 151 batch_loss: 0.23288494348526\n",
      "training: 99 batch 152 batch_loss: 0.23225310444831848\n",
      "training: 99 batch 153 batch_loss: 0.2312847077846527\n",
      "training: 99 batch 154 batch_loss: 0.23241183161735535\n",
      "training: 99 batch 155 batch_loss: 0.22942668199539185\n",
      "training: 99 batch 156 batch_loss: 0.2342056930065155\n",
      "training: 99 batch 157 batch_loss: 0.2334062159061432\n",
      "training: 99 batch 158 batch_loss: 0.23007696866989136\n",
      "training: 99 batch 159 batch_loss: 0.23228368163108826\n",
      "training: 99 batch 160 batch_loss: 0.22870910167694092\n",
      "training: 99 batch 161 batch_loss: 0.23100626468658447\n",
      "training: 99 batch 162 batch_loss: 0.23475438356399536\n",
      "training: 99 batch 163 batch_loss: 0.23121392726898193\n",
      "training: 99 batch 164 batch_loss: 0.23045998811721802\n",
      "training: 99 batch 165 batch_loss: 0.23856866359710693\n",
      "training: 99 batch 166 batch_loss: 0.2323228120803833\n",
      "training: 99 batch 167 batch_loss: 0.23439174890518188\n",
      "training: 99 batch 168 batch_loss: 0.2281084656715393\n",
      "training: 99 batch 169 batch_loss: 0.2334522306919098\n",
      "training: 99 batch 170 batch_loss: 0.23237115144729614\n",
      "training: 99 batch 171 batch_loss: 0.2283036708831787\n",
      "training: 99 batch 172 batch_loss: 0.23513275384902954\n",
      "training: 99 batch 173 batch_loss: 0.23526531457901\n",
      "training: 99 batch 174 batch_loss: 0.23056039214134216\n",
      "training: 99 batch 175 batch_loss: 0.23625263571739197\n",
      "training: 99 batch 176 batch_loss: 0.2320300042629242\n",
      "training: 99 batch 177 batch_loss: 0.2317168414592743\n",
      "training: 99 batch 178 batch_loss: 0.2292899787425995\n",
      "training: 99 batch 179 batch_loss: 0.23014777898788452\n",
      "training: 99 batch 180 batch_loss: 0.2331608533859253\n",
      "training: 99 batch 181 batch_loss: 0.23417621850967407\n",
      "training: 99 batch 182 batch_loss: 0.23274526000022888\n",
      "training: 99 batch 183 batch_loss: 0.23551437258720398\n",
      "training: 99 batch 184 batch_loss: 0.23465842008590698\n",
      "training: 99 batch 185 batch_loss: 0.2341194748878479\n",
      "training: 99 batch 186 batch_loss: 0.22944635152816772\n",
      "training: 99 batch 187 batch_loss: 0.2286209762096405\n",
      "training: 99 batch 188 batch_loss: 0.2316608726978302\n",
      "training: 99 batch 189 batch_loss: 0.23396822810173035\n",
      "training: 99 batch 190 batch_loss: 0.2333056926727295\n",
      "training: 99 batch 191 batch_loss: 0.2304358184337616\n",
      "training: 99 batch 192 batch_loss: 0.23066961765289307\n",
      "training: 99 batch 193 batch_loss: 0.23050457239151\n",
      "training: 99 batch 194 batch_loss: 0.23003041744232178\n",
      "training: 99 batch 195 batch_loss: 0.23111793398857117\n",
      "training: 99 batch 196 batch_loss: 0.23409658670425415\n",
      "training: 99 batch 197 batch_loss: 0.2352251410484314\n",
      "training: 99 batch 198 batch_loss: 0.2334488332271576\n",
      "training: 99 batch 199 batch_loss: 0.22882917523384094\n",
      "training: 99 batch 200 batch_loss: 0.23445552587509155\n",
      "training: 99 batch 201 batch_loss: 0.23451006412506104\n",
      "training: 99 batch 202 batch_loss: 0.23691415786743164\n",
      "training: 99 batch 203 batch_loss: 0.23014014959335327\n",
      "training: 99 batch 204 batch_loss: 0.23435631394386292\n",
      "training: 99 batch 205 batch_loss: 0.23395711183547974\n",
      "training: 99 batch 206 batch_loss: 0.23090136051177979\n",
      "training: 99 batch 207 batch_loss: 0.23023837804794312\n",
      "training: 99 batch 208 batch_loss: 0.22823208570480347\n",
      "training: 99 batch 209 batch_loss: 0.23006892204284668\n",
      "training: 99 batch 210 batch_loss: 0.23087236285209656\n",
      "training: 99 batch 211 batch_loss: 0.23382946848869324\n",
      "training: 99 batch 212 batch_loss: 0.23151347041130066\n",
      "training: 99 batch 213 batch_loss: 0.23494616150856018\n",
      "training: 99 batch 214 batch_loss: 0.23573678731918335\n",
      "training: 99 batch 215 batch_loss: 0.23329514265060425\n",
      "training: 99 batch 216 batch_loss: 0.23186072707176208\n",
      "training: 99 batch 217 batch_loss: 0.23039880394935608\n",
      "training: 99 batch 218 batch_loss: 0.23392993211746216\n",
      "training: 99 batch 219 batch_loss: 0.23206055164337158\n",
      "training: 99 batch 220 batch_loss: 0.2328193187713623\n",
      "training: 99 batch 221 batch_loss: 0.23355510830879211\n",
      "training: 99 batch 222 batch_loss: 0.23522266745567322\n",
      "training: 99 batch 223 batch_loss: 0.233341783285141\n",
      "training: 99 batch 224 batch_loss: 0.23669147491455078\n",
      "training: 99 batch 225 batch_loss: 0.23312580585479736\n",
      "training: 99 batch 226 batch_loss: 0.22999781370162964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 99 batch 227 batch_loss: 0.22760871052742004\n",
      "training: 99 batch 228 batch_loss: 0.23737752437591553\n",
      "training: 99 batch 229 batch_loss: 0.2307623326778412\n",
      "training: 99 batch 230 batch_loss: 0.2292974591255188\n",
      "training: 99 batch 231 batch_loss: 0.23509037494659424\n",
      "training: 99 batch 232 batch_loss: 0.22968465089797974\n",
      "training: 99 batch 233 batch_loss: 0.23073327541351318\n",
      "training: 99 batch 234 batch_loss: 0.23544704914093018\n",
      "training: 99 batch 235 batch_loss: 0.23687273263931274\n",
      "training: 99 batch 236 batch_loss: 0.23516210913658142\n",
      "training: 99 batch 237 batch_loss: 0.22944515943527222\n",
      "training: 99 batch 238 batch_loss: 0.2267613708972931\n",
      "training: 99 batch 239 batch_loss: 0.2326124906539917\n",
      "training: 99 batch 240 batch_loss: 0.23561140894889832\n",
      "training: 99 batch 241 batch_loss: 0.23422250151634216\n",
      "training: 99 batch 242 batch_loss: 0.23151344060897827\n",
      "training: 99 batch 243 batch_loss: 0.2340323030948639\n",
      "training: 99 batch 244 batch_loss: 0.23746371269226074\n",
      "training: 99 batch 245 batch_loss: 0.23257389664649963\n",
      "training: 99 batch 246 batch_loss: 0.2286285161972046\n",
      "training: 99 batch 247 batch_loss: 0.23480570316314697\n",
      "training: 99 batch 248 batch_loss: 0.23000037670135498\n",
      "training: 99 batch 249 batch_loss: 0.23198843002319336\n",
      "training: 99 batch 250 batch_loss: 0.23241308331489563\n",
      "training: 99 batch 251 batch_loss: 0.23170197010040283\n",
      "training: 99 batch 252 batch_loss: 0.23226317763328552\n",
      "training: 99 batch 253 batch_loss: 0.22920888662338257\n",
      "training: 99 batch 254 batch_loss: 0.23059019446372986\n",
      "training: 99 batch 255 batch_loss: 0.2360972762107849\n",
      "training: 99 batch 256 batch_loss: 0.23520833253860474\n",
      "training: 99 batch 257 batch_loss: 0.23124957084655762\n",
      "training: 99 batch 258 batch_loss: 0.2337368130683899\n",
      "training: 99 batch 259 batch_loss: 0.2350669503211975\n",
      "training: 99 batch 260 batch_loss: 0.22997093200683594\n",
      "training: 99 batch 261 batch_loss: 0.23148569464683533\n",
      "training: 99 batch 262 batch_loss: 0.23550480604171753\n",
      "training: 99 batch 263 batch_loss: 0.234236478805542\n",
      "training: 99 batch 264 batch_loss: 0.2372191846370697\n",
      "training: 99 batch 265 batch_loss: 0.23166900873184204\n",
      "training: 99 batch 266 batch_loss: 0.23316583037376404\n",
      "training: 99 batch 267 batch_loss: 0.23551306128501892\n",
      "training: 99 batch 268 batch_loss: 0.232852041721344\n",
      "training: 99 batch 269 batch_loss: 0.23734453320503235\n",
      "training: 99 batch 270 batch_loss: 0.23590686917304993\n",
      "training: 99 batch 271 batch_loss: 0.22973895072937012\n",
      "training: 99 batch 272 batch_loss: 0.23294606804847717\n",
      "training: 99 batch 273 batch_loss: 0.23725885152816772\n",
      "training: 99 batch 274 batch_loss: 0.23340481519699097\n",
      "training: 99 batch 275 batch_loss: 0.2347896695137024\n",
      "training: 99 batch 276 batch_loss: 0.23101919889450073\n",
      "training: 99 batch 277 batch_loss: 0.233216792345047\n",
      "training: 99 batch 278 batch_loss: 0.23575660586357117\n",
      "training: 99 batch 279 batch_loss: 0.23284614086151123\n",
      "training: 99 batch 280 batch_loss: 0.2367439866065979\n",
      "training: 99 batch 281 batch_loss: 0.23533064126968384\n",
      "training: 99 batch 282 batch_loss: 0.23415321111679077\n",
      "training: 99 batch 283 batch_loss: 0.23028206825256348\n",
      "training: 99 batch 284 batch_loss: 0.23603978753089905\n",
      "training: 99 batch 285 batch_loss: 0.23531568050384521\n",
      "training: 99 batch 286 batch_loss: 0.2364138960838318\n",
      "training: 99 batch 287 batch_loss: 0.23480093479156494\n",
      "training: 99 batch 288 batch_loss: 0.23446381092071533\n",
      "training: 99 batch 289 batch_loss: 0.2347390055656433\n",
      "training: 99 batch 290 batch_loss: 0.23271316289901733\n",
      "training: 99 batch 291 batch_loss: 0.22954359650611877\n",
      "training: 99 batch 292 batch_loss: 0.23693960905075073\n",
      "training: 99 batch 293 batch_loss: 0.23102852702140808\n",
      "training: 99 batch 294 batch_loss: 0.22907602787017822\n",
      "training: 99 batch 295 batch_loss: 0.228226900100708\n",
      "training: 99 batch 296 batch_loss: 0.2334223985671997\n",
      "training: 99 batch 297 batch_loss: 0.23107299208641052\n",
      "training: 99 batch 298 batch_loss: 0.235670268535614\n",
      "training: 99 batch 299 batch_loss: 0.23065480589866638\n",
      "training: 99 batch 300 batch_loss: 0.23646953701972961\n",
      "training: 99 batch 301 batch_loss: 0.2336578667163849\n",
      "training: 99 batch 302 batch_loss: 0.22975197434425354\n",
      "training: 99 batch 303 batch_loss: 0.23439174890518188\n",
      "training: 99 batch 304 batch_loss: 0.23391786217689514\n",
      "training: 99 batch 305 batch_loss: 0.23476499319076538\n",
      "training: 99 batch 306 batch_loss: 0.23657652735710144\n",
      "training: 99 batch 307 batch_loss: 0.23184967041015625\n",
      "training: 99 batch 308 batch_loss: 0.22696071863174438\n",
      "training: 99 batch 309 batch_loss: 0.2311723828315735\n",
      "training: 99 batch 310 batch_loss: 0.23410320281982422\n",
      "training: 99 batch 311 batch_loss: 0.23111295700073242\n",
      "training: 99 batch 312 batch_loss: 0.2297823429107666\n",
      "training: 99 batch 313 batch_loss: 0.23606422543525696\n",
      "training: 99 batch 314 batch_loss: 0.2319028377532959\n",
      "training: 99 batch 315 batch_loss: 0.23601019382476807\n",
      "training: 99 batch 316 batch_loss: 0.23087060451507568\n",
      "training: 99 batch 317 batch_loss: 0.2373555600643158\n",
      "training: 99 batch 318 batch_loss: 0.23580008745193481\n",
      "training: 99 batch 319 batch_loss: 0.2301226556301117\n",
      "training: 99 batch 320 batch_loss: 0.23019546270370483\n",
      "training: 99 batch 321 batch_loss: 0.2335299253463745\n",
      "training: 99 batch 322 batch_loss: 0.22961273789405823\n",
      "training: 99 batch 323 batch_loss: 0.23647230863571167\n",
      "training: 99 batch 324 batch_loss: 0.232467383146286\n",
      "training: 99 batch 325 batch_loss: 0.23309314250946045\n",
      "training: 99 batch 326 batch_loss: 0.23605045676231384\n",
      "training: 99 batch 327 batch_loss: 0.23739516735076904\n",
      "training: 99 batch 328 batch_loss: 0.23494082689285278\n",
      "training: 99 batch 329 batch_loss: 0.23651301860809326\n",
      "training: 99 batch 330 batch_loss: 0.2311251163482666\n",
      "training: 99 batch 331 batch_loss: 0.23274895548820496\n",
      "training: 99 batch 332 batch_loss: 0.23098713159561157\n",
      "training: 99 batch 333 batch_loss: 0.23511794209480286\n",
      "training: 99 batch 334 batch_loss: 0.2407422661781311\n",
      "training: 99 batch 335 batch_loss: 0.23173049092292786\n",
      "training: 99 batch 336 batch_loss: 0.23368829488754272\n",
      "training: 99 batch 337 batch_loss: 0.23279836773872375\n",
      "training: 99 batch 338 batch_loss: 0.23275619745254517\n",
      "training: 99 batch 339 batch_loss: 0.23352423310279846\n",
      "training: 99 batch 340 batch_loss: 0.2310384213924408\n",
      "training: 99 batch 341 batch_loss: 0.2334001064300537\n",
      "training: 99 batch 342 batch_loss: 0.23556560277938843\n",
      "training: 99 batch 343 batch_loss: 0.23222437500953674\n",
      "training: 99 batch 344 batch_loss: 0.23270225524902344\n",
      "training: 99 batch 345 batch_loss: 0.23559114336967468\n",
      "training: 99 batch 346 batch_loss: 0.2264007329940796\n",
      "training: 99 batch 347 batch_loss: 0.2324286699295044\n",
      "training: 99 batch 348 batch_loss: 0.23223251104354858\n",
      "training: 99 batch 349 batch_loss: 0.23077493906021118\n",
      "training: 99 batch 350 batch_loss: 0.23375776410102844\n",
      "training: 99 batch 351 batch_loss: 0.23042526841163635\n",
      "training: 99 batch 352 batch_loss: 0.23219269514083862\n",
      "training: 99 batch 353 batch_loss: 0.2331010401248932\n",
      "training: 99 batch 354 batch_loss: 0.23709678649902344\n",
      "training: 99 batch 355 batch_loss: 0.23592233657836914\n",
      "training: 99 batch 356 batch_loss: 0.23459434509277344\n",
      "training: 99 batch 357 batch_loss: 0.2333219349384308\n",
      "training: 99 batch 358 batch_loss: 0.23107212781906128\n",
      "training: 99 batch 359 batch_loss: 0.22977334260940552\n",
      "training: 99 batch 360 batch_loss: 0.23505425453186035\n",
      "training: 99 batch 361 batch_loss: 0.23552265763282776\n",
      "training: 99 batch 362 batch_loss: 0.23173010349273682\n",
      "training: 99 batch 363 batch_loss: 0.23598414659500122\n",
      "training: 99 batch 364 batch_loss: 0.23673993349075317\n",
      "training: 99 batch 365 batch_loss: 0.23397094011306763\n",
      "training: 99 batch 366 batch_loss: 0.23420938849449158\n",
      "training: 99 batch 367 batch_loss: 0.23290488123893738\n",
      "training: 99 batch 368 batch_loss: 0.2349914312362671\n",
      "training: 99 batch 369 batch_loss: 0.23254111409187317\n",
      "training: 99 batch 370 batch_loss: 0.23435088992118835\n",
      "training: 99 batch 371 batch_loss: 0.22945097088813782\n",
      "training: 99 batch 372 batch_loss: 0.23517781496047974\n",
      "training: 99 batch 373 batch_loss: 0.23069310188293457\n",
      "training: 99 batch 374 batch_loss: 0.23418021202087402\n",
      "training: 99 batch 375 batch_loss: 0.232285737991333\n",
      "training: 99 batch 376 batch_loss: 0.23495256900787354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 99 batch 377 batch_loss: 0.23839423060417175\n",
      "training: 99 batch 378 batch_loss: 0.2320578396320343\n",
      "training: 99 batch 379 batch_loss: 0.23340600728988647\n",
      "training: 99 batch 380 batch_loss: 0.24075329303741455\n",
      "training: 99 batch 381 batch_loss: 0.23597726225852966\n",
      "training: 99 batch 382 batch_loss: 0.23359641432762146\n",
      "training: 99 batch 383 batch_loss: 0.23692578077316284\n",
      "training: 99 batch 384 batch_loss: 0.23686766624450684\n",
      "training: 99 batch 385 batch_loss: 0.23413696885108948\n",
      "training: 99 batch 386 batch_loss: 0.23362958431243896\n",
      "training: 99 batch 387 batch_loss: 0.23661106824874878\n",
      "training: 99 batch 388 batch_loss: 0.2325751781463623\n",
      "training: 99 batch 389 batch_loss: 0.23111873865127563\n",
      "training: 99 batch 390 batch_loss: 0.2306310534477234\n",
      "training: 99 batch 391 batch_loss: 0.23457270860671997\n",
      "training: 99 batch 392 batch_loss: 0.23445338010787964\n",
      "training: 99 batch 393 batch_loss: 0.23200881481170654\n",
      "training: 99 batch 394 batch_loss: 0.2303980588912964\n",
      "training: 99 batch 395 batch_loss: 0.2371620535850525\n",
      "training: 99 batch 396 batch_loss: 0.23474052548408508\n",
      "training: 99 batch 397 batch_loss: 0.2337166965007782\n",
      "training: 99 batch 398 batch_loss: 0.23720940947532654\n",
      "training: 99 batch 399 batch_loss: 0.23703736066818237\n",
      "training: 99 batch 400 batch_loss: 0.23405125737190247\n",
      "training: 99 batch 401 batch_loss: 0.23133063316345215\n",
      "training: 99 batch 402 batch_loss: 0.2285003662109375\n",
      "training: 99 batch 403 batch_loss: 0.23822060227394104\n",
      "training: 99 batch 404 batch_loss: 0.23397910594940186\n",
      "training: 99 batch 405 batch_loss: 0.23118478059768677\n",
      "training: 99 batch 406 batch_loss: 0.234431654214859\n",
      "training: 99 batch 407 batch_loss: 0.22781866788864136\n",
      "training: 99 batch 408 batch_loss: 0.23658791184425354\n",
      "training: 99 batch 409 batch_loss: 0.23717749118804932\n",
      "training: 99 batch 410 batch_loss: 0.2347756028175354\n",
      "training: 99 batch 411 batch_loss: 0.23414653539657593\n",
      "training: 99 batch 412 batch_loss: 0.23091676831245422\n",
      "training: 99 batch 413 batch_loss: 0.23440340161323547\n",
      "training: 99 batch 414 batch_loss: 0.2335999608039856\n",
      "training: 99 batch 415 batch_loss: 0.23550796508789062\n",
      "training: 99 batch 416 batch_loss: 0.229653000831604\n",
      "training: 99 batch 417 batch_loss: 0.23542988300323486\n",
      "training: 99 batch 418 batch_loss: 0.23248502612113953\n",
      "training: 99 batch 419 batch_loss: 0.2366328239440918\n",
      "training: 99 batch 420 batch_loss: 0.23145166039466858\n",
      "training: 99 batch 421 batch_loss: 0.2357146441936493\n",
      "training: 99 batch 422 batch_loss: 0.23058202862739563\n",
      "training: 99 batch 423 batch_loss: 0.2329007089138031\n",
      "training: 99 batch 424 batch_loss: 0.2299855649471283\n",
      "training: 99 batch 425 batch_loss: 0.231947124004364\n",
      "training: 99 batch 426 batch_loss: 0.23113220930099487\n",
      "training: 99 batch 427 batch_loss: 0.2326103150844574\n",
      "training: 99 batch 428 batch_loss: 0.23379749059677124\n",
      "training: 99 batch 429 batch_loss: 0.23081955313682556\n",
      "training: 99 batch 430 batch_loss: 0.23410627245903015\n",
      "training: 99 batch 431 batch_loss: 0.23219552636146545\n",
      "training: 99 batch 432 batch_loss: 0.23704826831817627\n",
      "training: 99 batch 433 batch_loss: 0.23012501001358032\n",
      "training: 99 batch 434 batch_loss: 0.23070186376571655\n",
      "training: 99 batch 435 batch_loss: 0.2358582317829132\n",
      "training: 99 batch 436 batch_loss: 0.23536676168441772\n",
      "training: 99 batch 437 batch_loss: 0.22640573978424072\n",
      "training: 99 batch 438 batch_loss: 0.23845097422599792\n",
      "training: 99 batch 439 batch_loss: 0.227882981300354\n",
      "training: 99 batch 440 batch_loss: 0.23344188928604126\n",
      "training: 99 batch 441 batch_loss: 0.23247578740119934\n",
      "training: 99 batch 442 batch_loss: 0.23637962341308594\n",
      "training: 99 batch 443 batch_loss: 0.23967266082763672\n",
      "training: 99 batch 444 batch_loss: 0.23062938451766968\n",
      "training: 99 batch 445 batch_loss: 0.22939953207969666\n",
      "training: 99 batch 446 batch_loss: 0.23508262634277344\n",
      "training: 99 batch 447 batch_loss: 0.23160532116889954\n",
      "training: 99 batch 448 batch_loss: 0.237626850605011\n",
      "training: 99 batch 449 batch_loss: 0.23546278476715088\n",
      "training: 99 batch 450 batch_loss: 0.2332439422607422\n",
      "training: 99 batch 451 batch_loss: 0.2391257882118225\n",
      "training: 99 batch 452 batch_loss: 0.23223859071731567\n",
      "training: 99 batch 453 batch_loss: 0.23632478713989258\n",
      "training: 99 batch 454 batch_loss: 0.22747182846069336\n",
      "training: 99 batch 455 batch_loss: 0.23289042711257935\n",
      "training: 99 batch 456 batch_loss: 0.2320738434791565\n",
      "training: 99 batch 457 batch_loss: 0.2323976755142212\n",
      "training: 99 batch 458 batch_loss: 0.2369961142539978\n",
      "training: 99 batch 459 batch_loss: 0.22808107733726501\n",
      "training: 99 batch 460 batch_loss: 0.2306472659111023\n",
      "training: 99 batch 461 batch_loss: 0.23205101490020752\n",
      "training: 99 batch 462 batch_loss: 0.23416486382484436\n",
      "training: 99 batch 463 batch_loss: 0.23165065050125122\n",
      "training: 99 batch 464 batch_loss: 0.23321977257728577\n",
      "training: 99 batch 465 batch_loss: 0.23020115494728088\n",
      "training: 99 batch 466 batch_loss: 0.23381105065345764\n",
      "training: 99 batch 467 batch_loss: 0.23465359210968018\n",
      "training: 99 batch 468 batch_loss: 0.23130932450294495\n",
      "training: 99 batch 469 batch_loss: 0.233872652053833\n",
      "training: 99 batch 470 batch_loss: 0.2336544692516327\n",
      "training: 99 batch 471 batch_loss: 0.22945374250411987\n",
      "training: 99 batch 472 batch_loss: 0.2345588207244873\n",
      "training: 99 batch 473 batch_loss: 0.23013556003570557\n",
      "training: 99 batch 474 batch_loss: 0.23753318190574646\n",
      "training: 99 batch 475 batch_loss: 0.23181021213531494\n",
      "training: 99 batch 476 batch_loss: 0.23021668195724487\n",
      "training: 99 batch 477 batch_loss: 0.23044684529304504\n",
      "training: 99 batch 478 batch_loss: 0.2311842441558838\n",
      "training: 99 batch 479 batch_loss: 0.23532971739768982\n",
      "training: 99 batch 480 batch_loss: 0.2365598976612091\n",
      "training: 99 batch 481 batch_loss: 0.23554033041000366\n",
      "training: 99 batch 482 batch_loss: 0.23108232021331787\n",
      "training: 99 batch 483 batch_loss: 0.23082810640335083\n",
      "training: 99 batch 484 batch_loss: 0.23398619890213013\n",
      "training: 99 batch 485 batch_loss: 0.23534530401229858\n",
      "training: 99 batch 486 batch_loss: 0.23450517654418945\n",
      "training: 99 batch 487 batch_loss: 0.23297402262687683\n",
      "training: 99 batch 488 batch_loss: 0.23301982879638672\n",
      "training: 99 batch 489 batch_loss: 0.23438099026679993\n",
      "training: 99 batch 490 batch_loss: 0.23004531860351562\n",
      "training: 99 batch 491 batch_loss: 0.23613348603248596\n",
      "training: 99 batch 492 batch_loss: 0.2318284511566162\n",
      "training: 99 batch 493 batch_loss: 0.2332036793231964\n",
      "training: 99 batch 494 batch_loss: 0.23764729499816895\n",
      "training: 99 batch 495 batch_loss: 0.23407742381095886\n",
      "training: 99 batch 496 batch_loss: 0.23449504375457764\n",
      "training: 99 batch 497 batch_loss: 0.24165427684783936\n",
      "training: 99 batch 498 batch_loss: 0.2364940643310547\n",
      "training: 99 batch 499 batch_loss: 0.22996267676353455\n",
      "training: 99 batch 500 batch_loss: 0.2333422303199768\n",
      "training: 99 batch 501 batch_loss: 0.23564189672470093\n",
      "training: 99 batch 502 batch_loss: 0.2324221432209015\n",
      "training: 99 batch 503 batch_loss: 0.23519980907440186\n",
      "training: 99 batch 504 batch_loss: 0.23193660378456116\n",
      "training: 99 batch 505 batch_loss: 0.23292794823646545\n",
      "training: 99 batch 506 batch_loss: 0.23464253544807434\n",
      "training: 99 batch 507 batch_loss: 0.23388531804084778\n",
      "training: 99 batch 508 batch_loss: 0.23096808791160583\n",
      "training: 99 batch 509 batch_loss: 0.23524713516235352\n",
      "training: 99 batch 510 batch_loss: 0.23472189903259277\n",
      "training: 99 batch 511 batch_loss: 0.23316538333892822\n",
      "training: 99 batch 512 batch_loss: 0.23776140809059143\n",
      "training: 99 batch 513 batch_loss: 0.2311270833015442\n",
      "training: 99 batch 514 batch_loss: 0.23558685183525085\n",
      "training: 99 batch 515 batch_loss: 0.23415237665176392\n",
      "training: 99 batch 516 batch_loss: 0.23179683089256287\n",
      "training: 99 batch 517 batch_loss: 0.23544737696647644\n",
      "training: 99 batch 518 batch_loss: 0.234291672706604\n",
      "training: 99 batch 519 batch_loss: 0.23309674859046936\n",
      "training: 99 batch 520 batch_loss: 0.23539668321609497\n",
      "training: 99 batch 521 batch_loss: 0.23307102918624878\n",
      "training: 99 batch 522 batch_loss: 0.23312032222747803\n",
      "training: 99 batch 523 batch_loss: 0.232980877161026\n",
      "training: 99 batch 524 batch_loss: 0.23739928007125854\n",
      "training: 99 batch 525 batch_loss: 0.23559051752090454\n",
      "training: 99 batch 526 batch_loss: 0.23510295152664185\n",
      "training: 99 batch 527 batch_loss: 0.23502296209335327\n",
      "training: 99 batch 528 batch_loss: 0.2347104549407959\n",
      "training: 99 batch 529 batch_loss: 0.23606577515602112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 99 batch 530 batch_loss: 0.23307088017463684\n",
      "training: 99 batch 531 batch_loss: 0.23092809319496155\n",
      "training: 99 batch 532 batch_loss: 0.23099377751350403\n",
      "training: 99 batch 533 batch_loss: 0.23096397519111633\n",
      "training: 99 batch 534 batch_loss: 0.23216372728347778\n",
      "training: 99 batch 535 batch_loss: 0.23849061131477356\n",
      "training: 99 batch 536 batch_loss: 0.2333989143371582\n",
      "training: 99 batch 537 batch_loss: 0.2373054325580597\n",
      "training: 99 batch 538 batch_loss: 0.23407882452011108\n",
      "training: 99 batch 539 batch_loss: 0.23677301406860352\n",
      "training: 99 batch 540 batch_loss: 0.23242783546447754\n",
      "training: 99 batch 541 batch_loss: 0.2377263605594635\n",
      "training: 99 batch 542 batch_loss: 0.23656395077705383\n",
      "training: 99 batch 543 batch_loss: 0.23363199830055237\n",
      "training: 99 batch 544 batch_loss: 0.23528772592544556\n",
      "training: 99 batch 545 batch_loss: 0.23506468534469604\n",
      "training: 99 batch 546 batch_loss: 0.2304341197013855\n",
      "training: 99 batch 547 batch_loss: 0.2345607578754425\n",
      "training: 99 batch 548 batch_loss: 0.23389753699302673\n",
      "training: 99 batch 549 batch_loss: 0.23312386870384216\n",
      "training: 99 batch 550 batch_loss: 0.23021596670150757\n",
      "training: 99 batch 551 batch_loss: 0.23671382665634155\n",
      "training: 99 batch 552 batch_loss: 0.23170894384384155\n",
      "training: 99 batch 553 batch_loss: 0.23772570490837097\n",
      "training: 99 batch 554 batch_loss: 0.23525717854499817\n",
      "training: 99 batch 555 batch_loss: 0.2343578338623047\n",
      "training: 99 batch 556 batch_loss: 0.2312309443950653\n",
      "training: 99 batch 557 batch_loss: 0.23543861508369446\n",
      "training: 99 batch 558 batch_loss: 0.2337779402732849\n",
      "training: 99 batch 559 batch_loss: 0.2328857183456421\n",
      "training: 99 batch 560 batch_loss: 0.2385091781616211\n",
      "training: 99 batch 561 batch_loss: 0.2305760383605957\n",
      "training: 99 batch 562 batch_loss: 0.23333221673965454\n",
      "training: 99 batch 563 batch_loss: 0.23865294456481934\n",
      "training: 99 batch 564 batch_loss: 0.23313075304031372\n",
      "training: 99 batch 565 batch_loss: 0.23505616188049316\n",
      "training: 99 batch 566 batch_loss: 0.23093783855438232\n",
      "training: 99 batch 567 batch_loss: 0.2348087728023529\n",
      "training: 99 batch 568 batch_loss: 0.23583033680915833\n",
      "training: 99 batch 569 batch_loss: 0.23630166053771973\n",
      "training: 99 batch 570 batch_loss: 0.232677161693573\n",
      "training: 99 batch 571 batch_loss: 0.23579102754592896\n",
      "training: 99 batch 572 batch_loss: 0.23081940412521362\n",
      "training: 99 batch 573 batch_loss: 0.2334105372428894\n",
      "training: 99 batch 574 batch_loss: 0.2347014844417572\n",
      "training: 99 batch 575 batch_loss: 0.23119917511940002\n",
      "training: 99 batch 576 batch_loss: 0.23544526100158691\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 99, Hit Ratio:0.0338249059771547 | Precision:0.04990661555096825 | Recall:0.06602263117426514 | NDCG:0.0652407062072511\n",
      "*Best Performance* \n",
      "Epoch: 97, Hit Ratio:0.03426795428274477 | Precision:0.0505603066941905 | Recall:0.06646838612031453 | MDCG:0.06605620545333978\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "training: 100 batch 0 batch_loss: 0.2335149049758911\n",
      "training: 100 batch 1 batch_loss: 0.23143529891967773\n",
      "training: 100 batch 2 batch_loss: 0.23179396986961365\n",
      "training: 100 batch 3 batch_loss: 0.23350602388381958\n",
      "training: 100 batch 4 batch_loss: 0.23130697011947632\n",
      "training: 100 batch 5 batch_loss: 0.2347944676876068\n",
      "training: 100 batch 6 batch_loss: 0.23150259256362915\n",
      "training: 100 batch 7 batch_loss: 0.233331561088562\n",
      "training: 100 batch 8 batch_loss: 0.23251968622207642\n",
      "training: 100 batch 9 batch_loss: 0.23205983638763428\n",
      "training: 100 batch 10 batch_loss: 0.23216763138771057\n",
      "training: 100 batch 11 batch_loss: 0.233498215675354\n",
      "training: 100 batch 12 batch_loss: 0.23049402236938477\n",
      "training: 100 batch 13 batch_loss: 0.22787347435951233\n",
      "training: 100 batch 14 batch_loss: 0.23675286769866943\n",
      "training: 100 batch 15 batch_loss: 0.23686790466308594\n",
      "training: 100 batch 16 batch_loss: 0.23373723030090332\n",
      "training: 100 batch 17 batch_loss: 0.23347210884094238\n",
      "training: 100 batch 18 batch_loss: 0.23131322860717773\n",
      "training: 100 batch 19 batch_loss: 0.23427814245224\n",
      "training: 100 batch 20 batch_loss: 0.2341342270374298\n",
      "training: 100 batch 21 batch_loss: 0.2330314815044403\n",
      "training: 100 batch 22 batch_loss: 0.230796217918396\n",
      "training: 100 batch 23 batch_loss: 0.23888295888900757\n",
      "training: 100 batch 24 batch_loss: 0.23605245351791382\n",
      "training: 100 batch 25 batch_loss: 0.23379433155059814\n",
      "training: 100 batch 26 batch_loss: 0.22687268257141113\n",
      "training: 100 batch 27 batch_loss: 0.230342298746109\n",
      "training: 100 batch 28 batch_loss: 0.2331399917602539\n",
      "training: 100 batch 29 batch_loss: 0.22890210151672363\n",
      "training: 100 batch 30 batch_loss: 0.23444288969039917\n",
      "training: 100 batch 31 batch_loss: 0.23397108912467957\n",
      "training: 100 batch 32 batch_loss: 0.23263195157051086\n",
      "training: 100 batch 33 batch_loss: 0.22718727588653564\n",
      "training: 100 batch 34 batch_loss: 0.2301461398601532\n",
      "training: 100 batch 35 batch_loss: 0.23129740357398987\n",
      "training: 100 batch 36 batch_loss: 0.23443710803985596\n",
      "training: 100 batch 37 batch_loss: 0.23390406370162964\n",
      "training: 100 batch 38 batch_loss: 0.22826355695724487\n",
      "training: 100 batch 39 batch_loss: 0.2301698923110962\n",
      "training: 100 batch 40 batch_loss: 0.2317473292350769\n",
      "training: 100 batch 41 batch_loss: 0.23477959632873535\n",
      "training: 100 batch 42 batch_loss: 0.2373197376728058\n",
      "training: 100 batch 43 batch_loss: 0.22920018434524536\n",
      "training: 100 batch 44 batch_loss: 0.23075354099273682\n",
      "training: 100 batch 45 batch_loss: 0.22945460677146912\n",
      "training: 100 batch 46 batch_loss: 0.23048636317253113\n",
      "training: 100 batch 47 batch_loss: 0.2326669692993164\n",
      "training: 100 batch 48 batch_loss: 0.23023828864097595\n",
      "training: 100 batch 49 batch_loss: 0.23207175731658936\n",
      "training: 100 batch 50 batch_loss: 0.226541668176651\n",
      "training: 100 batch 51 batch_loss: 0.23058480024337769\n",
      "training: 100 batch 52 batch_loss: 0.22952541708946228\n",
      "training: 100 batch 53 batch_loss: 0.23451650142669678\n",
      "training: 100 batch 54 batch_loss: 0.23717698454856873\n",
      "training: 100 batch 55 batch_loss: 0.23025014996528625\n",
      "training: 100 batch 56 batch_loss: 0.2302493155002594\n",
      "training: 100 batch 57 batch_loss: 0.22966787219047546\n",
      "training: 100 batch 58 batch_loss: 0.22840142250061035\n",
      "training: 100 batch 59 batch_loss: 0.23934897780418396\n",
      "training: 100 batch 60 batch_loss: 0.22965288162231445\n",
      "training: 100 batch 61 batch_loss: 0.23583859205245972\n",
      "training: 100 batch 62 batch_loss: 0.23648428916931152\n",
      "training: 100 batch 63 batch_loss: 0.23174279928207397\n",
      "training: 100 batch 64 batch_loss: 0.23254680633544922\n",
      "training: 100 batch 65 batch_loss: 0.23410436511039734\n",
      "training: 100 batch 66 batch_loss: 0.2330630123615265\n",
      "training: 100 batch 67 batch_loss: 0.23484230041503906\n",
      "training: 100 batch 68 batch_loss: 0.22850435972213745\n",
      "training: 100 batch 69 batch_loss: 0.23493793606758118\n",
      "training: 100 batch 70 batch_loss: 0.23198699951171875\n",
      "training: 100 batch 71 batch_loss: 0.23194938898086548\n",
      "training: 100 batch 72 batch_loss: 0.23367929458618164\n",
      "training: 100 batch 73 batch_loss: 0.23308491706848145\n",
      "training: 100 batch 74 batch_loss: 0.2344886064529419\n",
      "training: 100 batch 75 batch_loss: 0.2294781506061554\n",
      "training: 100 batch 76 batch_loss: 0.23548299074172974\n",
      "training: 100 batch 77 batch_loss: 0.23071444034576416\n",
      "training: 100 batch 78 batch_loss: 0.229844868183136\n",
      "training: 100 batch 79 batch_loss: 0.2288963794708252\n",
      "training: 100 batch 80 batch_loss: 0.2290809452533722\n",
      "training: 100 batch 81 batch_loss: 0.23720315098762512\n",
      "training: 100 batch 82 batch_loss: 0.23277848958969116\n",
      "training: 100 batch 83 batch_loss: 0.23219898343086243\n",
      "training: 100 batch 84 batch_loss: 0.23105931282043457\n",
      "training: 100 batch 85 batch_loss: 0.23333188891410828\n",
      "training: 100 batch 86 batch_loss: 0.23425441980361938\n",
      "training: 100 batch 87 batch_loss: 0.23502391576766968\n",
      "training: 100 batch 88 batch_loss: 0.22702190279960632\n",
      "training: 100 batch 89 batch_loss: 0.23110228776931763\n",
      "training: 100 batch 90 batch_loss: 0.23258832097053528\n",
      "training: 100 batch 91 batch_loss: 0.2344222068786621\n",
      "training: 100 batch 92 batch_loss: 0.2373799979686737\n",
      "training: 100 batch 93 batch_loss: 0.22792571783065796\n",
      "training: 100 batch 94 batch_loss: 0.23384222388267517\n",
      "training: 100 batch 95 batch_loss: 0.234686940908432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 100 batch 96 batch_loss: 0.23691010475158691\n",
      "training: 100 batch 97 batch_loss: 0.23151710629463196\n",
      "training: 100 batch 98 batch_loss: 0.23028254508972168\n",
      "training: 100 batch 99 batch_loss: 0.231582909822464\n",
      "training: 100 batch 100 batch_loss: 0.23193782567977905\n",
      "training: 100 batch 101 batch_loss: 0.23597726225852966\n",
      "training: 100 batch 102 batch_loss: 0.23197397589683533\n",
      "training: 100 batch 103 batch_loss: 0.23250672221183777\n",
      "training: 100 batch 104 batch_loss: 0.23417222499847412\n",
      "training: 100 batch 105 batch_loss: 0.23085129261016846\n",
      "training: 100 batch 106 batch_loss: 0.23454326391220093\n",
      "training: 100 batch 107 batch_loss: 0.23380431532859802\n",
      "training: 100 batch 108 batch_loss: 0.2357233762741089\n",
      "training: 100 batch 109 batch_loss: 0.23247894644737244\n",
      "training: 100 batch 110 batch_loss: 0.23017922043800354\n",
      "training: 100 batch 111 batch_loss: 0.23337265849113464\n",
      "training: 100 batch 112 batch_loss: 0.23421373963356018\n",
      "training: 100 batch 113 batch_loss: 0.23021313548088074\n",
      "training: 100 batch 114 batch_loss: 0.23215654492378235\n",
      "training: 100 batch 115 batch_loss: 0.23096489906311035\n",
      "training: 100 batch 116 batch_loss: 0.2317902147769928\n",
      "training: 100 batch 117 batch_loss: 0.23387202620506287\n",
      "training: 100 batch 118 batch_loss: 0.2353273630142212\n",
      "training: 100 batch 119 batch_loss: 0.23343950510025024\n",
      "training: 100 batch 120 batch_loss: 0.23103058338165283\n",
      "training: 100 batch 121 batch_loss: 0.23144006729125977\n",
      "training: 100 batch 122 batch_loss: 0.239424467086792\n",
      "training: 100 batch 123 batch_loss: 0.23751699924468994\n",
      "training: 100 batch 124 batch_loss: 0.23511886596679688\n",
      "training: 100 batch 125 batch_loss: 0.2294057011604309\n",
      "training: 100 batch 126 batch_loss: 0.23148968815803528\n",
      "training: 100 batch 127 batch_loss: 0.23821887373924255\n",
      "training: 100 batch 128 batch_loss: 0.23295950889587402\n",
      "training: 100 batch 129 batch_loss: 0.2308844029903412\n",
      "training: 100 batch 130 batch_loss: 0.23139381408691406\n",
      "training: 100 batch 131 batch_loss: 0.23533084988594055\n",
      "training: 100 batch 132 batch_loss: 0.23030278086662292\n",
      "training: 100 batch 133 batch_loss: 0.23568353056907654\n",
      "training: 100 batch 134 batch_loss: 0.2367744743824005\n",
      "training: 100 batch 135 batch_loss: 0.23175173997879028\n",
      "training: 100 batch 136 batch_loss: 0.23136374354362488\n",
      "training: 100 batch 137 batch_loss: 0.23388144373893738\n",
      "training: 100 batch 138 batch_loss: 0.2348070740699768\n",
      "training: 100 batch 139 batch_loss: 0.2352503538131714\n",
      "training: 100 batch 140 batch_loss: 0.24103516340255737\n",
      "training: 100 batch 141 batch_loss: 0.23152098059654236\n",
      "training: 100 batch 142 batch_loss: 0.23222076892852783\n",
      "training: 100 batch 143 batch_loss: 0.23017486929893494\n",
      "training: 100 batch 144 batch_loss: 0.23191356658935547\n",
      "training: 100 batch 145 batch_loss: 0.2339453101158142\n",
      "training: 100 batch 146 batch_loss: 0.22936463356018066\n",
      "training: 100 batch 147 batch_loss: 0.23373115062713623\n",
      "training: 100 batch 148 batch_loss: 0.23477128148078918\n",
      "training: 100 batch 149 batch_loss: 0.23242142796516418\n",
      "training: 100 batch 150 batch_loss: 0.22759774327278137\n",
      "training: 100 batch 151 batch_loss: 0.22789019346237183\n",
      "training: 100 batch 152 batch_loss: 0.23462095856666565\n",
      "training: 100 batch 153 batch_loss: 0.23462596535682678\n",
      "training: 100 batch 154 batch_loss: 0.23548856377601624\n",
      "training: 100 batch 155 batch_loss: 0.2313479483127594\n",
      "training: 100 batch 156 batch_loss: 0.2340613603591919\n",
      "training: 100 batch 157 batch_loss: 0.23008042573928833\n",
      "training: 100 batch 158 batch_loss: 0.23353606462478638\n",
      "training: 100 batch 159 batch_loss: 0.2337472140789032\n",
      "training: 100 batch 160 batch_loss: 0.23717793822288513\n",
      "training: 100 batch 161 batch_loss: 0.23025688529014587\n",
      "training: 100 batch 162 batch_loss: 0.23104429244995117\n",
      "training: 100 batch 163 batch_loss: 0.2318689227104187\n",
      "training: 100 batch 164 batch_loss: 0.23549294471740723\n",
      "training: 100 batch 165 batch_loss: 0.23137745261192322\n",
      "training: 100 batch 166 batch_loss: 0.2335420846939087\n",
      "training: 100 batch 167 batch_loss: 0.23585787415504456\n",
      "training: 100 batch 168 batch_loss: 0.23815560340881348\n",
      "training: 100 batch 169 batch_loss: 0.2375098466873169\n",
      "training: 100 batch 170 batch_loss: 0.23155051469802856\n",
      "training: 100 batch 171 batch_loss: 0.23701125383377075\n",
      "training: 100 batch 172 batch_loss: 0.23573634028434753\n",
      "training: 100 batch 173 batch_loss: 0.23505863547325134\n",
      "training: 100 batch 174 batch_loss: 0.2310587763786316\n",
      "training: 100 batch 175 batch_loss: 0.23115020990371704\n",
      "training: 100 batch 176 batch_loss: 0.23251685500144958\n",
      "training: 100 batch 177 batch_loss: 0.23278331756591797\n",
      "training: 100 batch 178 batch_loss: 0.23489731550216675\n",
      "training: 100 batch 179 batch_loss: 0.22958844900131226\n",
      "training: 100 batch 180 batch_loss: 0.23163089156150818\n",
      "training: 100 batch 181 batch_loss: 0.23603546619415283\n",
      "training: 100 batch 182 batch_loss: 0.23378613591194153\n",
      "training: 100 batch 183 batch_loss: 0.23786669969558716\n",
      "training: 100 batch 184 batch_loss: 0.22980618476867676\n",
      "training: 100 batch 185 batch_loss: 0.2329138219356537\n",
      "training: 100 batch 186 batch_loss: 0.2285207211971283\n",
      "training: 100 batch 187 batch_loss: 0.234095960855484\n",
      "training: 100 batch 188 batch_loss: 0.23362666368484497\n",
      "training: 100 batch 189 batch_loss: 0.22922149300575256\n",
      "training: 100 batch 190 batch_loss: 0.2325032651424408\n",
      "training: 100 batch 191 batch_loss: 0.22998115420341492\n",
      "training: 100 batch 192 batch_loss: 0.22604110836982727\n",
      "training: 100 batch 193 batch_loss: 0.23553818464279175\n",
      "training: 100 batch 194 batch_loss: 0.23146075010299683\n",
      "training: 100 batch 195 batch_loss: 0.22980594635009766\n",
      "training: 100 batch 196 batch_loss: 0.23247000575065613\n",
      "training: 100 batch 197 batch_loss: 0.23063009977340698\n",
      "training: 100 batch 198 batch_loss: 0.23258143663406372\n",
      "training: 100 batch 199 batch_loss: 0.23825526237487793\n",
      "training: 100 batch 200 batch_loss: 0.23060816526412964\n",
      "training: 100 batch 201 batch_loss: 0.23196399211883545\n",
      "training: 100 batch 202 batch_loss: 0.2342720627784729\n",
      "training: 100 batch 203 batch_loss: 0.2326408326625824\n",
      "training: 100 batch 204 batch_loss: 0.2312816083431244\n",
      "training: 100 batch 205 batch_loss: 0.23545435070991516\n",
      "training: 100 batch 206 batch_loss: 0.2268798053264618\n",
      "training: 100 batch 207 batch_loss: 0.23623499274253845\n",
      "training: 100 batch 208 batch_loss: 0.2340601682662964\n",
      "training: 100 batch 209 batch_loss: 0.23041865229606628\n",
      "training: 100 batch 210 batch_loss: 0.22920945286750793\n",
      "training: 100 batch 211 batch_loss: 0.23146900534629822\n",
      "training: 100 batch 212 batch_loss: 0.2323441207408905\n",
      "training: 100 batch 213 batch_loss: 0.23167014122009277\n",
      "training: 100 batch 214 batch_loss: 0.23160752654075623\n",
      "training: 100 batch 215 batch_loss: 0.23421478271484375\n",
      "training: 100 batch 216 batch_loss: 0.23437240719795227\n",
      "training: 100 batch 217 batch_loss: 0.23380863666534424\n",
      "training: 100 batch 218 batch_loss: 0.2331794798374176\n",
      "training: 100 batch 219 batch_loss: 0.23328393697738647\n",
      "training: 100 batch 220 batch_loss: 0.23294565081596375\n",
      "training: 100 batch 221 batch_loss: 0.23533421754837036\n",
      "training: 100 batch 222 batch_loss: 0.23046737909317017\n",
      "training: 100 batch 223 batch_loss: 0.2313627302646637\n",
      "training: 100 batch 224 batch_loss: 0.23017269372940063\n",
      "training: 100 batch 225 batch_loss: 0.23787301778793335\n",
      "training: 100 batch 226 batch_loss: 0.23184680938720703\n",
      "training: 100 batch 227 batch_loss: 0.2293764352798462\n",
      "training: 100 batch 228 batch_loss: 0.23150259256362915\n",
      "training: 100 batch 229 batch_loss: 0.2350941300392151\n",
      "training: 100 batch 230 batch_loss: 0.23198682069778442\n",
      "training: 100 batch 231 batch_loss: 0.23136001825332642\n",
      "training: 100 batch 232 batch_loss: 0.23205292224884033\n",
      "training: 100 batch 233 batch_loss: 0.23788359761238098\n",
      "training: 100 batch 234 batch_loss: 0.23337271809577942\n",
      "training: 100 batch 235 batch_loss: 0.23103263974189758\n",
      "training: 100 batch 236 batch_loss: 0.23441502451896667\n",
      "training: 100 batch 237 batch_loss: 0.2366366982460022\n",
      "training: 100 batch 238 batch_loss: 0.2343350648880005\n",
      "training: 100 batch 239 batch_loss: 0.23229292035102844\n",
      "training: 100 batch 240 batch_loss: 0.23761558532714844\n",
      "training: 100 batch 241 batch_loss: 0.23159778118133545\n",
      "training: 100 batch 242 batch_loss: 0.23303961753845215\n",
      "training: 100 batch 243 batch_loss: 0.23251450061798096\n",
      "training: 100 batch 244 batch_loss: 0.234075129032135\n",
      "training: 100 batch 245 batch_loss: 0.23350676894187927\n",
      "training: 100 batch 246 batch_loss: 0.23228812217712402\n",
      "training: 100 batch 247 batch_loss: 0.23424160480499268\n",
      "training: 100 batch 248 batch_loss: 0.23070764541625977\n",
      "training: 100 batch 249 batch_loss: 0.23139351606369019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 100 batch 250 batch_loss: 0.23184102773666382\n",
      "training: 100 batch 251 batch_loss: 0.23405951261520386\n",
      "training: 100 batch 252 batch_loss: 0.23148012161254883\n",
      "training: 100 batch 253 batch_loss: 0.2321680188179016\n",
      "training: 100 batch 254 batch_loss: 0.23525896668434143\n",
      "training: 100 batch 255 batch_loss: 0.2304132580757141\n",
      "training: 100 batch 256 batch_loss: 0.23688074946403503\n",
      "training: 100 batch 257 batch_loss: 0.2360404133796692\n",
      "training: 100 batch 258 batch_loss: 0.22891291975975037\n",
      "training: 100 batch 259 batch_loss: 0.23349040746688843\n",
      "training: 100 batch 260 batch_loss: 0.2351764440536499\n",
      "training: 100 batch 261 batch_loss: 0.23766794800758362\n",
      "training: 100 batch 262 batch_loss: 0.23381230235099792\n",
      "training: 100 batch 263 batch_loss: 0.23411080241203308\n",
      "training: 100 batch 264 batch_loss: 0.233216792345047\n",
      "training: 100 batch 265 batch_loss: 0.2321879267692566\n",
      "training: 100 batch 266 batch_loss: 0.240780770778656\n",
      "training: 100 batch 267 batch_loss: 0.23531806468963623\n",
      "training: 100 batch 268 batch_loss: 0.23961806297302246\n",
      "training: 100 batch 269 batch_loss: 0.23349764943122864\n",
      "training: 100 batch 270 batch_loss: 0.2333729863166809\n",
      "training: 100 batch 271 batch_loss: 0.23546400666236877\n",
      "training: 100 batch 272 batch_loss: 0.23418131470680237\n",
      "training: 100 batch 273 batch_loss: 0.23132780194282532\n",
      "training: 100 batch 274 batch_loss: 0.23386773467063904\n",
      "training: 100 batch 275 batch_loss: 0.23424625396728516\n",
      "training: 100 batch 276 batch_loss: 0.24049866199493408\n",
      "training: 100 batch 277 batch_loss: 0.23248672485351562\n",
      "training: 100 batch 278 batch_loss: 0.23227223753929138\n",
      "training: 100 batch 279 batch_loss: 0.23178616166114807\n",
      "training: 100 batch 280 batch_loss: 0.23505625128746033\n",
      "training: 100 batch 281 batch_loss: 0.23575228452682495\n",
      "training: 100 batch 282 batch_loss: 0.2339935004711151\n",
      "training: 100 batch 283 batch_loss: 0.23101601004600525\n",
      "training: 100 batch 284 batch_loss: 0.229619562625885\n",
      "training: 100 batch 285 batch_loss: 0.23269319534301758\n",
      "training: 100 batch 286 batch_loss: 0.23525667190551758\n",
      "training: 100 batch 287 batch_loss: 0.23669907450675964\n",
      "training: 100 batch 288 batch_loss: 0.23007196187973022\n",
      "training: 100 batch 289 batch_loss: 0.23351553082466125\n",
      "training: 100 batch 290 batch_loss: 0.23259729146957397\n",
      "training: 100 batch 291 batch_loss: 0.23384404182434082\n",
      "training: 100 batch 292 batch_loss: 0.23178964853286743\n",
      "training: 100 batch 293 batch_loss: 0.23818480968475342\n",
      "training: 100 batch 294 batch_loss: 0.23270940780639648\n",
      "training: 100 batch 295 batch_loss: 0.23259511590003967\n",
      "training: 100 batch 296 batch_loss: 0.2243824303150177\n",
      "training: 100 batch 297 batch_loss: 0.2314719557762146\n",
      "training: 100 batch 298 batch_loss: 0.2361375093460083\n",
      "training: 100 batch 299 batch_loss: 0.23627346754074097\n",
      "training: 100 batch 300 batch_loss: 0.23638689517974854\n",
      "training: 100 batch 301 batch_loss: 0.23548904061317444\n",
      "training: 100 batch 302 batch_loss: 0.23000553250312805\n",
      "training: 100 batch 303 batch_loss: 0.23157325387001038\n",
      "training: 100 batch 304 batch_loss: 0.23212122917175293\n",
      "training: 100 batch 305 batch_loss: 0.23585203289985657\n",
      "training: 100 batch 306 batch_loss: 0.23662495613098145\n",
      "training: 100 batch 307 batch_loss: 0.2351844310760498\n",
      "training: 100 batch 308 batch_loss: 0.23527494072914124\n",
      "training: 100 batch 309 batch_loss: 0.23581308126449585\n",
      "training: 100 batch 310 batch_loss: 0.23493006825447083\n",
      "training: 100 batch 311 batch_loss: 0.23683583736419678\n",
      "training: 100 batch 312 batch_loss: 0.2342471480369568\n",
      "training: 100 batch 313 batch_loss: 0.2308393120765686\n",
      "training: 100 batch 314 batch_loss: 0.22837942838668823\n",
      "training: 100 batch 315 batch_loss: 0.23131316900253296\n",
      "training: 100 batch 316 batch_loss: 0.23129042983055115\n",
      "training: 100 batch 317 batch_loss: 0.2366161346435547\n",
      "training: 100 batch 318 batch_loss: 0.23531091213226318\n",
      "training: 100 batch 319 batch_loss: 0.23611995577812195\n",
      "training: 100 batch 320 batch_loss: 0.2366257905960083\n",
      "training: 100 batch 321 batch_loss: 0.23401674628257751\n",
      "training: 100 batch 322 batch_loss: 0.23069602251052856\n",
      "training: 100 batch 323 batch_loss: 0.23497438430786133\n",
      "training: 100 batch 324 batch_loss: 0.23248952627182007\n",
      "training: 100 batch 325 batch_loss: 0.2345649003982544\n",
      "training: 100 batch 326 batch_loss: 0.2314109206199646\n",
      "training: 100 batch 327 batch_loss: 0.23340114951133728\n",
      "training: 100 batch 328 batch_loss: 0.23287007212638855\n",
      "training: 100 batch 329 batch_loss: 0.23764905333518982\n",
      "training: 100 batch 330 batch_loss: 0.2286202311515808\n",
      "training: 100 batch 331 batch_loss: 0.2349812388420105\n",
      "training: 100 batch 332 batch_loss: 0.23525628447532654\n",
      "training: 100 batch 333 batch_loss: 0.2298353910446167\n",
      "training: 100 batch 334 batch_loss: 0.23734372854232788\n",
      "training: 100 batch 335 batch_loss: 0.23453354835510254\n",
      "training: 100 batch 336 batch_loss: 0.23321393132209778\n",
      "training: 100 batch 337 batch_loss: 0.2322227954864502\n",
      "training: 100 batch 338 batch_loss: 0.2324763536453247\n",
      "training: 100 batch 339 batch_loss: 0.23720216751098633\n",
      "training: 100 batch 340 batch_loss: 0.23620456457138062\n",
      "training: 100 batch 341 batch_loss: 0.2348703145980835\n",
      "training: 100 batch 342 batch_loss: 0.2343710958957672\n",
      "training: 100 batch 343 batch_loss: 0.2343786060810089\n",
      "training: 100 batch 344 batch_loss: 0.2320961058139801\n",
      "training: 100 batch 345 batch_loss: 0.23903420567512512\n",
      "training: 100 batch 346 batch_loss: 0.23181813955307007\n",
      "training: 100 batch 347 batch_loss: 0.2354615032672882\n",
      "training: 100 batch 348 batch_loss: 0.2357211410999298\n",
      "training: 100 batch 349 batch_loss: 0.23575222492218018\n",
      "training: 100 batch 350 batch_loss: 0.23515993356704712\n",
      "training: 100 batch 351 batch_loss: 0.23326954245567322\n",
      "training: 100 batch 352 batch_loss: 0.23273149132728577\n",
      "training: 100 batch 353 batch_loss: 0.23697948455810547\n",
      "training: 100 batch 354 batch_loss: 0.23209530115127563\n",
      "training: 100 batch 355 batch_loss: 0.2362602949142456\n",
      "training: 100 batch 356 batch_loss: 0.23436152935028076\n",
      "training: 100 batch 357 batch_loss: 0.23256617784500122\n",
      "training: 100 batch 358 batch_loss: 0.23665276169776917\n",
      "training: 100 batch 359 batch_loss: 0.23213785886764526\n",
      "training: 100 batch 360 batch_loss: 0.2318899929523468\n",
      "training: 100 batch 361 batch_loss: 0.23243963718414307\n",
      "training: 100 batch 362 batch_loss: 0.2370055913925171\n",
      "training: 100 batch 363 batch_loss: 0.23205220699310303\n",
      "training: 100 batch 364 batch_loss: 0.23622667789459229\n",
      "training: 100 batch 365 batch_loss: 0.23585104942321777\n",
      "training: 100 batch 366 batch_loss: 0.23508566617965698\n",
      "training: 100 batch 367 batch_loss: 0.23360159993171692\n",
      "training: 100 batch 368 batch_loss: 0.23422271013259888\n",
      "training: 100 batch 369 batch_loss: 0.23437485098838806\n",
      "training: 100 batch 370 batch_loss: 0.2322825789451599\n",
      "training: 100 batch 371 batch_loss: 0.2341865599155426\n",
      "training: 100 batch 372 batch_loss: 0.2327185869216919\n",
      "training: 100 batch 373 batch_loss: 0.23223650455474854\n",
      "training: 100 batch 374 batch_loss: 0.23460814356803894\n",
      "training: 100 batch 375 batch_loss: 0.2390197515487671\n",
      "training: 100 batch 376 batch_loss: 0.23622947931289673\n",
      "training: 100 batch 377 batch_loss: 0.2352776825428009\n",
      "training: 100 batch 378 batch_loss: 0.23280778527259827\n",
      "training: 100 batch 379 batch_loss: 0.23429006338119507\n",
      "training: 100 batch 380 batch_loss: 0.23386400938034058\n",
      "training: 100 batch 381 batch_loss: 0.23419049382209778\n",
      "training: 100 batch 382 batch_loss: 0.23580342531204224\n",
      "training: 100 batch 383 batch_loss: 0.23386803269386292\n",
      "training: 100 batch 384 batch_loss: 0.23251956701278687\n",
      "training: 100 batch 385 batch_loss: 0.23365157842636108\n",
      "training: 100 batch 386 batch_loss: 0.23859280347824097\n",
      "training: 100 batch 387 batch_loss: 0.23533442616462708\n",
      "training: 100 batch 388 batch_loss: 0.2329539954662323\n",
      "training: 100 batch 389 batch_loss: 0.23085233569145203\n",
      "training: 100 batch 390 batch_loss: 0.2303612232208252\n",
      "training: 100 batch 391 batch_loss: 0.23094069957733154\n",
      "training: 100 batch 392 batch_loss: 0.23549401760101318\n",
      "training: 100 batch 393 batch_loss: 0.23580175638198853\n",
      "training: 100 batch 394 batch_loss: 0.23581400513648987\n",
      "training: 100 batch 395 batch_loss: 0.23575416207313538\n",
      "training: 100 batch 396 batch_loss: 0.23178625106811523\n",
      "training: 100 batch 397 batch_loss: 0.23604550957679749\n",
      "training: 100 batch 398 batch_loss: 0.23472997546195984\n",
      "training: 100 batch 399 batch_loss: 0.2349335253238678\n",
      "training: 100 batch 400 batch_loss: 0.23321273922920227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 100 batch 401 batch_loss: 0.23416635394096375\n",
      "training: 100 batch 402 batch_loss: 0.2303803563117981\n",
      "training: 100 batch 403 batch_loss: 0.23467779159545898\n",
      "training: 100 batch 404 batch_loss: 0.23423904180526733\n",
      "training: 100 batch 405 batch_loss: 0.23115190863609314\n",
      "training: 100 batch 406 batch_loss: 0.23866459727287292\n",
      "training: 100 batch 407 batch_loss: 0.23123705387115479\n",
      "training: 100 batch 408 batch_loss: 0.2335985004901886\n",
      "training: 100 batch 409 batch_loss: 0.2409539818763733\n",
      "training: 100 batch 410 batch_loss: 0.22909408807754517\n",
      "training: 100 batch 411 batch_loss: 0.23607909679412842\n",
      "training: 100 batch 412 batch_loss: 0.2329120934009552\n",
      "training: 100 batch 413 batch_loss: 0.2340809404850006\n",
      "training: 100 batch 414 batch_loss: 0.23339763283729553\n",
      "training: 100 batch 415 batch_loss: 0.237937331199646\n",
      "training: 100 batch 416 batch_loss: 0.23341122269630432\n",
      "training: 100 batch 417 batch_loss: 0.2394712269306183\n",
      "training: 100 batch 418 batch_loss: 0.23330727219581604\n",
      "training: 100 batch 419 batch_loss: 0.2389822006225586\n",
      "training: 100 batch 420 batch_loss: 0.23502010107040405\n",
      "training: 100 batch 421 batch_loss: 0.23348212242126465\n",
      "training: 100 batch 422 batch_loss: 0.2312607765197754\n",
      "training: 100 batch 423 batch_loss: 0.23320242762565613\n",
      "training: 100 batch 424 batch_loss: 0.23690646886825562\n",
      "training: 100 batch 425 batch_loss: 0.22929158806800842\n",
      "training: 100 batch 426 batch_loss: 0.23540562391281128\n",
      "training: 100 batch 427 batch_loss: 0.2306687831878662\n",
      "training: 100 batch 428 batch_loss: 0.23145389556884766\n",
      "training: 100 batch 429 batch_loss: 0.23527079820632935\n",
      "training: 100 batch 430 batch_loss: 0.230841726064682\n",
      "training: 100 batch 431 batch_loss: 0.2332209348678589\n",
      "training: 100 batch 432 batch_loss: 0.23373112082481384\n",
      "training: 100 batch 433 batch_loss: 0.23109981417655945\n",
      "training: 100 batch 434 batch_loss: 0.2343890368938446\n",
      "training: 100 batch 435 batch_loss: 0.23001229763031006\n",
      "training: 100 batch 436 batch_loss: 0.23334836959838867\n",
      "training: 100 batch 437 batch_loss: 0.2333388328552246\n",
      "training: 100 batch 438 batch_loss: 0.23205554485321045\n",
      "training: 100 batch 439 batch_loss: 0.23572757840156555\n",
      "training: 100 batch 440 batch_loss: 0.2316940426826477\n",
      "training: 100 batch 441 batch_loss: 0.23091259598731995\n",
      "training: 100 batch 442 batch_loss: 0.23225194215774536\n",
      "training: 100 batch 443 batch_loss: 0.2368817925453186\n",
      "training: 100 batch 444 batch_loss: 0.2368001937866211\n",
      "training: 100 batch 445 batch_loss: 0.22837448120117188\n",
      "training: 100 batch 446 batch_loss: 0.23637306690216064\n",
      "training: 100 batch 447 batch_loss: 0.23251166939735413\n",
      "training: 100 batch 448 batch_loss: 0.23111885786056519\n",
      "training: 100 batch 449 batch_loss: 0.22809046506881714\n",
      "training: 100 batch 450 batch_loss: 0.23603177070617676\n",
      "training: 100 batch 451 batch_loss: 0.22909575700759888\n",
      "training: 100 batch 452 batch_loss: 0.2369825839996338\n",
      "training: 100 batch 453 batch_loss: 0.2386513352394104\n",
      "training: 100 batch 454 batch_loss: 0.23246073722839355\n",
      "training: 100 batch 455 batch_loss: 0.23518088459968567\n",
      "training: 100 batch 456 batch_loss: 0.235622376203537\n",
      "training: 100 batch 457 batch_loss: 0.23498404026031494\n",
      "training: 100 batch 458 batch_loss: 0.2331271767616272\n",
      "training: 100 batch 459 batch_loss: 0.2330627143383026\n",
      "training: 100 batch 460 batch_loss: 0.23573413491249084\n",
      "training: 100 batch 461 batch_loss: 0.229640930891037\n",
      "training: 100 batch 462 batch_loss: 0.2328253984451294\n",
      "training: 100 batch 463 batch_loss: 0.2371666133403778\n",
      "training: 100 batch 464 batch_loss: 0.2334502637386322\n",
      "training: 100 batch 465 batch_loss: 0.2379879355430603\n",
      "training: 100 batch 466 batch_loss: 0.2340562641620636\n",
      "training: 100 batch 467 batch_loss: 0.23466917872428894\n",
      "training: 100 batch 468 batch_loss: 0.23570209741592407\n",
      "training: 100 batch 469 batch_loss: 0.23286837339401245\n",
      "training: 100 batch 470 batch_loss: 0.23116227984428406\n",
      "training: 100 batch 471 batch_loss: 0.23456844687461853\n",
      "training: 100 batch 472 batch_loss: 0.2348915934562683\n",
      "training: 100 batch 473 batch_loss: 0.234358012676239\n",
      "training: 100 batch 474 batch_loss: 0.23383790254592896\n",
      "training: 100 batch 475 batch_loss: 0.23493248224258423\n",
      "training: 100 batch 476 batch_loss: 0.23371285200119019\n",
      "training: 100 batch 477 batch_loss: 0.23443540930747986\n",
      "training: 100 batch 478 batch_loss: 0.23825126886367798\n",
      "training: 100 batch 479 batch_loss: 0.23199719190597534\n",
      "training: 100 batch 480 batch_loss: 0.23159974813461304\n",
      "training: 100 batch 481 batch_loss: 0.23264682292938232\n",
      "training: 100 batch 482 batch_loss: 0.23059934377670288\n",
      "training: 100 batch 483 batch_loss: 0.23509341478347778\n",
      "training: 100 batch 484 batch_loss: 0.23353731632232666\n",
      "training: 100 batch 485 batch_loss: 0.23383840918540955\n",
      "training: 100 batch 486 batch_loss: 0.2316787838935852\n",
      "training: 100 batch 487 batch_loss: 0.2284635603427887\n",
      "training: 100 batch 488 batch_loss: 0.2394859790802002\n",
      "training: 100 batch 489 batch_loss: 0.2349725067615509\n",
      "training: 100 batch 490 batch_loss: 0.23510438203811646\n",
      "training: 100 batch 491 batch_loss: 0.22984099388122559\n",
      "training: 100 batch 492 batch_loss: 0.23296436667442322\n",
      "training: 100 batch 493 batch_loss: 0.23249492049217224\n",
      "training: 100 batch 494 batch_loss: 0.23447191715240479\n",
      "training: 100 batch 495 batch_loss: 0.23428133130073547\n",
      "training: 100 batch 496 batch_loss: 0.23993608355522156\n",
      "training: 100 batch 497 batch_loss: 0.23313561081886292\n",
      "training: 100 batch 498 batch_loss: 0.23391693830490112\n",
      "training: 100 batch 499 batch_loss: 0.23825669288635254\n",
      "training: 100 batch 500 batch_loss: 0.23599755764007568\n",
      "training: 100 batch 501 batch_loss: 0.2331114113330841\n",
      "training: 100 batch 502 batch_loss: 0.24033114314079285\n",
      "training: 100 batch 503 batch_loss: 0.23595410585403442\n",
      "training: 100 batch 504 batch_loss: 0.23570653796195984\n",
      "training: 100 batch 505 batch_loss: 0.2328397035598755\n",
      "training: 100 batch 506 batch_loss: 0.23700189590454102\n",
      "training: 100 batch 507 batch_loss: 0.2332932949066162\n",
      "training: 100 batch 508 batch_loss: 0.23003780841827393\n",
      "training: 100 batch 509 batch_loss: 0.23599261045455933\n",
      "training: 100 batch 510 batch_loss: 0.2316296100616455\n",
      "training: 100 batch 511 batch_loss: 0.23617732524871826\n",
      "training: 100 batch 512 batch_loss: 0.2303127646446228\n",
      "training: 100 batch 513 batch_loss: 0.23023167252540588\n",
      "training: 100 batch 514 batch_loss: 0.2365049123764038\n",
      "training: 100 batch 515 batch_loss: 0.23660144209861755\n",
      "training: 100 batch 516 batch_loss: 0.23414090275764465\n",
      "training: 100 batch 517 batch_loss: 0.24002861976623535\n",
      "training: 100 batch 518 batch_loss: 0.23050981760025024\n",
      "training: 100 batch 519 batch_loss: 0.23589354753494263\n",
      "training: 100 batch 520 batch_loss: 0.23192918300628662\n",
      "training: 100 batch 521 batch_loss: 0.2335546314716339\n",
      "training: 100 batch 522 batch_loss: 0.23433291912078857\n",
      "training: 100 batch 523 batch_loss: 0.23360031843185425\n",
      "training: 100 batch 524 batch_loss: 0.23583224415779114\n",
      "training: 100 batch 525 batch_loss: 0.22992631793022156\n",
      "training: 100 batch 526 batch_loss: 0.2378096580505371\n",
      "training: 100 batch 527 batch_loss: 0.23581594228744507\n",
      "training: 100 batch 528 batch_loss: 0.23548004031181335\n",
      "training: 100 batch 529 batch_loss: 0.23208990693092346\n",
      "training: 100 batch 530 batch_loss: 0.23901930451393127\n",
      "training: 100 batch 531 batch_loss: 0.2373386025428772\n",
      "training: 100 batch 532 batch_loss: 0.23560503125190735\n",
      "training: 100 batch 533 batch_loss: 0.2384929060935974\n",
      "training: 100 batch 534 batch_loss: 0.23413747549057007\n",
      "training: 100 batch 535 batch_loss: 0.23084986209869385\n",
      "training: 100 batch 536 batch_loss: 0.2361791729927063\n",
      "training: 100 batch 537 batch_loss: 0.23795950412750244\n",
      "training: 100 batch 538 batch_loss: 0.2382553219795227\n",
      "training: 100 batch 539 batch_loss: 0.23646026849746704\n",
      "training: 100 batch 540 batch_loss: 0.23238739371299744\n",
      "training: 100 batch 541 batch_loss: 0.23798543214797974\n",
      "training: 100 batch 542 batch_loss: 0.2348823845386505\n",
      "training: 100 batch 543 batch_loss: 0.2388787865638733\n",
      "training: 100 batch 544 batch_loss: 0.23826414346694946\n",
      "training: 100 batch 545 batch_loss: 0.2358495593070984\n",
      "training: 100 batch 546 batch_loss: 0.2351912558078766\n",
      "training: 100 batch 547 batch_loss: 0.2362137734889984\n",
      "training: 100 batch 548 batch_loss: 0.2365952730178833\n",
      "training: 100 batch 549 batch_loss: 0.2395634949207306\n",
      "training: 100 batch 550 batch_loss: 0.23144873976707458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 100 batch 551 batch_loss: 0.23193752765655518\n",
      "training: 100 batch 552 batch_loss: 0.23048776388168335\n",
      "training: 100 batch 553 batch_loss: 0.2326829433441162\n",
      "training: 100 batch 554 batch_loss: 0.23642849922180176\n",
      "training: 100 batch 555 batch_loss: 0.232804536819458\n",
      "training: 100 batch 556 batch_loss: 0.2358502447605133\n",
      "training: 100 batch 557 batch_loss: 0.2379085123538971\n",
      "training: 100 batch 558 batch_loss: 0.2317456603050232\n",
      "training: 100 batch 559 batch_loss: 0.23546987771987915\n",
      "training: 100 batch 560 batch_loss: 0.23264408111572266\n",
      "training: 100 batch 561 batch_loss: 0.23442035913467407\n",
      "training: 100 batch 562 batch_loss: 0.23135730624198914\n",
      "training: 100 batch 563 batch_loss: 0.23717889189720154\n",
      "training: 100 batch 564 batch_loss: 0.2306515872478485\n",
      "training: 100 batch 565 batch_loss: 0.23159468173980713\n",
      "training: 100 batch 566 batch_loss: 0.23156940937042236\n",
      "training: 100 batch 567 batch_loss: 0.23557940125465393\n",
      "training: 100 batch 568 batch_loss: 0.2353474497795105\n",
      "training: 100 batch 569 batch_loss: 0.22994306683540344\n",
      "training: 100 batch 570 batch_loss: 0.2391791045665741\n",
      "training: 100 batch 571 batch_loss: 0.23740431666374207\n",
      "training: 100 batch 572 batch_loss: 0.23342996835708618\n",
      "training: 100 batch 573 batch_loss: 0.23510560393333435\n",
      "training: 100 batch 574 batch_loss: 0.2354247272014618\n",
      "training: 100 batch 575 batch_loss: 0.2356840968132019\n",
      "training: 100 batch 576 batch_loss: 0.2364388406276703\n",
      "Evaluating the model...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Real-Time Ranking Performance  (Top-20 Item Recommendation)\n",
      "*Current Performance*\n",
      "Epoch: 100, Hit Ratio:0.03440120189344855 | Precision:0.050756905534257345 | Recall:0.06690833773455611 | NDCG:0.06625942678709931\n",
      "*Best Performance* \n",
      "Epoch: 100, Hit Ratio:0.03440120189344855 | Precision:0.050756905534257345 | Recall:0.06690833773455611 | MDCG:0.06625942678709931\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Testing...\n",
      "Progress: [++++++++++++++++++++++++++++++++++++++++++++++++++]100%\n",
      "Evaluating...\n",
      "The result has been output to  /home/windy/512Project/SELFRec/results .\n",
      "The result of SelfCF:\n",
      "Top 10\n",
      "Hit Ratio:0.016762549426535597\n",
      "Precision:0.04946426816081785\n",
      "Recall:0.03639490554461259\n",
      "NDCG:0.05598795609866094\n",
      "Top 20\n",
      "Hit Ratio:0.03440120189344855\n",
      "Precision:0.050756905534257345\n",
      "Recall:0.06690833773455611\n",
      "NDCG:0.06625942678709931\n",
      "\n",
      "Running time: 2326.921267 s\n"
     ]
    }
   ],
   "source": [
    "# Register your model here\n",
    "baseline = ['LightGCN','DirectAU','MF']\n",
    "graph_models = ['SGL', 'SimGCL', 'SEPT', 'MHCN', 'BUIR', 'SelfCF', 'SSL4Rec', 'XSimGCL', 'NCL','MixGCF']\n",
    "applying_models = ['SelfCF_anime1','SelfCF_anime2','XSimGCL_anime1','XSimGCL_anime2','NCL_anime1','NCL_anime2']\n",
    "sequential_models = []\n",
    "\n",
    "print('=' * 80)\n",
    "print('   SELFRec: A library for self-supervised recommendation.   ')\n",
    "print('=' * 80)\n",
    "\n",
    "print('Baseline Models:')\n",
    "print('   '.join(baseline))\n",
    "print('-' * 80)\n",
    "print('Graph-Based Models:')\n",
    "print('   '.join(graph_models))\n",
    "print('-' * 80)\n",
    "print('Applying Models:')\n",
    "print('   '.join(applying_models))\n",
    "\n",
    "print('=' * 80)\n",
    "model = input('Please enter the model you want to run:')\n",
    "import time\n",
    "\n",
    "s = time.time()\n",
    "if model in baseline or model in graph_models or model in sequential_models or model in applying_models:\n",
    "    conf = ModelConf('./conf/' + model + '.conf')\n",
    "else:\n",
    "    print('Wrong model name!')\n",
    "    exit(-1)\n",
    "rec = SELFRec(conf)\n",
    "rec.execute()\n",
    "e = time.time()\n",
    "print(\"Running time: %f s\" % (e - s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c2662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-DKM]",
   "language": "python",
   "name": "conda-env-.conda-DKM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
